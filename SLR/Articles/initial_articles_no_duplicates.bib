% Encoding: UTF-8
@inproceedings{Aderaldo:2017:BRM:3101282.3101285,
 author = {Aderaldo, Carlos M. and Mendon\c{c}a, Nabor C. and Pahl, Claus and Jamshidi, Pooyan},
 title = {Benchmark Requirements for Microservices Architecture Research},
 booktitle = {Proceedings of the 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering},
 series = {ECASE '17},
 year = {2017},
 isbn = {978-1-5386-0417-5},
 location = {Buenos Aires, Argentina},
 pages = {8--13},
 numpages = {6},
 url = {https://doi.org/10.1109/ECASE.2017..4},
 doi = {10.1109/ECASE.2017..4},
 acmid = {3101285},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {microservices, research benchmark, software architecture},
}

@inproceedings{Iosup:2013:ICB:2462307.2462309,
 author = {Iosup, Alexandru},
 title = {IaaS Cloud Benchmarking: Approaches, Challenges, and Experience},
 booktitle = {Proceedings of the 2013 International Workshop on Hot Topics in Cloud Services},
 series = {HotTopiCS '13},
 year = {2013},
 isbn = {978-1-4503-2051-1},
 location = {Prague, Czech Republic},
 pages = {1--2},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2462307.2462309},
 doi = {10.1145/2462307.2462309},
 acmid = {2462309},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud benchmarking, cloud computing, experimental research, performance measures, service-oriented architectures},
}

@article{Ouni:2018:HAI:3283809.3226593,
 author = {Ouni, Ali and Wang, Hanzhang and Kessentini, Marouane and Bouktif, Salah and Inoue, Katsuro},
 title = {A Hybrid Approach for Improving the Design Quality of Web Service Interfaces},
 journal = {ACM Trans. Internet Technol.},
 issue_date = {March 2019},
 volume = {19},
 number = {1},
 month = dec,
 year = {2018},
 issn = {1533-5399},
 pages = {4:1--4:24},
 articleno = {4},
 numpages = {24},
 url = {http://doi.acm.org/10.1145/3226593},
 doi = {10.1145/3226593},
 acmid = {3226593},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Web service design, Web services, antipattern, search-based software engineering, service-oriented computing},
}

@inproceedings{Zhou:2018:BMS:3183440.3194991,
 author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Xu, Chenjie and Ji, Chao and Zhao, Wenyun},
 title = {Benchmarking Microservice Systems for Software Engineering Research},
 booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
 series = {ICSE '18},
 year = {2018},
 isbn = {978-1-4503-5663-3},
 location = {Gothenburg, Sweden},
 pages = {323--324},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3183440.3194991},
 doi = {10.1145/3183440.3194991},
 acmid = {3194991},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark, debugging, failure diagnosis, microservice, tracing, visualization},
}

@inproceedings{Zhu:2006:MDB:1138486.1138494,
 author = {Zhu, Liming and Gorton, Ian and Liu, Yan and Bui, Ngoc Bao},
 title = {Model Driven Benchmark Generation for Web Services},
 booktitle = {Proceedings of the 2006 International Workshop on Service-oriented Software Engineering},
 series = {SOSE '06},
 year = {2006},
 isbn = {1-59593-398-0},
 location = {Shanghai, China},
 pages = {33--39},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1138486.1138494},
 doi = {10.1145/1138486.1138494},
 acmid = {1138494},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MDA, code, model-driven development, performance, testing},
}

@inproceedings{Ker:2007:USB:1288869.1288889,
 author = {Ker, Andrew D.},
 title = {The Ultimate Steganalysis Benchmark?},
 booktitle = {Proceedings of the 9th Workshop on Multimedia \& Security},
 series = {MM\&\#38;Sec '07},
 year = {2007},
 isbn = {978-1-59593-857-2},
 location = {Dallas, Texas, USA},
 pages = {141--148},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1288869.1288889},
 doi = {10.1145/1288869.1288889},
 acmid = {1288889},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, steganalysis, steganographic capacity},
}

@inproceedings{Dullmann:2017:MGM:3053600.3053627,
 author = {D\"{u}llmann, Thomas F. and van Hoorn, Andr{\'e}},
 title = {Model-driven Generation of Microservice Architectures for Benchmarking Performance and Resilience Engineering Approaches},
 booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
 series = {ICPE '17 Companion},
 year = {2017},
 isbn = {978-1-4503-4899-7},
 location = {L'Aquila, Italy},
 pages = {171--172},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3053600.3053627},
 doi = {10.1145/3053600.3053627},
 acmid = {3053627},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, microservice architecture, model-driven generation, software performance, software resilience},
}

@inproceedings{Ouni:2015:WSA:2739480.2754724,
 author = {Ouni, Ali and Gaikovina Kula, Raula and Kessentini, Marouane and Inoue, Katsuro},
 title = {Web Service Antipatterns Detection Using Genetic Programming},
 booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '15},
 year = {2015},
 isbn = {978-1-4503-3472-3},
 location = {Madrid, Spain},
 pages = {1351--1358},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2739480.2754724},
 doi = {10.1145/2739480.2754724},
 acmid = {2754724},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {antipatterns, search-based software engineering, web services},
}

@inproceedings{Crnkovic:2002:TIW:581339.581429,
 author = {Crnkovic, Ivica and Schmidt, Heinz and Stafford, Judith and Wallnau, Kurt},
 title = {5th ICSE Workshop on Component-based Software Engineering: Benchmarks for Predictable Assembly},
 booktitle = {Proceedings of the 24th International Conference on Software Engineering},
 series = {ICSE '02},
 year = {2002},
 isbn = {1-58113-472-X},
 location = {Orlando, Florida},
 pages = {655--656},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/581339.581429},
 doi = {10.1145/581339.581429},
 acmid = {581429},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {analysis, certification, component, composition languages, software architecture, trusted components},
}

@inproceedings{Gorbenko:2008:TUS:1479772.1479781,
 author = {Gorbenko, Anatoliy and Kharchenko, Vyacheslav and Tarasyuk, Olga and Chen, Yuhui and Romanovsky, Alexander},
 title = {The Threat of Uncertainty in Service-oriented Architecture},
 booktitle = {Proceedings of the 2008 RISE/EFTS Joint International Workshop on Software Engineering for Resilient Systems},
 series = {SERENE '08},
 year = {2008},
 isbn = {978-1-60558-275-7},
 location = {Newcastle upon Tyne, United Kingdom},
 pages = {49--54},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1479772.1479781},
 doi = {10.1145/1479772.1479781},
 acmid = {1479781},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependability, instability, performance analysis, resilience, service-oriented systems, web service benchmarking},
}

@inproceedings{Gorbenko:2008:EEP:1454268.1454269,
 author = {Gorbenko, Anatoliy and Romanovsky, Alexander and Kharchenko, Vyacheslav and Mikhaylichenko, Alexey},
 title = {Experimenting with Exception Propagation Mechanisms in Service-oriented Architecture},
 booktitle = {Proceedings of the 4th International Workshop on Exception Handling},
 series = {WEH '08},
 year = {2008},
 isbn = {978-1-60558-229-0},
 location = {Atlanta, Georgia},
 pages = {1--7},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1454268.1454269},
 doi = {10.1145/1454268.1454269},
 acmid = {1454269},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependability benchmarking, exception handling, exception propagation mechanisms, fault tolerance, robustness, service-oriented architecture},
}

@inproceedings{Hasselbring:2011:RED:2031759.2031765,
 author = {Hasselbring, Wilhelm},
 title = {Reverse Engineering of Dependency Graphs via Dynamic Analysis},
 booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
 series = {ECSA '11},
 year = {2011},
 isbn = {978-1-4503-0618-8},
 location = {Essen, Germany},
 pages = {5:1--5:2},
 articleno = {5},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2031759.2031765},
 doi = {10.1145/2031759.2031765},
 acmid = {2031765},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dependency graphs, dynamic analysis, monitoring, reverse engineering, software engineering},
}

@inproceedings{Coenen:2019:BFD:3328905.3332506,
 author = {Coenen, Manuel and Wagner, Christoph and Echler, Alexander and Frischbier, Sebastian},
 title = {Benchmarking Financial Data Feed Systems},
 booktitle = {Proceedings of the 13th ACM International Conference on Distributed and Event-based Systems},
 series = {DEBS '19},
 year = {2019},
 isbn = {978-1-4503-6794-3},
 location = {Darmstadt, Germany},
 pages = {252--253},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/3328905.3332506},
 doi = {10.1145/3328905.3332506},
 acmid = {3332506},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Event-processing, benchmarking, big data, event bus, financial data, publish/subscribe, requirements, stream-processing, workload},
}

@inproceedings{Mo:2018:EAA:3238147.3240467,
 author = {Mo, Ran and Snipes, Will and Cai, Yuanfang and Ramaswamy, Srini and Kazman, Rick and Naedele, Martin},
 title = {Experiences Applying Automated Architecture Analysis Tool Suites},
 booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
 series = {ASE 2018},
 year = {2018},
 isbn = {978-1-4503-5937-5},
 location = {Montpellier, France},
 pages = {779--789},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/3238147.3240467},
 doi = {10.1145/3238147.3240467},
 acmid = {3240467},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Software Architecture, Software Maintenance, Software Quality},
}

@inproceedings{Zhou:2018:DDM:3238147.3240730,
 author = {Zhou, Xiang and Peng, Xin and Xie, Tao and Sun, Jun and Li, Wenhai and Ji, Chao and Ding, Dan},
 title = {Delta Debugging Microservice Systems},
 booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
 series = {ASE 2018},
 year = {2018},
 isbn = {978-1-4503-5937-5},
 location = {Montpellier, France},
 pages = {802--807},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3238147.3240730},
 doi = {10.1145/3238147.3240730},
 acmid = {3240730},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Delta Debugging, Microservice, Testing},
}

@inproceedings{Reinecke:2008:AMP:1383559.1383585,
 author = {Reinecke, Philipp and Wolter, Katinka},
 title = {Adaptivity Metric and Performance for Restart Strategies in Web Services Reliable Messaging},
 booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
 series = {WOSP '08},
 year = {2008},
 isbn = {978-1-59593-873-2},
 location = {Princeton, NJ, USA},
 pages = {201--212},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1383559.1383585},
 doi = {10.1145/1383559.1383585},
 acmid = {1383585},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {adaptivity, adaptivity metrics, service-oriented architectures, web-services reliable messaging},
}

@inproceedings{Konersmann:2013:TAE:2465478.2465496,
 author = {Konersmann, Marco and Durdik, Zoya and Goedicke, Michael and Reussner, Ralf H.},
 title = {Towards Architecture-centric Evolution of Long-living Systems (the ADVERT Approach)},
 booktitle = {Proceedings of the 9th International ACM Sigsoft Conference on Quality of Software Architectures},
 series = {QoSA '13},
 year = {2013},
 isbn = {978-1-4503-2126-6},
 location = {Vancouver, British Columbia, Canada},
 pages = {163--168},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2465478.2465496},
 doi = {10.1145/2465478.2465496},
 acmid = {2465496},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {design decisions, embedded architecture, software architecture, software evolution},
}

@inproceedings{Pankratius:2008:SEM:1370082.1370096,
 author = {Pankratius, Victor and Schaefer, Christoph and Jannesari, Ali and Tichy, Walter F.},
 title = {Software Engineering for Multicore Systems: An Experience Report},
 booktitle = {Proceedings of the 1st International Workshop on Multicore Software Engineering},
 series = {IWMSE '08},
 year = {2008},
 isbn = {978-1-60558-031-9},
 location = {Leipzig, Germany},
 pages = {53--60},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1370082.1370096},
 doi = {10.1145/1370082.1370096},
 acmid = {1370096},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {autotuning, design patterns, multicore systems, openMP},
}

@inproceedings{Kale:2010:CPP:1808954.1808969,
 author = {Kale, Vivek L.},
 title = {The Correlation Between Parallel Patterns and Multi-core Benchmarks},
 booktitle = {Proceedings of the 3rd International Workshop on Multicore Software Engineering},
 series = {IWMSE '10},
 year = {2010},
 isbn = {978-1-60558-964-0},
 location = {Cape Town, South Africa},
 pages = {54--55},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1808954.1808969},
 doi = {10.1145/1808954.1808969},
 acmid = {1808969},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, multi-core, parallel patterns, parallel programming, performance tuning, scientific codes, software},
}

@inproceedings{Franks:2011:PMM:2000259.2000270,
 author = {Franks, Greg and Lau, Danny and Hrischuk, Curtis},
 title = {Performance Measurements and Modeling of a Java-based Session Initiation Protocol (SIP) Application Server},
 booktitle = {Proceedings of the Joint ACM SIGSOFT Conference -- QoSA and ACM SIGSOFT Symposium -- ISARCS on Quality of Software Architectures -- QoSA and Architecting Critical Systems -- ISARCS},
 series = {QoSA-ISARCS '11},
 year = {2011},
 isbn = {978-1-4503-0724-6},
 location = {Boulder, Colorado, USA},
 pages = {63--72},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2000259.2000270},
 doi = {10.1145/2000259.2000270},
 acmid = {2000270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ethernet, layered queueing network, performance analysis, session initiation protocol, two-phase server},
}

@inproceedings{Casalicchio:2017:MDP:3053600.3053605,
 author = {Casalicchio, Emiliano and Perciballi, Vanessa},
 title = {Measuring Docker Performance: What a Mess!!!},
 booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
 series = {ICPE '17 Companion},
 year = {2017},
 isbn = {978-1-4503-4899-7},
 location = {L'Aquila, Italy},
 pages = {11--16},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3053600.3053605},
 doi = {10.1145/3053600.3053605},
 acmid = {3053605},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud computing, container, docker, internet of service, microservices, monitoring, performance evaluation},
}

@inproceedings{Dorofeev:2018:VAD:3274856.3274869,
 author = {Dorofeev, Dmitriy and Shestakov, Sergey},
 title = {2tierr vs. 3-tier Architectures for Data Processing Software},
 booktitle = {Proceedings of the 3rd International Conference on Applications in Information Technology},
 series = {ICAIT'2018},
 year = {2018},
 isbn = {978-1-4503-6516-1},
 location = {Aizu-Wakamatsu, Japan},
 pages = {63--68},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3274856.3274869},
 doi = {10.1145/3274856.3274869},
 acmid = {3274869},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {2-tier, 3-tier, benchmarks, data centric, data processing, software architecture},
}

@inproceedings{Huang:2011:TAM:2000229.2000252,
 author = {Huang, Gang and Wu, Yihan},
 title = {Towards Architecture-level Middleware-enabled Exception Handling of Component-based Systems},
 booktitle = {Proceedings of the 14th International ACM Sigsoft Symposium on Component Based Software Engineering},
 series = {CBSE '11},
 year = {2011},
 isbn = {978-1-4503-0723-9},
 location = {Boulder, Colorado, USA},
 pages = {159--168},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2000229.2000252},
 doi = {10.1145/2000229.2000252},
 acmid = {2000252},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {exception handling, jee, middleware, software architecture},
}

@inproceedings{Ker:2008:SRL:1411328.1411349,
 author = {Ker, Andrew D. and Pevn\'{y}, Tom\'{a}\v{s} and Kodovsk\'{y}, Jan and Fridrich, Jessica},
 title = {The Square Root Law of Steganographic Capacity},
 booktitle = {Proceedings of the 10th ACM Workshop on Multimedia and Security},
 series = {MM\&\#38;Sec '08},
 year = {2008},
 isbn = {978-1-60558-058-6},
 location = {Oxford, United Kingdom},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1411328.1411349},
 doi = {10.1145/1411328.1411349},
 acmid = {1411349},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmarking, steganalysis, steganographic capacity, steganography},
}

@inproceedings{Cardarelli:2019:EDA:3297280.3297400,
 author = {Cardarelli, Mario and Iovino, Ludovico and Di Francesco, Paolo and Di Salle, Amleto and Malavolta, Ivano and Lago, Patricia},
 title = {An Extensible Data-driven Approach for Evaluating the Quality of Microservice Architectures},
 booktitle = {Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing},
 series = {SAC '19},
 year = {2019},
 isbn = {978-1-4503-5933-7},
 location = {Limassol, Cyprus},
 pages = {1225--1234},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3297280.3297400},
 doi = {10.1145/3297280.3297400},
 acmid = {3297400},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture recovery, microservices, model-driven, software quality},
}

@inproceedings{Schutze:2017:ASR:3079368.3079386,
 author = {Sch\"{u}tze, Lars and Castrillon, Jeronimo},
 title = {Analyzing State-of-the-Art Role-based Programming Languages},
 booktitle = {Companion to the First International Conference on the Art, Science and Engineering of Programming},
 series = {Programming '17},
 year = {2017},
 isbn = {978-1-4503-4836-2},
 location = {Brussels, Belgium},
 pages = {9:1--9:6},
 articleno = {9},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3079368.3079386},
 doi = {10.1145/3079368.3079386},
 acmid = {3079386},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Benchmarking, Optimization, Role-based Programming},
}

@inproceedings{Happe:2008:PPC:1383559.1383581,
 author = {Happe, Jens and Friedrich, Holger and Becker, Steffen and Reussner, Ralf H.},
 title = {A Pattern-based Performance Completion for Message-oriented Middleware},
 booktitle = {Proceedings of the 7th International Workshop on Software and Performance},
 series = {WOSP '08},
 year = {2008},
 isbn = {978-1-59593-873-2},
 location = {Princeton, NJ, USA},
 pages = {165--176},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1383559.1383581},
 doi = {10.1145/1383559.1383581},
 acmid = {1383581},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {message-oriented middleware, model-based performance prediction, software architecture},
}

@inproceedings{Scholz:2008:WWS:1368088.1368188,
 author = {Scholz, Andreas and Buckl, Christian and Kemper, Alfons and Knoll, Alois and Heuer, J\"{o}rg and Winter, Martin},
 title = {WS-AMUSE - Web Service Architecture for Multimedia Services},
 booktitle = {Proceedings of the 30th International Conference on Software Engineering},
 series = {ICSE '08},
 year = {2008},
 isbn = {978-1-60558-079-1},
 location = {Leipzig, Germany},
 pages = {703--712},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1368088.1368188},
 doi = {10.1145/1368088.1368188},
 acmid = {1368188},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bpel, soa, vod, voip, web services},
}

@inproceedings{Bog:2012:IPM:2213836.2213921,
 author = {Bog, Anja and Sachs, Kai and Plattner, Hasso},
 title = {Interactive Performance Monitoring of a Composite OLTP and OLAP Workload},
 booktitle = {Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data},
 series = {SIGMOD '12},
 year = {2012},
 isbn = {978-1-4503-1247-9},
 location = {Scottsdale, Arizona, USA},
 pages = {645--648},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2213836.2213921},
 doi = {10.1145/2213836.2213921},
 acmid = {2213921},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {database benchmarking, mixed oltp and olap workload, performance monitoring},
}

@article{Rellermeyer:2007:CSP:1272998.1273022,
 author = {Rellermeyer, Jan S. and Alonso, Gustavo},
 title = {Concierge: A Service Platform for Resource-constrained Devices},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {June 2007},
 volume = {41},
 number = {3},
 month = mar,
 year = {2007},
 issn = {0163-5980},
 pages = {245--258},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1272998.1273022},
 doi = {10.1145/1272998.1273022},
 acmid = {1273022},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OSGi, average bundle, concierge, resource-constrained devices, service oriented architecture},
}

@inproceedings{Rellermeyer:2007:CSP:1272996.1273022,
 author = {Rellermeyer, Jan S. and Alonso, Gustavo},
 title = {Concierge: A Service Platform for Resource-constrained Devices},
 booktitle = {Proceedings of the 2Nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007},
 series = {EuroSys '07},
 year = {2007},
 isbn = {978-1-59593-636-3},
 location = {Lisbon, Portugal},
 pages = {245--258},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1272996.1273022},
 doi = {10.1145/1272996.1273022},
 acmid = {1273022},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OSGi, average bundle, concierge, resource-constrained devices, service oriented architecture},
}

@inproceedings{Attouchi:2014:MMM:2602458.2602467,
 author = {Attouchi, Koutheir and Thomas, Ga\"{e}l and Bottaro, Andr{\'e} and Muller, Gilles},
 title = {Memory Monitoring in a Multi-tenant OSGi Execution Environment},
 booktitle = {Proceedings of the 17th International ACM Sigsoft Symposium on Component-based Software Engineering},
 series = {CBSE '14},
 year = {2014},
 isbn = {978-1-4503-2577-6},
 location = {Marcq-en-Bareul, France},
 pages = {107--116},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2602458.2602467},
 doi = {10.1145/2602458.2602467},
 acmid = {2602467},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {home gateway, memory monitoring, multitenancy, osgi technology, smart home, software platform},
}

@inproceedings{Bui:2006:DAD:1169086.1169087,
 author = {Bui, Ngoc Bao and Jeffery, Ross},
 title = {DSLBench: Applying DSL in Benchmark Generation},
 booktitle = {Proceedings of the 1st Workshop on MOdel Driven Development for Middleware (MODDM '06)},
 series = {MODDM '06},
 year = {2006},
 isbn = {1-59593-423-5},
 location = {Melbourne, Australia},
 pages = {1--6},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1169086.1169087},
 doi = {10.1145/1169086.1169087},
 acmid = {1169087},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {domain specific language, domain specific modelling, model driven development, performance, testing},
}

@inproceedings{Hall:2004:BMS:1035167.1035191,
 author = {Hall, Robert J. and Zisman, Andrea},
 title = {Behavioral Models As Service Descriptions},
 booktitle = {Proceedings of the 2Nd International Conference on Service Oriented Computing},
 series = {ICSOC '04},
 year = {2004},
 isbn = {1-58113-871-7},
 location = {New York, NY, USA},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1035167.1035191},
 doi = {10.1145/1035167.1035191},
 acmid = {1035191},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {behavioral model, executable specification, execution monitoring, service discovery, validation},
}

@inproceedings{Brunnert:2014:UAP:2602576.2602587,
 author = {Brunnert, Andreas and Wischer, Kilian and Krcmar, Helmut},
 title = {Using Architecture-level Performance Models As Resource Profiles for Enterprise Applications},
 booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
 series = {QoSA '14},
 year = {2014},
 isbn = {978-1-4503-2576-9},
 location = {Marcq-en-Bareul, France},
 pages = {53--62},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2602576.2602587},
 doi = {10.1145/2602576.2602587},
 acmid = {2602587},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {capacity planning, energy consumption, palladio component model, performance modeling, resource profile},
}

@inproceedings{Shahriari:2015:WRW:2809563.2809593,
 author = {Shahriari, Mohsen and Krott, Sebastian and Klamma, Ralf},
 title = {WebOCD: A RESTful Web-based Overlapping Community Detection Framework},
 booktitle = {Proceedings of the 15th International Conference on Knowledge Technologies and Data-driven Business},
 series = {i-KNOW '15},
 year = {2015},
 isbn = {978-1-4503-3721-2},
 location = {Graz, Austria},
 pages = {51:1--51:4},
 articleno = {51},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2809563.2809593},
 doi = {10.1145/2809563.2809593},
 acmid = {2809593},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark graphs, community detection, web services},
}

@inproceedings{Zhu:2005:MTC:1094855.1094919,
 author = {Zhu, Liming and Liu, Yan and Gorton, Ian and Bui, Ngoc Bao},
 title = {MDAbench: A Tool for Customized Benchmark Generation Using MDA},
 booktitle = {Companion to the 20th Annual ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications},
 series = {OOPSLA '05},
 year = {2005},
 isbn = {1-59593-193-7},
 location = {San Diego, CA, USA},
 pages = {171--172},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/1094855.1094919},
 doi = {10.1145/1094855.1094919},
 acmid = {1094919},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {CASE tools, MDA, code generation, model-driven development, performance, testing},
}

@inproceedings{Liu:2005:PPJ:2154570.2154572,
 author = {Liu, Yan and Gorton, Ian},
 title = {Performance Prediction of J2EE Applications Using Messaging Protocols},
 booktitle = {Proceedings of the 8th International Conference on Component-Based Software Engineering},
 series = {CBSE'05},
 year = {2005},
 isbn = {3-540-25877-9, 978-3-540-25877-3},
 location = {St. Louis, MO},
 pages = {1--16},
 numpages = {16},
 url = {http://dx.doi.org/10.1007/11424529_1},
 doi = {10.1007/11424529_1},
 acmid = {2154572},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

@inproceedings{Dufour:2008:STC:1453101.1453111,
 author = {Dufour, Bruno and Ryder, Barbara G. and Sevitsky, Gary},
 title = {A Scalable Technique for Characterizing the Usage of Temporaries in Framework-intensive Java Applications},
 booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
 series = {SIGSOFT '08/FSE-16},
 year = {2008},
 isbn = {978-1-59593-995-1},
 location = {Atlanta, Georgia},
 pages = {59--70},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1453101.1453111},
 doi = {10.1145/1453101.1453111},
 acmid = {1453111},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, dataflow analysis, escape analysis, framework-intensive applications, performance, program understanding},
}

@inproceedings{Jin:2007:PEP:1248820.1248885,
 author = {Jin, Yan and Tang, Antony and Han, Jun and Liu, Yan},
 title = {Performance Evaluation and Prediction for Legacy Information Systems},
 booktitle = {Proceedings of the 29th International Conference on Software Engineering},
 series = {ICSE '07},
 year = {2007},
 isbn = {0-7695-2828-7},
 pages = {540--549},
 numpages = {10},
 url = {https://doi.org/10.1109/ICSE.2007.64},
 doi = {10.1109/ICSE.2007.64},
 acmid = {1248885},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@inproceedings{vanEyk:2018:SRC:3185768.3186308,
 author = {van Eyk, Erwin and Iosup, Alexandru and Abad, Cristina L. and Grohmann, Johannes and Eismann, Simon},
 title = {A SPEC RG Cloud Group's Vision on the Performance Challenges of FaaS Cloud Architectures},
 booktitle = {Companion of the 2018 ACM/SPEC International Conference on Performance Engineering},
 series = {ICPE '18},
 year = {2018},
 isbn = {978-1-4503-5629-9},
 location = {Berlin, Germany},
 pages = {21--24},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/3185768.3186308},
 doi = {10.1145/3185768.3186308},
 acmid = {3186308},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FaaS, benchmarking, function-as-a-service, performance evaluation, reference architecture, serverless computing},
}

@inproceedings{Gan:2019:OBS:3297858.3304013,
 author = {Gan, Yu and Zhang, Yanqi and Cheng, Dailun and Shetty, Ankitha and Rathi, Priyal and Katarki, Nayan and Bruno, Ariana and Hu, Justin and Ritchken, Brian and Jackson, Brendon and Hu, Kelvin and Pancholi, Meghna and He, Yuan and Clancy, Brett and Colen, Chris and Wen, Fukang and Leung, Catherine and Wang, Siyuan and Zaruvinsky, Leon and Espinosa, Mateo and Lin, Rick and Liu, Zhongling and Padilla, Jake and Delimitrou, Christina},
 title = {An Open-Source Benchmark Suite for Microservices and Their Hardware-Software Implications for Cloud \&\#38; Edge Systems},
 booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '19},
 year = {2019},
 isbn = {978-1-4503-6240-5},
 location = {Providence, RI, USA},
 pages = {3--18},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/3297858.3304013},
 doi = {10.1145/3297858.3304013},
 acmid = {3304013},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {acceleration, cloud computing, cluster management, datacenters, fpga, microservices, qos, serverless},
}

@article{Stein:2009:FPW:1462159.1462161,
 author = {Stein, Sebastian and Payne, Terry R. and Jennings, Nicholas R.},
 title = {Flexible Provisioning of Web Service Workflows},
 journal = {ACM Trans. Internet Technol.},
 issue_date = {February 2009},
 volume = {9},
 number = {1},
 month = feb,
 year = {2009},
 issn = {1533-5399},
 pages = {2:1--2:45},
 articleno = {2},
 numpages = {45},
 url = {http://doi.acm.org/10.1145/1462159.1462161},
 doi = {10.1145/1462159.1462161},
 acmid = {1462161},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Web services, semantic Web services, service composition, service provisioning, service-oriented computing, workflows},
}

@inproceedings{FonsecaC.:2014:RTA:2652524.2652600,
 author = {Fonseca C., Efra\'{\i}n R. and Dieste, Oscar and Juristo, Natalia and Serral, Estefan\'{\i}a and Biffl, Stefan},
 title = {Reviewing Technical Approaches for Sharing and Preservation of Experimental Data},
 booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
 series = {ESEM '14},
 year = {2014},
 isbn = {978-1-4503-2774-9},
 location = {Torino, Italy},
 pages = {71:1--71:1},
 articleno = {71},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2652524.2652600},
 doi = {10.1145/2652524.2652600},
 acmid = {2652600},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {preservation, semantic integration, sharing},
}

@inproceedings{Pitakrat:2013:CML:2465470.2465473,
 author = {Pitakrat, Teerat and van Hoorn, Andr{\'e} and Grunske, Lars},
 title = {A Comparison of Machine Learning Algorithms for Proactive Hard Disk Drive Failure Detection},
 booktitle = {Proceedings of the 4th International ACM Sigsoft Symposium on Architecting Critical Systems},
 series = {ISARCS '13},
 year = {2013},
 isbn = {978-1-4503-2123-5},
 location = {Vancouver, British Columbia, Canada},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2465470.2465473},
 doi = {10.1145/2465470.2465473},
 acmid = {2465473},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {hard disk drive failures, machine learning, proactive failure detection},
}

@inproceedings{Thies:2010:ECS:1854273.1854319,
 author = {Thies, William and Amarasinghe, Saman},
 title = {An Empirical Characterization of Stream Programs and Its Implications for Language and Compiler Design},
 booktitle = {Proceedings of the 19th International Conference on Parallel Architectures and Compilation Techniques},
 series = {PACT '10},
 year = {2010},
 isbn = {978-1-4503-0178-7},
 location = {Vienna, Austria},
 pages = {365--376},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1854273.1854319},
 doi = {10.1145/1854273.1854319},
 acmid = {1854319},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {benchmark suite, stream programming, streamit, synchronous dataflow, workload characterization},
}

@inproceedings{Wert:2014:ADP:2602576.2602579,
 author = {Wert, Alexander and Oehler, Marius and Heger, Christoph and Farahbod, Roozbeh},
 title = {Automatic Detection of Performance Anti-patterns in Inter-component Communications},
 booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
 series = {QoSA '14},
 year = {2014},
 isbn = {978-1-4503-2576-9},
 location = {Marcq-en-Bareul, France},
 pages = {3--12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2602576.2602579},
 doi = {10.1145/2602576.2602579},
 acmid = {2602579},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {performance anti-patterns, performance problem diagnostics, performance testing},
}

@inproceedings{deGooijer:2014:EMM:2602576.2602584,
 author = {de Gooijer, Thijmen and Harper, K. Eric},
 title = {Experiences with Modeling Memory Contention for Multi-core Industrial Real-time Systems},
 booktitle = {Proceedings of the 10th International ACM Sigsoft Conference on Quality of Software Architectures},
 series = {QoSA '14},
 year = {2014},
 isbn = {978-1-4503-2576-9},
 location = {Marcq-en-Bareul, France},
 pages = {43--52},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2602576.2602584},
 doi = {10.1145/2602576.2602584},
 acmid = {2602584},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {experience report, industry, multicore, performance modeling, queuing petri nets, realtime systems},
}

@inproceedings{Gay:2015:IWS:2819009.2819250,
 author = {Gay, Gregory and Antoniol, Giuliano},
 title = {8th International Workshop on Search-based Software Testing (SBST 2015)},
 booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
 series = {ICSE '15},
 year = {2015},
 location = {Florence, Italy},
 pages = {1001--1002},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=2819009.2819250},
 acmid = {2819250},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@inproceedings{Krebs:2012:MTQ:2304696.2304713,
 author = {Krebs, Rouven and Momm, Christof and Kounev, Samuel},
 title = {Metrics and Techniques for Quantifying Performance Isolation in Cloud Environments},
 booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
 series = {QoSA '12},
 year = {2012},
 isbn = {978-1-4503-1346-9},
 location = {Bertinoro, Italy},
 pages = {91--100},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2304696.2304713},
 doi = {10.1145/2304696.2304713},
 acmid = {2304713},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cloud, isolation, metric, multi-tenancy, performance, saas, virtualization},
}

@inproceedings{Schmalz:2008:IWM:1414558.1414567,
 author = {Schmalz, Mark S. and Conway, Lynn},
 title = {It/Cs Workshop: Multimodal, Multimedia Courseware for Teaching Technical Concepts in Humanistic Context},
 booktitle = {Proceedings of the 9th ACM SIGITE Conference on Information Technology Education},
 series = {SIGITE '08},
 year = {2008},
 isbn = {978-1-60558-329-7},
 location = {Cincinnati, OH, USA},
 pages = {23--30},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1414558.1414567},
 doi = {10.1145/1414558.1414567},
 acmid = {1414567},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {courseware, distributed learning, performance measurement},
}

@inproceedings{Espinha:2012:SSP:2666048.2666057,
 author = {Espinha, Tiago and Chen, Cuiting and Zaidman, Andy and Gross, Hans-Gerhard},
 title = {Spicy Stonehenge: Proposing a SOA Case Study},
 booktitle = {Proceedings of the 4th International Workshop on Principles of Engineering Service-Oriented Systems},
 series = {PESOS '12},
 year = {2012},
 isbn = {978-1-4673-1755-9},
 location = {Zurich, Switzerland},
 pages = {57--58},
 numpages = {2},
 url = {http://dl.acm.org/citation.cfm?id=2666048.2666057},
 acmid = {2666057},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@inproceedings{Zhang:2003:CIB:783106.783129,
 author = {Zhang, Yan and Liu, Anna and Qu, Wei},
 title = {Comparing Industry Benchmarks for J2EE Application Server: IBM's Trade2 vs Sun's ECperf},
 booktitle = {Proceedings of the 26th Australasian Computer Science Conference - Volume 16},
 series = {ACSC '03},
 year = {2003},
 isbn = {0-909-92594-1},
 location = {Adelaide, Australia},
 pages = {199--206},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=783106.783129},
 acmid = {783129},
 publisher = {Australian Computer Society, Inc.},
 address = {Darlinghurst, Australia, Australia},
 keywords = {COTS, benchmark, component-based system, empirical results, middleware},
}

@inproceedings{Bondi:2016:ISP:2851553.2858668,
 author = {Bondi, Andr{\'e} B.},
 title = {Incorporating Software Performance Engineering Methods and Practices into the Software Development Life Cycle},
 booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
 series = {ICPE '16},
 year = {2016},
 isbn = {978-1-4503-4080-9},
 location = {Delft, The Netherlands},
 pages = {327--330},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2851553.2858668},
 doi = {10.1145/2851553.2858668},
 acmid = {2858668},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture, modeling, performance measurement and testing, software life cycle, software performance engineering},
}

@inproceedings{Chowdhury:2019:GES:3339505.3339644,
 author = {Chowdhury, Shaiful Alam and Hindle, Abram and Kazman, Rick and Shuto, Takumi and Matsui, Ken and Kamei, Yasutaka},
 title = {GreenBundle: An Empirical Study on the Energy Impact of Bundled Processing},
 booktitle = {Proceedings of the 41st International Conference on Software Engineering},
 series = {ICSE '19},
 year = {2019},
 location = {Montreal, Quebec, Canada},
 pages = {1107--1118},
 numpages = {12},
 url = {https://doi.org/10.1109/ICSE.2019.00114},
 doi = {10.1109/ICSE.2019.00114},
 acmid = {3339644},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
 keywords = {MVC, MVP, architecture, energy consumption},
}

@inproceedings{Brosig:2011:AEA:2190078.2190173,
 author = {Brosig, Fabian and Huber, Nikolaus and Kounev, Samuel},
 title = {Automated Extraction of Architecture-level Performance Models of Distributed Component-based Systems},
 booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
 series = {ASE '11},
 year = {2011},
 isbn = {978-1-4577-1638-6},
 pages = {183--192},
 numpages = {10},
 url = {http://dx.doi.org/10.1109/ASE.2011.6100052},
 doi = {10.1109/ASE.2011.6100052},
 acmid = {2190173},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@inproceedings{Brosig:2012:MPC:2304736.2304740,
 author = {Brosig, Fabian and Huber, Nikolaus and Kounev, Samuel},
 title = {Modeling Parameter and Context Dependencies in Online Architecture-level Performance Models},
 booktitle = {Proceedings of the 15th ACM SIGSOFT Symposium on Component Based Software Engineering},
 series = {CBSE '12},
 year = {2012},
 isbn = {978-1-4503-1345-2},
 location = {Bertinoro, Italy},
 pages = {3--12},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2304736.2304740},
 doi = {10.1145/2304736.2304740},
 acmid = {2304740},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture-level performance model, parameter dependencies},
}

@inproceedings{Atencio:2018:CAC:3193077.3193083,
 author = {Atencio, Luis and Aybar, Bruno and Padilla, Alfredo Barrientos},
 title = {Comparative Analysis of Cross-Platform Communication Mechanisms},
 booktitle = {Proceedings of the 2Nd International Conference on Compute and Data Analysis},
 series = {ICCDA 2018},
 year = {2018},
 isbn = {978-1-4503-6359-4},
 location = {DeKalb, IL, USA},
 pages = {75--79},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3193077.3193083},
 doi = {10.1145/3193077.3193083},
 acmid = {3193083},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Android, Apps, Cross-device, Cross-platform, Mobile development, Real-time communication, Software architecture, iOS},
}

@inproceedings{Kale:2010:TUI:1953611.1953623,
 author = {Kale, Vivek},
 title = {Towards Using and Improving the NAS Parallel Benchmarks: A Parallel Patterns Approach},
 booktitle = {Proceedings of the 2010 Workshop on Parallel Programming Patterns},
 series = {ParaPLoP '10},
 year = {2010},
 isbn = {978-1-4503-0127-5},
 location = {Carefree, Arizona, USA},
 pages = {12:1--12:6},
 articleno = {12},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1953611.1953623},
 doi = {10.1145/1953611.1953623},
 acmid = {1953623},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Jannesari:2008:ORD:1390841.1390847,
 author = {Jannesari, Ali and Tichy, Walter F.},
 title = {On-the-fly Race Detection in Multi-threaded Programs},
 booktitle = {Proceedings of the 6th Workshop on Parallel and Distributed Systems: Testing, Analysis, and Debugging},
 series = {PADTAD '08},
 year = {2008},
 isbn = {978-1-60558-052-4},
 location = {Seattle, Washington},
 pages = {6:1--6:10},
 articleno = {6},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1390841.1390847},
 doi = {10.1145/1390841.1390847},
 acmid = {1390847},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {debugging, dynamic analysis, happens-before, lockset, multi-threaded programming, parallel programs, race conditions, race detection},
}

@inproceedings{Dong:2007:EDP:1268984.1269060,
 author = {Dong, Jing and Zhao, Yajing},
 title = {Experiments on Design Pattern Discovery},
 booktitle = {Proceedings of the Third International Workshop on Predictor Models in Software Engineering},
 series = {PROMISE '07},
 year = {2007},
 isbn = {0-7695-2954-2},
 pages = {12--},
 url = {http://dx.doi.org/10.1109/PROMISE.2007.6},
 doi = {10.1109/PROMISE.2007.6},
 acmid = {1269060},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@proceedings{Dujmovic:2004:974044,
 title = {WOSP '04: Proceedings of the 4th International Workshop on Software and Performance},
 year = {2004},
 isbn = {1-58113-673-0},
 issn = {0163-5948},
 location = {Redwood Shores, California},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Canas:2016:EPS:2933267.2933297,
 author = {Canas, Cesar and Zhang, Kaiwen and Kemme, Bettina and Kienzle, J\"{o}rg and Jacobsen, Hans-Arno},
 title = {Evolving Pub/Sub Subscriptions for Multiplayer Online Games: Demo},
 booktitle = {Proceedings of the 10th ACM International Conference on Distributed and Event-based Systems},
 series = {DEBS '16},
 year = {2016},
 isbn = {978-1-4503-4021-2},
 location = {Irvine, California},
 pages = {344--347},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2933267.2933297},
 doi = {10.1145/2933267.2933297},
 acmid = {2933297},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dead reckoning, online games, publish/subscribe},
}

@inproceedings{Misra:2014:RCC:2729104.2729131,
 author = {Misra, Harekrishna and Das, Umanag},
 title = {Role of Connectivity in Citizen Centered E-Governance in Myanmar: Learning from Indian Experience},
 booktitle = {Proceedings of the 2014 Conference on Electronic Governance and Open Society: Challenges in Eurasia},
 series = {EGOSE '14},
 year = {2014},
 isbn = {978-1-4503-3401-3},
 location = {St. Petersburg, Russian Federation},
 pages = {121--126},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2729104.2729131},
 doi = {10.1145/2729104.2729131},
 acmid = {2729131},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Connectivity, E-Governance Infrastructure, E-Governance Services, Rural-Governance, Service Oriented Architecture},
}

@inproceedings{Gama:2008:UFP:1462802.1462804,
 author = {Gama, Kiev and Rudametkin, Walter and Donsez, Didier},
 title = {Using Fail-stop Proxies for Enhancing Services Isolation in the OSGi Service Platform},
 booktitle = {Proceedings of the 3rd Workshop on Middleware for Service Oriented Computing},
 series = {MW4SOC '08},
 year = {2008},
 isbn = {978-1-60558-368-6},
 location = {Leuven, Belgium},
 pages = {7--12},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1462802.1462804},
 doi = {10.1145/1462802.1462804},
 acmid = {1462804},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {OSGi, fail-stop services, service isolation, stale references},
}

@inproceedings{Jung:2007:MAS:1314483.1314486,
 author = {Jung, Gueyoung and Pu, Calton and Swint, Galen},
 title = {Mulini: An Automated Staging Framework for QoS of Distributed Multi-tier Applications},
 booktitle = {Proceedings of the 2007 Workshop on Automating Service Quality: Held at the International Conference on Automated Software Engineering (ASE)},
 series = {WRASQ '07},
 year = {2007},
 isbn = {978-1-59593-878-7},
 location = {Atlanta, Georgia},
 pages = {10--15},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1314483.1314486},
 doi = {10.1145/1314483.1314486},
 acmid = {1314486},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {QoS, code generation, mulini, multi-tier application, staging},
}

@article{Shindi:2006:EPC:1185875.1185645,
 author = {Shindi, Rajaa S. and Cooper, Shaun},
 title = {Evaluate the Performance Changes of Processor Simulator Benchmarks When Context Switches Are Incorporated},
 journal = {Ada Lett.},
 issue_date = {December 2006},
 volume = {XXVI},
 number = {3},
 month = nov,
 year = {2006},
 issn = {1094-3641},
 pages = {9--14},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1185875.1185645},
 doi = {10.1145/1185875.1185645},
 acmid = {1185645},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, context switches, cpu, processor simulators, sim-alpha},
}

@inproceedings{Shindi:2006:EPC:1185642.1185645,
 author = {Shindi, Rajaa S. and Cooper, Shaun},
 title = {Evaluate the Performance Changes of Processor Simulator Benchmarks When Context Switches Are Incorporated},
 booktitle = {Proceedings of the 2006 Annual ACM SIGAda International Conference on Ada},
 series = {SIGAda '06},
 year = {2006},
 isbn = {1-59593-563-0},
 location = {Albuquerque, New Mexico, USA},
 pages = {9--14},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1185642.1185645},
 doi = {10.1145/1185642.1185645},
 acmid = {1185645},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache, context switches, cpu, processor simulators, sim-alpha},
}

@proceedings{Koziolek:2009:1596473,
 title = {QUASOSS '09: Proceedings of the 1st International Workshop on Quality of Service-oriented Software Systems},
 year = {2009},
 isbn = {978-1-60558-709-7},
 location = {Amsterdam, The Netherlands},
 note = {594094},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@proceedings{Arbab:2007:1294917,
 title = {SYANCO '07: International Workshop on Synthesis and Analysis of Component Connectors: In Conjunction with the 6th ESEC/FSE Joint Meeting},
 year = {2007},
 isbn = {978-1-59593-720-9},
 location = {Dubrovnik, Croatia},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Chatterjee:2016:DSR:2972206.2972215,
 author = {Chatterjee, Arghya and Gvoka, Branko and Xue, Bing and Budimlic, Zoran and Imam, Shams and Sarkar, Vivek},
 title = {A Distributed Selectors Runtime System for Java Applications},
 booktitle = {Proceedings of the 13th International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
 series = {PPPJ '16},
 year = {2016},
 isbn = {978-1-4503-4135-6},
 location = {Lugano, Switzerland},
 pages = {3:1--3:11},
 articleno = {3},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/2972206.2972215},
 doi = {10.1145/2972206.2972215},
 acmid = {2972215},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Actor Model, Distributed Selectors, Remote Messaging, Remote Synchronization, Selector Model},
}

@inproceedings{Gunarathne:2009:EAW:1658260.1658270,
 author = {Gunarathne, Thilina and Herath, Chathura and Chinthaka, Eran and Marru, Suresh},
 title = {Experience with Adapting a WS-BPEL Runtime for eScience Workflows},
 booktitle = {Proceedings of the 5th Grid Computing Environments Workshop},
 series = {GCE '09},
 year = {2009},
 isbn = {978-1-60558-887-2},
 location = {Portland, Oregon},
 pages = {7:1--7:10},
 articleno = {7},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1658260.1658270},
 doi = {10.1145/1658260.1658270},
 acmid = {1658270},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {eScience, scientific application abstractions, workflows},
}

@inproceedings{Bermbach:2011:ECS:2093185.2093186,
 author = {Bermbach, David and Tai, Stefan},
 title = {Eventual Consistency: How Soon is Eventual? An Evaluation of Amazon S3's Consistency Behavior},
 booktitle = {Proceedings of the 6th Workshop on Middleware for Service Oriented Computing},
 series = {MW4SOC '11},
 year = {2011},
 isbn = {978-1-4503-1067-3},
 location = {Lisbon, Portugal},
 pages = {1:1--1:6},
 articleno = {1},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2093185.2093186},
 doi = {10.1145/2093185.2093186},
 acmid = {2093186},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Amazon S3, cloud computing, eventual consistency},
}

@article{Barbaria:2006:SMS:1185875.1185652,
 author = {Barbaria, Khaled and Pautet, Laurent and Perseil, Isabelle},
 title = {Schizophrenic Middleware Support for Fault Tolerance},
 journal = {Ada Lett.},
 issue_date = {December 2006},
 volume = {XXVI},
 number = {3},
 month = nov,
 year = {2006},
 issn = {1094-3641},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1185875.1185652},
 doi = {10.1145/1185875.1185652},
 acmid = {1185652},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Ada, CORBA, fault tolerance, middleware, software architecture},
}

@inproceedings{Barbaria:2006:SMS:1185642.1185652,
 author = {Barbaria, Khaled and Pautet, Laurent and Perseil, Isabelle},
 title = {Schizophrenic Middleware Support for Fault Tolerance},
 booktitle = {Proceedings of the 2006 Annual ACM SIGAda International Conference on Ada},
 series = {SIGAda '06},
 year = {2006},
 isbn = {1-59593-563-0},
 location = {Albuquerque, New Mexico, USA},
 pages = {51--60},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1185642.1185652},
 doi = {10.1145/1185642.1185652},
 acmid = {1185652},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Ada, CORBA, fault tolerance, middleware, software architecture},
}

@inproceedings{Sinkala:2018:MSS:3241403.3241454,
 author = {Sinkala, Zipani Tom and Blom, Martin and Herold, Sebastian},
 title = {A Mapping Study of Software Architecture Recovery for Software Product Lines},
 booktitle = {Proceedings of the 12th European Conference on Software Architecture: Companion Proceedings},
 series = {ECSA '18},
 year = {2018},
 isbn = {978-1-4503-6483-6},
 location = {Madrid, Spain},
 pages = {49:1--49:7},
 articleno = {49},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3241403.3241454},
 doi = {10.1145/3241403.3241454},
 acmid = {3241454},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mapping study, software architecture recovery, software product lines},
}

@inproceedings{Dufour:2007:BAP:1273463.1273480,
 author = {Dufour, Bruno and Ryder, Barbara G. and Sevitsky, Gary},
 title = {Blended Analysis for Performance Understanding of Framework-based Applications},
 booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
 series = {ISSTA '07},
 year = {2007},
 isbn = {978-1-59593-734-6},
 location = {London, United Kingdom},
 pages = {118--128},
 numpages = {11},
 url = {http://doi.acm.org/10.1145/1273463.1273480},
 doi = {10.1145/1273463.1273480},
 acmid = {1273480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow analysis, escape analysis, framework-intensive applications, java, program understandingperfor-mance},
}

@inproceedings{ElMaghraoui:2010:MSF:1712605.1712611,
 author = {El Maghraoui, Kaoutar and Kandiraju, Gokul and Jann, Joefon and Pattnaik, Pratap},
 title = {Modeling and Simulating Flash Based Solid-state Disks for Operating Systems},
 booktitle = {Proceedings of the First Joint WOSP/SIPEW International Conference on Performance Engineering},
 series = {WOSP/SIPEW '10},
 year = {2010},
 isbn = {978-1-60558-563-5},
 location = {San Jose, California, USA},
 pages = {15--26},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1712605.1712611},
 doi = {10.1145/1712605.1712611},
 acmid = {1712611},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {modeling, nand flash memory, simulator, solid state disks (ssd)},
}

@inproceedings{Ha:2012:ODD:2338967.2336808,
 author = {Ha, Ok-Kyoon and Kuh, In-Bon and Tchamgoue, Guy Martin and Jun, Yong-Kee},
 title = {On-the-fly Detection of Data Races in OpenMP Programs},
 booktitle = {Proceedings of the 2012 Workshop on Parallel and Distributed Systems: Testing, Analysis, and Debugging},
 series = {PADTAD 2012},
 year = {2012},
 isbn = {978-1-4503-1456-5},
 location = {Minneapolis, MN, USA},
 pages = {1--10},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2338967.2336808},
 doi = {10.1145/2338967.2336808},
 acmid = {2336808},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Ghosh:2018:DSE:3236466.3140256,
 author = {Ghosh, Rajrup and Simmhan, Yogesh},
 title = {Distributed Scheduling of Event Analytics Across Edge and Cloud},
 journal = {ACM Trans. Cyber-Phys. Syst.},
 issue_date = {September 2018},
 volume = {2},
 number = {4},
 month = jul,
 year = {2018},
 issn = {2378-962X},
 pages = {24:1--24:28},
 articleno = {24},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/3140256},
 doi = {10.1145/3140256},
 acmid = {3140256},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Internet of things (IoT), big data platforms, cloud computing, complex event processing (CEP), distributed scheduling, edge computing, energy-aware scheduling, low power processing, meta-heuristics, query partitioning},
}

@inproceedings{Mueller:2009:PPF:1569901.1570090,
 author = {Mueller, Christian L. and Baumgartner, Benedikt and Ofenbeck, Georg and Schrader, Birte and Sbalzarini, Ivo F.},
 title = {pCMALib: A Parallel Fortran 90 Library for the Evolution Strategy with Covariance Matrix Adaptation},
 booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO '09},
 year = {2009},
 isbn = {978-1-60558-325-9},
 location = {Montreal, Qu\&\#233;bec, Canada},
 pages = {1411--1418},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1569901.1570090},
 doi = {10.1145/1569901.1570090},
 acmid = {1570090},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cma-es, evolution strategies, parallel island model, software library},
}

@inproceedings{Burger:2019:BIP:3297663.3309670,
 author = {Burger, Andreas and Koziolek, Heiko and R\"{u}ckert, Julius and Platenius-Mohr, Marie and Stomberg, G\"{o}sta},
 title = {Bottleneck Identification and Performance Modeling of OPC UA Communication Models},
 booktitle = {Proceedings of the 2019 ACM/SPEC International Conference on Performance Engineering},
 series = {ICPE '19},
 year = {2019},
 isbn = {978-1-4503-6239-9},
 location = {Mumbai, India},
 pages = {231--242},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/3297663.3309670},
 doi = {10.1145/3297663.3309670},
 acmid = {3309670},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {bottleneck identification, client/server, dynamic multicast filtering, m2m communication, opc ua, performance evaluation, performance modeling, pub/sub, resource-constrained devices},
}

@article{Lamps:2018:TIE:3174299.3154386,
 author = {Lamps, Jereme and Babu, Vignesh and Nicol, David M. and Adam, Vladimir and Kumar, Rakesh},
 title = {Temporal Integration of Emulation and Network Simulators on Linux Multiprocessors},
 journal = {ACM Trans. Model. Comput. Simul.},
 issue_date = {January 2018},
 volume = {28},
 number = {1},
 month = jan,
 year = {2018},
 issn = {1049-3301},
 pages = {1:1--1:25},
 articleno = {1},
 numpages = {25},
 url = {http://doi.acm.org/10.1145/3154386},
 doi = {10.1145/3154386},
 acmid = {3154386},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {EMANE, LXCs, Linux kernel, S3F, Simulation, core, emulation, ns-3, time dilation, virtualization},
}

@inproceedings{Soltero:2013:GAE:2491661.2481428,
 author = {Soltero, Philip and Bridges, Patrick and Arnold, Dorian and Lang, Michael},
 title = {A Gossip-based Approach to Exascale System Services},
 booktitle = {Proceedings of the 3rd International Workshop on Runtime and Operating Systems for Supercomputers},
 series = {ROSS '13},
 year = {2013},
 isbn = {978-1-4503-2146-4},
 location = {Eugene, Oregon},
 pages = {3:1--3:7},
 articleno = {3},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2491661.2481428},
 doi = {10.1145/2491661.2481428},
 acmid = {2481428},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Khalid:1997:PKC:271003.271007,
 author = {Khalid, Humayun},
 title = {Performance of the KORA-2 Cache Replacement Scheme},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {Sept. 1997},
 volume = {25},
 number = {4},
 month = sep,
 year = {1997},
 issn = {0163-5964},
 pages = {17--21},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/271003.271007},
 doi = {10.1145/271003.271007},
 acmid = {271007},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache memory, neural networks, performance evaluation},
}

@inproceedings{Arora:2006:SAE:1146909.1147040,
 author = {Arora, Divya and Raghunathan, Anand and Ravi, Srivaths and Sankaradass, Murugan and Jha, Niraj K. and Chakradhar, Srimat T.},
 title = {Software Architecture Exploration for High-performance Security Processing on a Multiprocessor Mobile SoC},
 booktitle = {Proceedings of the 43rd Annual Design Automation Conference},
 series = {DAC '06},
 year = {2006},
 isbn = {1-59593-381-6},
 location = {San Francisco, CA, USA},
 pages = {496--501},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1146909.1147040},
 doi = {10.1145/1146909.1147040},
 acmid = {1147040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {computation offloading, software partitioning},
}

@inproceedings{Schloss:2007:SPE:1272457.1272464,
 author = {Schloss, Hermann and Scholtes, Ingo and Sturm, Peter},
 title = {SISC: Providing Efficient XML-based Service-orientation for Core OS Functionality},
 booktitle = {Proceedings of the 2007 Workshop on Service-oriented Computing Performance: Aspects, Issues, and Approaches},
 series = {SOCP '07},
 year = {2007},
 isbn = {978-1-59593-717-9},
 location = {Monterey, California, USA},
 pages = {45--52},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1272457.1272464},
 doi = {10.1145/1272457.1272464},
 acmid = {1272464},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {XML, language-supported communication, middleware, operating system, service-orientation, shared memory},
}

@inproceedings{Khalloof:2018:GDM:3205651.3208253,
 author = {Khalloof, Hatem and Jakob, Wilfried and Liu, Jianlei and Braun, Eric and Shahoud, Shadi and Duepmeier, Clemens and Hagenmeyer, Veit},
 title = {A Generic Distributed Microservices and Container Based Framework for Metaheuristic Optimization},
 booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
 series = {GECCO '18},
 year = {2018},
 isbn = {978-1-4503-5764-7},
 location = {Kyoto, Japan},
 pages = {1363--1370},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/3205651.3208253},
 doi = {10.1145/3205651.3208253},
 acmid = {3208253},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cluster, container, evolutionary algorithms, metaheuristic optimization, microservices, parallel computing, scalability, virtualization},
}

@article{Rasmussen:2013:TBE:2427631.2427634,
 author = {Rasmussen, Alexander and Porter, George and Conley, Michael and Madhyastha, Harsha V. and Mysore, Radhika Niranjan and Pucher, Alexander and Vahdat, Amin},
 title = {TritonSort: A Balanced and Energy-Efficient Large-Scale Sorting System},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {February 2013},
 volume = {31},
 number = {1},
 month = feb,
 year = {2013},
 issn = {0734-2071},
 pages = {3:1--3:28},
 articleno = {3},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/2427631.2427634},
 doi = {10.1145/2427631.2427634},
 acmid = {2427634},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data-intensive computing, balanced systems, sorting, system optimization},
}

@inproceedings{Aragon:2019:WCS:3308560.3316466,
 author = {Aragon, Harold and Braganza, Samuel and Boza, Edwin and Parrales, Jonathan and Abad, Cristina},
 title = {Workload Characterization of a Software-as-a-Service Web Application Implemented with a Microservices Architecture},
 booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
 series = {WWW '19},
 year = {2019},
 isbn = {978-1-4503-6675-5},
 location = {San Francisco, USA},
 pages = {746--750},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3308560.3316466},
 doi = {10.1145/3308560.3316466},
 acmid = {3316466},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {microservices, software-as-a-service, workload characterization},
}

@inproceedings{Tazaki:2013:DCE:2535372.2535374,
 author = {Tazaki, Hajime and Uarbani, Fr{\'e}d{\'e}ric and Mancini, Emilio and Lacage, Mathieu and Camara, Daniel and Turletti, Thierry and Dabbous, Walid},
 title = {Direct Code Execution: Revisiting Library OS Architecture for Reproducible Network Experiments},
 booktitle = {Proceedings of the Ninth ACM Conference on Emerging Networking Experiments and Technologies},
 series = {CoNEXT '13},
 year = {2013},
 isbn = {978-1-4503-2101-3},
 location = {Santa Barbara, California, USA},
 pages = {217--228},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2535372.2535374},
 doi = {10.1145/2535372.2535374},
 acmid = {2535374},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {direct code execution, emulation, experimentation, linux, network stack, simulation, software development},
}

@article{Strickland:2013:CFC:2542180.2518189,
 author = {Strickland, T. Stephen and Dimoulas, Christos and Takikawa, Asumu and Felleisen, Matthias},
 title = {Contracts for First-Class Classes},
 journal = {ACM Trans. Program. Lang. Syst.},
 issue_date = {November 2013},
 volume = {35},
 number = {3},
 month = nov,
 year = {2013},
 issn = {0164-0925},
 pages = {11:1--11:58},
 articleno = {11},
 numpages = {58},
 url = {http://doi.acm.org/10.1145/2518189},
 doi = {10.1145/2518189},
 acmid = {2518189},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Contracts, first-class class systems},
}

@proceedings{Becker:2010:1858263,
 title = {QUASOSS '10: Proceedings of the 2Nd International Workshop on the Quality of Service-Oriented Software Systems},
 year = {2010},
 isbn = {978-1-4503-0239-5},
 location = {Oslo, Norway},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Chen:2005:EEM:1071690.1064230,
 author = {Chen, Zhifeng and Zhang, Yan and Zhou, Yuanyuan and Scott, Heidi and Schiefer, Berni},
 title = {Empirical Evaluation of Multi-level Buffer Cache Collaboration for Storage Systems},
 journal = {SIGMETRICS Perform. Eval. Rev.},
 issue_date = {June 2005},
 volume = {33},
 number = {1},
 month = jun,
 year = {2005},
 issn = {0163-5999},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1071690.1064230},
 doi = {10.1145/1071690.1064230},
 acmid = {1064230},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative caching, database, file system, storage system},
}

@inproceedings{Chen:2005:EEM:1064212.1064230,
 author = {Chen, Zhifeng and Zhang, Yan and Zhou, Yuanyuan and Scott, Heidi and Schiefer, Berni},
 title = {Empirical Evaluation of Multi-level Buffer Cache Collaboration for Storage Systems},
 booktitle = {Proceedings of the 2005 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems},
 series = {SIGMETRICS '05},
 year = {2005},
 isbn = {1-59593-022-1},
 location = {Banff, Alberta, Canada},
 pages = {145--156},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1064212.1064230},
 doi = {10.1145/1064212.1064230},
 acmid = {1064230},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {collaborative caching, database, file system, storage system},
}

@inproceedings{Dashtbozorgi:2009:SMA:1626195.1626226,
 author = {Dashtbozorgi, Mahdi and Abdollahi Azgomi, Mohammad},
 title = {A Scalable Multi-core Aware Software Architecture for High-performance Network Monitoring},
 booktitle = {Proceedings of the 2Nd International Conference on Security of Information and Networks},
 series = {SIN '09},
 year = {2009},
 isbn = {978-1-60558-412-6},
 location = {Famagusta, North Cyprus},
 pages = {117--122},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1626195.1626226},
 doi = {10.1145/1626195.1626226},
 acmid = {1626226},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {high-performance network monitoring, multi-core aware architecture, scalable architecture},
}

@inproceedings{Plishker:2009:MGM:1629911.1630148,
 author = {Plishker, William and Sane, Nimish and Bhattacharyya, Shuvra S.},
 title = {Mode Grouping for More Effective Generalized Scheduling of Dynamic Dataflow Applications},
 booktitle = {Proceedings of the 46th Annual Design Automation Conference},
 series = {DAC '09},
 year = {2009},
 isbn = {978-1-60558-497-3},
 location = {San Francisco, California},
 pages = {923--926},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/1629911.1630148},
 doi = {10.1145/1629911.1630148},
 acmid = {1630148},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {dataflow, mode grouping, scheduling},
}

@inproceedings{Rajgarhia:2010:PEU:1774088.1774130,
 author = {Rajgarhia, Aditya and Gehani, Ashish},
 title = {Performance and Extension of User Space File Systems},
 booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
 series = {SAC '10},
 year = {2010},
 isbn = {978-1-60558-639-7},
 location = {Sierre, Switzerland},
 pages = {206--213},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1774088.1774130},
 doi = {10.1145/1774088.1774130},
 acmid = {1774130},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FUSE, Java, language binding, performance, user space},
}

@article{Oliveira:2014:HCS:2670967.2670969,
 author = {Oliveira, Daniela and Wetzel, Nicholas and Bucci, Max and Navarro, Jesus and Sullivan, Dean and Jin, Yier},
 title = {Hardware-software Collaboration for Secure Coexistence with Kernel Extensions},
 journal = {SIGAPP Appl. Comput. Rev.},
 issue_date = {September 2014},
 volume = {14},
 number = {3},
 month = sep,
 year = {2014},
 issn = {1559-6915},
 pages = {22--35},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/2670967.2670969},
 doi = {10.1145/2670967.2670969},
 acmid = {2670969},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {HW-SW collaboration, OS defense, immune system, kernel extensions},
}

@inproceedings{You:2019:TAF:3339186.3339192,
 author = {You, Yi-Ping and Lin, Tsung-Chun and Yang, Wuu},
 title = {Translating AArch64 Floating-Point Instruction Set to the x86-64 Platform},
 booktitle = {Proceedings of the 48th International Conference on Parallel Processing: Workshops},
 series = {ICPP 2019},
 year = {2019},
 isbn = {978-1-4503-7196-4},
 location = {Kyoto, Japan},
 pages = {12:1--12:7},
 articleno = {12},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3339186.3339192},
 doi = {10.1145/3339186.3339192},
 acmid = {3339192},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {AArch64, ARM v8, LLVM, binary translation, mc2llvm, x86-64},
}

@article{Shasha:1995:TCA:211414.211427,
 author = {Shasha, Dennis and Llirbat, Francois and Simon, Eric and Valduriez, Patrick},
 title = {Transaction Chopping: Algorithms and Performance Studies},
 journal = {ACM Trans. Database Syst.},
 issue_date = {Sept. 1995},
 volume = {20},
 number = {3},
 month = sep,
 year = {1995},
 issn = {0362-5915},
 pages = {325--363},
 numpages = {39},
 url = {http://doi.acm.org/10.1145/211414.211427},
 doi = {10.1145/211414.211427},
 acmid = {211427},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {locking, multidatabase, serializability, tuning},
}

@inproceedings{Zink:2005:MCC:1072530.1072539,
 author = {Zink, Michael and Westbrook, David and Abdallah, Sherief and Horling, Bryan and Lakamraju, Vijay and Lyons, Eric and Manfredi, Victoria and Kurose, Jim and Hondl, Kurt},
 title = {Meteorological Command and Control: An End-to-end Architecture for a Hazardous Weather Detection Sensor Network},
 booktitle = {Proceedings of the 2005 Workshop on End-to-end, Sense-and-respond Systems, Applications and Services},
 series = {EESR '05},
 year = {2005},
 isbn = {1-931971-32-3},
 location = {Seattle, Washington},
 pages = {37--42},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1072530.1072539},
 acmid = {1072539},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@inproceedings{Mahdavi-Hezavehi:2016:HMQ:2993412.3010822,
 author = {Mahdavi-Hezavehi, Sara},
 title = {Handling Multiple Quality Attributes Trade-off in Architecture-based Self-adaptive Systems},
 booktitle = {Proccedings of the 10th European Conference on Software Architecture Workshops},
 series = {ECSAW '16},
 year = {2016},
 isbn = {978-1-4503-4781-5},
 location = {Copenhagen, Denmark},
 pages = {41:1--41:2},
 articleno = {41},
 numpages = {2},
 url = {http://doi.acm.org/10.1145/2993412.3010822},
 doi = {10.1145/2993412.3010822},
 acmid = {3010822},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {architecture-based, quality attribute, self-adaptive, trade-off},
}

@inproceedings{Christos:2008:ERB:1497308.1497355,
 author = {Christos, Kareliotis and Vassilakis, Costas and Rouvas, Efstathios and Georgiadis, Panayiotis},
 title = {Exception Resolution for BPEL Processes: A Middleware-based Framework and Performance Evaluation},
 booktitle = {Proceedings of the 10th International Conference on Information Integration and Web-based Applications \& Services},
 series = {iiWAS '08},
 year = {2008},
 isbn = {978-1-60558-349-5},
 location = {Linz, Austria},
 pages = {248--256},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/1497308.1497355},
 doi = {10.1145/1497308.1497355},
 acmid = {1497355},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {exception handling, middleware, performance metrics, quality of service (QoS), scalability, web services},
}

@inproceedings{Kordic:2019:PSP:3352700.3352701,
 author = {Kordic, Branislav and Popovic, Marko and Popovic, Miroslav and Goldstein, Moshe and Amitay, Moshe and Dayan, David},
 title = {A Protein Structure Prediction Program Architecture Based on a Software Transactional Memory},
 booktitle = {Proceedings of the 6th Conference on the Engineering of Computer Based Systems},
 series = {ECBS '19},
 year = {2019},
 isbn = {978-1-4503-7636-5},
 location = {Bucharest, Romania},
 pages = {1:1--1:9},
 articleno = {1},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/3352700.3352701},
 doi = {10.1145/3352700.3352701},
 acmid = {3352701},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {DEEPSAM, Evolutionary Programming, PSTM, Python, Software Transactional Memory},
}

@inproceedings{Pediaditakis:2014:FRN:2658260.2658274,
 author = {Pediaditakis, Dimosthenis and Rotsos, Charalampos and Moore, Andrew William},
 title = {Faithful Reproduction of Network Experiments},
 booktitle = {Proceedings of the Tenth ACM/IEEE Symposium on Architectures for Networking and Communications Systems},
 series = {ANCS '14},
 year = {2014},
 isbn = {978-1-4503-2839-5},
 location = {Los Angeles, California, USA},
 pages = {41--52},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2658260.2658274},
 doi = {10.1145/2658260.2658274},
 acmid = {2658274},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {emulation, network experimentation, time-dilation, virtualization},
}

@article{2001:CMM:374308.374365,
 title = {Customizable Middleware for Modular Distributed Software},
 journal = {Commun. ACM},
 issue_date = {May 2001},
 volume = {44},
 number = {5},
 month = may,
 year = {2001},
 issn = {0001-0782},
 pages = {99--107},
 numpages = {9},
 url = {http://doi.acm.org/10.1145/374308.374365},
 doi = {10.1145/374308.374365},
 acmid = {374365},
 publisher = {ACM},
 address = {New York, NY, USA},
key = {{$\!\!$}} ,
}

@inproceedings{Yan:2015:VTS:2769458.2769480,
 author = {Yan, Jiaqi and Jin, Dong},
 title = {A Virtual Time System for Linux-container-based Emulation of Software-defined Networks},
 booktitle = {Proceedings of the 3rd ACM SIGSIM Conference on Principles of Advanced Discrete Simulation},
 series = {SIGSIM PADS '15},
 year = {2015},
 isbn = {978-1-4503-3583-6},
 location = {London, United Kingdom},
 pages = {235--246},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2769458.2769480},
 doi = {10.1145/2769458.2769480},
 acmid = {2769480},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {linux container, time dilation, virtual time},
}

@article{Sebastio:2018:HAC:3190505.3155336,
 author = {Sebastio, Stefano and Amoretti, Michele and Lafuente, Alberto Lluch and Scala, Antonio},
 title = {A Holistic Approach for Collaborative Workload Execution in Volunteer Clouds},
 journal = {ACM Trans. Model. Comput. Simul.},
 issue_date = {April 2018},
 volume = {28},
 number = {2},
 month = mar,
 year = {2018},
 issn = {1049-3301},
 pages = {14:1--14:27},
 articleno = {14},
 numpages = {27},
 url = {http://doi.acm.org/10.1145/3155336},
 doi = {10.1145/3155336},
 acmid = {3155336},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Collective adaptive systems, ant colony optimization (ACO), autonomic computing, cloud computing, collaborative computing, computational fields, multiagent optimization, peer-to-peer (P2P), task scheduling},
}

@inproceedings{Lu:2006:MVM:1134744.1134754,
 author = {Lu, Xiaoqi and Smith, Scott F.},
 title = {A Microkernel Virtual Machine:: Building Security with Clear Interfaces},
 booktitle = {Proceedings of the 2006 Workshop on Programming Languages and Analysis for Security},
 series = {PLAS '06},
 year = {2006},
 isbn = {1-59593-374-3},
 location = {Ottawa, Ontario, Canada},
 pages = {47--56},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1134744.1134754},
 doi = {10.1145/1134744.1134754},
 acmid = {1134754},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Java, access control, frameworks, interface, kernel, language-based security, virtual machine},
}

@inproceedings{Ditter:2017:FBM:3069383.3069386,
 author = {Ditter, Alexander and Graf, Gabriel and Fey, Dietmar},
 title = {Fe2vCl2: From Bare Metal to High Performance Computing on Virtual Clusters and Cloud Infrastructure},
 booktitle = {Proceedings of the 4th Workshop on CrossCloud Infrastructures \& Platforms},
 series = {Crosscloud'17},
 year = {2017},
 isbn = {978-1-4503-4934-5},
 location = {Belgrade, Serbia},
 pages = {3:1--3:7},
 articleno = {3},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3069383.3069386},
 doi = {10.1145/3069383.3069386},
 acmid = {3069386},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Cloud computing, Container, High Performance Computing, Software as a Service},
}

@article{Cai:2005:ATM:1060576.1060577,
 author = {Cai, Wentong and Turner, Stephen J. and Lee, Bu-Sung and Zhou, Junlan},
 title = {An Alternative Time Management Mechanism for Distributed Simulations},
 journal = {ACM Trans. Model. Comput. Simul.},
 issue_date = {April 2005},
 volume = {15},
 number = {2},
 month = apr,
 year = {2005},
 issn = {1049-3301},
 pages = {109--137},
 numpages = {29},
 url = {http://doi.acm.org/10.1145/1060576.1060577},
 doi = {10.1145/1060576.1060577},
 acmid = {1060577},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Distributed simulation, causal order, high level architecture, time management},
}

@article{DeSutter:2007:LCO:1210268.1210273,
 author = {De Sutter, Bjorn and Van Put, Ludo and Chanet, Dominique and De Bus, Bruno and De Bosschere, Koen},
 title = {Link-time Compaction and Optimization of ARM Executables},
 journal = {ACM Trans. Embed. Comput. Syst.},
 issue_date = {February 2007},
 volume = {6},
 number = {1},
 month = feb,
 year = {2007},
 issn = {1539-9087},
 articleno = {5},
 url = {http://doi.acm.org/10.1145/1210268.1210273},
 doi = {10.1145/1210268.1210273},
 acmid = {1210273},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Performance, compaction, linker, optimization},
}

@inproceedings{Lei:2016:HAL:2847263.2847305,
 author = {Lei, Jie and Chen, Yuting and Li, Yunsong and Cong, Jason},
 title = {A High-throughput Architecture for Lossless Decompression on FPGA Designed Using HLS (Abstract Only)},
 booktitle = {Proceedings of the 2016 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
 series = {FPGA '16},
 year = {2016},
 isbn = {978-1-4503-3856-1},
 location = {Monterey, California, USA},
 pages = {277--277},
 numpages = {1},
 url = {http://doi.acm.org/10.1145/2847263.2847305},
 doi = {10.1145/2847263.2847305},
 acmid = {2847305},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {accelerator, fpga, hls, lossless decompression},
}

@inproceedings{Gao:2003:ASD:775152.775217,
 author = {Gao, Lei and Dahlin, Mike and Nayate, Amol and Zheng, Jiandan and Iyengar, Arun},
 title = {Application Specific Data Replication for Edge Services},
 booktitle = {Proceedings of the 12th International Conference on World Wide Web},
 series = {WWW '03},
 year = {2003},
 isbn = {1-58113-680-3},
 location = {Budapest, Hungary},
 pages = {449--460},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/775152.775217},
 doi = {10.1145/775152.775217},
 acmid = {775217},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {availability, data replication, distributed objects, edge services, performance, wide area networks (WAN)},
}

@inproceedings{Jackson:2010:CPP:1879021.1879027,
 author = {Jackson, Ethan K. and Kang, Eunsuk and Dahlweid, Markus and Seifert, Dirk and Santen, Thomas},
 title = {Components, Platforms and Possibilities: Towards Generic Automation for MDA},
 booktitle = {Proceedings of the Tenth ACM International Conference on Embedded Software},
 series = {EMSOFT '10},
 year = {2010},
 isbn = {978-1-60558-904-6},
 location = {Scottsdale, Arizona, USA},
 pages = {39--48},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1879021.1879027},
 doi = {10.1145/1879021.1879027},
 acmid = {1879027},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {formal specification, logic programming, model synthesis, model-driven architecture},
}

@inproceedings{Caheny:2016:RCC:2967938.2967962,
 author = {Caheny, Paul and Casas, Marc and Moret\'{o}, Miquel and Gloaguen, Herv{\'e} and Saintes, Maxime and Ayguad{\'e}, Eduard and Labarta, Jes\'{u}s and Valero, Mateo},
 title = {Reducing Cache Coherence Traffic with Hierarchical Directory Cache and NUMA-Aware Runtime Scheduling},
 booktitle = {Proceedings of the 2016 International Conference on Parallel Architectures and Compilation},
 series = {PACT '16},
 year = {2016},
 isbn = {978-1-4503-4121-9},
 location = {Haifa, Israel},
 pages = {275--286},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2967938.2967962},
 doi = {10.1145/2967938.2967962},
 acmid = {2967962},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cache coherence, numa, task-based programming models},
}

@article{Vandin:2018:RCR:3190505.3182167,
 author = {Vandin, Andrea},
 title = {Replicated Computations Results (RCR) Report for \&\#x0201C;A Holistic Approach for Collaborative Workload Execution in Volunteer Clouds\&\#x0201D;},
 journal = {ACM Trans. Model. Comput. Simul.},
 issue_date = {April 2018},
 volume = {28},
 number = {2},
 month = feb,
 year = {2018},
 issn = {1049-3301},
 pages = {15:1--15:3},
 articleno = {15},
 numpages = {3},
 url = {http://doi.acm.org/10.1145/3182167},
 doi = {10.1145/3182167},
 acmid = {3182167},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RCR report, ant colony optimization (ACO), autonomic computing, cloud computing, collaborative computing, collective adaptive systems, computational fields, multiagent optimization, peer-to-peer (P2P), task scheduling},
}

@inproceedings{Hofstatter:2013:DME:2536853.2536942,
 author = {Hofst\"{a}tter, Harald and Anjomshoaa, Amin and Tjoa, A. Min},
 title = {A Data Mashup Engine for Smartphones Based on XProc},
 booktitle = {Proceedings of International Conference on Advances in Mobile Computing \&\#38; Multimedia},
 series = {MoMM '13},
 year = {2013},
 isbn = {978-1-4503-2106-8},
 location = {Vienna, Austria},
 pages = {440:440--440:444},
 articleno = {440},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/2536853.2536942},
 doi = {10.1145/2536853.2536942},
 acmid = {2536942},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Data Mashup, Smartphone Programming, Workflow},
}

@inproceedings{Mehrotra:2014:SMI:2663165.2663331,
 author = {Mehrotra, Abhinav and Pejovic, Veljko and Musolesi, Mirco},
 title = {SenSocial: A Middleware for Integrating Online Social Networks and Mobile Sensing Data Streams},
 booktitle = {Proceedings of the 15th International Middleware Conference},
 series = {Middleware '14},
 year = {2014},
 isbn = {978-1-4503-2785-5},
 location = {Bordeaux, France},
 pages = {205--216},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2663165.2663331},
 doi = {10.1145/2663165.2663331},
 acmid = {2663331},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mobile middleware, mobile sensing, social sensing, ubiquitous computing},
}

@inproceedings{Fox:2014:TUF:2737909.2737912,
 author = {Fox, Geoffrey C. and Jha, Shantenu and Qiu, Judy and Luckow, Andre},
 title = {Towards an Understanding of Facets and Exemplars of Big Data Applications},
 booktitle = {Proceedings of the 20 Years of Beowulf Workshop on Honor of Thomas Sterling's 65th Birthday},
 series = {Beowulf '14},
 year = {2015},
 isbn = {978-1-4503-3031-2},
 location = {Annapolis, MD, USA},
 pages = {7--16},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2737909.2737912},
 doi = {10.1145/2737909.2737912},
 acmid = {2737912},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Salako:1972:ATD:800194.805881,
 author = {Salako, Abimbola},
 title = {An Approach to the Total Design of Instructional Systems by Simulation},
 booktitle = {Proceedings of the ACM Annual Conference - Volume 2},
 series = {ACM '72},
 year = {1972},
 location = {Boston, Massachusetts, USA},
 pages = {935--949},
 numpages = {15},
 url = {http://doi.acm.org/10.1145/800194.805881},
 doi = {10.1145/800194.805881},
 acmid = {805881},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Applications, Computer assisted instruction, Instructional systems, Integrated hardware/software design, Modeling, Operating systems, Simulation, System performance, Systems design},
}

@inproceedings{Olaya:2014:RPS:2616498.2616547,
 author = {Olaya, Julio C. and Romero, Rodrigo A.},
 title = {Runtime Pipeline Scheduling System for Heterogeneous Architectures},
 booktitle = {Proceedings of the 2014 Annual Conference on Extreme Science and Engineering Discovery Environment},
 series = {XSEDE '14},
 year = {2014},
 isbn = {978-1-4503-2893-7},
 location = {Atlanta, GA, USA},
 pages = {45:1--45:7},
 articleno = {45},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/2616498.2616547},
 doi = {10.1145/2616498.2616547},
 acmid = {2616547},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Concurrency, heterogeneous architecture, over-lapping, pipeline, runtime, scheduling, streams},
}

@inproceedings{Zhao:2007:UMI:1251974.1252550,
 author = {Zhao, Qin and Rabbah, Rodric and Amarasinghe, Saman and Rudolph, Larry and Wong, Weng-Fai},
 title = {Ubiquitous Memory Introspection},
 booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
 series = {CGO '07},
 year = {2007},
 isbn = {0-7695-2764-7},
 pages = {299--311},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=1251974.1252550},
 acmid = {1252550},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@article{Fraser:1989:SDC:66488.66492,
 author = {Fraser, Martin D. and Gagliano, Ross A. and Scheafer, Mark E.},
 title = {The Simulation of a Distributed Control Model for Resource Allocation and the Implied Pricing},
 journal = {SIGSIM Simul. Dig.},
 issue_date = {March 1989},
 volume = {20},
 number = {1},
 month = mar,
 year = {1989},
 issn = {0163-6103},
 pages = {81--92},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/66488.66492},
 doi = {10.1145/66488.66492},
 acmid = {66492},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Nguyen:2001:DPS:366413.364597,
 author = {Nguyen, Dung ``Zung'' and Wong, Stephen B.},
 title = {Design Patterns for Sorting},
 journal = {SIGCSE Bull.},
 issue_date = {March 2001},
 volume = {33},
 number = {1},
 month = feb,
 year = {2001},
 issn = {0097-8418},
 pages = {263--267},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/366413.364597},
 doi = {10.1145/366413.364597},
 acmid = {364597},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Nguyen:2001:DPS:364447.364597,
 author = {Nguyen, Dung ``Zung'' and Wong, Stephen B.},
 title = {Design Patterns for Sorting},
 booktitle = {Proceedings of the Thirty-second SIGCSE Technical Symposium on Computer Science Education},
 series = {SIGCSE '01},
 year = {2001},
 isbn = {1-58113-329-4},
 location = {Charlotte, North Carolina, USA},
 pages = {263--267},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/364447.364597},
 doi = {10.1145/364447.364597},
 acmid = {364597},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Narzt:2015:RII:2750858.2805841,
 author = {Narzt, Wolfgang and Pomberger, Gustav and Weichselbaum, Otto and Draxler, Reinhard and Welser, Markus},
 title = {From Research to Industry: Interactive Mobile Services for Accelerating Logistics Processes},
 booktitle = {Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
 series = {UbiComp '15},
 year = {2015},
 isbn = {978-1-4503-3574-4},
 location = {Osaka, Japan},
 pages = {611--615},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/2750858.2805841},
 doi = {10.1145/2750858.2805841},
 acmid = {2805841},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {location-triggered code execution, logistics processes, mobile computing, mobile interaction services},
}

@inproceedings{Sadilek:2007:ECW:1376860.1376865,
 author = {Sadilek, Daniel A.},
 title = {Energy-aware Compilation for Wireless Sensor Networks},
 booktitle = {Proceedings of the 2Nd International Workshop on Middleware for Sensor Networks},
 series = {MidSens '07},
 year = {2007},
 isbn = {978-1-59593-929-6},
 location = {Newport Beach, California},
 pages = {25--30},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1376860.1376865},
 doi = {10.1145/1376860.1376865},
 acmid = {1376865},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {compilation, energy-awareness, high-level languages, network reprogramming, virtual machines, wireless sensor networks},
}

@inproceedings{Jang:2019:HIE:3297858.3304021,
 author = {Jang, Insu and Tang, Adrian and Kim, Taehoon and Sethumadhavan, Simha and Huh, Jaehyuk},
 title = {Heterogeneous Isolated Execution for Commodity GPUs},
 booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
 series = {ASPLOS '19},
 year = {2019},
 isbn = {978-1-4503-6240-5},
 location = {Providence, RI, USA},
 pages = {455--468},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/3297858.3304021},
 doi = {10.1145/3297858.3304021},
 acmid = {3304021},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gpu security, heterogeneous computing, trusted execution},
}

@article{Johnson:2010:ASA:1920841.1920928,
 author = {Johnson, Ryan and Pandis, Ippokratis and Stoica, Radu and Athanassoulis, Manos and Ailamaki, Anastasia},
 title = {Aether: A Scalable Approach to Logging},
 journal = {Proc. VLDB Endow.},
 issue_date = {September 2010},
 volume = {3},
 number = {1-2},
 month = sep,
 year = {2010},
 issn = {2150-8097},
 pages = {681--692},
 numpages = {12},
 url = {http://dx.doi.org/10.14778/1920841.1920928},
 doi = {10.14778/1920841.1920928},
 acmid = {1920928},
 publisher = {VLDB Endowment},
}

@article{Soltesz:2007:COS:1272998.1273025,
 author = {Soltesz, Stephen and P\"{o}tzl, Herbert and Fiuczynski, Marc E. and Bavier, Andy and Peterson, Larry},
 title = {Container-based Operating System Virtualization: A Scalable, High-performance Alternative to Hypervisors},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {June 2007},
 volume = {41},
 number = {3},
 month = mar,
 year = {2007},
 issn = {0163-5980},
 pages = {275--287},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1272998.1273025},
 doi = {10.1145/1272998.1273025},
 acmid = {1273025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Linux-VServer, Xen, alternative, container, hypervisor, operating, system, virtualization},
}

@inproceedings{Soltesz:2007:COS:1272996.1273025,
 author = {Soltesz, Stephen and P\"{o}tzl, Herbert and Fiuczynski, Marc E. and Bavier, Andy and Peterson, Larry},
 title = {Container-based Operating System Virtualization: A Scalable, High-performance Alternative to Hypervisors},
 booktitle = {Proceedings of the 2Nd ACM SIGOPS/EuroSys European Conference on Computer Systems 2007},
 series = {EuroSys '07},
 year = {2007},
 isbn = {978-1-59593-636-3},
 location = {Lisbon, Portugal},
 pages = {275--287},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1272996.1273025},
 doi = {10.1145/1272996.1273025},
 acmid = {1273025},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Linux-VServer, Xen, alternative, container, hypervisor, operating, system, virtualization},
}

@inproceedings{Pienaar:2011:MPM:1995896.1995933,
 author = {Pienaar, Jacques A. and Raghunathan, Anand and Chakradhar, Srimat},
 title = {MDR: Performance Model Driven Runtime for Heterogeneous Parallel Platforms},
 booktitle = {Proceedings of the International Conference on Supercomputing},
 series = {ICS '11},
 year = {2011},
 isbn = {978-1-4503-0102-2},
 location = {Tucson, Arizona, USA},
 pages = {225--234},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/1995896.1995933},
 doi = {10.1145/1995896.1995933},
 acmid = {1995933},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {gpus, heterogeneous platforms, many-core, multi-core, parallel computing, performance model, runtime system},
}

@article{Atkins:1992:ACC:146937.146939,
 author = {Atkins, M. S. and Coady, M. Y.},
 title = {Adaptable Concurrency Control for Atomic Data Types},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {Aug. 1992},
 volume = {10},
 number = {3},
 month = aug,
 year = {1992},
 issn = {0734-2071},
 pages = {190--225},
 numpages = {36},
 url = {http://doi.acm.org/10.1145/146937.146939},
 doi = {10.1145/146937.146939},
 acmid = {146939},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {concurrent access to shared data, hybrid locking, optimistic locking, pessimistic locking, transactions serializability},
}

@inproceedings{Bondarev:2007:CTD:1266366.1266588,
 author = {Bondarev, Egor and Chaudron, Michel and de With, Peter H. N.},
 title = {CARAT: A Toolkit for Design and Performance Analysis of Component-based Embedded Systems},
 booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
 series = {DATE '07},
 year = {2007},
 isbn = {978-3-9810801-2-4},
 location = {Nice, France},
 pages = {1024--1029},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1266366.1266588},
 acmid = {1266588},
 publisher = {EDA Consortium},
 address = {San Jose, CA, USA},
}

@inproceedings{Voigt:2010:MCM:1774088.1774563,
 author = {Voigt, Konrad and Ivanov, Petko and Rummler, Andreas},
 title = {MatchBox: Combined Meta-model Matching for Semi-automatic Mapping Generation},
 booktitle = {Proceedings of the 2010 ACM Symposium on Applied Computing},
 series = {SAC '10},
 year = {2010},
 isbn = {978-1-60558-639-7},
 location = {Sierre, Switzerland},
 pages = {2281--2288},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1774088.1774563},
 doi = {10.1145/1774088.1774563},
 acmid = {1774563},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {mapping generation, meta-model matching, model engineering},
}

@inproceedings{Yamashita:2010:PES:1899721.1899906,
 author = {Yamashita, Koichiro},
 title = {-Possibility of ESL-: A Software Centric System Design for Multicore SoC in the Upstream Phase},
 booktitle = {Proceedings of the 2010 Asia and South Pacific Design Automation Conference},
 series = {ASPDAC '10},
 year = {2010},
 isbn = {978-1-60558-837-7},
 location = {Taipei, Taiwan},
 pages = {805--808},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=1899721.1899906},
 acmid = {1899906},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@article{Cattaneo:2015:AIS:2836331.2842615,
 author = {Cattaneo, Riccardo and Natale, Giuseppe and Sicignano, Carlo and Sciuto, Donatella and Santambrogio, Marco Domenico},
 title = {On How to Accelerate Iterative Stencil Loops: A Scalable Streaming-Based Approach},
 journal = {ACM Trans. Archit. Code Optim.},
 issue_date = {January 2016},
 volume = {12},
 number = {4},
 month = dec,
 year = {2015},
 issn = {1544-3566},
 pages = {53:1--53:26},
 articleno = {53},
 numpages = {26},
 url = {http://doi.acm.org/10.1145/2842615},
 doi = {10.1145/2842615},
 acmid = {2842615},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {FPGAs, power efficiency, streaming architectures},
}

@inproceedings{Zhou:2014:TMH:2656045.2656074,
 author = {Zhou, Husheng and Liu, Cong},
 title = {Task Mapping in Heterogeneous Embedded Systems for Fast Completion Time},
 booktitle = {Proceedings of the 14th International Conference on Embedded Software},
 series = {EMSOFT '14},
 year = {2014},
 isbn = {978-1-4503-3052-7},
 location = {New Delhi, India},
 pages = {22:1--22:10},
 articleno = {22},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2656045.2656074},
 doi = {10.1145/2656045.2656074},
 acmid = {2656074},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {GPU, heterogeneous scheduling},
}

@inproceedings{Snavely:2002:FPM:762761.762785,
 author = {Snavely, Allan and Carrington, Laura and Wolter, Nicole and Labarta, Jesus and Badia, Rosa and Purkayastha, Avi},
 title = {A Framework for Performance Modeling and Prediction},
 booktitle = {Proceedings of the 2002 ACM/IEEE Conference on Supercomputing},
 series = {SC '02},
 year = {2002},
 isbn = {0-7695-1524-X},
 location = {Baltimore, Maryland},
 pages = {1--17},
 numpages = {17},
 url = {http://dl.acm.org/citation.cfm?id=762761.762785},
 acmid = {762785},
 publisher = {IEEE Computer Society Press},
 address = {Los Alamitos, CA, USA},
}

@inproceedings{Morganstein:1971:SSM:800024.808357,
 author = {Morganstein, S. J. and Winograd, J. and Herman, R.},
 title = {Sim/61: A Simulation Measurement Tool for a Time-shared, Demand Paging Operating System},
 booktitle = {Proceedings of the SIGOPS Workshop on System Performance Evaluation},
 year = {1971},
 pages = {142--172},
 numpages = {31},
 url = {http://doi.acm.org/10.1145/800024.808357},
 doi = {10.1145/800024.808357},
 acmid = {808357},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Shliferstein:1982:EUI:800263.809243,
 author = {Shliferstein, Abe},
 title = {Experiments Using Interactive Color Raster Graphics for CAD},
 booktitle = {Proceedings of the 19th Design Automation Conference},
 series = {DAC '82},
 year = {1982},
 isbn = {0-89791-020-6},
 pages = {445--452},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=800263.809243},
 acmid = {809243},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@article{Nicoara:2008:CSE:1357010.1352617,
 author = {Nicoara, Angela and Alonso, Gustavo and Roscoe, Timothy},
 title = {Controlled, Systematic, and Efficient Code Replacement for Running Java Programs},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {May 2008},
 volume = {42},
 number = {4},
 month = apr,
 year = {2008},
 issn = {0163-5980},
 pages = {233--246},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1357010.1352617},
 doi = {10.1145/1357010.1352617},
 acmid = {1352617},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {PROSE, dynamic bytecode instrumentation, inlining, run-time method code replacement, run-time modification},
}

@inproceedings{Nicoara:2008:CSE:1352592.1352617,
 author = {Nicoara, Angela and Alonso, Gustavo and Roscoe, Timothy},
 title = {Controlled, Systematic, and Efficient Code Replacement for Running Java Programs},
 booktitle = {Proceedings of the 3rd ACM SIGOPS/EuroSys European Conference on Computer Systems 2008},
 series = {Eurosys '08},
 year = {2008},
 isbn = {978-1-60558-013-5},
 location = {Glasgow, Scotland UK},
 pages = {233--246},
 numpages = {14},
 url = {http://doi.acm.org/10.1145/1352592.1352617},
 doi = {10.1145/1352592.1352617},
 acmid = {1352617},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {PROSE, dynamic bytecode instrumentation, inlining, run-time method code replacement, run-time modification},
}

@article{Anderson:1996:SNF:225535.225537,
 author = {Anderson, Thomas E. and Dahlin, Michael D. and Neefe, Jeanna M. and Patterson, David A. and Roselli, Drew S. and Wang, Randolph Y.},
 title = {Serverless Network File Systems},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {Feb. 1996},
 volume = {14},
 number = {1},
 month = feb,
 year = {1996},
 issn = {0734-2071},
 pages = {41--79},
 numpages = {39},
 url = {http://doi.acm.org/10.1145/225535.225537},
 doi = {10.1145/225535.225537},
 acmid = {225537},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {RAID, log cleaning, log structured, log-based striping, logging, redundant data storage, scalable performance},
}

@inproceedings{Zhao:2012:AFA:2361999.2362004,
 author = {Zhao, Liang and Sakr, Sherif and Zhu, Liming and Xu, Xiwei and Liu, Anna},
 title = {An Architecture Framework for Application-managed Scaling of Cloud-hosted Relational Databases},
 booktitle = {Proceedings of the WICSA/ECSA 2012 Companion Volume},
 series = {WICSA/ECSA '12},
 year = {2012},
 isbn = {978-1-4503-1568-5},
 location = {Helsinki, Finland},
 pages = {21--28},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/2361999.2362004},
 doi = {10.1145/2361999.2362004},
 acmid = {2362004},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {actuator, amazon EC2, application-managed, cloud, framework, monitor, sensor},
}

@inproceedings{Rinard:2010:PSA:1869459.1869525,
 author = {Rinard, Martin and Hoffmann, Henry and Misailovic, Sasa and Sidiroglou, Stelios},
 title = {Patterns and Statistical Analysis for Understanding Reduced Resource Computing},
 booktitle = {Proceedings of the ACM International Conference on Object Oriented Programming Systems Languages and Applications},
 series = {OOPSLA '10},
 year = {2010},
 isbn = {978-1-4503-0203-6},
 location = {Reno/Tahoe, Nevada, USA},
 pages = {806--821},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/1869459.1869525},
 doi = {10.1145/1869459.1869525},
 acmid = {1869525},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cyclic memory allocation, discarding tasks, loop perforation, reduced resource computing, statistical analysis},
}

@article{Rinard:2010:PSA:1932682.1869525,
 author = {Rinard, Martin and Hoffmann, Henry and Misailovic, Sasa and Sidiroglou, Stelios},
 title = {Patterns and Statistical Analysis for Understanding Reduced Resource Computing},
 journal = {SIGPLAN Not.},
 issue_date = {October 2010},
 volume = {45},
 number = {10},
 month = oct,
 year = {2010},
 issn = {0362-1340},
 pages = {806--821},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/1932682.1869525},
 doi = {10.1145/1932682.1869525},
 acmid = {1869525},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {cyclic memory allocation, discarding tasks, loop perforation, reduced resource computing, statistical analysis},
}

@inproceedings{Blom:2009:PPE:1570256.1570265,
 author = {Blom, Hendrik and K\"{u}ch, Christiane and Losemann, Katja and Schwiegelshohn, Chris},
 title = {PEPPA: A Project for Evolutionary Predator Prey Algorithms},
 booktitle = {Proceedings of the 11th Annual Conference Companion on Genetic and Evolutionary Computation Conference: Late Breaking Papers},
 series = {GECCO '09},
 year = {2009},
 isbn = {978-1-60558-505-5},
 location = {Montreal, Qu\&\#233;bec, Canada},
 pages = {1993--1998},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/1570256.1570265},
 doi = {10.1145/1570256.1570265},
 acmid = {1570265},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {evolutionary algorithms, framework, multi-objective optimization,, objectoriented programming,},
}

@inproceedings{Sohda:2001:IPS:376656.376845,
 author = {Sohda, Yukihiko and Nakada, Hidemoto and Matsuoka, Satoshi},
 title = {Implementation of a Portable Software DSM in Java},
 booktitle = {Proceedings of the 2001 Joint ACM-ISCOPE Conference on Java Grande},
 series = {JGI '01},
 year = {2001},
 isbn = {1-58113-359-6},
 location = {Palo Alto, California, USA},
 pages = {163--172},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/376656.376845},
 doi = {10.1145/376656.376845},
 acmid = {376845},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Wingert:2017:PIU:3132272.3135078,
 author = {Wingert, Benjamin and Sch\"{o}llhorn, Isabel and Bues, Matthias},
 title = {ProDesk: An Interactive Ubiquitous Desktop Surface},
 booktitle = {Proceedings of the 2017 ACM International Conference on Interactive Surfaces and Spaces},
 series = {ISS '17},
 year = {2017},
 isbn = {978-1-4503-4691-7},
 location = {Brighton, United Kingdom},
 pages = {366--371},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/3132272.3135078},
 doi = {10.1145/3132272.3135078},
 acmid = {3135078},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Enhancing Desktop Systems, Horizontal Touch Interaction, Interactive Tabletop, Interactive Touch Projection, Post-WIMP, Ubiquitokw Desktop, VD1, Virtual Desktop One},
}

@inproceedings{Kristien:2019:MJC:3313808.3313818,
 author = {Kristien, Martin and Spink, Tom and Wagstaff, Harry and Franke, Bj\"{o}rn and B\"{o}hm, Igor and Topham, Nigel},
 title = {Mitigating JIT Compilation Latency in Virtual Execution Environments},
 booktitle = {Proceedings of the 15th ACM SIGPLAN/SIGOPS International Conference on Virtual Execution Environments},
 series = {VEE 2019},
 year = {2019},
 isbn = {978-1-4503-6020-3},
 location = {Providence, RI, USA},
 pages = {101--107},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/3313808.3313818},
 doi = {10.1145/3313808.3313818},
 acmid = {3313818},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {JIT compilation, compilation latency, performance, scheduling},
}

@article{Wu:2015:SDE:2821757.2785969,
 author = {Wu, Ming-Ju and Tsai, Chun-Jen},
 title = {A Storage Device Emulator for System Performance Evaluation},
 journal = {ACM Trans. Embed. Comput. Syst.},
 issue_date = {December 2015},
 volume = {14},
 number = {4},
 month = oct,
 year = {2015},
 issn = {1539-9087},
 pages = {82:1--82:27},
 articleno = {82},
 numpages = {27},
 url = {http://doi.acm.org/10.1145/2785969},
 doi = {10.1145/2785969},
 acmid = {2785969},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Storage emulation, storage evaluation},
}

@inproceedings{Hussein:2006:FVL:1188966.1189015,
 author = {Hussein, Ashraf S. and El-Shishiny, Hisham},
 title = {A Framework for Visualization of Large Time-varying Volume Data for High Performance Computing},
 booktitle = {Proceedings of the 2006 Conference of the Center for Advanced Studies on Collaborative Research},
 series = {CASCON '06},
 year = {2006},
 location = {Toronto, Ontario, Canada},
 articleno = {36},
 url = {http://dx.doi.org/10.1145/1188966.1189015},
 doi = {10.1145/1188966.1189015},
 acmid = {1189015},
 publisher = {IBM Corp.},
 address = {Riverton, NJ, USA},
}

@inproceedings{Perelman:2003:PSV:942806.943854,
 author = {Perelman, Erez and Hamerly, Greg and Calder, Brad},
 title = {Picking Statistically Valid and Early Simulation Points},
 booktitle = {Proceedings of the 12th International Conference on Parallel Architectures and Compilation Techniques},
 series = {PACT '03},
 year = {2003},
 isbn = {0-7695-2021-9},
 pages = {244--},
 url = {http://dl.acm.org/citation.cfm?id=942806.943854},
 acmid = {943854},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@inproceedings{MedeirosAraujo:2009:DSB:1529282.1529322,
 author = {Medeiros Ara\'{u}jo, Gustavo and Siqueira, Frank},
 title = {The Device Service Bus: A Solution for Embedded Device Integration Through Web Services},
 booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
 series = {SAC '09},
 year = {2009},
 isbn = {978-1-60558-166-8},
 location = {Honolulu, Hawaii},
 pages = {185--189},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/1529282.1529322},
 doi = {10.1145/1529282.1529322},
 acmid = {1529322},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded systems, interoperability, ubiquitous computing, web services},
}

@inproceedings{Park:2004:EAE:1017753.1017762,
 author = {Park, Sangsoo and Lee, Yonghee and Shin, Heonshik},
 title = {An Experimental Analysis of the Effect of the Operating System on Memory Performance in Embedded Multimedia Computing},
 booktitle = {Proceedings of the 4th ACM International Conference on Embedded Software},
 series = {EMSOFT '04},
 year = {2004},
 isbn = {1-58113-860-1},
 location = {Pisa, Italy},
 pages = {26--33},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1017753.1017762},
 doi = {10.1145/1017753.1017762},
 acmid = {1017762},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {embedded system, memory performance, multimedia, operating system},
}

@inproceedings{Hankins:2003:SCR:956417.956541,
 author = {Hankins, Richard A. and Diep, Trung and Annavaram, Murali and Hirano, Brian and Eri, Harald and Nueckel, Hubert and Shen, John P.},
 title = {Scaling and Charact Rizing Database Workloads: Bridging the Gap Between Research and Practice},
 booktitle = {Proceedings of the 36th Annual IEEE/ACM International Symposium on Microarchitecture},
 series = {MICRO 36},
 year = {2003},
 isbn = {0-7695-2043-X},
 pages = {151--},
 url = {http://dl.acm.org/citation.cfm?id=956417.956541},
 acmid = {956541},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@article{Atkins:1988:ESD:48012.48041,
 author = {Atkins, M. Stella},
 title = {Experiments in SR with Different Upcall Program Structures},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {Nov. 1988},
 volume = {6},
 number = {4},
 month = nov,
 year = {1988},
 issn = {0734-2071},
 pages = {365--392},
 numpages = {28},
 url = {http://doi.acm.org/10.1145/48012.48041},
 doi = {10.1145/48012.48041},
 acmid = {48041},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Taylor:2017:AOO:3140582.3081040,
 author = {Taylor, Ben and Marco, Vicent Sanz and Wang, Zheng},
 title = {Adaptive Optimization for OpenCL Programs on Embedded Heterogeneous Systems},
 journal = {SIGPLAN Not.},
 issue_date = {May 2017},
 volume = {52},
 number = {5},
 month = jun,
 year = {2017},
 issn = {0362-1340},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3140582.3081040},
 doi = {10.1145/3140582.3081040},
 acmid = {3081040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Energy Efficiency, Heterogeneous Multi-cores, OpenCL, Predictive Modeling},
}

@inproceedings{Taylor:2017:AOO:3078633.3081040,
 author = {Taylor, Ben and Marco, Vicent Sanz and Wang, Zheng},
 title = {Adaptive Optimization for OpenCL Programs on Embedded Heterogeneous Systems},
 booktitle = {Proceedings of the 18th ACM SIGPLAN/SIGBED Conference on Languages, Compilers, and Tools for Embedded Systems},
 series = {LCTES 2017},
 year = {2017},
 isbn = {978-1-4503-5030-3},
 location = {Barcelona, Spain},
 pages = {11--20},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/3078633.3081040},
 doi = {10.1145/3078633.3081040},
 acmid = {3081040},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Energy Efficiency, Heterogeneous Multi-cores, OpenCL, Predictive Modeling},
}

@inproceedings{Perumalla:2010:MST:1808143.1808221,
 author = {Perumalla, Kalyan S.},
 title = {\&Mu;{\$\Pi\$}: A Scalable and Transparent System for Simulating MPI Programs},
 booktitle = {Proceedings of the 3rd International ICST Conference on Simulation Tools and Techniques},
 series = {SIMUTools '10},
 year = {2010},
 isbn = {978-963-9799-87-5},
 location = {Torremolinos, Malaga, Spain},
 pages = {62:1--62:6},
 articleno = {62},
 numpages = {6},
 url = {https://doi.org/10.4108/ICST.SIMUTOOLS2010.8692},
 doi = {10.4108/ICST.SIMUTOOLS2010.8692},
 acmid = {1808221},
 publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
 address = {ICST, Brussels, Belgium, Belgium},
 keywords = {MPI, parallel algorithms, supercomputing, synchronization, virtual execution},
}

@article{Minnich:2006:RKO:1131322.1131331,
 author = {Minnich, Ronald G. and Sottile, Matthew J. and Choi, Sung-Eun and Hendriks, Erik and McKie, Jim},
 title = {Right-weight Kernels: An Off-the-shelf Alternative to Custom Light-weight Kernels},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {April 2006},
 volume = {40},
 number = {2},
 month = apr,
 year = {2006},
 issn = {0163-5980},
 pages = {22--28},
 numpages = {7},
 url = {http://doi.acm.org/10.1145/1131322.1131331},
 doi = {10.1145/1131322.1131331},
 acmid = {1131331},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Ben-Othman:2010:PEH:1868521.1868574,
 author = {Ben-Othman, Jalel and Diagne, Serigne and Mokdad, Lynda and Yahya, Bashir},
 title = {Performance Evaluation of a Hybrid MAC Protocol for Wireless Sensor Networks},
 booktitle = {Proceedings of the 13th ACM International Conference on Modeling, Analysis, and Simulation of Wireless and Mobile Systems},
 series = {MSWIM '10},
 year = {2010},
 isbn = {978-1-4503-0274-6},
 location = {Bodrum, Turkey},
 pages = {327--334},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/1868521.1868574},
 doi = {10.1145/1868521.1868574},
 acmid = {1868574},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {colored petri-nets, energy efficiency, mac protocol, qos},
}

@inproceedings{Qu:1999:PMU:339492.340037,
 author = {Qu, Gang and Qu, Gang and Potkonjak, Miodrag and Potkonjak, Miodrag},
 title = {Power Minimization Using System-level Partitioning of Applications with Quality of Service Requirements},
 booktitle = {Proceedings of the 1999 IEEE/ACM International Conference on Computer-aided Design},
 series = {ICCAD '99},
 year = {1999},
 isbn = {0-7803-5832-5},
 location = {San Jose, California, USA},
 pages = {343--346},
 numpages = {4},
 url = {http://dl.acm.org/citation.cfm?id=339492.340037},
 acmid = {340037},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@article{Cox:1989:ICM:74851.74855,
 author = {Cox, A. and Fowler, R.},
 title = {The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with Platinum},
 journal = {SIGOPS Oper. Syst. Rev.},
 issue_date = {Dec. 3\&\#8211;6, 1989},
 volume = {23},
 number = {5},
 month = nov,
 year = {1989},
 issn = {0163-5980},
 pages = {32--44},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/74851.74855},
 doi = {10.1145/74851.74855},
 acmid = {74855},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Cox:1989:ICM:74850.74855,
 author = {Cox, A. and Fowler, R.},
 title = {The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with Platinum},
 booktitle = {Proceedings of the Twelfth ACM Symposium on Operating Systems Principles},
 series = {SOSP '89},
 year = {1989},
 isbn = {0-89791-338-8},
 pages = {32--44},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/74850.74855},
 doi = {10.1145/74850.74855},
 acmid = {74855},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Mery:2014:FEL:2676585.2676599,
 author = {M{\'e}ry, Dominique and Singh, Neeraj Kumar},
 title = {Formal Evaluation of Landing Gear System},
 booktitle = {Proceedings of the Fifth Symposium on Information and Communication Technology},
 series = {SoICT '14},
 year = {2014},
 isbn = {978-1-4503-2930-9},
 location = {Hanoi, Viet Nam},
 pages = {75--84},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2676585.2676599},
 doi = {10.1145/2676585.2676599},
 acmid = {2676599},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {abstract model, event-b, event-driven approach, landing gear system, proof-based development, refinement},
}

@inproceedings{Aaby:2010:ESA:1808143.1808181,
 author = {Aaby, Brandon G. and Perumalla, Kalyan S. and Seal, Sudip K.},
 title = {Efficient Simulation of Agent-based Models on multi-GPU and Multi-core Clusters},
 booktitle = {Proceedings of the 3rd International ICST Conference on Simulation Tools and Techniques},
 series = {SIMUTools '10},
 year = {2010},
 isbn = {978-963-9799-87-5},
 location = {Torremolinos, Malaga, Spain},
 pages = {29:1--29:10},
 articleno = {29},
 numpages = {10},
 url = {https://doi.org/10.4108/ICST.SIMUTOOLS2010.8822},
 doi = {10.4108/ICST.SIMUTOOLS2010.8822},
 acmid = {1808181},
 publisher = {ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering)},
 address = {ICST, Brussels, Belgium, Belgium},
 keywords = {CUDA, GPU, MPI, agent-based simulation, cluster, computational hierarchy, latency hiding, multi-core, threads},
}

@article{Tan:2010:CFF:1816038.1815999,
 author = {Tan, Zhangxi and Waterman, Andrew and Cook, Henry and Bird, Sarah and Asanovi\'{c}, Krste and Patterson, David},
 title = {A Case for FAME: FPGA Architecture Model Execution},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {June 2010},
 volume = {38},
 number = {3},
 month = jun,
 year = {2010},
 issn = {0163-5964},
 pages = {290--301},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1816038.1815999},
 doi = {10.1145/1816038.1815999},
 acmid = {1815999},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fpga, microprocessors, simulation},
}

@inproceedings{Tan:2010:CFF:1815961.1815999,
 author = {Tan, Zhangxi and Waterman, Andrew and Cook, Henry and Bird, Sarah and Asanovi\'{c}, Krste and Patterson, David},
 title = {A Case for FAME: FPGA Architecture Model Execution},
 booktitle = {Proceedings of the 37th Annual International Symposium on Computer Architecture},
 series = {ISCA '10},
 year = {2010},
 isbn = {978-1-4503-0053-7},
 location = {Saint-Malo, France},
 pages = {290--301},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1815961.1815999},
 doi = {10.1145/1815961.1815999},
 acmid = {1815999},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {fpga, microprocessors, simulation},
}

@article{Stamos:1984:SGS:190.194,
 author = {Stamos, James W.},
 title = {Static Grouping of Small Objects to Enhance Performance of a Paged Virtual Memory},
 journal = {ACM Trans. Comput. Syst.},
 issue_date = {May 1984},
 volume = {2},
 number = {2},
 month = may,
 year = {1984},
 issn = {0734-2071},
 pages = {155--180},
 numpages = {26},
 url = {http://doi.acm.org/10.1145/190.194},
 doi = {10.1145/190.194},
 acmid = {194},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Smalltalk, initial placement, object-oriented, paging, programing restructuring, reference trace compression, static grouping, virtual memory},
}

@article{Waluyo:2009:DEL:1613011.1613032,
 author = {Waluyo, Agustinus Borgy and Pek, Isaac and Chen, Xiang and Yeoh, Wee-Soon},
 title = {Design and Evaluation of Lightweight Middleware for Personal Wireless Body Area Network},
 journal = {Personal Ubiquitous Comput.},
 issue_date = {October   2009},
 volume = {13},
 number = {7},
 month = oct,
 year = {2009},
 issn = {1617-4909},
 pages = {509--525},
 numpages = {17},
 url = {http://dx.doi.org/10.1007/s00779-009-0222-y},
 doi = {10.1007/s00779-009-0222-y},
 acmid = {1613032},
 publisher = {Springer-Verlag},
 address = {London, UK, UK},
 keywords = {Lightweight middleware, Mobile middleware, Wireless body area network middleware},
}

@inproceedings{Gharsellaoui:2014:GBS:2598394.2605440,
 author = {Gharsellaoui, Hamza and Hasni, Hamadi and Ben Ahmed, Samir},
 title = {A Genetic Based Scheduling Approach of Real-time Reconfigurable Embedded Systems},
 booktitle = {Proceedings of the Companion Publication of the 2014 Annual Conference on Genetic and Evolutionary Computation},
 series = {GECCO Comp '14},
 year = {2014},
 isbn = {978-1-4503-2881-4},
 location = {Vancouver, BC, Canada},
 pages = {993--998},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/2598394.2605440},
 doi = {10.1145/2598394.2605440},
 acmid = {2605440},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Sourek:2016:DSM:2893706.2893708,
 author = {\v{S}ourek, Gustav and Po\v{s}\'{\i}k, Petr},
 title = {Dynamic System Modeling of Evolutionary Algorithms},
 journal = {SIGAPP Appl. Comput. Rev.},
 issue_date = {December 2015},
 volume = {15},
 number = {4},
 month = feb,
 year = {2016},
 issn = {1559-6915},
 pages = {19--30},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/2893706.2893708},
 doi = {10.1145/2893706.2893708},
 acmid = {2893708},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data flow, dynamic systems, evolutionary algorithms, matlab simulink, optimization, visual programming},
}

@inproceedings{Tembey:2013:ICM:2524211.2524213,
 author = {Tembey, Priyanka and Gavrilovska, Ada and Schwan, Karsten},
 title = {inTune: Coordinating Multicore Islands to Achieve Global Policy Objectives},
 booktitle = {Proceedings of the First ACM SIGOPS Conference on Timely Results in Operating Systems},
 series = {TRIOS '13},
 year = {2013},
 isbn = {978-1-4503-2463-2},
 location = {Farmington, Pennsylvania},
 pages = {4:1--4:16},
 articleno = {4},
 numpages = {16},
 url = {http://doi.acm.org/10.1145/2524211.2524213},
 doi = {10.1145/2524211.2524213},
 acmid = {2524213},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Rasmussen:2011:TBL:1972457.1972461,
 author = {Rasmussen, Alexander and Porter, George and Conley, Michael and Madhyastha, Harsha V. and Mysore, Radhika Niranjan and Pucher, Alexander and Vahdat, Amin},
 title = {TritonSort: A Balanced Large-scale Sorting System},
 booktitle = {Proceedings of the 8th USENIX Conference on Networked Systems Design and Implementation},
 series = {NSDI'11},
 year = {2011},
 location = {Boston, MA},
 pages = {29--42},
 numpages = {14},
 url = {http://dl.acm.org/citation.cfm?id=1972457.1972461},
 acmid = {1972461},
 publisher = {USENIX Association},
 address = {Berkeley, CA, USA},
}

@proceedings{Ailamaki:2005:1114252,
 title = {DaMoN '05: Proceedings of the 1st International Workshop on Data Management on New Hardware},
 year = {2005},
 location = {Baltimore, Maryland},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Johnson:2012:SWL:2205457.2205463,
 author = {Johnson, Ryan and Pandis, Ippokratis and Stoica, Radu and Athanassoulis, Manos and Ailamaki, Anastasia},
 title = {Scalability of Write-ahead Logging on Multicore and Multisocket Hardware},
 journal = {The VLDB Journal},
 issue_date = {April     2012},
 volume = {21},
 number = {2},
 month = apr,
 year = {2012},
 issn = {1066-8888},
 pages = {239--263},
 numpages = {25},
 url = {http://dx.doi.org/10.1007/s00778-011-0260-8},
 doi = {10.1007/s00778-011-0260-8},
 acmid = {2205463},
 publisher = {Springer-Verlag New York, Inc.},
 address = {Secaucus, NJ, USA},
 keywords = {Consolidation array, Early lock release, Flush pipelining, Log buffer contention, Log manager, Scaling to multisockets},
}

@inproceedings{Kurian:1992:MLE:139669.140380,
 author = {Kurian, Lizyamma and Hulina, Paul T. and Coraor, Lee D.},
 title = {Memory Latency Effects in Decoupled Architectures with a Single Data Memory Module},
 booktitle = {Proceedings of the 19th Annual International Symposium on Computer Architecture},
 series = {ISCA '92},
 year = {1992},
 isbn = {0-89791-509-7},
 location = {Queensland, Australia},
 pages = {236--245},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/139669.140380},
 doi = {10.1145/139669.140380},
 acmid = {140380},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@article{Kurian:1992:MLE:146628.140380,
 author = {Kurian, Lizyamma and Hulina, Paul T. and Coraor, Lee D.},
 title = {Memory Latency Effects in Decoupled Architectures with a Single Data Memory Module},
 journal = {SIGARCH Comput. Archit. News},
 issue_date = {May 1992},
 volume = {20},
 number = {2},
 month = apr,
 year = {1992},
 issn = {0163-5964},
 pages = {236--245},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/146628.140380},
 doi = {10.1145/146628.140380},
 acmid = {140380},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Price:1973:SPN:800280.811029,
 author = {Price, W. L.},
 title = {Simulation of Packet-switching Networks Controlled on Isarithmic Principles},
 booktitle = {Proceedings of the Third ACM Symposium on Data Communications and Data Networks: Analysis and Design},
 series = {DATACOMM '73},
 year = {1973},
 pages = {44--49},
 numpages = {6},
 url = {http://doi.acm.org/10.1145/800280.811029},
 doi = {10.1145/800280.811029},
 acmid = {811029},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@inproceedings{Gokhale:2006:PAR:1898699.1898867,
 author = {Gokhale, Swapna and Gokhale, Aniruddha and Gray, Jeff and Vandal, Paul and Praphamontripong, Upsorn},
 title = {Performance Analysis of the Reactor Pattern in Network Services},
 booktitle = {Proceedings of the 20th International Conference on Parallel and Distributed Processing},
 series = {IPDPS'06},
 year = {2006},
 isbn = {1-4244-0054-6},
 location = {Rhodes Island, Greece},
 pages = {327--327},
 numpages = {1},
 url = {http://dl.acm.org/citation.cfm?id=1898699.1898867},
 acmid = {1898867},
 publisher = {IEEE Computer Society},
 address = {Washington, DC, USA},
}

@InProceedings{8449560,
  author    = {X. {Zhou} and X. {Peng} and T. {Xie} and J. {Sun} and C. {Xu} and C. {Ji} and W. {Zhao}},
  title     = {Poster: Benchmarking Microservice Systems for Software Engineering Research},
  booktitle = {2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)},
  year      = {2018},
  pages     = {323-324},
  month     = {May},
  abstract  = {Despite the prevalence and importance of microservices in industry, there exists limited research on microservices, partly due to lacking a benchmark system that reflects the characteristics of industrial microservice systems. To fill this gap, we conduct a review of literature and open source systems to identify the gap between existing benchmark systems and industrial microservice systems. Based on the results of the gap analysis, we then develop and release a medium-size benchmark system of microservice architecture.},
  keywords  = {benchmark testing;public domain software;small-to-medium enterprises;software architecture;industrial microservice systems;open source systems;medium-size benchmark system;microservice architecture;benchmarking microservice systems;software engineering research;Benchmark testing;Synchronization;Software engineering;Debugging;Business;Computer science;Microservice;benchmark;tracing;visualization;debugging;failure diagnosis},
}

@InProceedings{8539184,
  author    = {A. {van Hoorn} and A. {Aleti} and T. F. {Düllmann} and T. {Pitakrat}},
  title     = {ORCAS: Efficient Resilience Benchmarking of Microservice Architectures},
  booktitle = {2018 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW)},
  year      = {2018},
  pages     = {146-147},
  month     = {Oct},
  abstract  = {Resilience benchmarking aims to assess a software system's and an organization's ability to cope with failures, e.g., by injecting faults and observing their effects in both, testing and production environments. However, existing resilience benchmarks are ad-hoc and based on randomly injected faults. In this paper, we give an overview of the vision and the current state of our ORCAS approach for a more efficient resilience benchmarking for microservice architectures. ORCAS leverages the following characteristics: i) relationship between resilience patterns, antipatterns, and fault injections; ii) automatically extracted architectural knowledge to generate and refine resilience benchmarks; iii) use of simulations to further reduce the number of benchmarks to execute in testing and production systems.},
  doi       = {10.1109/ISSREW.2018.00-10},
  keywords  = {benchmark testing;software architecture;software fault tolerance;resilience patterns;fault injections;architectural knowledge;testing;production systems;microservice architectures;software system;organization;production environments;randomly injected faults;ORCAS approach;resilience benchmarking;Resilience;Benchmark testing;Circuit faults;Computer architecture;Software;Production;resilience benchmarking;microservices;fault injection;chaos engineering},
}

@InProceedings{7958455,
  author    = {G. {Granchelli} and M. {Cardarelli} and P. D. {Francesco} and I. {Malavolta} and L. {Iovino} and A. D. {Salle}},
  title     = {Towards Recovering the Software Architecture of Microservice-Based Systems},
  booktitle = {2017 IEEE International Conference on Software Architecture Workshops (ICSAW)},
  year      = {2017},
  pages     = {46-53},
  month     = {April},
  abstract  = {Today the microservice architectural style is being adopted by many key technological players such as Netflix, Amazon, The Guardian. A microservice architecture is composed of a large set of small services, each running in its own process and communicating with lightweight mechanisms (often via REST APIs). If on one side having a large set of independently developed services helps in terms of developer productivity, scalability, maintainability, on the other side it is very difficult to have a clear understanding of the overall architecture of a microservice-based software system, specially when the deployment and operation of the involved microservices evolves at run-time. In this paper we present MicroART, an architecture recovery approach for microservice-based systems. By using Model-Driven Engineering techniques, we leverage a suitably defined domain-specific language for representing the key aspects of the architecture of a microservice-based system and provide a tool-chain for automatically extracting architecture models of the system. The only inputs of MicroART are: (i) a GitHub repository containing the source code of the system and (ii) a reference to the container engine managing it. We validated MicroART on a publicly available benchmark system, with promising results.},
  doi       = {10.1109/ICSAW.2017.48},
  keywords  = {programming languages;service-oriented architecture;software maintenance;source code (software);microservice architectural style;developer scalability;developer maintainability;microservice-based software system;MicroART;model-driven engineering;domain-specific language;software architecture recovery;tool-chain;GitHub repository;source code;container engine;Computer architecture;Containers;Data mining;Software;Reverse engineering;Software architecture;Ports (Computers);Microservices;Architecture recovery;Model-Driven Engineering},
}

@InProceedings{4222650,
  author    = {L. {Zhu} and Y. {Liu} and N. B. {Bui} and I. {Gorton}},
  title     = {Revel8or: Model Driven Capacity Planning Tool Suite},
  booktitle = {29th International Conference on Software Engineering (ICSE'07)},
  year      = {2007},
  pages     = {797-800},
  month     = {May},
  abstract  = {Designing complex multi-tier applications that must meet strict performance requirements is a challenging software engineering problem. Ideally, the application architect could derive accurate performance predictions early in the project life-cycle, leveraging initial application design-level models and a description of the target software and hardware platforms. To this end, we have developed a capacity planning tool suite for component-based applications, called Revel8tor. The tool adheres to the model driven development paradigm and supports benchmarking and performance prediction for J2EE, .Net and Web services platforms. The suite is composed of three different tools: MDAPerf MDABench and DSLBench. MDAPerf allows annotation of design diagrams and derives performance analysis models. MDABench allows a customized benchmark application to be modeled in the UML 2.0 Testing Profile and automatically generates a deployable application, with measurement automatically conducted. DSLBench allows the same benchmark modeling and generation to be conducted using a simple performance engineering Domain Specific Language (DSL) in Microsoft Visual Studio. DSLBench integrates with Visual Studio and reuses its load testing infrastructure. Together, the tool suite can assist capacity planning across platforms in an automated fashion.},
  doi       = {10.1109/ICSE.2007.73},
  keywords  = {formal specification;Java;network operating systems;object-oriented programming;program testing;project management;software architecture;software metrics;software performance evaluation;software reusability;Unified Modeling Language;Web services;capacity planning tool suite;complex multitier application design;software performance requirement;software engineering;software project life-cycle;component-based application;model driven development;J2EE;.Net;Web service;design diagram annotation;customized benchmark application;UML 2.0 testing profile;software measurement;domain specific language;Microsoft Visual Studio;software reuse;load testing;software architecture;Capacity planning;Application software;Benchmark testing;Predictive models;Software engineering;Software performance;Hardware;Web services;Performance analysis;Unified modeling language},
}

@InProceedings{6228393,
  author    = {W. {Lee} and K. {Hsu} and J. {Lee}},
  title     = {Designing Software Architecture with Use Case Blocks Using the Design Structure Matrix},
  booktitle = {2012 International Symposium on Computer, Consumer and Control},
  year      = {2012},
  pages     = {654-657},
  month     = {June},
  abstract  = {Use case driven and architecture-centric approaches have been widely used to develop software systems, which also impose a great demand for a systematic approach to derive software architecture from requirements. To establish software architectures from goals and use cases, we propose, in this work, a software architecture design approach with use case blocks based on design structure matrix (DSM). The approach identifies relations between goals and use cases in the DSM and uses DSM partitioning to group goals and use cases in the blocks to form an initial system architecture which includes subsystems or high level components. The scenarios of the use cases are used to design low level components/classes of the corresponding subsystems/components. The proposed approach is illustrated by a benchmark problem domain of a meeting scheduler system.},
  doi       = {10.1109/IS3C.2012.170},
  keywords  = {matrix algebra;object-oriented methods;program diagnostics;software architecture;software architecture design;use case blocks;design structure matrix;use case driven approach;architecture-centric approach;software system development;DSM;initial system architecture;benchmark problem;meeting scheduler system;traceability relations;Computer architecture;Software architecture;Matrix decomposition;Unified modeling language;Analytical models;Software systems;design structure matrix;software architecture;use case block;traceability relations},
}

@Article{8651324,
  author   = {L. {Bao} and C. {Wu} and X. {Bu} and N. {Ren} and M. {Shen}},
  title    = {Performance Modeling and Workflow Scheduling of Microservice-Based Applications in Clouds},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2019},
  volume   = {30},
  number   = {9},
  pages    = {2114-2129},
  month    = {Sep.},
  abstract = {Microservice has been increasingly recognized as a promising architectural style for constructing large-scale cloud-based applications within and across organizational boundaries. This microservice-based architecture greatly increases application scalability, but meanwhile incurs an expensive performance overhead, which calls for a careful design of performance modeling and task scheduling. However, these problems have thus far remained largely unexplored. In this paper, we develop a performance modeling and prediction method for independent microservices, design a three-layer performance model for microservice-based applications, formulate a Microservice-based Application Workflow Scheduling problem for minimum end-to-end delay under a user-specified Budget Constraint (MAWS-BC), and propose a heuristic microservice scheduling algorithm. The performance modeling and prediction method are validated and justified by experimental results generated through a well-known microservice benchmark on disparate computing nodes, and the performance superiority of the proposed scheduling solution is illustrated by extensive simulation results in comparison with existing algorithms.},
  doi      = {10.1109/TPDS.2019.2901467},
  keywords = {cloud computing;scheduling;service-oriented architecture;workflow management software;heuristic microservice scheduling algorithm;architectural style;microservice-based application workflow scheduling problem;organizational boundaries;minimum end-to-end delay;user-specified budget constraint;computing nodes;MAWS-BC;three-layer performance model;prediction method;task scheduling;performance modeling;application scalability;microservice-based architecture;large-scale cloud-based applications;Computational modeling;Predictive models;Scheduling;Computer architecture;Cloud computing;Processor scheduling;Benchmark testing;Microservice;performance modeling and prediction;task scheduling;cloud computing},
}

@InProceedings{7091220,
  author    = {A. {Dragomir} and H. {Lichter}},
  title     = {Towards an Architecture Quality Index for the Behavior of Software Systems},
  booktitle = {2014 21st Asia-Pacific Software Engineering Conference},
  year      = {2014},
  volume    = {2},
  pages     = {75-82},
  month     = {Dec},
  abstract  = {Software architecture lies at the backbone of any software system and its choice directly influences important non-functional characteristics such as maintainability, extensibility, etc. Up-to-date software architecture descriptions should be at any time available to support the analysis and evaluation of the current state of the architecture. However the current state of the art lacks both methodologies and tools for ensuring availability of architecture descriptions and fails to offer objective means for evaluating software architectures. Currently, no generally accepted method for comparing software from an architecture point of view exists. In this paper, we present our current results towards creating a so-called architecture quality index that includes a bidirectional architecture quality model as well as a quality benchmark created for the context of the ARAMIS research project. The proposed architecture quality index aims to support the architects to evaluate and compare the architecture of software systems based on information extracted during the considered systems' run-time.},
  doi       = {10.1109/APSEC.2014.97},
  keywords  = {software architecture;software maintenance;software quality;architecture quality index;software system behavior;maintainability;extensibility;software architecture evaluation;software architecture descriptions;bidirectional architecture quality model;quality benchmark;ARAMIS research project;information extraction;Computer architecture;Indexes;Benchmark testing;Context;Monitoring;Software systems;Architecture;Architecture Quality Index;Software Architecture Reconstruction;Communication Integrity;Software Architecture Evaluation},
}

@InProceedings{7968049,
  author    = {C. M. {Aderaldo} and N. C. {Mendonça} and C. {Pahl} and P. {Jamshidi}},
  title     = {Benchmark Requirements for Microservices Architecture Research},
  booktitle = {2017 IEEE/ACM 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering (ECASE)},
  year      = {2017},
  pages     = {8-13},
  month     = {May},
  abstract  = {Microservices have recently emerged as a new architectural style in which distributed applications are broken up into small independently deployable services, each running in its own process and communicating via lightweight mechanisms. However, there is still a lack of repeatable empirical research on the design, development and evaluation of microservices applications. As a first step towards filling this gap, this paper proposes, discusses and illustrates the use of an initial set of requirements that may be useful in selecting a community-owned architecture benchmark to support repeatable microservices research.},
  doi       = {10.1109/ECASE.2017.4},
  keywords  = {benchmark testing;formal specification;software architecture;benchmark requirements;microservice architecture research;distributed applications;lightweight mechanisms;community-owned architecture benchmark;Benchmark testing;Tools;Software;Containers;Software engineering;Computer architecture;Microservices;software architecture;research benchmark},
}

@InProceedings{8312533,
  author    = {X. {Li} and L. {Zhang} and N. {Ge}},
  title     = {Framework Information Based Java Software Architecture Recovery},
  booktitle = {2017 24th Asia-Pacific Software Engineering Conference Workshops (APSECW)},
  year      = {2017},
  pages     = {114-120},
  month     = {Dec},
  abstract  = {Software systems tend to become more and more complex as they evolve, which makes it difficult to review, understand, and maintain the source code without complete architectural information, especially in case of large-scale systems. Software architecture recovery is considered an important method contributing to solving this problem. Hierarchical clustering is one of the techniques used to extract architectural information from lower level software representations, such as the source code. This paper is aimed at improving the accuracy of existing hierarchical clustering algorithms by allowing users to parameterize and configure framework information as framework-specific features. We have implemented our approach as an Eclipse plugin and have applied it to recovering the architecture of Java programs. Experiments are carried out on our benchmark built upon Java web applications in which the Spring Framework is used. The experimental results show that our approach can improve the accuracy of the recovered architecture to some extent.},
  doi       = {10.1109/APSECW.2017.15},
  keywords  = {Java;large-scale systems;pattern clustering;software architecture;software maintenance;framework information;Java software architecture recovery;software systems;large-scale systems;hierarchical clustering algorithms;framework-specific features;Java programs;Spring Framework;architectural information;Java Web applications;lower level software representation;Clustering algorithms;Software algorithms;Java;Software architecture;Software;Computer architecture;Feature extraction;software architecture recovery;hierarchical clustering;framework information},
}

@InProceedings{6402908,
  author    = {E. {Deniz} and A. {Sen} and J. {Holt} and B. {Kahne}},
  title     = {Using software architectural patterns for synthetic embedded multicore benchmark development},
  booktitle = {2012 IEEE International Symposium on Workload Characterization (IISWC)},
  year      = {2012},
  pages     = {89-99},
  month     = {Nov},
  abstract  = {Benchmarks capture the essence of many important real-world applications and allow performance, and power analysis while developing new systems. Synthetic benchmarks are a miniaturized form of benchmarks that allow high simulation speeds and act as proxies to proprietary applications. Software architecture principles guide the development of new applications and benchmarks. We leverage software architectural patterns in developing synthetic benchmarks for embedded multicore systems. We developed an automated framework complete with characterization and synthesis components and performed experiments on PARSEC and Rodinia benchmarks. Our benchmarks can be run on any given infrastructure, that is, SMP or message passing, unlike previously developed benchmarks. Hence, this allows us to target heterogeneous embedded multicore systems. Our results show that the synthetic benchmarks and the real applications are similar with respect to various micro-architecture dependent as well as independent metrics.},
  doi       = {10.1109/IISWC.2012.6402908},
  keywords  = {application program interfaces;benchmark testing;embedded systems;message passing;multiprocessing systems;software architecture;software performance evaluation;software architectural pattern;synthetic embedded multicore benchmark development;power analysis;software architecture principles;synthesis components;PARSEC;Rodinia benchmarks;SMP;message passing;heterogeneous embedded multicore systems;microarchitecture dependent metrics;microarchitecture independent metrics;performance analysis;Benchmark testing;Multicore processing;Message systems;Software;Measurement;Pattern recognition;Software architecture;multicore;workload characterization;software architectural pattern;synthetic benchmark;embedded multicore},
}

@InProceedings{1182977,
  author    = {{Yan Liu} and I. {Gorton} and A. {Liu} and {Shiping Chen}},
  title     = {Evaluating the scalability of Enterprise JavaBeans technology},
  booktitle = {Ninth Asia-Pacific Software Engineering Conference, 2002.},
  year      = {2002},
  pages     = {74-83},
  month     = {Dec},
  abstract  = {One of the major problems in building large-scale distributed systems is to anticipate the performance of the eventual solution before it has been built. This problem is especially germane to Internet-based e-business applications, where failure to provide high performance and scalability can lead to application and business failure. The fundamental software engineering problem is compounded by many factors, including individual application diversity, software architecture trade-offs, COTS component integration requirements, and differences in performance of various software and hardware infrastructures. We describe the results of an empirical investigation into the scalability of a widely used distributed component technology, Enterprise JavaBeans (EJB). A benchmark application is developed and tested to measure the performance of a system as both the client load and component infrastructure are scaled up. A scalability metric from the literature is then applied to analyze the scalability of the EJB component infrastructure under two different architectural solutions.},
  doi       = {10.1109/APSEC.2002.1182977},
  keywords  = {distributed object management;Java;software performance evaluation;electronic commerce;software architecture;software metrics;software reusability;scalability;Enterprise JavaBeans;large-scale distributed systems;performance;Internet-based e-business;software engineering;software architecture;COTS component integration requirements;benchmark application;scalability metric;Scalability;Java;Application software;Large-scale systems;Internet;Software engineering;Software architecture;Software performance;Hardware;Benchmark testing},
}

@InProceedings{5224151,
  author    = {M. H. {Jamal} and G. {Mustafa} and A. {Waheed} and W. {Mahmood}},
  title     = {An extensible infrastructure for benchmarking Multi-core Processors based systems},
  booktitle = {2009 International Symposium on Performance Evaluation of Computer Telecommunication Systems},
  year      = {2009},
  volume    = {41},
  pages     = {13-20},
  month     = {July},
  abstract  = {With wide adoption of multi-core processor based systems, there is a need for benchmarking such systems at both application and operating system levels. Developing benchmarks for multi-core systems is a cumbersome task due to underlying parallel architecture and complexity of parallel programming paradigms. In this paper, we introduce multi-core processor architecture and communication (MPAC) benchmarking library, which provides a common infrastructure for developing specification-driven micro-benchmarks, application benchmarks, and network traffic load generators. We describe the software architecture of MPAC and demonstrate its efficacy by implementing the specifications of well-known Stream and Netperf micro-benchmarks. We use these benchmarks to validate MPAC based performance measurements for single thread on Intel, AMD, and Cavium multi-core processors based platforms. We also develop a CPU micro-benchmark using our own specifications. In addition, we extend these micro-benchmarks through MPAC library to measure the scaling characteristics of our target multi-core processors based platforms.},
  keywords  = {benchmark testing;formal specification;multiprocessing systems;operating systems (computers);parallel architectures;parallel programming;software architecture;software libraries;multicore processor-based system;operating system level;parallel architecture;parallel programming paradigm;MPAC;multicore processor architecture-and-communication;benchmarking library;specification-driven micro-benchmark;application benchmark;network traffic load generator;software architecture;Multicore processing;Operating systems;Parallel architectures;Parallel programming;Computer architecture;Software libraries;Application software;Telecommunication traffic;Software architecture;Measurement;Performance measurement;benchmarking;load generation;Multi-core Processors;experimental design analysis;control},
}

@InProceedings{1183069,
  author    = {J. {Cahill} and J. M. {Hogan} and R. {Thomas}},
  title     = {The Java Metrics Reporter - an extensible tool for OO software analysis},
  booktitle = {Ninth Asia-Pacific Software Engineering Conference, 2002.},
  year      = {2002},
  pages     = {507-516},
  month     = {Dec},
  abstract  = {It has been argued for many years that software engineering lacks the repeatability and well-defined monitoring characteristic of the traditional engineering disciplines. Over time, numerous authors have addressed this issue by proposing a range of software metrics although it is generally agreed that no one measure is sufficient to capture software quality, and a well chosen suite of metrics must be employed. While substantial progress has been made, adoption of metrics has been limited in the software development community, and metrics have long suffered from a lack of comprehensibility. Further, critics have argued that many metrics have been introduced in isolation, with little regard for their relationship to existing measures, and without appropriate validation against a sufficient body of source code. This work introduces the Java Metrics Reporter, a new tool which addresses a number of these concerns in the domain of object oriented languages. The tool provides integrated tutorial support and benchmarking of user code against professional code bases. Moreover, the architecture allows for adaptation to other languages, and extension to other metrics through a straightforward plug-in approach. The paper provides detailed consideration of the architecture and the metrics selected, together with aspects of the tool which lend to its usability and assist in interpretation of metrics. Finally, the paper outlines plans for the further development of the tool, together with its release to the professional and research communities.},
  doi       = {10.1109/APSEC.2002.1183069},
  keywords  = {software metrics;Java;object-oriented programming;software tools;software architecture;Java Metrics Reporter;software tool;object oriented software analysis;software metrics;software quality;object oriented languages;benchmarking;software architecture;usability;Java;Software tools;Software engineering;Monitoring;Software metrics;Software measurement;Time measurement;Software quality;Programming;Usability},
}

@InProceedings{4055086,
  author    = {R. {Inkol} and C. {Wilson} and M. {Eidus}},
  title     = {Applications of Performance Benchmarking to the Development of Signal Processing Systems Based on Personal Computer Technology},
  booktitle = {2006 Canadian Conference on Electrical and Computer Engineering},
  year      = {2006},
  pages     = {41-45},
  month     = {May},
  abstract  = {Highly optimized libraries of vector and matrix math functions, such as the Intel integrated performance primitive (IPP) libraries, can be used to quickly and economically construct high performance signal processing systems built up around personal computer technology. This paper describes a portable benchmarking software suite that has been developed for benchmarking the performance obtainable with IPP implementations of common signal processing algorithms, including the fast Fourier transform (FFT) and finite impulse response (FIR) filter. The results presented provide useful insights into system design choices concerning algorithms, software architecture and processors},
  doi       = {10.1109/CCECE.2006.277339},
  keywords  = {benchmark testing;fast Fourier transforms;FIR filters;mathematics computing;microcomputer applications;signal processing;software libraries;benchmarking;signal processing system;personal computer technology;Intel integrated performance primitive library;fast Fourier transform;finite impulse response filter;Application software;Signal processing;Microcomputers;Signal processing algorithms;Software libraries;Finite impulse response filter;Software performance;Fast Fourier transforms;Software algorithms;Software architecture;Signal processing;Single Instruction Multiple Data;Fast Fourier Transform;multirate filter;benchmark},
}

@InProceedings{8712159,
  author    = {D. {Gesvindr} and B. {Buhnova}},
  title     = {PaaSArch: Quality Evaluation Tool for PaaS Cloud Applications Using Generated Prototypes},
  booktitle = {2019 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  year      = {2019},
  pages     = {170-173},
  month     = {March},
  abstract  = {Platform as a Service (PaaS) cloud offers a plethora of architectural options when designing new PaaS cloud applications. The quality evaluation of each architectural alternative is very difficult due to unknown performance and internal architecture of the used PaaS cloud services. The PaasArch tool supports software architects in quality evaluation of PaaS cloud applications using generated prototypes. Within the tool, fully functional cloud applications can be automatically generated based on their model, deployed to the cloud, and benchmarked. Our experiments show that the benchmarks give very good guidance to the software architect about the quality (namely performance) of evaluated architectural alternatives.},
  doi       = {10.1109/ICSA-C.2019.00038},
  keywords  = {cloud computing;software architecture;software performance evaluation;software quality;PaaS cloud applications;PaaS cloud services;PaasArch tool;fully functional cloud applications;evaluated architectural alternatives;quality evaluation tool;platform as a service cloud;software architecture;Prototypes;Tools;Unified modeling language;Cloud computing;Software;Computer architecture;Benchmark testing},
}

@Article{159342,
  title    = {IEEE Standard Glossary of Software Engineering Terminology},
  journal  = {IEEE Std 610.12-1990},
  year     = {1990},
  pages    = {1-84},
  month    = {Dec},
  abstract = {This IEEE Standards product is part of the family on Software Engineering. This standard identifies terms currently in use in the field of Software Engineering. Standard definitions for those terms are established.},
  doi      = {10.1109/IEEESTD.1990.101064},
  keywords = {glossaries;software engineering;standards;IEEE Std 610.12-1990;standard glossary;software engineering terminology;Terminology;Software engineering;Standards;glossary;terminology;dictionary;Software engineering;Definitions},
}

@InProceedings{557681,
  author    = {N. I. {Kamenoff}},
  title     = {One approach for generalization of real-time distributed systems benchmarking},
  booktitle = {Proceedings of the 4th International Workshop on Parallel and Distributed Real-Time Systems},
  year      = {1996},
  pages     = {202-203},
  month     = {April},
  abstract  = {Practically, there are endless variations of the implementation of real-time distributed systems (RTDSs). These variations are the consequences of the different hardware, system software architectures and application requirements. These variations make the testing of RTDSs more implementation dependent. To the contrary benchmarking can be considered to be domain specific. In general any RTDS can be considered to have three scheduling domains: the node's processor scheduling domain, the node's communication scheduling domain, and the nodes' priority domain. The only way a benchmark would be able to integrate these three scheduling domains and also be hardware and system software architecture independent, is to present itself as an application to the RTDS. From the application point of view, the RTDS can be seen as set of tasks (processes) located on different nodes, which communicate with each other by exchanging messages. The tasks as well the messages have timing constraints. The Hartstone Distributed Benchmark (HDB) can be viewed as an application-oriented benchmark applicable to any type of RTDS. The HDB provides figures of merit for the performance evaluation of the end-to-end scheduling of messages.},
  doi       = {10.1109/WPDRTS.1996.557681},
  keywords  = {performance evaluation;real-time distributed systems benchmarking;system software architectures;application requirements;hardware;testing;processor scheduling;communication scheduling;priority domain.;exchanging messages;timing constraints;Hartstone Distributed Benchmark;application-oriented benchmark;performance evaluation;end-to-end scheduling;Real time systems;Benchmark testing;Processor scheduling;Hardware;Timing;Computer architecture;Application software;System software;Software engineering;Control systems},
}

@InProceedings{4400181,
  author    = {Q. {Zhu}},
  title     = {An Experimental Platform for Root Cause Diagnosis Research},
  booktitle = {14th Working Conference on Reverse Engineering (WCRE 2007)},
  year      = {2007},
  pages     = {293-296},
  month     = {Oct},
  abstract  = {To obtain a healthy integrated production system that achieves defined quality goals in service oriented architecture (SOA), such as availability and performance, the timely detection and resolution of failures is needed. The goal of this thesis identify the primary or root causes (faults) of a set of observed symptoms in an integrated production system that indicate degradation and failure in system components leading to abnormal system performance. Our hypothesis is, if we combine more diagnosis methods (such as more symptom repository, dynamic analysis capabilities, different artificial intelligent (AI) reasoning capabilities or other available technologies and tools) into an existing tool such as open source AspectJ based diagnostic tool Glassbox, we can find more amount of root causes and more precise - "actual" root causes. We are building an experiment platform to validate this hypothesis.},
  doi       = {10.1109/WCRE.2007.12},
  keywords  = {fault diagnosis;object-oriented programming;program diagnostics;public domain software;software architecture;software fault tolerance;software quality;software tools;Web services;root cause diagnosis research;experimental platform;healthy integrated production system;service oriented architecture;Glassbox open source AspectJ based diagnostic tool;software quality;failure detection;failure resolution;symptom repository;dynamic analysis capability;artificial intelligent reasoning capability;Web service;Artificial intelligence;Production systems;Service oriented architecture;Databases;Fault diagnosis;Application software;Web services;Statistics;Benchmark testing;Availability},
}

@InProceedings{6178890,
  author    = {F. A. {Fontana} and A. {Caracciolo} and M. {Zanoni}},
  title     = {DPB: A Benchmark for Design Pattern Detection Tools},
  booktitle = {2012 16th European Conference on Software Maintenance and Reengineering},
  year      = {2012},
  pages     = {235-244},
  month     = {March},
  abstract  = {Many activities can be done to support software evolution and reverse engineering of a system. Design pattern detection is one of these activities. It is useful to gain knowledge on the design issues of an existing system, on its architecture and design quality, improving the comprehension of the system and hence its maintainability and evolution. Several tools for design pattern detection have been developed in the literature, but they usually provide different results when analyzing the same systems. Some works have been proposed in the literature to compare these results, but a standard and widely-accepted benchmark is not yet available. In this work we propose our benchmark web application for design pattern detection, based on a community driven evaluation.},
  doi       = {10.1109/CSMR.2012.32},
  keywords  = {Internet;pattern classification;reverse engineering;software architecture;software maintenance;software quality;software tools;design pattern detection tools;DPB;software evolution;reverse engineering;software architecture;software design quality;software maintainability;software evolution;widely-accepted benchmark;benchmark Web application;community driven evaluation;Benchmark testing;Unified modeling language;Java;Context;Communities;Image color analysis;Analytical models;design pattern detection;benchmark;meta-model},
}

@InProceedings{8573515,
  author    = {A. {Sriraman} and T. F. {Wenisch}},
  title     = {μ Suite: A Benchmark Suite for Microservices},
  booktitle = {2018 IEEE International Symposium on Workload Characterization (IISWC)},
  year      = {2018},
  pages     = {1-12},
  month     = {Sep.},
  abstract  = {Modern On-Line Data Intensive (OLDI) applications have evolved from monolithic systems to instead comprise numerous, distributed microservices interacting via Remote Procedure Calls (RPCs). Microservices face single-digit millisecond RPC latency goals (implying sub-ms medians)-much tighter than their monolithic ancestors that must meet ≥ 100 ms latency targets. Sub-ms-scale OS/network overheads that were once insignificant for such monoliths can now come to dominate in the sub-msscale microservice regime. It is therefore vital to characterize the influence of OSand network-based effects on microservices. Unfortunately, widely-used academic data center benchmark suites are unsuitable to aid this characterization as they (1) use monolithic rather than microservice architectures, and (2) largely have request service times ≥ 100 ms. In this paper, we investigate how OS and network overheads impact microservice median and tail latency by developing a complete suite of microservices called μSuite that we use to facilitate our study. μSuite comprises four OLDI services composed of microservices: image similarity search, protocol routing for key-value stores, set algebra on posting lists for document search, and recommender systems. Our characterization reveals that the relationship between optimal OS/network parameters and service load is complex. Our primary finding is that non-optimal OS scheduler decisions can degrade microservice tail latency by up to ~87%.},
  doi       = {10.1109/IISWC.2018.8573515},
  keywords  = {operating systems (computers);remote procedure calls;service-oriented architecture;Web services;academic data center benchmark suites;sub-ms-scale OS network overheads;sub-msscale microservice;OLDI services;image similarity search;protocol routing;set algebra;recommender systems;Service Level Objectives;monolithic software architectures;Remote Procedure Calls;distributed microservices;monolithic systems;On-Line Data Intensive applications;μSuite;microservice architectures;Benchmark testing;Feature extraction;Open source software;Protocols;Google;Indexing;Computer architecture;OLDI;microservice;mid-tier;benchmark suite;OS and network overheads.},
}

@InProceedings{989805,
  author    = {J. {Grundy} and {Yuhong Cai} and A. {Liu}},
  title     = {Generation of distributed system test-beds from high-level software architecture descriptions},
  booktitle = {Proceedings 16th Annual International Conference on Automated Software Engineering (ASE 2001)},
  year      = {2001},
  pages     = {193-200},
  month     = {Nov},
  abstract  = {Most distributed system specifications have performance benchmark requirements. However, determining the likely performance of complex distributed system architectures during development is very challenging. We describe a system where software architects sketch an outline of their proposed system architecture at a high level of abstraction, including indicating client requests, server services, and choosing particular kinds of middleware and database technologies. A fully working implementation of this system is then automatically generated, allowing multiple clients and servers to be run. Performance tests are then automatically run for this generated code and results are displayed back in the original high-level architectural diagrams. Architects may change performance parameters and architecture characteristics, comparing multiple test run results to determine the most suitable abstractions to refine to detailed designs for actual system implementation. We demonstrate the utility of this approach and the accuracy of our generated performance test-beds for validating architectural choices during early system development.},
  doi       = {10.1109/ASE.2001.989805},
  keywords  = {distributed programming;software architecture;program compilers;formal specification;client-server systems;distributed system test-bed generation;high-level software architecture descriptions;distributed system specifications;performance benchmark requirements;complex distributed system architectures;software architects;system architecture;client requests;server services;middleware;database technologies;multiple clients;multiple servers;generated code;high-level architectural diagrams;performance parameters;architecture characteristics;multiple test run results;architectural choices;early system development;Software testing;System testing;Software architecture;Middleware;Computer architecture;Databases;Automatic testing;XML;Virtual prototyping;Monitoring},
}

@InProceedings{6225940,
  author    = {T. {Espinha} and C. {Chen} and A. {Zaidman} and H. {Gross}},
  title     = {Spicy stonehenge: Proposing a SOA case study},
  booktitle = {2012 4th International Workshop on Principles of Engineering Service-Oriented Systems (PESOS)},
  year      = {2012},
  pages     = {57-58},
  month     = {June},
  abstract  = {Maintenance research in the context of Service Oriented Architecture (SOA) is currently lacking a suitable standard case study that can be used by scientists in order to (1) develop and assess their research ideas and (2) compare and benchmark their solution(s). It is also well established in different fields that having such a standard case study system brings many benefits, in that it helps determine which approaches work best for specific problems. For this reason, we decided to build upon an existing open-source system and make it available for other researchers to use. This system is Spicy Stonehenge.},
  doi       = {10.1109/PESOS.2012.6225940},
  keywords  = {public domain software;service-oriented architecture;spicy stonehenge;SOA;service oriented architecture;open-source system;Service oriented architecture;Benchmark testing;Databases;Stock markets;Business;Standards;Communities},
}

@InProceedings{1270737,
  author    = {S. A. {Bohner} and D. {Gracanin}},
  title     = {Software impact analysis in a virtual environment},
  booktitle = {28th Annual NASA Goddard Software Engineering Workshop, 2003. Proceedings.},
  year      = {2003},
  pages     = {143-151},
  month     = {Dec},
  abstract  = {With the relentless growth in software, automated support for visualizing and navigating software artifacts is no longer a luxury. As packaged software components and middleware occupy more and more of the software landscape, interoperability relationships point to increasingly relevant software change impacts. Packaged software now represents over thirty-two percent of the software portfolio in most organizations benchmark (Rubin, 2001). While traceability and dependency analysis has effectively supported impact analysis in the past, they fall short today as their webs of dependency information extend beyond most software engineers ability to comprehend them. This paper describes research for extending current software change impact analysis to incorporate software architecture dependency relationships. We discuss how we address the extensive dependency information involved, extending impact analysis using software visualization, and outline our approach to employing the software impact analysis virtual environment.},
  doi       = {10.1109/SEW.2003.1270737},
  keywords  = {middleware;open systems;object-oriented programming;software architecture;program visualisation;software impact analysis;virtual environment;software visualization;software navigation;software architecture;Virtual environment;Security;Information analysis;Software packages;Packaging;Software systems;Computer architecture;Application software;Visualization;Software architecture},
}

@InProceedings{5623383,
  author    = {A. {Gorbenko} and V. {Kharchenko} and S. {Mamutov} and O. {Tarasyuk} and Y. {Chen} and A. {Romanovsky}},
  title     = {Real Distribution of Response Time Instability in Service-Oriented Architecture},
  booktitle = {2010 29th IEEE Symposium on Reliable Distributed Systems},
  year      = {2010},
  pages     = {92-99},
  month     = {Oct},
  abstract  = {This paper reports our practical experience of benchmarking a complex System Biology Web Service, and investigates the instability of its behaviour and the delays induced by the communication medium. We present the results of our statistical data analysis and distributions which fit and predict the response time instability typical of Service-Oriented Architectures (SOAs) built over the Internet. Our experiment has shown that the request processing time of the target e-science Web Service (WS) has a higher instability than the network round trip time. It has been found that by using a particular theoretical distribution, within short time intervals the request processing time can be represented better than the network round trip time. Moreover, certain characteristics of the probability distribution series of the round trip time make it particularly difficult to fit them theoretically. The experimental work reported in the paper supports our claim that dealing with the uncertainty inherent in the very nature of SOA and WSs is one of the main challenges in building dependable service-oriented systems. In particular, this uncertainty exhibits itself through very unstable web service response times and Internet data transfer delays that are hard to predict. Our findings indicate that the more experimental data is considered the less precise distributional approximations become. The paper concludes with a discussion of the lessons learnt about the analysis techniques to be used in such experiments, the validity of the data, the main causes of uncertainty and possible remedial actions.},
  doi       = {10.1109/SRDS.2010.40},
  keywords  = {bioinformatics;software architecture;statistical analysis;real distribution;response time instability;service oriented architecture;system biology web service;statistical data analysis;SOA;network round trip time;service oriented systems;Time factors;Service oriented architecture;Uncertainty;Delay;Approximation methods;service-oriented architecture;response time;instability;distribution law},
}

@InProceedings{1115027,
  author    = {G. {Csertan} and G. {Huszerl} and I. {Majzik} and Z. {Pap} and A. {Pataricza} and D. {Varro}},
  title     = {VIATRA - visual automated transformations for formal verification and validation of UML models},
  booktitle = {Proceedings 17th IEEE International Conference on Automated Software Engineering,},
  year      = {2002},
  pages     = {267-270},
  month     = {Sep.},
  abstract  = {The VIATRA (visual automated model transformations) framework is the core of a transformation-based verification and validation environment for improving the quality of systems designed using the Unified Modeling Language by automatically checking consistency, completeness, and dependability requirements. In the current paper, we present an overview of (i) the major design goals and decisions, (ii) the underlying formal methodology based on metamodeling and graph transformation, (iii) the software architecture based upon the XMI standard, and (iv) several benchmark applications of the VIATRA framework.},
  doi       = {10.1109/ASE.2002.1115027},
  keywords  = {specification languages;formal verification;VIATRA;visual automated model transformations framework;transformation-based verification and validation environment;quality;UML models;formal verification;formal validation;automatic consistency requirements checking;automatic dependability requirements checking;automatic completeness requirements checking;metamodeling;graph transformation;software architecture;XMI standard;benchmark applications;Formal verification;Unified modeling language;Mathematical model;Object oriented modeling;Application software;Software architecture;Computer aided software engineering;System analysis and design;Mathematical analysis;Environmental economics},
}

@InProceedings{6178885,
  author    = {T. {Espinha} and C. {Chen} and A. {Zaidman} and H. {Gross}},
  title     = {Maintenance Research in SOA - Towards a Standard Case Study},
  booktitle = {2012 16th European Conference on Software Maintenance and Reengineering},
  year      = {2012},
  pages     = {391-396},
  month     = {March},
  abstract  = {Maintenance research in the context of Service Oriented Architecture (SOA) is currently lacking a suitable standard case study that can be used by scientists in order to develop and assess their research ideas, and for comparison, and benchmarking purposes. It is also well established in different fields that having such a standard case study system brings many benefits, in that it helps determine which approaches work best for specific problems. For this reason, we decided to build upon an existing open-source system and make it available for other researchers to use. This system, Spicy Stonehenge, provides many advantages for carrying out maintenance research: it is complex, extensible and, most importantly, openly available for anyone to use and build upon. With this paper, we introduce Spicy Stonehenge as the standard case study SOA system, and we also present our research vision in the field of maintenance, reengineering, evolution and testing of SOA systems, and how these goals fit together with Spicy Stonehenge.},
  doi       = {10.1109/CSMR.2012.49},
  keywords  = {program testing;public domain software;research and development;service-oriented architecture;software maintenance;maintenance research;standard case study;service oriented architecture;benchmarking purposes;open-source system;spicy stonehenge;SOA system testing;SOA system maintenance;SOA system reengineering;SOA system evolution;Service oriented architecture;Business;Monitoring;Maintenance engineering;Runtime;Industries;SOA;SOA maintenance;Software maintenance;Maintenance Research;Software evolution;Software reengineering;Service oriented;services},
}

@InProceedings{6340191,
  author    = {V. {Vedam} and J. {Vemulapati}},
  title     = {Demystifying Cloud Benchmarking Paradigm - An in Depth View},
  booktitle = {2012 IEEE 36th Annual Computer Software and Applications Conference},
  year      = {2012},
  pages     = {416-421},
  month     = {July},
  abstract  = {Industry has been boomeranging with "Cloud" in every forte and IaaS (Infrastructure as a Service) layer has always been the backbone of any cloud computing service. The infrastructure layer of any cloud service consists of an array of physical processors along with storage and network components with a virtualization layer on the top that can expose the combined processing power and storage of the array as a set of virtual instances. To ensure that a particular software system has the right virtual infrastructure attributes, it becomes imperative that the cloud infrastructure in question be audited or measured by the software designer before deploying the system on a particular cloud. Whether you are a cloud company that provides this virtual infrastructure or you are a software architect in charge of choosing the right infrastructure provider, you need to have the right set of tools for measuring the performance of the infrastructure. In other words, you need to benchmark the cloud service in specific areas to find out how the service is performing. This is called Cloud Benchmarking. At a conceptual level, cloud benchmarking is not any different from regular computing system benchmarking. However, there is a vast difference between the two when it comes to implementation of the benchmarking methodology and measurement of the critical QOS parameters. This research paper discusses the relevant aspects of cloud benchmarking such as the cloud characteristics to benchmark, the parameters to measure, the role of automation in cloud benchmarking and finally the performance-cost normalization of the benchmarking results so that the cloud provider offering the best value for money can be determined.},
  doi       = {10.1109/COMPSAC.2012.61},
  keywords  = {cloud computing;software architecture;virtualisation;cloud benchmarking;IaaS;infrastructure as a service;cloud computing service;physical processor;storage component;network component;virtualization layer;software system;cloud infrastructure;cloud company;virtual infrastructure;software architecture;infrastructure provider;system benchmarking;QOS parameter;cloud characteristics;performance-cost normalization;cloud provider;Benchmark testing;Cloud computing;Automation;Arrays;Standards;Cloud Benchmarking;Cloud Characteristics;Disk I/O;memory;CPU},
}

@InProceedings{5552755,
  author    = {R. {Akkiraju} and H. {van Geel}},
  title     = {Estimating the Cost of Developing Customizations to Packaged Application Software Using Service Oriented Architecture},
  booktitle = {2010 IEEE International Conference on Web Services},
  year      = {2010},
  pages     = {433-440},
  month     = {July},
  abstract  = {Service-oriented way of building customizations to packaged applications is an emerging alternative to the traditional way of modifying the package application directly to implement customizations. Estimating the effort at an early stage (before detailed design) in the services engagement is important to reduce the perceived risk of the new approach and to demonstrate the potential cost efficiencies. In this paper we present a method to estimate the effort and cost involved in developing customizations to packaged application software (such as SAP and Oracle ERP software) using service-oriented architecture/design (SOA) style. Taking only a description of business processes to be customized, we estimate the effort and costs of implementing those customizations. We use novel artifact-centric and linguistic analysis approaches to estimate the business object count which lies at the center of our cost estimation model. The model is currently being piloted at a large IT services organization involved in implementing packaged application software for clients. Initial experiments reveal that the cost and effort estimates are within 10% range of the benchmark estimates developed after requirements gathering and detailed design.},
  doi       = {10.1109/ICWS.2010.94},
  keywords  = {application program interfaces;costing;software architecture;developing customizations cost estimation;packaged application software;service oriented architecture;Business;Estimation;Unified modeling language;Application software;Benchmark testing;User interfaces;Web services;SOA application cost estimation;packaged application cost estimation},
}

@InProceedings{1625550,
  author    = {T. {Shinozaki} and E. {Kawai} and S. {Yamaguchi} and H. {Yamamoto}},
  title     = {Performance Anomalies of Advanced Web Server Architectures in Realistic Environments},
  booktitle = {2006 8th International Conference Advanced Communication Technology},
  year      = {2006},
  volume    = {1},
  pages     = {169-174},
  month     = {Feb},
  abstract  = {When we discuss the performance of network servers, simple benchmark tests can mislead us into inaccurate and wrong results. In this paper, we present precise results of Web server performance evaluations on emulated networks whose parameters include network delay and packet loss ratio. We chose three types of target Web servers: Apache, thttpd, and TUX. Their software architectures are largely different from each other and their pros and cons are discussed from a general viewpoint in a number of previous literatures. However, we still found some performance anomalies in the experimental results, which revealed implementation issues in those servers},
  doi       = {10.1109/ICACT.2006.205945},
  keywords  = {file servers;Internet;software architecture;performance anomalies;advanced Web server architectures;realistic environments;network servers;benchmark tests;Web server performance evaluations;emulated networks;network delay;packet loss ratio;Apache Web server;thttpd Web server;TUX Web server;software architectures;Web server;Service oriented architecture;Network servers;Benchmark testing;Performance loss;Software architecture;Computer architecture;File systems;Protocols;Intelligent networks;Performance;web;server;network;delay;loss;Apache;thttpd;TUX},
}

@InProceedings{995440,
  author    = {X. T. {Nguyen} and P. {Heuer}},
  title     = {Automated intent assessment simulation environment},
  booktitle = {Final Program and Abstracts on Information, Decision and Control},
  year      = {2002},
  pages     = {395-400},
  month     = {Feb},
  abstract  = {There are different ways of assessing the intent of a target. However, the most effective way is one that can offer the most accurate estimates compared to human-defined benchmarks. To allow various intent assessment technologies to be evaluated against the benchmarks, a reconfigurable simulation environment has been designed using a sound software engineering process which is the Rational Unified Process and the Unified Modelling Language. In order to cope with future tests with different technologies, the software architecture was structured in such a way that the simulation environment talks to different intent assessment technologies via an interface, hence forming a plug-and-play simulation environment.},
  doi       = {10.1109/IDC.2002.995440},
  keywords  = {software architecture;digital simulation;military computing;specification languages;automated intent assessment simulation environment;intent assessment technologies;human-defined benchmarks;reconfigurable simulation environment;Rational Unified Process;Unified Modelling Language;software architecture;plug-and-play simulation environment;Bayesian methods;Benchmark testing;Robustness;Software engineering;Unified modeling language;Software testing;Software architecture;Kinematics},
}

@InProceedings{8456340,
  author    = {T. {Bhagya} and J. {Dietrich} and H. {Guesgen} and S. {Versteeg}},
  title     = {GHTraffic: A Dataset for Reproducible Research in Service-Oriented Computing},
  booktitle = {2018 IEEE International Conference on Web Services (ICWS)},
  year      = {2018},
  pages     = {123-130},
  month     = {July},
  abstract  = {We present GHTraffic, a dataset of significant size comprising HTTP transactions extracted from GitHub data and augmented with synthetic transaction data. The dataset facilitates reproducible research on many aspects of service-oriented computing. This paper discusses use cases for such a dataset and extracts a set of requirements from these use cases. We then discuss the design of GHTraffic, and the methods and tool used to construct it. We conclude our contribution with some selective metrics that characterise GHTraffic.},
  doi       = {10.1109/ICWS.2018.00023},
  keywords  = {data mining;data visualisation;public domain software;Web services;service-oriented computing;HTTP transactions;GitHub data;synthetic transaction data;GHTraffic dataset;Benchmark testing;Standards;Web servers;HTTP, dataset, Web services, REST, benchmarking, reproducibility, service oriented computing, service virtualisation, API, GitHub},
}

@InProceedings{7581269,
  author    = {T. {Ueda} and T. {Nakaike} and M. {Ohara}},
  title     = {Workload characterization for microservices},
  booktitle = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
  year      = {2016},
  pages     = {1-10},
  month     = {Sep.},
  abstract  = {The microservice architecture is a new framework to construct a Web service as a collection of small services that communicate with each other. It is becoming increasingly popular because it can accelerate agile software development, deployment, and operation practices. As a result, cloud service providers are expected to host an increasing number of microservices that can generate significant resource pressure on the cloud infrastructure. We want to understand the characteristics of microservice workloads to design an infrastructure optimized for microservices. In this paper, we used Acme Air, an open-source benchmark for Web services, and analyzed the behavior of two versions of the benchmark, microservice and monolithic, for two widely used language runtimes, Node.js and Java. We observed a significant overhead due to the microservice architecture; the performance of the microservice version can be 79.2% lower than the monolithic version on the same hardware configuration. On Node.js, the microservice version consumed 4.22 times more time in the libraries of Node.js than the monolithic version to process one user request. On Java, the microservice version also consumed more time in the application server than the monolithic version. We explain these performance differences from both hardware and software perspectives. We discuss the network virtualization in Docker, an infrastructure for microservices that has nonnegligible impact on performance. These findings give clues to develop optimization techniques in a language runtime and hardware for microservice workloads.},
  doi       = {10.1109/IISWC.2016.7581269},
  keywords  = {cloud computing;Java;public domain software;software architecture;software prototyping;Web services;microservice workload characterization;microservice architecture;Web service;agile software development;cloud service providers;resource pressure;cloud infrastructure;Acme Air;open-source benchmark;Java;Node.js;network virtualization;Docker;optimization techniques;Containers;Computer architecture;Java;Service-oriented architecture;Runtime;Cloud computing;microservices;microservice architecture;Node.js;WebSphere Liberty;Java;Docker;container},
}

@InProceedings{4159670,
  author    = {N. B. {Bui} and L. {Zhu} and I. {Gorton} and Y. {Liu}},
  title     = {Benchmark Generation Using Domain Specific Modeling},
  booktitle = {2007 Australian Software Engineering Conference (ASWEC'07)},
  year      = {2007},
  pages     = {169-180},
  month     = {April},
  abstract  = {Performance benchmarks are domain specific applications that are specialized to a certain set of technologies and platforms. The development of a benchmark application requires mapping the performance specific domain concepts to an implementation and producing complex technology and platform specific code. Domain specific modeling (DSM) promises to bridge the gap between application domains and implementations by allowing designers to specify solutions in domain-specific abstractions and semantics through domain specific languages (DSL). This allows generation of a final implementation automatically from high level models. The modeling and task automation benefits obtained from this approach usually justify the upfront cost involved. This paper employs a DSM based approach to invent a new DSL, DSLBench, for benchmark generation. DSLBench and its associated code generation facilities allow the design and generation of a completely deployable benchmark application for performance testing from a high level model. DSLBench is implemented using Microsoft domain specific language toolkit. It is integrated with the Visual Studio 2005 Team Suite as a plug-in to provide extra modeling capabilities for performance testing. We illustrate the approach using a case study based on .Net and C#.},
  doi       = {10.1109/ASWEC.2007.13},
  keywords  = {object-oriented programming;program compilers;program testing;software architecture;software performance evaluation;software tools;specification languages;benchmark generation;domain specific modeling;domain-specific abstractions;task automation;DSLBench;Microsoft domain specific language toolkit;Visual Studio 2005 Team Suite;code generation;Benchmark testing;DSL;Australia;Application software;Domain specific languages;Automation;Software engineering;Costs;Middleware;Automatic testing},
}

@InProceedings{8530129,
  author    = {R. {Nakazawa} and T. {Ueda} and M. {Enoki} and H. {Horii}},
  title     = {Visualization Tool for Designing Microservices with the Monolith-First Approach},
  booktitle = {2018 IEEE Working Conference on Software Visualization (VISSOFT)},
  year      = {2018},
  pages     = {32-42},
  month     = {Sep.},
  abstract  = {The microservice architecture is essential for agile development and deployment of the application components; however, designing microservices for a web application is not a straight-forward task. One of the best ways to design microservices is to decompose a monolithic prototype of an application into microservices on the basis of both the complexity in engineering and the component boundaries of the application in the early phase of development. We propose a visualization tool allowing developers to interactively design microservice applications on the basis of the characteristics of source codes and the behaviors of a monolithic prototype. This visualization tool first constructs a calling-context tree from profile data taken in a dry-run of the application. Next, it generates an initial microservice design while considering keyword features in the source codes or amount of function calls between components. Developers can interactively refine this design via this visual interface by taking four-choice actions to revise boundaries of microservices while considering expected communications between them. This interaction will have a significant impact on runtime performance. Case studies of two open-source benchmark applications demonstrate the proposed tool enables interactive design of microservices. The results of the demonstration show that compared to the official microservice designs of the applications, the proposed tool can effectively design microservice applications.},
  doi       = {10.1109/VISSOFT.2018.00012},
  keywords  = {data visualisation;Internet;service-oriented architecture;software prototyping;user interfaces;visualization tool;microservice architecture;agile development;application components;web application;monolithic prototype;source codes;initial microservice design;open-source benchmark applications;interactive design;official microservice designs;microservice applications;calling-context tree;Tools;Visualization;Data visualization;Prototypes;Computer architecture;Software;Measurement;Software visualization;microservices;monolithic application;calling context;call graph},
}

@InProceedings{8745224,
  author    = {O. {Panait} and L. {Dumitriu} and I. {Susnea}},
  title     = {Hardware and Software Architecture for Accelerating Hash Functions Based on SoC},
  booktitle = {2019 22nd International Conference on Control Systems and Computer Science (CSCS)},
  year      = {2019},
  pages     = {136-139},
  month     = {May},
  abstract  = {This paper presents an architecture for fast prototyping of hardware designs aiming to accelerate cryptographic hash functions on Xilinx ZYNQ SoC. The design include both hardware and software components, using DMA and custom AXI4-Stream IPs for maximum performance and easy integration. The communication link between the hardware module that accelerates hash functions and the processor running Linux is accessible directly from the user-space, providing fast debugging/benchmarking capabilities. As a proof-of-concept, a custom, pipelined and optimized bitcoin miner module was developed in order to demonstrate the feasibility of this infrastructure, and to evaluate its performance. The proposed solution was implemented on an Arty Z7-20 development board.},
  doi       = {10.1109/CSCS.2019.00031},
  keywords  = {cryptography;hardware-software codesign;integrated circuit design;software architecture;system-on-chip;bitcoin miner module;hardware designs;cryptographic hash functions;Xilinx ZYNQ SoC;software architecture;hardware architecture;SoC;DMA;AXI4-Stream IP;Arty Z7-20 development board;cryptography, Hash functions, FPGA, Zynq SoC, AXI4-Stream, DMA},
}

@InProceedings{6032512,
  author    = {B. {Curtis} and J. {Sappidi} and J. {Subramanyam}},
  title     = {An evaluation of the internal quality of business applications: does size matter?},
  booktitle = {2011 33rd International Conference on Software Engineering (ICSE)},
  year      = {2011},
  pages     = {711-715},
  month     = {May},
  abstract  = {This study summarizes results of a study of the internal, structural quality of 288 business applications comprising 108 million lines of code collected from 75 companies in 8 industry segments. These applications were submitted to a static analysis that evaluates quality within and across application components that may be coded in different languages. The analysis consists of evaluating the application against a repository of over 900 rules of good architectural and coding practice. Results are presented for measures of security, performance, and changeability. The effect of size on quality is evaluated, and the ability of modularity to reduce the impact of size is suggested by the results.},
  doi       = {10.1145/1985793.1985893},
  keywords  = {business data processing;software architecture;software quality;business application;internal quality;static analysis;architectural practice;coding practice;quality size;software quality;Security;Complexity theory;Industries;Government;ISO standards;Encoding;benchmarking;internal quality;maintainability performance efficiency;software metrics;software quality;static analysis},
}

@InProceedings{8811956,
  author    = {S. A. {Chowdhury} and A. {Hindle} and R. {Kazman} and T. {Shuto} and K. {Matsui} and Y. {Kamei}},
  title     = {GreenBundle: An Empirical Study on the Energy Impact of Bundled Processing},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  year      = {2019},
  pages     = {1107-1118},
  month     = {May},
  abstract  = {Energy consumption is a concern in the data-center and at the edge, on mobile devices such as smartphones. Software that consumes too much energy threatens the utility of the end-user's mobile device. Energy consumption is fundamentally a systemic kind of performance and hence it should be addressed at design time via a software architecture that supports it, rather than after release, via some form of refactoring. Unfortunately developers often lack knowledge of what kinds of designs and architectures can help address software energy consumption. In this paper we show that some simple design choices can have significant effects on energy consumption. In particular we examine the Model-View-Controller architectural pattern and demonstrate how converting to Model-View-Presenter with bundling can improve the energy performance of both benchmark systems and real world applications. We show the relationship between energy consumption and bundled and delayed view updates: bundling events in the presenter can often reduce energy consumption by 30%.},
  doi       = {10.1109/ICSE.2019.00114},
  keywords  = {energy consumption;mobile computing;power aware computing;software architecture;software maintenance;energy impact;software energy consumption;energy performance;model-view-presenter;model-view-controller architectural pattern;Energy consumption;Benchmark testing;Computer architecture;Unified modeling language;Observers;Software;Mobile handsets;software energy consumption, MVP, MVC, software architecture},
}

@InProceedings{1254495,
  author    = {F. T. {Dabous} and F. A. {Rabhi} and {Hairong YU}},
  title     = {Performance issues in integrating a capital market surveillance system using Web services},
  booktitle = {Proceedings of the Fourth International Conference on Web Information Systems Engineering, 2003. WISE 2003.},
  year      = {2003},
  pages     = {287-290},
  month     = {Dec},
  abstract  = {Internet-based technologies have opened new opportunities for conducting business within and across enterprises that were never possible a few years ago. This paper presents our experience in using Web services for prototyping a service-oriented architecture for capital market systems (CMSs). Our work exposes a world-class surveillance system's functionality into a number of Web services. Our work also includes benchmarking the performance of this legacy system and investigating the associated overheads of using SOAP as a wire format for Web services. Even though other research studies have tried to explain SOAP's performance inefficiency, there is lack of studies that evaluate SOAP in the context of a realistic business application. This initial investigation shows that system's integration opportunities introduced by Web services can outweigh the performance overheads. This occurs in some aspects of real-time CMSs that are not performance-demanding such as the dissemination of market alerts to the analysts.},
  doi       = {10.1109/WISE.2003.1254495},
  keywords  = {Internet;distributed object management;software architecture;stock markets;financial data processing;capital market surveillance system;Web services;Internet;business processes;enterprises;service-oriented architecture;benchmarking;legacy system;SOAP;business application;market alerts;distributed computing;distributed object technologies;Surveillance;Web services;Collision mitigation;Service oriented architecture;Management information systems;Web and internet services;Simple object access protocol;Degradation;Throughput;Real time systems},
}

@Article{8580420,
  author   = {X. {Zhou} and X. {Peng} and T. {Xie} and J. {Sun} and C. {Ji} and W. {Li} and D. {Ding}},
  title    = {Fault Analysis and Debugging of Microservice Systems: Industrial Survey, Benchmark System, and Empirical Study},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2018},
  pages    = {1-1},
  abstract = {The complexity and dynamism of microservice systems pose unique challenges to a variety of software engineering tasks such as fault analysis and debugging. In spite of the prevalence and importance of microservices in industry, there is limited research on the fault analysis and debugging of microservice systems. To fill this gap, we conduct an industrial survey to learn typical faults of microservice systems, current practice of debugging, and the challenges faced by developers in practice. We then develop a medium-size benchmark microservice system (being the largest and most complex open source microservice system within our knowledge) and replicate 22 industrial fault cases on it. Based on the benchmark system and the replicated fault cases, we conduct an empirical study to investigate the effectiveness of existing industrial debugging practices and whether they can be further improved by introducing the state-of-the-art tracing and visualization techniques for distributed systems. The results show that the current industrial practices of microservice debugging can be improved by employing proper tracing and visualization techniques and strategies. Our findings also suggest that there is a strong need for more intelligent trace analysis and visualization, e.g., by combining trace visualization and improved fault localization, and employing data-driven and learning-based recommendation for guided visual exploration and comparison of traces.},
  doi      = {10.1109/TSE.2018.2887384},
  keywords = {Debugging;Benchmark testing;Companies;Computer architecture;Visualization;Industries;Runtime;microservices;fault localization;tracing;visualization;debugging},
}

@Article{1049203,
  author   = {M. {Host} and E. {Johansson} and A. {Noren} and L. {Bratthall}},
  title    = {Benchmarking of processes for managing product platforms: a case study},
  journal  = {IEE Proceedings - Software},
  year     = {2002},
  volume   = {149},
  number   = {5},
  pages    = {137-142},
  month    = {Oct},
  abstract = {A case study is presented in which two organisations have participated in a benchmarking initiative to discover improvement suggestions for their processes for managing product platforms. The initiative, based on an instrument which consists of a list of questions, has been developed as part of this study and contains eight major categories of questions that guide the participating organisations to describe their processes. The descriptions were then reviewed by the organisations cross-wise in order to identify areas for improvement. The major objective of the case study is to evaluate the benchmarking procedure and instrument in practice. The result is that the benchmarking procedure with the benchmarking instrument has been well received in the study. It was therefore concluded that the approach is probably applicable for other similar organisations as well.},
  doi      = {10.1049/ip-sen:20020633},
  keywords = {software process improvement;software development management;software performance evaluation;software architecture;product platforms;benchmarking initiative;improvement suggestions;benchmarking procedure;software process improvement;software development;software management processes},
}

@InProceedings{5470885,
  author    = {S. D. {Hammond} and G. R. {Mudalige} and J. A. {Smith} and J. A. {Davis} and S. A. {Jarvis} and J. {Holt} and I. {Miller} and J. A. {Herdman} and A. {Vadgama}},
  title     = {To upgrade or not to upgrade? Catamount vs. Cray Linux Environment},
  booktitle = {2010 IEEE International Symposium on Parallel Distributed Processing, Workshops and Phd Forum (IPDPSW)},
  year      = {2010},
  pages     = {1-8},
  month     = {April},
  abstract  = {Modern supercomputers are growing in diversity and complexity - the arrival of technologies such as multi-core processors, general purpose-GPUs and specialised compute accelerators has increased the potential scientific delivery possible from such machines. This is not however without some cost, including significant increases in the sophistication and complexity of supporting operating systems and software libraries. This paper documents the development and application of methods to assess the potential performance of selecting one hardware, operating system (OS) and software stack combination against another. This is of particular interest to supercomputing centres, which routinely examine prospective software/architecture combinations and possible machine upgrades. A case study is presented that assesses the potential performance of a particle transport code on AWE's Cray XT3 8,000-core supercomputer running images of the Catamount and the Cray Linux Environment (CLE) operating systems. This work demonstrates that by running a number of small benchmarks on a test machine and network, and observing factors such as operating system noise, it is possible to speculate as to the performance impact of upgrading from one operating system to another on the system as a whole. This use of performance modelling represents an inexpensive method of examining the likely behaviour of a large supercomputer before and after an operating system upgrade; this method is also attractive if it is desirable to minimise system downtime while exploring software-system upgrades. The results show that benchmark tests run on less than 256 cores would suggest that the impact (overhead) of upgrading the operating system to CLE was less than 10%; model projections suggest that this is not the case at scale.},
  doi       = {10.1109/IPDPSW.2010.5470885},
  keywords  = {benchmark testing;Linux;software architecture;software performance evaluation;Catamount Linux Environment;Cray Linux Environment;supercomputers;multicore processors;general purpose-GPUs;specialised compute accelerators;operating systems;software library;software stack combination;software architecture;transport code;AWE's Cray XT3 8,000-core supercomputer;operating system noise;performance modelling;software-system upgrades;benchmark tests;Linux;Operating systems;Supercomputers;Benchmark testing;System testing;Multicore processing;Costs;Software libraries;Application software;Hardware},
}

@InProceedings{8526888,
  author    = {J. {von Kistowski} and S. {Eismann} and N. {Schmitt} and A. {Bauer} and J. {Grohmann} and S. {Kounev}},
  title     = {TeaStore: A Micro-Service Reference Application for Benchmarking, Modeling and Resource Management Research},
  booktitle = {2018 IEEE 26th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)},
  year      = {2018},
  pages     = {223-236},
  month     = {Sep.},
  abstract  = {Modern distributed applications offer complex performance behavior and many degrees of freedom regarding deployment and configuration. Researchers employ various methods of analysis, modeling, and management that leverage these degrees of freedom to predict or improve non-functional properties of the software under consideration. In order to demonstrate and evaluate their applicability in the real world, methods resulting from such research areas require test and reference applications that offer a range of different behaviors, as well as the necessary degrees of freedom. Existing production software is often inaccessible for researchers or closed off to instrumentation. Existing testing and benchmarking frameworks, on the other hand, are either designed for specific testing scenarios, or they do not offer the necessary degrees of freedom. Further, most test applications are difficult to deploy and run, or are outdated. In this paper, we introduce the TeaStore, a state-of-the-art micro-service-based test and reference application. TeaStore offers services with different performance characteristics and many degrees of freedom regarding deployment and configuration to be used as a benchmarking framework for researchers. The TeaStore allows evaluating performance modeling and resource management techniques; it also offers instrumented variants to enable extensive run-time analysis. We demonstrate TeaStore's use in three contexts: performance modeling, cloud resource management, and energy efficiency analysis. Our experiments show that TeaStore can be used for evaluating novel approaches in these contexts and also motivates further research in the areas of performance modeling and resource management.},
  doi       = {10.1109/MASCOTS.2018.00030},
  keywords  = {cloud computing;energy conservation;power aware computing;program testing;resource allocation;production software;benchmarking frameworks;specific testing scenarios;test applications;benchmarking framework;performance modeling;extensive run-time analysis;cloud resource management;reference applications;distributed applications;TeaStore;microservice reference;Benchmark testing;Software;Analytical models;Resource management;Predictive models;Instruments;Microservice;Benchmarking;Performance;Power;Energy Efficiency;Models;Auto Scaler;Container;Cloud},
}

@InProceedings{8035152,
  author    = {A. {Pandey} and L. {Vu} and V. {Puthiyaveettil} and H. {Sivaraman} and U. {Kurkure} and A. {Bappanadu}},
  title     = {An Automation Framework for Benchmarking and Optimizing Performance of Remote Desktops in the Cloud},
  booktitle = {2017 International Conference on High Performance Computing Simulation (HPCS)},
  year      = {2017},
  pages     = {745-752},
  month     = {July},
  abstract  = {In the current trend of moving everything into the cloud, cloud-based remote desktops are not an exception. Benchmarking virtual remote desktops for performance optimization is an important task for the successful development and deployment planning of virtual desktop infrastructure (VDI) used to deliver remote desktops. This task is very challenging at cloud scale because of rapid evolution of VDI software architectures with a very large number of remote desktops to be managed. In this paper, we present a new framework for evaluating VDI performance that has the capabilities of simulating real world VDI workloads and measuring important performance metrics at scale. Its design aims to provide facilities to easily automate the performance benchmarking tasks and the flexibility of adapting to changes in VDI software architecture, which are two major limitations of the existing solution. For evaluation, we present performance results of this framework.},
  doi       = {10.1109/HPCS.2017.113},
  keywords  = {cloud computing;software architecture;software performance evaluation;virtual machines;virtualisation;VDI software architecture;performance optimization;virtual desktop infrastructure;cloud scale;performance metrics;virtual remote desktop performance benchmarking;Benchmark testing;Cloud computing;Protocols;Servers;Tools;Virtual machining;benchmarking;remote desktop;VDI;cloud computing;performance optimization},
}

@InProceedings{1253497,
  author    = {A. J. {Kornecki} and J. {Zalewski}},
  title     = {Software development for real-time safety-critical applications},
  booktitle = {28th Annual NASA/IEEE Software Engineering Workshop, 2003. Proceedings. 28th Annual NASA/IEEE},
  year      = {2003},
  pages     = {1-72},
  month     = {Dec},
  abstract  = {We develop a software for real-time safety-critical applications. We study the fundamental concepts of real-time systems, design principles for real-time architectures, real-time programming. Rate monotonic analysis (RMA) refers to the real-time performance analysis of a system that uses static priority driven scheduling. Three application case studies: benchmark problem, air traffic control system, satellite ground control station is discussed.},
  doi       = {10.1109/SEW.2003.1253497},
  keywords  = {real-time systems;safety-critical software;software performance evaluation;software architecture;software development;real-time safety-critical applications;rate monotonic analysis;real-time performance analysis;static priority driven scheduling;benchmark problem;air traffic control system;satellite ground control station;real-time programming architecture;Programming;Application software;NASA;Conferences},
}

@InProceedings{6472569,
  author    = {H. {Soubra} and K. {Chaaban}},
  title     = {Functional Size Measurement of Electronic Control Units Software Designed Following the AUTOSAR Standard: A Measurement Guideline Based on the COSMIC ISO 19761 Standard},
  booktitle = {2012 Joint Conference of the 22nd International Workshop on Software Measurement and the 2012 Seventh International Conference on Software Process and Product Measurement},
  year      = {2012},
  pages     = {78-84},
  month     = {Oct},
  abstract  = {In their early age, E/E (electrical and electronic) systems were OEM (Original Equipment Manufacturers) specific. Adapting existing components to different environments was both time and effort consuming. AUTOSAR (AUTomotive Open System Architecture) is both a consortium founded by the major automotive companies and an architecture standard that allows collaboration on basic E/E functions while providing a platform to develop new innovative ones. One of AUTOSAR main objectives is to standardize a large number of E/E software modules in order to benefit from the reuse of these modules, it also aims to prepare for the increase in functional scope of E/E systems. Functional size measurement (FSM) has become an important task in software development projects for real-time embedded systems. Functional size can be used to estimate development effort, to manage project scope changes, to measure productivity, to benchmark, and to normalize quality and maintenance ratios. Hence, FSM goes hand in hand with AUTOSAR objectives in terms of managing automotive software projects. This paper discusses FSM in connection with AUTOSAR. In particular, it presents a guideline for measuring ECU (Electronic Control Unit) Application Software in the context of AUTOSAR.},
  doi       = {10.1109/IWSM-MENSURA.2012.19},
  keywords  = {automotive electronics;embedded systems;maintenance engineering;mechanical engineering computing;open systems;project management;size measurement;software architecture;software management;functional size measurement;electronic control units software;AUTOSAR standard;measurement guideline;COSMIC ISO 19761 standard;E/E systems;electrical-and-electronic systems;OEM;original equipment manufacturers;automotive open system architecture;automotive companies;architecture standard;E/E software modules;FSM;software development projects;real-time embedded systems;development effort estimation;project scope change management;productivity measurement;quality ratio normalization;maintenance ratio normalization;automotive software project management;ECU application software;electronic control unit;Software;Size measurement;Software measurement;Ports (Computers);Computer architecture;Standards;Actuators;AUTOSAR;E/E architecture;Electronic Control Units;Functional size measurement;real-time systems;COSMIC ISO 19761},
}

@InProceedings{6424584,
  author    = {A. {Shina} and K. {Ootsu} and T. {Ohkawa} and T. {Yokota} and T. {Baba}},
  title     = {Proposal of Incremental Software Simulation for Reduction of Evaluation Time},
  booktitle = {2012 Third International Conference on Networking and Computing},
  year      = {2012},
  pages     = {311-315},
  month     = {Dec},
  abstract  = {Software optimization techniques are necessary to fully utilize modern high-performance computer architectures. In development of the optimization techniques, repetitive simulations are needed to evaluate multiple candidates of optimization methods on partial code of program. Simulation of entire program codes generally takes huge amount of time. Therefore, a problem occurs that repeated simulations make the evaluation time enormously long. In this paper, we propose an incremental software simulation, a method for reduction of evaluation time for the development of software optimization technique by using check pointing and code substitution. Check pointing technique allows us to save and restore simulation process image. We can limit execution of the simulation only to necessary parts of the target programs by using code substitution. Our method can reduce the evaluation time by check pointing the simulation process and by resuming the execution from the checkpoint after modification of the check pointed process image. Evaluation results show that the total simulation time is reduced 34 percents on average in SPEC CPU2000 benchmark programs.},
  doi       = {10.1109/ICNC.2012.58},
  keywords  = {checkpointing;optimisation;software architecture;incremental software simulation;evaluation time;software optimization;computer architectures;program codes;check pointing;code substitution;SPEC CPU2000 benchmark programs;Computational modeling;Software;Optimization;Checkpointing;Integrated circuit modeling;Computer architecture;Image segmentation;software optimization techniques;architecture simulation;code substitution;checkpointing},
}

@InProceedings{1420112,
  author    = {V. A. {Saletore} and P. M. {Stillwell} and J. A. {Wiegert} and P. {Cayton} and J. {Gray} and G. J. {Regnier}},
  title     = {Efficient direct user level sockets for an Intel/spl reg/ Xeon/spl trade/ processor based TCP on-load engine},
  booktitle = {19th IEEE International Parallel and Distributed Processing Symposium},
  year      = {2005},
  pages     = {8 pp.-},
  month     = {April},
  abstract  = {Intel Labs has continued development of the embedded transport acceleration (ETA) software prototype that uses one of the Intel/spl reg/ Xeon/spl trade/ processors in a multi-processor server as a packet processing engine (PPE) that is closely tied to the server's core CPU and memory complex. We have further developed the prototype to provide support for user-level, asynchronous interface for sockets. The direct user socket interface (DUSI) allows user-level applications to interface directly to the PPE using familiar socket commands and semantics. The prototype runs in an asymmetric multiprocessing mode, in that the PPE does not run as a general computing resource for the host operating system. We describe the prototype software architecture, the DUSI application interface, and detail our measurement and analysis of some micro-benchmarks. In particular, we measure throughput for transactions and end-to-end latency as the key metrics for the analysis.},
  doi       = {10.1109/IPDPS.2005.191},
  keywords  = {application program interfaces;operating systems (computers);software architecture;message passing;benchmark testing;multiprocessing systems;software prototyping;embedded transport acceleration;software prototype;Intel Xeon processors;packet processing engine;direct user socket interface;asymmetric multiprocessing mode;host operating system;software architecture;TCP on-load engine;Sockets;Engines;Software prototyping;Prototypes;Application software;Acceleration;Time of arrival estimation;Embedded software;Operating systems;Software architecture},
}

@InProceedings{1620089,
  author    = {{Liming Zhu} and {Yan Liu} and I. {Gorton} and {Ngoc Bao Bui}},
  title     = {Customized Benchmark Generation Using MDA},
  booktitle = {5th Working IEEE/IFIP Conference on Software Architecture (WICSA'05)},
  year      = {2005},
  pages     = {35-44},
  month     = {Nov},
  abstract  = {This paper describes an approach for generating customized benchmark applications from a software architecture description using a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation is based on widely used open source MDA frameworks. The business logic of the benchmark application is modeled in UML and generated by taking advantage of the existing generation "cartridges" so that the current component technology can be exploited in the benchmark. This greatly reduces the effort and expertise needed for benchmarking with complex component technology. We have also extended the MDA framework to model and generate a load testing suite and automatic performance measurement infrastructure. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. This provides the potential for tightly integrating runtime performance measurement with model-based prediction either for model validation or improving model prediction accuracy. We illustrate the approach using a case study based on EJB component technology.},
  doi       = {10.1109/WICSA.2005.26},
  keywords  = {Predictive models;Benchmark testing;Application software;Measurement;Software architecture;Computer architecture;Logic;Unified modeling language;Automatic testing;Performance analysis},
}

@InProceedings{4678606,
  author    = {M. {He} and Z. {Zheng} and G. {Xue} and X. {Du}},
  title     = {Event Driven RFID Based Exhaust Gas Detection Services Oriented System Research},
  booktitle = {2008 4th International Conference on Wireless Communications, Networking and Mobile Computing},
  year      = {2008},
  pages     = {1-4},
  month     = {Oct},
  abstract  = {The vehicle exhaust gas emission is directly related with the quality of air. This paper describes the research and development of an event driven RFID based exhaust gas detection system applying the service oriented architecture for the purpose of environmental protection. An edge engine supporting the EPCglobal ALE specification, a complex event processing engine dealing with the complex streaming RFID events and service bus, which are involved in the architecture of the system, are discussed both separately and in a whole picture. The edge engine is composed of four modules, which are kernel module, device management module, events filter module, and configuration module. It provides features to encapsulate the applications from device interfaces; to process the raw observations captured by the readers and sensors; and to provide an application-level interface for managing readers and querying RFID observations. Event processing engine consists of a network of event processing agents running in parallel that interact using a dedicated event processing infrastructure, which provides an abstract communication mechanism and allows dynamic reconfiguration of the communication topology between agents at run-time. Web Services and asynchronous APIs are pivotal system implementation for flexible components reuse, dynamic configuration and optimized performance. Service bus, through its service integration and management capabilities, is the backbone of the system. Lastly, the system is proven to be effective with high performance in the benchmark.},
  doi       = {10.1109/WiCom.2008.698},
  keywords  = {air pollution;application program interfaces;environmental science computing;exhaust systems;gas sensors;radiofrequency identification;software architecture;Web services;event driven RFID;exhaust gas detection services oriented system research;vehicle exhaust gas emission;service oriented architecture;environmental protection;edge engine;EPCglobal ALE specification;complex event processing engine;complex streaming RFID events;service bus;device management;events filter;application-level interface;abstract communication;dynamic reconfiguration;communication topology;Web services;asynchronous API;flexible components reuse;Radiofrequency identification;Event detection;Engines;Vehicle dynamics;Vehicles;Research and development;Service oriented architecture;Protection;Kernel;Filters},
}

@InProceedings{4343728,
  author    = {Y. {Mitani} and T. {Matsumura} and M. {Barker} and S. {Tsuruho} and K. {Inoue} and K. {Matsumoto}},
  title     = {Proposal of a Complete Life Cycle In-Process Measurement Model Based on Evaluation of an In-Process Measurement Experiment Using a Standardized Requirement Definition Process},
  booktitle = {First International Symposium on Empirical Software Engineering and Measurement (ESEM 2007)},
  year      = {2007},
  pages     = {11-20},
  month     = {Sep.},
  abstract  = {This paper focuses on in-process measurements during requirements definition where measurements of processes and products are relatively difficult. However, development processes in Japan based on the enterprise architecture method provide standardized formats for such upstream processes and products, allowing in-process measurements. Based on previous work and on this examination of in-process measurements of requirements definition with the enterprise architecture method and previous results of empirical studies of in-process measurements and empirically validates of later development processes, this paper proposes a new measurement model, the "full in-process process and product (I-PAP) measurement model," which includes the complete software development process from requirements to maintenance. Standardization of the requirements definition phase using the enterprise architecture method in Japan allows in-process measurement across the complete development lifecycle. Combining this with collaborative filtering and a project benchmark database will support project evaluation, estimation, and prediction.},
  doi       = {10.1109/ESEM.2007.27},
  keywords  = {software architecture;software metrics;standardisation;complete life cycle in-process measurement model;in-process measurement experiment evaluation;standardized requirement definition process;development processes;Japan;enterprise architecture;full in-process process and product measurement model;complete software development process;standardization;requirements definition phase;complete development lifecycle;I-PAP;Proposals;Software measurement;Phase measurement;Programming;Testing;Computer architecture;Collaboration;Filtering;Project management;History;Empirical software engineering;Software process measurement;In-process measurement;Enterprise Architecture;Requirement definition phase measurement.},
}

@InProceedings{7203089,
  author    = {S. {Bandyopadhyay} and D. {Sarkar} and C. {Mandal}},
  title     = {Poster: An Efficient Equivalence Checking Method for Petri Net Based Models of Programs},
  booktitle = {2015 IEEE/ACM 37th IEEE International Conference on Software Engineering},
  year      = {2015},
  volume    = {2},
  pages     = {827-828},
  month     = {May},
  abstract  = {The initial behavioural specification of any software programs goes through significant optimizing and parallelizing transformations, automated and also human guided, before being mapped to an architecture. Establishing validity of these transformations is crucial to ensure that they preserve the original behaviour. PRES+ model (Petri net based Representation of Embedded Systems) encompassing data processing is used to model parallel behaviours. Being value based with inherent scope of capturing parallelism, PRES+ models depict such data dependencies more directly; accordingly, they are likely to be more convenient as the intermediate representations (IRs) of both the source and the transformed codes for translation validation than strictly sequential variable-based IRs like Finite State Machines with Data path (FSMDs) (which are essentially sequential control flow graphs (CFGs)). In this work, a path based equivalence checking method for PRES+ models is presented.},
  doi       = {10.1109/ICSE.2015.268},
  keywords  = {embedded systems;formal specification;parallel programming;Petri nets;program interpreters;program verification;software architecture;source code (software);Petri net based models;behavioural specification;software programs;optimizing transformations;parallelizing transformations;architecture;PRES+ model;Petri net based representation of embedded systems;data processing;parallel behaviours;parallelism;data dependencies;intermediate representations;source codes;transformed codes;translation validation;sequential variable-based IR;finite state machines with data path;FSMD;sequential control flow graphs;CFG;path based equivalence checking method;Computational modeling;Petri nets;Data models;Sparks;Benchmark testing;Embedded systems;Equivalence checking;PRES+ model;FSMD model},
}

@Article{8556019,
  author   = {L. H. {Kellogg} and L. J. {Hwang} and R. {Gassmöller} and W. {Bangerth} and T. {Heister}},
  title    = {The Role of Scientific Communities in Creating Reusable Software: Lessons From Geophysics},
  journal  = {Computing in Science Engineering},
  year     = {2019},
  volume   = {21},
  number   = {2},
  pages    = {25-35},
  month    = {March},
  abstract = {The domain of geophysics has historically been a driver of scientific software development due to the size, complexity, and societal importance of the research questions. Geophysical computation complements field observation, laboratory analysis, experiment, and theory. Specialized scientific software is regularly developed by geophysicists in collaboration with computational scientists and applied mathematicians; in this cross-disciplinary environment, reusability is critically important both to preserve the intellectual investment and to ensure the quality of the research and its replicability. The Computational Infrastructure for Geodynamics (CIG) is a “community of practice” that advances Earth science by developing and disseminating software for geophysics and related fields. We discuss CIG's best practices, lessons learned, and community practices, and highlight how development of high-quality, reusable scientific software has accelerated scientific discovery by enabling simulations of the dynamics of Earth's surface and interior across a wide spectrum of problems using resources from laptops to leadership-class supercomputers.},
  doi      = {10.1109/MCSE.2018.2883326},
  keywords = {geophysics computing;parallel machines;software architecture;software engineering;software reusability;scientific communities;creating reusable software;geophysics;scientific software development;societal importance;geophysical computation complements field observation;laboratory analysis;specialized scientific software;computational scientists;cross-disciplinary environment;reusability;intellectual investment;community practices;reusable scientific software;scientific discovery;Earth science;computational infrastructure;Software;Best practices;Geodynamics;Tutorials;Benchmark testing;Training},
}

@InProceedings{6522016,
  author    = {L. {Lafi} and J. {Feki} and S. {Hammoudi}},
  title     = {Metamodel matching techniques evaluation and benchmarking},
  booktitle = {2013 International Conference on Computer Applications Technology (ICCAT)},
  year      = {2013},
  pages     = {1-6},
  month     = {Jan},
  abstract  = {During the last decades, the matching operation has been thoroughly studied in database systems and ontology development. Matching is critical in many application domains such as semantic web, schema and data integration, data translation. In the Model Driven Engineering (MDE) context, matching techniques between two metamodels are the centerpiece of a semi-automatic transformation process. Metamodel matching allows discovering mappings between two metamodels and then generating transformation rules. In this paper, we propose an approach for metamodel matching techniques evaluation and benchmarking. This approach firstly provides a benchmark for metamodel matching and, secondly prepares to semi-automate the transformation process according to the Model Driven Architecture (MDA). A benchmark is useful to compare the strengths and weaknesses of different metamodel matching techniques. Experimental results comparing the effectiveness of the application of various implementations of four recent techniques (Similarity Flooding, ModeICVS, SAMT4MDE andAML) on real-world metamodels are given.},
  doi       = {10.1109/ICCAT.2013.6522016},
  keywords  = {benchmark testing;formal specification;learning (artificial intelligence);ontologies (artificial intelligence);pattern matching;software architecture;metamodel matching technique evaluation;AML;SAMT4MDE;ModeICVS;similarity flooding;MDA;model driven architecture;transformation rule generation;metamodel mapping;semiautomatic transformation process;model driven engineering;data translation;data integration;schema;semantic Web;ontology development;database system;matching operation;benchmarking;Benchmark testing;Pattern matching;Ontologies;Size measurement;Encoding;Context;Standards;Metamodel matching;Benchmarking;Comparison Criteria;Comparison and Evaluation},
}

@InProceedings{8247456,
  author    = {B. F. {Andrés} and M. {Pérez}},
  title     = {Transpiler-based architecture for multi-platform web applications},
  booktitle = {2017 IEEE Second Ecuador Technical Chapters Meeting (ETCM)},
  year      = {2017},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {A Transpiler is a set of tools that take the source code developed in a high-level programming language (source language), and after a transpilation process, it generates a translated source code written in another programming language (target language), that is syntactically equivalent. In this paper, it is presented a new approach for software architecture design, which uses a Transpiler (Haxe) as a programming language and as a generator of the business logic and service layer. The goal is that software developers write the business logic only once and transpile it to equivalent server layers for C#, Java, and PHP, making them reusable and runnable on several platforms. An experimental prototype was developed to test the idea (published on GitHub with the codename “Cornerstone”), that implements a benchmark to compare when running on different Operating Systems and Application Servers combinations. A single web interface is developed, using HTML, CSS, Javascript, and XML-HttpRequest, which connect to any of the transpiled services regardless of the deployment platform. Cloud instances were used to assure comparability. Preliminary results were obtained and analyzed to show the concept validity and which platform combination provided the highest performance.},
  doi       = {10.1109/ETCM.2017.8247456},
  keywords  = {Internet;Java;operating systems (computers);software architecture;XML;HTML;CSS;Javascript;XML-HttpRequest;cloud instances;transpilation process;source language;high-level programming language;multiplatform web applications;deployment platform;transpiled services;single web interface;Application Servers combinations;equivalent server layers;software developers;service layer;business logic;Transpiler;software architecture design;target language;translated source code;Nickel;Servers;Business;Java;Computer architecture;Software;Software Architecture;Software Design;Transpiler;Multi-Platform;Haxe;Web Applications;Business Logic;Benchmark;C#;Java;PHP},
}

@InProceedings{7781795,
  author    = {M. {Aniche} and C. {Treude} and A. {Zaidman} and A. v. {Deursen} and M. A. {Gerosa}},
  title     = {SATT: Tailoring Code Metric Thresholds for Different Software Architectures},
  booktitle = {2016 IEEE 16th International Working Conference on Source Code Analysis and Manipulation (SCAM)},
  year      = {2016},
  pages     = {41-50},
  month     = {Oct},
  abstract  = {Code metric analysis is a well-known approach for assessing the quality of a software system. However, current tools and techniques do not take the system architecture (e.g., MVC, Android) into account. This means that all classes are assessed similarly, regardless of their specific responsibilities. In this paper, we propose SATT (Software Architecture Tailored Thresholds), an approach that detects whether an architectural role is considerably different from others in the system in terms of code metrics, and provides a specific threshold for that role. We evaluated our approach on 2 different architectures (MVC and Android) in more than 400 projects. We also interviewed 6 experts in order to explain why some architectural roles are different from others. Our results shows that SATT can overcome issues that traditional approaches have, especially when some architectural role presents very different metric values than others.},
  doi       = {10.1109/SCAM.2016.19},
  keywords  = {software architecture;software quality;SATT;tailoring code metric thresholds;code metric analysis;software system quality;system architecture;software architecture tailored thresholds;Measurement;Benchmark testing;Androids;Humanoid robots;Systems architecture;Computer architecture;Software architecture},
}

@InProceedings{7930225,
  author    = {R. {Yasaweerasinghelage} and M. {Staples} and I. {Weber}},
  title     = {Predicting Latency of Blockchain-Based Systems Using Architectural Modelling and Simulation},
  booktitle = {2017 IEEE International Conference on Software Architecture (ICSA)},
  year      = {2017},
  pages     = {253-256},
  month     = {April},
  abstract  = {Blockchain is an emerging technology for sharing transactional data and computation without using a central trusted third party. It is an architectural choice to use a blockchain instead of traditional databases or protocols, and this creates trade-offs between non-functional requirements such as performance, cost, and security. However, little is known about predicting the behaviour of blockchain-based systems. This paper shows the feasibility of using architectural performance modelling and simulation tools to predict the latency of blockchain-based systems. We use established tools and techniques, but explore new blockchain-specific issues such as the configuration of the number of confirmation blocks and inter-block times. We report on a lab-based experimental study using an incident management system, showing predictions of median system level response time with a relative error mostly under 10%. We discuss how the approach can be used to support architectural decision-making, during the design of blockchain-based systems.},
  doi       = {10.1109/ICSA.2017.22},
  keywords  = {decision making;digital simulation;distributed databases;latency prediction;blockchain-based systems;architectural modelling;transactional data sharing;nonfunctional requirements;architectural performance modelling;simulation tools;blockchain-specific issues;incident management system;median system level response time;architectural decision-making;Benchmark testing;Predictive models;Contracts;Time measurement;Databases;Decision making;Software architecture;Software performance;Distributed databases},
}

@InProceedings{5090522,
  author    = {Y. {Chen} and A. {Romanovsky} and A. {Gorbenko} and V. {Kharchenko} and S. {Mamutov} and O. {Tarasyuk}},
  title     = {Benchmarking Dependability of a System Biology Application},
  booktitle = {2009 14th IEEE International Conference on Engineering of Complex Computer Systems},
  year      = {2009},
  pages     = {146-153},
  month     = {June},
  abstract  = {In this paper we report our practical experience in benchmarking a System Biology Web Service, and investigate instability of its performance and the delays induced by the communication medium. We discuss the results of a statistical data analysis and discuss the causes affecting the Web Service performance. The uncertainty discovered in Web Services operations reduces the overall dependability of Service-Oriented Architecture and require specific resilience techniques.},
  doi       = {10.1109/ICECCS.2009.20},
  keywords  = {bioinformatics;statistical analysis;Web services;benchmarking dependability;system biology Web service;communication medium;statistical data analysis;service-oriented architecture;bioinformatics;Systems biology;Web services;Service oriented architecture;Biology computing;Data analysis;Application software;Aging;Resilience;Web and internet services;Buildings;web services;system biology application;dependability;benchmarking;performance instability},
}

@InProceedings{6613845,
  author    = {L. {Pruijt} and C. {Köppe} and S. {Brinkkemper}},
  title     = {On the accuracy of Architecture Compliance Checking support Accuracy of dependency analysis and violation reporting},
  booktitle = {2013 21st International Conference on Program Comprehension (ICPC)},
  year      = {2013},
  pages     = {172-181},
  month     = {May},
  abstract  = {Architecture Compliance Checking (ACC) is useful to bridge the gap between architecture and implementation. ACC is an approach to verify conformance of implemented program code to high-level models of architectural design. Static ACC focuses on the modular software architecture and on the existence of rule violating dependencies between modules. Accurate tool support is essential for effective and efficient ACC. This paper presents a study on the accuracy of ACC tools regarding dependency analysis and violation reporting. Seven tools were tested and compared by means of a custom-made test application. In addition, the code of open source system Freemind was used to compare the tools on the number and precision of reported violation and dependency messages. On the average, 74 percent of 34 dependency types in our custom-made test software were reported, while 69 percent of 109 violating dependencies within a module of Freemind were reported. The test results show large differences between the tools, but all tools could improve the accuracy of the reported dependencies and violations.},
  doi       = {10.1109/ICPC.2013.6613845},
  keywords  = {conformance testing;program diagnostics;program testing;program verification;public domain software;software architecture;software tools;architecture compliance checking support accuracy;static ACC;dependency analysis;violation reporting;implemented program code conformance verification;architectural design;custom-made test application;open source system code;Freemind;dependency types;Computer architecture;Accuracy;Java;Benchmark testing;Sonar;Software architecture;Software;Software architecture;modular architecture;architecture compliance;architecture conformance;static analysis;dependency analysis;dependency detection;accuracy},
}

@InProceedings{6405430,
  author    = {N. {Antunes} and M. {Vieira}},
  title     = {Detecting Vulnerabilities in Service Oriented Architectures},
  booktitle = {2012 IEEE 23rd International Symposium on Software Reliability Engineering Workshops},
  year      = {2012},
  pages     = {134-139},
  month     = {Nov},
  abstract  = {The adoption of Service Oriented Architectures (SOAs) in a wide range of organizations, including business-critical systems, opens the door to new security challenges. Although the services used should be secure and reliable, they are often deployed with security bugs that can be maliciously exploited. The problem is that developers are frequently not specialized on security and the common time-to-market constraints limits an in depth test for vulnerabilities. Additionally, research and practice shows that the effectiveness of existing vulnerability detection tools is very poor. The goal of this work is to advance the state-of-the-art by investigating new techniques and tools to effectively detect vulnerabilities in SOAs in an automated manner. Instrumental in this work is to propose a benchmarking approach that allows assessing and comparing vulnerability detection tools, thus helping guiding tools development and improvement, and allowing users to select the most effective ones according to specific needs.},
  doi       = {10.1109/ISSREW.2012.33},
  keywords  = {security of data;service-oriented architecture;service oriented architectures;SOA;business-critical systems;security bugs;common time-to-market constraints limits;vulnerability detection tools;benchmarking approach;guiding tools development;Security;Benchmark testing;Service oriented architecture;Monitoring;Runtime;SOA;web services;vulnerability detection;security;vulnerabilities;benchmarking},
}

@InProceedings{4575113,
  author    = {A. {Andrzejak} and L. {Silva}},
  title     = {Using machine learning for non-intrusive modeling and prediction of software aging},
  booktitle = {NOMS 2008 - 2008 IEEE Network Operations and Management Symposium},
  year      = {2008},
  pages     = {25-32},
  month     = {April},
  abstract  = {The wide-spread phenomenon of software (running image) aging is known to cause performance degradation, transient failures or even crashes of applications. In this work we describe first a method for monitoring and modeling of performance degradation in SOA applications, particularly application servers. This method works for a large class of the aging processes caused by resource depletion (e.g. memory leaks). It can be deployed non-intrusively in a production environment, under arbitrary service request distributions. Based on this schema we investigate in the second part of the paper how machine learning (classification) algorithms can be used for proactive detection of performance degradation or sudden drops caused by aging. We leverage the predictive power of these algorithms with several techniques to make the measurement-based aging models more adaptive and more robust against transient failures. We evaluate several state-of-the-art classification methods for their accuracy and computational efficiency in this scenario. The studies are performed on a data set generated by a TPC-W benchmark instrumented with a memory leak injector. The results show that the probing method yields accurate aging models with low overhead and the machine learning approach gives statistically significant short-term predictions of degrading application performance. Both approaches can be used directly to fight aging via adaptive software rejuvenation (restart of the application), for operator alerting, or for short-term capacity planning.},
  doi       = {10.1109/NOMS.2008.4575113},
  keywords  = {learning (artificial intelligence);resource allocation;software architecture;software performance evaluation;storage management;system monitoring;machine learning;nonintrusive modeling;software aging prediction;performance degradation;transient failure;application crash;system monitoring;SOA applications;application servers;resource depletion;memory leaks;service request distribution;memory leak injector;probing method;adaptive software rejuvenation;capacity planning;Machine learning;Predictive models;Aging;Application software;Degradation;Machine learning algorithms;Software performance;Computer crashes;Condition monitoring;Service oriented architecture},
}

@InProceedings{6114843,
  author    = {V. B. C. {C.} and U. B. {Corrêa} and L. {Carro}},
  title     = {Performance Overhead from the Usage of Software Abstraction on Complex Embedded Systems},
  booktitle = {2011 Brazilian Symposium on Computing System Engineering},
  year      = {2011},
  pages     = {111-114},
  month     = {Nov},
  abstract  = {Nowadays major embedded systems functionalities are developed in software. Moreover, to attend the market exigencies the software productivity has to be improves. This work analyzes the overhead caused by the application of abstraction levels in embedded systems software development. This analysis was done based in Google Android. The obtained results showed that even about 80% of the executed instructions where in the lower levels from the layer architecture in applications that reuse framework components.},
  doi       = {10.1109/SBESC.2011.39},
  keywords  = {embedded systems;Linux;operating systems (computers);software architecture;software performance evaluation;software reusability;performance overhead;software abstraction;complex embedded systems;software productivity;embedded systems software development;Google Android;layer architecture;framework component reuse;Humanoid robots;Androids;Benchmark testing;Smart phones;Kernel;Google;Performance evaluation;layer architectures;Android},
}

@Article{7426807,
  author   = {S. {Hunold} and A. {Carpen-Amarie}},
  title    = {Reproducible MPI Benchmarking is Still Not as Easy as You Think},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2016},
  volume   = {27},
  number   = {12},
  pages    = {3617-3630},
  month    = {Dec},
  abstract = {The Message Passing Interface (MPI) is the prevalent programming model used on today's supercomputers. Therefore, MPI library developers are looking for the best possible performance (shortest run-time) of individual MPI functions across many different supercomputer architectures. Several MPI benchmark suites have been developed to assess the performance of MPI implementations. Unfortunately, the outcome of these benchmarks is often neither reproducible nor statistically sound. To overcome these issues, we show which experimental factors have an impact on the run-time of blocking collective MPI operations and how to measure their effect. Finally, we present a new experimental method that allows us to obtain reproducible and statistically sound measurements of MPI functions.},
  doi      = {10.1109/TPDS.2016.2539167},
  keywords = {application program interfaces;benchmark testing;message passing;parallel machines;software architecture;statistical analysis;MPI benchmarking;message passing interface;prevalent programming;MPI library;supercomputer architectures;statistical analysis;Benchmark testing;Time measurement;Statistical analysis;Supercomputers;MPI;benchmarking;reproducibility;statistical analysis},
}

@Article{5728829,
  author   = {A. {Kalbasi} and D. {Krishnamurthy} and J. {Rolia} and S. {Dawson}},
  title    = {DEC: Service Demand Estimation with Confidence},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2012},
  volume   = {38},
  number   = {3},
  pages    = {561-578},
  month    = {May},
  abstract = {We present a new technique for predicting the resource demand requirements of services implemented by multitier systems. Accurate demand estimates are essential to ensure the efficient provisioning of services in an increasingly service-oriented world. The demand estimation technique proposed in this paper has several advantages compared with regression-based demand estimation techniques, which many practitioners employ today. In contrast to regression, it does not suffer from the problem of multicollinearity, it provides more reliable aggregate resource demand and confidence interval predictions, and it offers a measurement-based validation test. The technique can be used to support system sizing and capacity planning exercises, costing and pricing exercises, and to predict the impact of changes to a service upon different service customers.},
  doi      = {10.1109/TSE.2011.23},
  keywords = {multiprocessing systems;regression analysis;service-oriented architecture;DEC;service demand estimation technique;resource demand requirements;multitier systems;service-oriented world;regression-based demand estimation techniques;multicollinearity;system sizing;capacity planning;Benchmark testing;Equations;Software;Mathematical model;Estimation;Frequency modulation;Computers;Benchmarking;resource demand prediction;statistical regression.},
}

@InProceedings{8117113,
  author    = {Y. {Wu} and M. {He} and S. {Shen} and X. {Chen} and Z. {Liu} and Y. {Zheng}},
  title     = {A Model-Based Fault Tolerance Configuration Framework for Component-Based Systems},
  booktitle = {2017 International Conference on Green Informatics (ICGI)},
  year      = {2017},
  pages     = {252-262},
  month     = {Aug},
  abstract  = {Fault tolerance (FT) is well studied and practicedin the past decades to enhance reliability of software systems. Although many sophisticated fault tolerance mechanisms have been developed, they are difficult to be applied to a given systemfor the following reasons. Firstly, the comprehensive analysis of the target system is a challenging task. Secondly, it is hard todetermine which fault tolerance mechanisms are suitable for a given system. Thirdly, the configuration of fault tolerance mechanisms is complex. To address these challenges, this paper presents a model-based framework to achieve semi-automatic configuration of fault tolerance mechanisms for component-basedsystems. The system administrators just need to construct the Runtime Software Architecture (RSA) of the target system. The other steps will be carried out automatically. To validate our proposed approach, extensive experiments are conducted to employ a popular Java enterprise benchmark running in our cloud platform.},
  doi       = {10.1109/ICGI.2017.40},
  keywords  = {cloud computing;Java;object-oriented programming;software architecture;software fault tolerance;software systems;sophisticated fault tolerance mechanisms;target system;system administrators;model-based fault tolerance configuration framework;component-based systems;FT;runtime software architecture;RSA;Java enterprise benchmark;cloud platform;Software reliability;Fault tolerance;Fault tolerant systems;Connectors;Unified modeling language;Runtime;Fault tolerance;reliability;model driven;fuzzy reasoning;runtime software architecture},
}

@InProceedings{7847993,
  author    = {R. {Yoshimoto} and T. {Kadono} and K. {Hisazumi} and A. {Fukuda}},
  title     = {A software energy analysis method using ExecutableUML},
  booktitle = {2016 IEEE Region 10 Conference (TENCON)},
  year      = {2016},
  pages     = {218-221},
  month     = {Nov},
  abstract  = {The power consumption of an embedded system has been increased. It is important to analyze energy consumption while taking software behaviors into account to reduce energy consumption. Some studies propose model-based energy analysis. Although these method can estimate energy usage within a short time as compared with code-based analysis, the problem is that these technique cannot detect the bottleneck of energy consumption in software behavior in the design stage. This paper proposes an energy consumption analysis method for multi-granularity of ExecutableUML models. The method is useful for detecting the bottleneck of energy consumption in software behavior in the design stage. We also demonstrate that the error of our energy analysis method is, on average, 9.0%.},
  doi       = {10.1109/TENCON.2016.7847993},
  keywords  = {power consumption;power engineering computing;software architecture;Unified Modeling Language;software energy analysis method;power consumption;embedded system;energy consumption;software behaviors;energy usage estimation;code-based analysis;ExecutableUML models;multigranularity;design stage;Unified modeling language;Energy consumption;Software;Power demand;Mathematical model;Analytical models;Benchmark testing},
}

@InProceedings{1659476,
  author    = {L. {Silva} and H. {Madeira} and J. G. {Silva}},
  title     = {Software Aging and Rejuvenation in a SOAP-based Server},
  booktitle = {Fifth IEEE International Symposium on Network Computing and Applications (NCA'06)},
  year      = {2006},
  pages     = {56-65},
  month     = {July},
  abstract  = {Web-services and service-oriented architectures are gaining momentum in the area of distributed systems and Internet applications. However, as we increase the abstraction level of the applications we are also increasing the complexity of the underlying middleware. In this paper, we present a dependability benchmarking study to evaluate and compare the robustness of some of the most popular SOAP-RPC implementations that are intensively used in the industry. The study was focused on Apache Axis where we have observed a high susceptibility of software aging. Building on these results we propose a new SLA-oriented software rejuvenation technique that proved to be a simple way to increase the dependability of the SOAP-server, the degree of self-healing and to maintain a sustained level of performance in the applications},
  doi       = {10.1109/NCA.2006.51},
  keywords  = {access protocols;file servers;Internet;middleware;remote procedure calls;software maintenance;software aging;software rejuvenation;SOAP-based server;Web-service;service-oriented architecture;distributed system;Internet;middleware;RPC implementation;remote procedure call;Aging;Service oriented architecture;Simple object access protocol;Application software;Middleware;Operating systems;Software tools;Production systems;Computer networks;Computer vision},
}

@InProceedings{8417120,
  author    = {S. {García} and C. {Menghi} and P. {Pelliccione} and T. {Berger} and R. {Wohlrab}},
  title     = {An Architecture for Decentralized, Collaborative, and Autonomous Robots},
  booktitle = {2018 IEEE International Conference on Software Architecture (ICSA)},
  year      = {2018},
  pages     = {75-7509},
  month     = {April},
  abstract  = {Robotic applications are typically realized using ad hoc and domain-specific solutions, which challenges the engineering and cross-project reuse of such applications. Especially in complex scenarios, where self-adaptive robots collaborate among themselves or with humans, the effective and systematic engineering of such applications is becoming increasingly important. Such scenarios require decentralized software architectures that foster fault-tolerant ways of managing large teams of (possibly) heterogeneous robots. To the best of our knowledge, no existing architecture for robot applications supports decentralized and self-adaptive collaboration. To address this gap, we conducted a design science study with 21 practitioners and experts in the field of robotics to develop an architecture fulfilling these requirements through several iterations. We present SERA, an architecture for robot applications that supports human-robot collaboration, as well as adaptation and coordination of single- and multi-robot systems in a decentralized fashion. SERA is based on layers that contain components that manage the adaptation at different levels of abstraction and communicate through well-defined interfaces. We successfully validated SERA by considering a set of real scenarios, by both using simulators and real robots, by involving robotic experts, and by benchmarking it with state-of-the-art solutions.},
  doi       = {10.1109/ICSA.2018.00017},
  keywords  = {control engineering computing;human-robot interaction;multi-robot systems;software architecture;autonomous robots;robotic applications;decentralized software architectures;heterogeneous robots;robot applications;self-adaptive collaboration;human-robot collaboration;multirobot systems;self-adaptive robots;SERA architecture;Robot kinematics;Computer architecture;Collaboration;Service robots;Task analysis;Software;Software architecture;Robotics;Human robot collaboration;Self-adaptive systems;Decentralized systems},
}

@InProceedings{8275316,
  author    = {M. {Moness} and A. M. {Moustafa} and A. H. {Muhammad} and A. A. {Younis}},
  title     = {Hybrid controller for a software-defined architecture of industrial internet lab-scale process},
  booktitle = {2017 12th International Conference on Computer Engineering and Systems (ICCES)},
  year      = {2017},
  pages     = {266-271},
  month     = {Dec},
  abstract  = {Internet of Things (IoT) is a thriving trend that has invaded many aspects of real life. The merging of IoT with industrial information is forming a new emerging direction of Industrial Internet of Things (I2oT). I2oT requires reliable methods for co-design of control and automation systems that can align their performance within deep and complex cyber layers of communication and computation. This paper investigates the utilization of hybrid control approach for modeling and analyzing distributed control of a quadruple-tank as a lab-scale benchmark process. The system is implemented via a software-defined architecture for I2oT using low-cost commercial IoT-enabled Intel Galileo Boards. The proposed I2oT software architecture utilizes Node.js and JavaScript for all different layers of the system with web sockets for handling real-time control packets. Node.js is considered reliable for event-driven scheme applications such as I2oT due to its optimal performance and resource utilization.},
  doi       = {10.1109/ICCES.2017.8275316},
  keywords  = {control engineering computing;distributed control;Internet of Things;Java;process control;production engineering computing;software architecture;software reliability;I2oT software architecture;quadruple-tank;Intel Galileo Boards;Node.js;JavaScript;web sockets;software-defined architecture;Industrial Internet of Things;complex cyber layers;deep cyber layers;automation systems;industrial information;industrial internet lab-scale process;hybrid controller;real-time control packets;low-cost commercial IoT;lab-scale benchmark process;distributed control;hybrid control approach;Computer architecture;Sensors;Mathematical model;Process control;Servers;Internet of Things;Cyber-physical systems (CPS);hybrid control;Industrial Internet;Industrial Internet of Things (I2oT);Internet of Things (IoT);Quadruple Tank Process (QTP);Node.js;software-defined architecture;switched systems},
}

@InProceedings{7095818,
  author    = {Z. {Hadjilambrou} and M. {Kleanthous} and Y. {Sazeides}},
  title     = {Characterization and analysis of a web search benchmark},
  booktitle = {2015 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2015},
  pages     = {328-337},
  month     = {March},
  abstract  = {Web search as a service is very impressive. Web search runs on thousands of servers which perform search on an index of billions of web pages. The search results must be both relevant to the user queries and reach the user in a fraction of a second. A web search service must guarantee the same QoS at all times even at the peak incoming traffic load. Not unjustifiably the web search service has attracted a lot of research attention. Despite the high research interest web search has gained, there are still plenty unknown about the functionality and the architecture of web search benchmarks. Much research has been done using commercial web search engines, like Bing or Google, but many details of these search engines are, of course, not disclosed to the public. We take an academically accepted web search benchmark and we perform a thorough characterization and analysis of it. We shed light in to the architecture and the functionality of the benchmark. We also investigate some prominent web search research issues. In particular, we study how intra-server index partitioning affects the response time and throughput and we also explore the potential use of low power servers for web search. Our results show that intra-server partitioning can reduce tail latencies and that low power servers given enough partitioning can provide same response times as conventional high performance servers.},
  doi       = {10.1109/ISPASS.2015.7095818},
  keywords  = {Internet;quality of service;query processing;search engines;software architecture;user query;QoS;Web search engine;benchmark architecture;benchmark functionality;intraserver index partitioning;Web search benchmark;Servers;Indexes;Benchmark testing;Web search;Time factors;Instruction sets;Engines},
}

@InProceedings{6950070,
  author    = {R. {Reeta} and A. K. {Mariappan}},
  title     = {An approach to assure QoS for dynamically reconfigurable component-based software systems},
  booktitle = {2014 International Conference on Communication and Signal Processing},
  year      = {2014},
  pages     = {1353-1357},
  month     = {April},
  abstract  = {The main concept of dynamic reconfiguration is to make a system to change from one configuration to another during run time, without shutting down and rebooting the system. This is very helpful to avoid downtime during software maintenance. Providing various quality of services (QoS) namely, adaptability, availability and maintainability are the major challenges faced during dynamic reconfiguration for component based systems. This work argues that the main benefit to make dynamic reconfiguration for component systems is to minimize application disruption. A quantitative evaluation for QoS assurance to the proposed work is conducted in two steps. First, reconfigurable component model is implemented using reconfiguration strategies. Second, each reconfiguration strategy is tested using reconfiguration benchmark results and then, testing results are evaluated. Thus the QoS characteristics can be achieved using some constraints under some acceptable environment.},
  doi       = {10.1109/ICCSP.2014.6950070},
  keywords  = {quality of service;software architecture;software maintenance;software quality;dynamically reconfigurable component-based software systems;software maintenance;quality of services;software adaptability;software availability;software maintainability;component based systems;QoS assurance;reconfigurable component model;reconfiguration strategies;Quality of service;Connectors;Operating systems;ISO standards;Analytical models;Conferences;dynamic reconfiguration;operating systems;support for adaptation;QoS assurance},
}

@InProceedings{6916614,
  author    = {G. {Boutheina} and Y. {Baghdadi} and N. {Kraiem}},
  title     = {Toward a guidance framework for Service Oriented method engineering},
  booktitle = {2014 World Congress on Computer Applications and Information Systems (WCCAIS)},
  year      = {2014},
  pages     = {1-6},
  month     = {Jan},
  abstract  = {The emergence of the paradigms Service Orientation (SO) and Service Oriented Computing (SOC) has resulted in new types of applications, referred to as Service Based Applications. This has entailed many service oriented methods for developing such kinds of applications. In this paper, we first present most of the well-known methods from academia and industry, focusing on their engineering approaches and comparison frameworks. Then we highlight their strengths and weaknesses. Finally, we introduce the elements of a comprehensive framework to guide the engineering of new methods or benchmarking the existing ones.},
  doi       = {10.1109/WCCAIS.2014.6916614},
  keywords  = {service-oriented architecture;service oriented computing;service based applications;engineering approaches;service oriented method engineering;guidance framework;Unified modeling language;Business;Aggregates;Buildings;Computer architecture;Architecture;Monitoring;Service Orientation;Service Oriented Software Engineering;Engineering Methods;Comparison Frameworks},
}

@InProceedings{6189228,
  author    = {D. {Tiwari} and Y. {Solihin}},
  title     = {Architectural characterization and similarity analysis of sunspider and Google's V8 Javascript benchmarks},
  booktitle = {2012 IEEE International Symposium on Performance Analysis of Systems Software},
  year      = {2012},
  pages     = {221-232},
  month     = {April},
  abstract  = {Today, more than 99% of web-browsers are enabled with Javascript capabilities, and Javascript's popularity is only going to increase in the future. However, due to bytecode interpretation, Javascript codes suffer from severe performance penalty (up to 50x slower) compared to the corresponding native C/C++ code. We recognize that the first step to bridge this performance gap is to understand the the architectural execution characteristics of Javascript benchmarks. Therefore, this paper presents an in-depth architectural characterization of widely used V8 and Sunspider Javascript benchmarks using Google's V8 javascript engine. Using statistical data analysis techniques, our characterization study discovers and explains correlation among different execution characteristics in microarchitecture dependent as well as microarchitecture independent fashion. Furthermore, our study measures (dis)similarity among 33 different Javascript benchmarks and discusses its implications. Given the widespread use of Javascripts, we believe our findings are useful for both performance analysis and benchmarking communities.},
  doi       = {10.1109/ISPASS.2012.6189228},
  keywords  = {benchmark testing;data analysis;Java;online front-ends;program diagnostics;software architecture;software performance evaluation;statistical analysis;architectural characterization;similarity analysis;Sunspider Javascript benchmark;Google V8 Javascript benchmark;Web browser;Javascript capability;bytecode interpretation;Javascript codes;performance penalty;native C code;C++ code;performance gap;architectural execution characteristics;Google V8 Javascript engine;performance analysis;statistical data analysis technique;Benchmark testing;Principal component analysis;Microarchitecture;Hardware;Radiation detectors;Cryptography;Google},
}

@InProceedings{7516002,
  author    = {N. R. {Tallent} and J. B. {Manzano} and N. A. {Gawande} and S. {Kang} and D. J. {Kerbyson} and A. {Hoisie} and J. K. {Cross}},
  title     = {Algorithm and Architecture Independent Benchmarking with SEAK},
  booktitle = {2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  year      = {2016},
  pages     = {63-72},
  month     = {May},
  abstract  = {Many applications of high performance embedded computing are constrained by performance or power bottlenecks. We designed a new benchmark suite, the Suite for Embedded Applications and Kernels (SEAK), (a) to capture these bottlenecks in a way that encourages creative solutions, and (b) to facilitate rigorous tradeoff evaluation for their solutions. To avoid biases toward existing solutions, both algorithms and architecture are variables. Thus, each benchmark has a mission-centric (abstracted from a particular algorithm) and goal-oriented (functional) specification. To encourage solutions that are any combination of software or hardware, we use an end-user black-box evaluation. To inform procurement decisions, evaluations capture tradeoffs between performance, power, accuracy, size, and weight. We call our benchmarks future proof because they remain useful despite shifting algorithmic/architectural preferences. To create both concise and precise mission-centric specifications, we introduce two distinct benchmark classes. This paper describes the SEAK suite and presents an evaluation of sample solutions that highlights power and performance tradeoffs.},
  doi       = {10.1109/IPDPS.2016.25},
  keywords  = {embedded systems;parallel processing;software architecture;software performance evaluation;suite for embedded applications and kernels;SEAK;high performance embedded computing;end-user black-box evaluation;algorithmic preference;architectural preference;performance benchmarking;Benchmark testing;Kernel;Parallel processing;Measurement;Computer architecture;Software algorithms;constraining problem;bottleneck specification;functional;mission interface;tradeoff evaluation},
}

@InProceedings{7161599,
  author    = {M. {Popov} and C. {Akel} and F. {Conti} and W. {Jalby} and P. d. O. {Castro}},
  title     = {PCERE: Fine-Grained Parallel Benchmark Decomposition for Scalability Prediction},
  booktitle = {2015 IEEE International Parallel and Distributed Processing Symposium},
  year      = {2015},
  pages     = {1151-1160},
  month     = {May},
  abstract  = {Evaluating the strong scalability of OpenMP applications is a costly and time-consuming process. It traditionally requires executing the whole application multiple times with different number of threads. We propose the Parallel Codelet Extractor and REplayer (PCERE), a tool to reduce the cost of scalability evaluation. PCERE decomposes applications into small pieces called codelets: each codelet maps to an OpenMP parallel region and can be replayed as a standalone program. To accelerate scalability prediction, PCERE replays codelets while varying the number of threads. Prediction speedup comes from two key ideas. First, the number of invocations during replay can be significantly reduced. Invocations that have the same performance are grouped together and a single representative is replayed. Second, sequential parts of the programs do not need to be replayed for each different thread configuration. PCERE codelets can be captured once and replayed accurately on multiple architectures, enabling cross-architecture parallel performance prediction. We evaluate PCERE on a C version of the NAS 3.0 Parallel Benchmarks (NPB). We achieve an average speed-up of 25 × on evaluating OpenMP applications scalability with an average error of 4.9% (median error of 1.7%).},
  doi       = {10.1109/IPDPS.2015.19},
  keywords  = {benchmark testing;parallel processing;software architecture;software performance evaluation;PCERE;fine-grained parallel benchmark decomposition;scalability prediction;OpenMP applications;parallel codelet extractor and replayer;thread configuration;cross-architecture parallel performance prediction;NAS 3.0 parallel benchmarks;NPB;Benchmark testing;Instruction sets;Scalability;Accuracy;Optimization;In vivo;Context;OpenMP applications;program replay;checkpoint restart;parallel code isolation;scalability prediction;cross-architecture performance prediction},
}

@InProceedings{5650208,
  author    = {L. {Van Ertvelde} and L. {Eeckhout}},
  title     = {Benchmark synthesis for architecture and compiler exploration},
  booktitle = {IEEE International Symposium on Workload Characterization (IISWC'10)},
  year      = {2010},
  pages     = {1-11},
  month     = {Dec},
  abstract  = {This paper presents a novel benchmark synthesis framework with three key features. First, it generates synthetic benchmarks in a high-level programming language (C in our case), in contrast to prior work in benchmark synthesis which generates synthetic benchmarks in assembly. Second, the synthetic benchmarks hide proprietary information from the original workloads they are built after. Hence, companies may want to distribute synthetic benchmark clones to third parties as proxies for their proprietary codes; third parties can then optimize the target system without having access to the original codes. Third, the synthetic benchmarks are shorter running than the original workloads they are modeled after, yet they are representative. In summary, the proposed framework generates small (thus quick to simulate) and representative benchmarks that can serve as proxies for other workloads without revealing proprietary information; and because the benchmarks are generated in a high-level programming language, they can be used to explore both the architecture and compiler spaces. The results obtained with our initial framework are promising. We demonstrate that we can generate synthetic proxy benchmarks for the MiBench benchmarks, and we show that they are representative across a range of machines with different instruction-set architectures, microarchitectures, and compilers and optimization levels, while being 30 times shorter running on average. We also verify using software plagiarism detection tools that the synthetic benchmark clones hide proprietary information from the original workloads.},
  doi       = {10.1109/IISWC.2010.5650208},
  keywords  = {C language;instruction sets;program compilers;software architecture;benchmark synthesis;compiler exploration;three key feature;high level programming language;synthetic benchmark;target system;proprietary information;compiler space;synthetic proxy;MiBench benchmark;instruction set architecture;microarchitecture;optimization level;software plagiarism detection tool;Benchmark testing;Optimization;Computer architecture;Cloning;Computer languages;Program processors;Hardware},
}

@InProceedings{5704324,
  author    = {T. {Wang} and X. {Zhou} and J. {Wei} and W. {Zhang} and X. {Zhu}},
  title     = {Component Monitoring of OSGi-Based Software},
  booktitle = {2010 IEEE 7th International Conference on E-Business Engineering},
  year      = {2010},
  pages     = {250-255},
  month     = {Nov},
  abstract  = {OSGi is a widely used service-oriented platform which provides support for the development and execution of component-based e-business applications. The component monitoring of OSGi-based software is important to the software reliability analysis and abnormality diagnosis. However, current approaches not only bring significant overhead, but also lack interaction behavior analysis. This paper focuses on monitoring components from resource utilization and interaction behavior perspectives. We propose an approach to trace thread execution for updating the relationship between threads and bundles, so that the CPU and memory utilization of bundles are calculated, and the service invocation graph is updated at runtime. A prototype tool is implemented and applied in an application server OnceAS based on OSGi. The experiments using TPC-W benchmark validate our approach without significant resource and performance overhead. Furthermore, simulation results show that our approach is of high accuracy for monitoring CPU utilization of bundles.},
  doi       = {10.1109/ICEBE.2010.15},
  keywords  = {business data processing;service-oriented architecture;software reliability;OSGi based software;service oriented platform;component based e-business applications;software reliability analysis;application server OnceAS;CPU bundle utilization;TPC-W benchmark;Monitoring;Java;Resource management;Instruction sets;Accuracy;Servers;software component;monitoring system;OSGi;resource utilization;service dependency},
}

@InProceedings{8641578,
  author    = {J. {Laukemann} and J. {Hammer} and J. {Hofmann} and G. {Hager} and G. {Wellein}},
  title     = {Automated Instruction Stream Throughput Prediction for Intel and AMD Microarchitectures},
  booktitle = {2018 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)},
  year      = {2018},
  pages     = {121-131},
  month     = {Nov},
  abstract  = {An accurate prediction of scheduling and execution of instruction streams is a necessary prerequisite for predicting the in-core performance behavior of throughput-bound loop kernels on out-of-order processor architectures. Such predictions are an indispensable component of analytical performance models, such as the Roofline and the Execution-Cache-Memory (ECM) model, and allow a deep understanding of the performance-relevant interactions between hardware architecture and loop code. We present the Open Source Architecture Code Analyzer (OSACA), a static analysis tool for predicting the execution time of sequential loops comprising x86 instructions under the assumption of an infinite first-level cache and perfect out-of-order scheduling. We show the process of building a machine model from available documentation and semi-automatic benchmarking, and carry it out for the latest Intel Skylake and AMD Zen micro-architectures. To validate the constructed models, we apply them to several assembly kernels and compare runtime predictions with actual measurements. Finally we give an outlook on how the method may be generalized to new architectures.},
  doi       = {10.1109/PMBS.2018.8641578},
  keywords  = {cache storage;program diagnostics;public domain software;scheduling;software architecture;in-core performance behavior;throughput-bound loop kernels;out-of-order processor architectures;performance-relevant interactions;hardware architecture;static analysis tool;infinite first-level cache;out-of-order scheduling;machine model;assembly kernels;Intel Skylake;automated instruction stream throughput prediction;execution-cache-memory model;open source architecture code analyzer;OSACA;AMD Zen micro-architectures;Out of order;Computer architecture;Analytical models;Computational modeling;Predictive models;Runtime;Benchmark testing;benchmarking;performance modeling;performance engineering;architecture analysis;static analysis},
}

@InProceedings{4437200,
  author    = {S. {Chauvie} and P. {Mendez Lorenzo} and A. {Lechner} and J. {Moscicki} and M. {Grazia Pia}},
  title     = {Benchmark of medical dosimetry simulation using the Grid},
  booktitle = {2007 IEEE Nuclear Science Symposium Conference Record},
  year      = {2007},
  volume    = {2},
  pages     = {1100-1106},
  month     = {Oct},
  abstract  = {The practical capability of using Grid resources to perform high-precision dosimetry simulation in radiation oncology is evaluated, taking into account the peculiar demands of different treatment modalities. For this purpose extensive benchmark tests on the LHC Computing Grid (LCG) are performed, involving the calculation of dose distributions as required for clinical practice. The software architecture of the test is based on Geant4 for simulation, on AIDA for data analysis, on the LCG middleware for grid computing and on DIANE as an intermediate layer between the application software and the computing environment.},
  doi       = {10.1109/NSSMIC.2007.4437200},
  keywords  = {benchmark testing;data analysis;dosimetry;grid computing;high energy physics instrumentation computing;medical dosimetry simulation;benchmark tests;LHC computing grid;Geant4 simulation;AIDA;data analysis;DIANE;Dosimetry;Medical simulation;Computational modeling;Grid computing;Benchmark testing;Performance evaluation;Oncology;Large Hadron Collider;Distributed computing;Software architecture;Index Terms;Monte Carlo;Geant4;parallel computing;grid;dosimetry},
}

@InProceedings{8502448,
  author    = {J. {Sini} and M. {Violante} and R. {Dessì}},
  title     = {Computer-Aided Design of Multi-Agent Cyber-Physical Systems},
  booktitle = {2018 IEEE 23rd International Conference on Emerging Technologies and Factory Automation (ETFA)},
  year      = {2018},
  volume    = {1},
  pages     = {677-684},
  month     = {Sep.},
  abstract  = {This paper presents a methodology to aid the development of multi-agent cyber-physical systems. The software architecture is structured in a multi-layer fashion, distributed on different devices (agents), making its design very challenging. A methodology is thus needed to meet the performance goal of such a system. The proposed methodology relies on Model-Based Software Design techniques, and on the Model/Software/Hardware-In-The-Loop (MIL, SIL, HIL) simulations. In particular, the HIL verification technique, which is widely adopted in the automotive and avionic industry also in the early development phases as a Computer-Aided Design tool, is the core tool of the real-time performances assessment verification. To speed-up the development process, and to obtain a toolchain coherent with the proposed approach, the adoption of a Model-Based Software Design methodology represents an optimal solution. Within those assumptions, we propose a benchmark of the methodology, applied on a customized multiagent system. The aim is to prove that the proposed method is able to achieve and verify a set of real-time requirements on the system behavior.},
  doi       = {10.1109/ETFA.2018.8502448},
  keywords  = {CAD;cyber-physical systems;formal verification;hardware-in-the loop simulation;multi-agent systems;real-time systems;software architecture;Computer-Aided Design tool;real-time performances assessment verification;Model-Based Software Design methodology;customized multiagent system;system behavior;software architecture;Model-Based Software Design techniques;HIL verification technique;multi-agent cyber-physical systems;multi-layer fashion;Model-In-The-Loop;Software-In-The-Loop;Hardware-In-The-Loop;Software;Solid modeling;Real-time systems;Robots;Task analysis;Testing;Hardware;CAD;Cyber-Physical Systems;Functional testing;Multi-Agent Robotic;Robotics;Software test;Software integration},
}

@InProceedings{4222585,
  author    = {C. {Fu} and B. G. {Ryder}},
  title     = {Exception-Chain Analysis: Revealing Exception Handling Architecture in Java Server Applications},
  booktitle = {29th International Conference on Software Engineering (ICSE'07)},
  year      = {2007},
  pages     = {230-239},
  month     = {May},
  abstract  = {Although it is common in large Java programs to rethrow exceptions, existing exception-flow analyses find only single exception-flow links, thus are unable to identify multiple-link exception propagation paths. This paper presents a new static analysis that, when combined with previous exception-flow analyses, computes chains of semantically-related exception-flow links, and thus reports entire exception propagation paths, instead of just discrete segments of them. These chains can be used 1) to show the error handling architecture of a system, 2) to assess the vulnerability of a single component and the whole system, 3) to support better testing of error recovery code, and 4) to facilitate the tracing of the root cause of a logged problem. Empirical findings and a case history for Tomcat show that a significant portion of the chains found in our benchmarks span multiple components, and thus are hard to find manually.},
  doi       = {10.1109/ICSE.2007.35},
  keywords  = {error handling;file servers;Java;program compilers;program diagnostics;program testing;software architecture;system recovery;exception-chain analysis;exception handling architecture;Java server application;static program analysis;exception-flow analysis;exception propagation path;semantically-related exception-flow link;error handling;error recovery code testing;program tracing;program compiler;Java;Information analysis;Computer architecture;Programming profession;Application software;System testing;History;Benchmark testing;Software engineering;Availability},
}

@InProceedings{6100052,
  author    = {F. {Brosig} and N. {Huber} and S. {Kounev}},
  title     = {Automated extraction of architecture-level performance models of distributed component-based systems},
  booktitle = {2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011)},
  year      = {2011},
  pages     = {183-192},
  month     = {Nov},
  abstract  = {Modern enterprise applications have to satisfy increasingly stringent Quality-of-Service requirements. To ensure that a system meets its performance requirements, the ability to predict its performance under different configurations and workloads is essential. Architecture-level performance models describe performance-relevant aspects of software architectures and execution environments allowing to evaluate different usage profiles as well as system deployment and configuration options. However, building performance models manually requires a lot of time and effort. In this paper, we present a novel automated method for the extraction of architecture-level performance models of distributed component-based systems, based on monitoring data collected at run-time. The method is validated in a case study with the industry-standard SPECjEnterprise2010 Enterprise Java benchmark, a representative software system executed in a realistic environment. The obtained performance predictions match the measurements on the real system within an error margin of mostly 10-20 percent.},
  doi       = {10.1109/ASE.2011.6100052},
  keywords  = {distributed processing;electronic commerce;object-oriented programming;software architecture;software metrics;software performance evaluation;architecture-level performance model automated extraction;distributed component-based systems;enterprise applications;quality-of-service requirements;performance requirements;software architectures;system deployment;configuration options;usage profile evaluation;data monitoring;SPECjEnterprise2010 Enterprise Java benchmark;real system measurement;Unified modeling language;Monitoring;Predictive models;Java;Context;Servers;Data mining},
}

@InProceedings{6628280,
  author    = {R. A. {Shafik} and G. {Rauwerda} and J. {Potman} and K. {Sunesen} and D. K. {Pradhan} and J. {Mathew} and I. {Sourdis}},
  title     = {Software Modification Aided Transient Error Tolerance for Embedded Systems},
  booktitle = {2013 Euromicro Conference on Digital System Design},
  year      = {2013},
  pages     = {219-226},
  month     = {Sep.},
  abstract  = {Commercial off-the-shelf (COTS) components are increasingly being employed in embedded systems due to their high performance at low cost. With emerging reliability requirements, design of these components using traditional hardware redundancy incur large overheads, time-demanding re-design and validation. To reduce the design time with shorter time-to-market requirements, software-only reliable design techniques can provide with an effective and low-cost alternative. This paper presents a novel, architecture-independent software modification tool, SMART (Software Modification Aided transient eRror Tolerance) for effective error detection and tolerance. To detect transient errors in processor data path, control flow and memory at reasonable system overheads, the tool incorporates selective and non-intrusive data duplication and dynamic signature comparison. Also, to mitigate the impact of the detected errors, it facilitates further software modification implementing software-based check-pointing. Due to automatic software based source-to-source modification tailored to a given reliability requirement, the tool requires no re-design effort, hardware- or compiler-level intervention. We evaluate the effectiveness of the tool using a Xentium processor based system as a case study of COTS based systems. Using various benchmark applications with single-event upset (SEUs) based error model, we show that up to 91% of the errors can be detected or masked with reasonable performance, energy and memory footprint overheads.},
  doi       = {10.1109/DSD.2013.32},
  keywords  = {checkpointing;embedded systems;error detection;software architecture;software fault tolerance;time to market;commercial off-the-shelf component;COTS components;embedded systems;memory footprint overhead;energy overhead;single-event upset based error model;COTS based systems;Xentium processor based system;automatic software based source-to-source modification;software-based check-pointing;dynamic signature;nonintrusive data duplication;selective data duplication;control flow;processor datapath;transient error detection;software modification aided transient error tolerance;SMART;architecture-independent software modification tool;reliability requirement;software-only reliable design technique;time-to-market requirements;design time reduction;Software;Transient analysis;Registers;Hardware;Computer architecture;Reliability;Libraries;Fault Tolerance;Error Detection;Reliable Computing;Embedded Systems},
}

@InProceedings{5261034,
  author    = {H. {Dworak}},
  title     = {A Concept of a Web Application Blending Thin and Fat Client Architectures},
  booktitle = {2009 Fourth International Conference on Dependability of Computer Systems},
  year      = {2009},
  pages     = {84-90},
  month     = {June},
  abstract  = {Rich Internet applications and Web application frameworks treat user agents as either thin or fat clients. Whereas the former approach does not take advantage of the increasing potential of modern Web browsers, the latter requires the availability of their scripting engine. Although the majority of users allow the source code to be interpreted by their clients, some consider it to be a security threat and forbid its use. Moreover, not all user agents support scripting, thus the requirement of code execution constitutes a Web accessibility issue. We propose an architecture allowing software developers to build modern Web applications with equivalent functionality regardless of whether the user agent supports client-side scripting. The concept eliminates the need to rewrite the code base in a server-side language and requires no customisation of other layers of a Web application.},
  doi       = {10.1109/DepCoS-RELCOMEX.2009.9},
  keywords  = {Internet;online front-ends;security of data;software architecture;Web application;rich Internet application;thin client architecture;fat client architecture;Web browser;scripting engine;user agent;software developer;client-side scripting;security threat;Service oriented architecture;Application software;Computer architecture;Web sites;Internet;Search engines;Java;Military computing;Security;Web server;Software dependability;Secure and dependable information society;Dependability benchmarking},
}

@InProceedings{4625066,
  author    = {S. {Kurkovsky} and D. {Strimple} and E. {Nuzzi} and K. {Verdecchia}},
  title     = {Convergence of Web 2.0 and SOA: Taking Advantage of Web Services to Implement a Multimodal Social Networking System},
  booktitle = {2008 11th IEEE International Conference on Computational Science and Engineering - Workshops},
  year      = {2008},
  pages     = {227-232},
  month     = {July},
  abstract  = {The term Web 2.0 describes Web-based applications such as social networking sites, wikis, and blogs that facilitate collaboration, creativity, and sharing among users. Web 2.0 applications are often enabled by reusing content from other Web-based applications or Web services. This trend is largely parallel to the principles of service-oriented architecture (SOA), which encourages integration and reuse of online services. In this paper, we discuss the possibility of convergence of Web 2.0 and SOA, which is illustrated by a multimodal social networking system, built using Web services. We also present the results of benchmarking experiments aimed to study the latency caused by Web service composition and integration and its effects on the end-user experience.},
  doi       = {10.1109/CSEW.2008.15},
  keywords  = {social sciences computing;software architecture;user interfaces;Web services;Web 2.0 application;SOA;service-oriented architecture;Web service composition;multimodal social networking system;online service reuse;end-user experience;multimodal interface;Service oriented architecture;Web services;Social network services;Benchmark testing;Servers;Speech;Mobile communication;web services;service-oriented architectures;Web 2.0;social networking;VoiceXML},
}

@InProceedings{6385047,
  author    = {M. {Jarschel} and F. {Lehrieder} and Z. {Magyari} and R. {Pries}},
  title     = {A Flexible OpenFlow-Controller Benchmark},
  booktitle = {2012 European Workshop on Software Defined Networking},
  year      = {2012},
  pages     = {48-53},
  month     = {Oct},
  abstract  = {Software defined networking (SDN) promises a way to more flexible networks that can adapt to changing demands. At the same time these networks should also benefit from simpler management mechanisms. This is achieved by moving the network control out of the forwarding devices to purpose-tailored software-applications on top of a "networking operating system". Currently, the most notable representative of this approach is OpenFlow. In the OpenFlow architecture the operating system is represented by the OpenFlow controller. As the key component of the OpenFlow ecosystem, the behavior and performance of the controller are significant for the entire network. Therefore, it is important to understand these influence factors, when planning an OpenFlow-based SDN deployment. In this work, we introduce a tool to help achieving just that - a flexible OpenFlow controller benchmark. The benchmark creates a set of message-generating virtual switches, which can be configured independently from each other to emulate a certain scenario and also keep their own statistics. This way a granular controller performance analysis is possible.},
  doi       = {10.1109/EWSDN.2012.15},
  keywords  = {operating systems (computers);software architecture;flexible OpenFlow-controller;software defined networking;OpenFlow ecosystem;OpenFlow-based SDN deployment;virtual switches;granular controller performance analysis;OpenFlow architecture;Benchmark testing;Control systems;Message systems;Operating systems;Computer architecture;Sockets;OpenFlow;controller;performance evaluation},
}

@Article{5567094,
  author   = {L. {Zhu} and X. {Liu}},
  title    = {Technical Target Setting in QFD for Web Service Systems Using an Artificial Neural Network},
  journal  = {IEEE Transactions on Services Computing},
  year     = {2010},
  volume   = {3},
  number   = {4},
  pages    = {338-352},
  month    = {Oct},
  abstract = {There are at least two challenges with quality management of service-oriented architecture based web service systems: 1) how to link its technical capabilities with customer's needs explicitly to satisfy customers' functional and nonfunctional requirements; and 2) how to determine targets of web service design attributes. Currently, the first issue is not addressed and the second one is dealt with subjectively. Quality Function Deployment (QFD), a quality management system, has found its success in improving quality of complex products although it has not been used for developing web service systems. In this paper, we analyze requirements for web services and their design attributes, and apply the QFD for developing web service systems by linking quality of service requirements to web service design attributes. A new method for technical target setting in QFD, based on an artificial neural network, is also presented. Compared with the conventional methods for technical target setting in QFD, such as benchmarking and the linear regression method, which fail to incorporate nonlinear relationships between design attributes and quality of service requirements, it sets up technical targets consistent with relationships between quality of web service requirements and design attributes, no matter whether they are linear or nonlinear.},
  doi      = {10.1109/TSC.2010.45},
  keywords = {neural nets;quality management;regression analysis;software architecture;Web services;technical target setting;QFD;Web service systems;artificial neural network;quality management system;service-oriented architecture;Web service design attributes;quality function deployment;linear regression method;benchmarking;Quality of service;Quality function deployment;Service oriented architecture;Artificial neural networks;Web and internet services;Neural networks;Bayes methods;Web service system;service quality management;Bayesian regularized neural network;quality function deployment (QFD);technical targets setting.},
}

@InProceedings{6226000,
  author    = {B. {Curtis} and J. {Sappidi} and A. {Szynkarski}},
  title     = {Estimating the size, cost, and types of Technical Debt},
  booktitle = {2012 Third International Workshop on Managing Technical Debt (MTD)},
  year      = {2012},
  pages     = {49-53},
  month     = {June},
  abstract  = {This study summarizes results of a study of Technical Debt across 745 business applications comprising 365 million lines of code collected from 160 companies in 10 industry segments. These applications were submitted to a static analysis that evaluates quality within and across application layers that may be coded in different languages. The analysis consists of evaluating the application against a repository of over 1200 rules of good architectural and coding practice. A formula for estimating Technical Debt with adjustable parameters is presented. Results are presented for Technical Debt across the entire sample as well as for different programming languages and quality factors.},
  doi       = {10.1109/MTD.2012.6226000},
  keywords  = {business data processing;program diagnostics;programming languages;software architecture;software cost estimation;software performance evaluation;software quality;size estimation;cost estimation;technical debt;business applications;industry segments;static analysis;quality evaluation;repository;architectural practice;coding practice;adjustable parameters;programming languages;quality factors;Organizations;Software;Production;Encoding;ISO standards;Industries;software metrics;software structural quality;technical debt;static analysis;benchmarking},
}

@InProceedings{1531259,
  author    = {N. {Wickramage} and S. {Weerawarana}},
  title     = {A benchmark for Web service frameworks},
  booktitle = {2005 IEEE International Conference on Services Computing (SCC'05) Vol-1},
  year      = {2005},
  volume    = {1},
  pages     = {233-240 vol.1},
  month     = {July},
  abstract  = {Considering the facts that existing benchmarks to measure the performance of Web service frameworks simulate only theoretical scenarios such as streaming homogeneous data structures and the computer industry has an established culture of developing performance benchmarks imitating real world scenarios, an effort was made to come up with a benchmark that closely represent the real world business services. The paper concludes that the benchmark represents an unbiased subset of actual scenarios because the ranking and performance patterns of the leading Web services frameworks used in the experiment are consistent with industry wide experiences. Additionally the paper introduces a performance model to analyze Web service frameworks and identifies complexity of the SOAP messages and size of the payloads they carry as two major factors that affect the RTT of the SOAP messages and reveals that a framework that is good at handling complex SOAP messages may not deal with messages that carry larger payloads equally well.},
  doi       = {10.1109/SCC.2005.9},
  keywords  = {Internet;communication complexity;protocols;benchmark testing;Web service framework;homogeneous data structure streaming;performance benchmark;real world business services;SOAP message complexity;Web services;Simple object access protocol;Computer architecture;Service oriented architecture;Testing;Computer industry;Payloads;Grid computing;Space technology;Application software},
}

@InProceedings{6717302,
  author    = {K. {Chen} and Y. {Chang} and P. {Liao} and P. {Yew} and S. {Cheng} and T. {Chen}},
  title     = {Selective Profiling for OS Scalability Study on Multicore Systems},
  booktitle = {2013 IEEE 6th International Conference on Service-Oriented Computing and Applications},
  year      = {2013},
  pages     = {174-181},
  month     = {Dec},
  abstract  = {With more cores becoming available in each future generation of microprocessors (i.e. the well-known Moore's Law), scalability is becoming an increasingly important issue. Scalability of the operating system, in particular, is critical to such systems. To study OS scalability and many other issues related to OS performance on multicore systems, software and hardware profilers are indispensable tools. Hardware profilers give detailed performance information on hardware components with minimal overhead, but are difficult to relate the collected information to specific software events. Hence, most of the OS profiling tools are software based. Such profilers often incur significant overheads if more precise measurements are required. The situation is exacerbated further because most of these tools have scalability issue themselves in that their overheads could grow more than proportionately to the number of cores and/or the number of threads in a system. Our results showed that such overheads not only cause much longer execution time (often by orders of magnitude), but also perturb program execution and produce misleading profiling results. In order to mitigate such problems, we propose an approach, called selective profiling, that uses a mix of profiling tools with different levels of precision and overheads to produce the desired results with tolerable overhead. In selective-profiling, potential scalability bottlenecks and hotspots are first identified by low-overhead tracers. More detailed information of the selected bottlenecks and hotspots are then collected by a sampler with more precision but heavier overheads. Since the sampler only focuses on the selected bottlenecks and hotspots instead of the entire program, the overhead can be substantially reduced. Using such an approach on some OS benchmarks, we show that the proposed selective-profiling approach can efficiently identify their scalability bottlenecks with much reduced overheads.},
  doi       = {10.1109/SOCA.2013.28},
  keywords  = {multiprocessing systems;operating systems (computers);system monitoring;selective profiling;OS scalability study;multicore systems;microprocessors;Moore's Law;operating system scalability;OS performance;software profilers;hardware profilers;performance information;hardware components;information collection;software events;OS profiling tools;program execution;scalability bottleneck identification;hotspot identification;low-overhead tracers;overhead reduction;Scalability;Kernel;Hardware;Throughput;Correlation coefficient;Benchmark testing;multicores;profiling;benchmarking},
}

@InProceedings{6557141,
  author    = {T. E. {Carlson} and W. {Heirman} and L. {Eeckhout}},
  title     = {Sampled simulation of multi-threaded applications},
  booktitle = {2013 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2013},
  pages     = {2-12},
  month     = {April},
  abstract  = {Sampling is a well-known workload reduction technique that allows one to speed up architectural simulation while accurately predicting performance. Previous sampling methods have been shown to accurately predict single-threaded application runtime based on its overall IPC. However, these previous approaches are unsuitable for general multi-threaded applications, for which IPC is not a good proxy for runtime. Additionally, we find that issues such as application periodicity and inter-thread synchronization play a significant role in determining how best to sample these applications. The proposed multi-threaded application sampling methodology is able to derive an effective sampling strategy for candidate applications using architecture-independent metrics. Using this methodology, large input sets can now be simulated which would otherwise be infeasible, allowing for more accurate conclusions to be made than from studies using scaled-down input sets. Through the use of the proposed methodology, we can simulate less than 10% of the total application runtime in detail. On the SPEComp, NPB and PARSEC benchmarks, running on an 8-core simulated system, we achieve an average absolute error of 3.5%.},
  doi       = {10.1109/ISPASS.2013.6557141},
  keywords  = {digital simulation;multi-threading;software architecture;software metrics;software performance evaluation;sampled simulation;multithreaded application sampling methodology;workload reduction technique;architectural simulation;performance prediction;application periodicity;interthread synchronization;architecture-independent metrics;sampling strategy;scaled-down input sets;SPEComp benchmark;NPB benchmark;PARSEC benchmark;8-core simulated system;Instruction sets;Runtime;Synchronization;Accuracy;Predictive models;Benchmark testing;Vectors},
}

@Article{6280665,
  author   = {O. {Mora} and G. {Engelbrecht} and J. {Bisbal}},
  title    = {A Service-Oriented Distributed Semantic Mediator: Integrating Multiscale Biomedical Information},
  journal  = {IEEE Transactions on Information Technology in Biomedicine},
  year     = {2012},
  volume   = {16},
  number   = {6},
  pages    = {1296-1303},
  month    = {Nov},
  abstract = {Biomedical research continuously generates large amounts of heterogeneous and multimodal data spread over multiple data sources. These data, if appropriately shared and exploited, could dramatically improve the research practice itself, and ultimately the quality of health care delivered. This paper presents DIstributed Semantic MEDiator (DISMED), an open source semantic mediator that provides a unified view of a federated environment of multiscale biomedical data sources. DISMED is a Web-based software application to query and retrieve information distributed over a set of registered data sources, using semantic technologies. It also offers a user-friendly interface specifically designed to simplify the usage of these technologies by nonexpert users. Although the architecture of the software mediator is generic and domain independent, in the context of this paper, DISMED has been evaluated for managing biomedical environments and facilitating research with respect to the handling of scientific data distributed in multiple heterogeneous data sources. As part of this contribution, a quantitative evaluation framework has been developed. It consist of a benchmarking scenario and the definition of five realistic use-cases. This framework, created entirely with public datasets, has been used to compare the performance of DISMED against other available mediators. It is also available to the scientific community in order to evaluate progress in the domain of semantic mediation, in a systematic and comparable manner. The results show an average improvement in the execution time by DISMED of 55% compared to the second best alternative in four out of the five use-cases of the experimental evaluation.},
  doi      = {10.1109/TITB.2012.2215045},
  keywords = {data integration;health care;human computer interaction;medical information systems;public domain software;query processing;semantic Web;service-oriented architecture;user interfaces;service-oriented Distributed Semantic Mediator;multiscale biomedical information;multimodal data;heterogeneous data;health care;DISMED;open source semantic mediator;multiscale biomedical data sources;Web-based software application;information retrieval;information querying;registered data sources;semantic technology;user-friendly interface;software mediator;biomedical environments;scientific data handling;multiple heterogeneous data sources;quantitative evaluation framework;public datasets;Resource description framework;Distributed databases;Data integration;Query processing;Semantic Web;Biomedical information;data integration;distributed query processing;RDF;semantic Web;software mediator;SPARQL;Algorithms;Biomedical Research;Computational Biology;Databases, Factual;Information Storage and Retrieval;Internet;Semantics;Software},
}

@InProceedings{6569838,
  author    = {K. {Mitropoulou} and V. {Porpodas} and M. {Cintra}},
  title     = {CASTED: Core-Adaptive Software Transient Error Detection for Tightly Coupled Cores},
  booktitle = {2013 IEEE 27th International Symposium on Parallel and Distributed Processing},
  year      = {2013},
  pages     = {513-524},
  month     = {May},
  abstract  = {Aggressive silicon process scaling over the last years has made transistors faster and less power consuming. Meanwhile, transistors have become more susceptible to errors. The need to maintain high reliability has led to the development of various software-based error detection methodologies which target either single-core or multi-core processors. In this work, we present CASTED, a Core-Adaptive Soft- ware Transient Error Detection methodology that focuses on improving the impact of error detection overhead on single- chip scalable architectures that are composed of tightly coupled cores. The proposed compiler methodology adaptively distributes the error detection overhead to the available resources across multiple cores, fully exploiting the abundant ILP of these architectures. CASTED adapts to a wide range of architecture configurations (issue-width, inter-core delay). We evaluate our technique on a range of architecture configurations using the MediabenchII video and SPEC CINT2000 benchmark suites. Our approach successfully adapts to (and regularly outperforms by up to 21.2%) the best fixed state-of- the-art approach while maintaining the same fault coverage.},
  doi       = {10.1109/IPDPS.2013.107},
  keywords  = {error detection;multiprocessing systems;program compilers;software architecture;software reliability;CASTED;core-adaptive software transient error detection;tightly coupled cores;aggressive silicon process scaling;power consuming;reliability;software-based error detection methodology;single-core processor;multicore processors;error detection overhead;single-chip scalable architectures;compiler methodology;ILP;architecture configurations;MediabenchII video;SPEC CINT2000 benchmark suites;Registers;Delays;Transient analysis;Computer architecture;Program processors;Hardware;Clustering algorithms;adaptivity;software error detection},
}

@InProceedings{4578524,
  author    = {N. {Laranjeiro} and S. {Canelas} and M. {Vieira}},
  title     = {wsrbench: An On-Line Tool for Robustness Benchmarking},
  booktitle = {2008 IEEE International Conference on Services Computing},
  year      = {2008},
  volume    = {2},
  pages     = {187-194},
  month     = {July},
  abstract  = {Testing Web services for robustness is a difficult task. In fact, existing development support tools do not provide any practical mean to assess Web services robustness in the presence of erroneous inputs. Previous works proposed that Web services robustness testing should be based on a set of robustness tests (i.e., invalid Web services call parameters) that are applied in order to discover both programming and design errors. Web services can be classified based on the failure modes observed. In this paper we present and discuss the architecture and use of an on-line tool that provides an easy interface for Web services robustness testing. This tool is publicly available and can be used by both web services providers (to assess the robustness of their Web services code) and consumers (to select the services that best fit their requirements). The tool is demonstrated by testing several Web services available in the Internet.},
  doi       = {10.1109/SCC.2008.123},
  keywords  = {software architecture;software tools;Web services;online tool;robustness benchmarking;Web services;failure modes;online tools;Internet;web services;robustness benchmarking;fault injection},
}

@InProceedings{5365909,
  author    = {M. {Masuda} and A. B. {Abdallah} and A. {Canedo}},
  title     = {Software and Hardware Design Issues for Low Complexity High Performance Processor Architecture},
  booktitle = {2009 International Conference on Parallel Processing Workshops},
  year      = {2009},
  pages     = {558-565},
  month     = {Sep.},
  abstract  = {Queue processor offers an attractive option in the design of general purpose and applications specific systems. This paper presents software and hardware design issues for extracting high instruction level parallelism for the 32-bit queuecore processor. We propose code generation algorithm for the queuecore architecture. Compiling for the queuecore requires a new approach since the concept of registers disappears. The compiler extracts more parallelism than the optimizing compiler for a RISC machine over a set of various numerical benchmark programs. In addition, we are able to generate in average about 23% denser code than two embedded RISC processors.},
  doi       = {10.1109/ICPPW.2009.60},
  keywords  = {hardware-software codesign;program compilers;reduced instruction set computing;software architecture;software-hardware design;high performance processor architecture;queue processor;high instruction level parallelism;RISC machine;numerical benchmark programs;Software performance;Hardware;Software design;Computer architecture;Registers;Parallel processing;Program processors;Processor scheduling;Reduced instruction set computing;Computational modeling},
}

@InProceedings{4725855,
  author    = {M. G. {Noll} and C. {Meinel}},
  title     = {Building a Scalable Collaborative Web Filter with Free and Open Source Software},
  booktitle = {2008 IEEE International Conference on Signal Image Technology and Internet Based Systems},
  year      = {2008},
  pages     = {563-571},
  month     = {Nov},
  abstract  = {In this case study, we describe the design and architecture of a scalable collaborative web filtering service, TaggyBear, which is powered by free and open source software. We will introduce the reader to the ideas and concepts behind TaggyBear, and discuss why we picked the software components that form the basis of the service. We will talk about how we combined or extended their functionality to build the TaggyBear service, and provide some initial benchmarking results and performance figures.},
  doi       = {10.1109/SITIS.2008.27},
  keywords  = {groupware;information filters;public domain software;scalable collaborative web filter;free software;open source software;TaggyBear service;Collaborative software;Information filters;Open source software;Information filtering;Web pages;Voting;Computer architecture;Web and internet services;International collaboration;Service oriented architecture;case study;scalability;open source;web filter;security},
}

@InProceedings{7936235,
  author    = {T. {Dallou} and D. C. S. {Lucas} and G. {Araujo} and L. {Morais} and E. F. {Barbosa} and M. {Frank} and R. {Bagley} and R. {Sayana}},
  title     = {Task parallel programming model + hardware acceleration = performance advantage},
  booktitle = {2016 IEEE Hot Chips 28 Symposium (HCS)},
  year      = {2016},
  pages     = {1-1},
  month     = {Aug},
  abstract  = {Presents a collection of slides covering the following topics: task parallel programming model; hardware acceleration; SoC; multi-core heterogeneous computing architecture; software architecture; and task graph accelerator.},
  doi       = {10.1109/HOTCHIPS.2016.7936235},
  keywords  = {multiprocessing systems;parallel programming;software architecture;system-on-chip;task parallel programming model;hardware acceleration;SoC;multicore heterogeneous computing architecture;software architecture;task graph accelerator;Benchmark testing;Runtime;Programming;Computer architecture;Hardware;Registers;Acceleration},
}

@InProceedings{5472507,
  author    = {M. {Chaudhry} and Q. {Ahmad} and I. {Sarwar} and A. H. {Akbar}},
  title     = {Comparative study of RFID middlewares-defining the roadmap to SOA-based middlewares},
  booktitle = {2010 IEEE International Conference on Industrial Technology},
  year      = {2010},
  pages     = {1386-1393},
  month     = {March},
  abstract  = {RFID technology finds immense applicability in various domains utilizing identification as the base operation such as in access control, localization, and tracking etc. It may be designed to work satisfactorily for either singular or a limited range of applications. However, the strict-coupling between applications and RFID equipment must be relaxed to scale up their usability. In order to achieve such a relaxation, RFID middlewares are employed that allow independence to applications from underlying complexities of RFID systems. In this paper, we present interplay of all that contemporary middlewares provide for RFID systems, and what are the actual expectations from one, in terms of desirable “features” and “aspects”. While the aspects are more business specific, features are implementation specific. Through literature survey, we extract implemented features of RFID middlewares which, are in turn, realizing design aspects. While breaking down these features into respective functions set, a) we unravel functional overlap between these features both horizontally and vertically, and b) we observe that each of these functions also implements part of system level business design aspects. Our analysis of these concludes that an appreciation of overlap both at the functional-level and at the design aspects-level allows necessary and simpler implementations of RFID middlewares. Further in this paper, we propose SOA-based strategy that helps to define and consolidate representative design aspects of a well-federated RFID middleware, such that these representative design aspects would serve as benchmarking basis for RFID middlewares. We have tested our benchmarking strategy to grade current implementations of SOA-based RFID middlewares. We found that none of these realizes complete business aspects of SOA.},
  doi       = {10.1109/ICIT.2010.5472507},
  keywords  = {commerce;middleware;radiofrequency identification;software architecture;RFID middlewares;roadmap;SOA-based middlewares;business design;benchmarking strategy;Radiofrequency identification;Middleware;Application software;Feature extraction;Benchmark testing;Semiconductor optical amplifiers;Scalability;Business communication;Collaboration;Service oriented architecture},
}

@InProceedings{4031789,
  author    = {E. {Bondarev} and M. {Chaudron} and H. {Byelas} and P. H. n. {De With}},
  title     = {A Toolkit for Design and Performance Analysis of Real-Time Component-Based Software Systems},
  booktitle = {2006 International Conference on Software Engineering Advances (ICSEA'06)},
  year      = {2006},
  pages     = {4-4},
  month     = {Oct},
  abstract  = {Software tools supporting the design and analysis of complex software-intensive systems are highly desirable, since they enable earlier decision making about system realization. This paper presents a tooling environment that supports the design and performance analysis of time-critical component-based software architectures deployed on complex multiprocessor platforms. The tooling environment contains a set of integrated tools for (a) component storage and retrieval, (b) graphics-based design of software and hardware architectures, (c) performance analysis of the defined architectures and, (d) automated code generation. The cornerstone of the toolkit is a performance analyzer that provides efficient simulation of the designed architectures and enables design-time prediction of key performance properties like response time, data throughout, and usage of hardware resources (processor, memory and bus). For every architecture alternative, the performance predictions can be quickly obtained, thereby enabling a fast and yet broad design space exploration. We demonstrate the efficiency and robustness of this toolkit on a Car Radio Navigation benchmark case.},
  doi       = {10.1109/ICSEA.2006.261260},
  keywords  = {Performance analysis;Real time systems;Software systems;Computer architecture;Software tools;Hardware;Decision making;Time factors;Software architecture;Storage automation},
}

@InProceedings{6685425,
  author    = {T. {Sakoda} and T. {Yamauchi} and H. {Taniguchi}},
  title     = {Evaluation of Load Balancing in Multicore Processor for AnT},
  booktitle = {2013 16th International Conference on Network-Based Information Systems},
  year      = {2013},
  pages     = {360-365},
  month     = {Sep.},
  abstract  = {Operating systems (OSes) that is based on micro kernel architecture have high adaptability and toughness. In addition, multicore processors have been developed along with the progress of LSI technology. By running a micro kernel OS on a multicore processor and distributing the OS server to multiple cores, it is possible to realize load balancing of the OS processing. In this method, transaction processing, which requires a large amount of OS processing, can be provided effectively in a multicore environment. This paper presents evaluations of distributed OS processing performances for various scenarios for AnT operating system that is based on the micro kernel architecture in a multicore environment. In these evaluations, we describe the differences in performance by distribution forms when referring the data in a block. Moreover, we use the Post Mark and Bonnie benchmark tools to evaluate the effects of load balancing for the distribution forms.},
  doi       = {10.1109/NBiS.2013.57},
  keywords  = {benchmark testing;multiprocessing systems;operating systems (computers);resource allocation;software architecture;software performance evaluation;load balancing evaluation;multicore processor;operating systems;microkernel architecture;LSI technology;microkernel OS processing;OS server distribution;distributed OS processing performance evaluation;AnT operating system;PostMark benchmark tool;Bonnie benchmark tool;Servers;Multicore processing;Benchmark testing;Load management;Kernel;Schedules;Educational institutions;operating system;multicore processor;microkernel;distributing process},
}

@InProceedings{4983226,
  author    = {G. S. {Tewolde} and D. M. {Hanna} and R. E. {Haskell}},
  title     = {Accelerating the performance of particle swarm optimization for embedded applications},
  booktitle = {2009 IEEE Congress on Evolutionary Computation},
  year      = {2009},
  pages     = {2294-2300},
  month     = {May},
  abstract  = {The ever increasing popularity of particle swarm optimization (PSO) algorithm is recently attracting attention to the embedded computing world. Although PSO is in general considered to be computationally efficient algorithm, its direct software implementation on complex problems, targeted on low capacity embedded processors could however suffer from poor execution performance. This paper first evaluates the performance of the standard PSO algorithm on a typical embedded platform (using a 16-bit microcontroller). Then, a modular, flexible and reusable architecture for a hardware PSO engine, for accelerating the algorithm's performance, will be presented. Finally, implementation test results of the proposed architecture targeted on Field Programmable Gate Array (FPGA) technology will be presented and its performance compared against software executions on benchmark test functions.},
  doi       = {10.1109/CEC.2009.4983226},
  keywords  = {embedded systems;field programmable gate arrays;particle swarm optimisation;software architecture;software performance evaluation;particle swarm optimization;embedded computing;software implementation;embedded processors;architecture;field programmable gate array;FPGA;benchmark test functions;Acceleration;Particle swarm optimization;Embedded computing;Software performance;Computer architecture;Software testing;Field programmable gate arrays;Application software;Software algorithms;Embedded software},
}

@InProceedings{7062496,
  author    = {Z. {Lai} and K. T. {Lam} and C. {Wang} and J. {Su}},
  title     = {Power and performance analysis of the Graph 500 benchmark on the Single-chip Cloud Computer},
  booktitle = {Proceedings of 2014 International Conference on Cloud Computing and Internet of Things},
  year      = {2014},
  pages     = {9-13},
  month     = {Dec},
  abstract  = {The concerns of data-intensiveness and energy awareness are actively reshaping the design of high-performance computing (HPC) systems nowadays. The Graph500 is a widely adopted benchmark for evaluating the performance of computing systems for data-intensive workloads. In this paper, we introduce a data-parallel implementation of Graph500 on the Intel Single-chip Cloud Computer (SCC). The SCC features a non-coherent many-core architecture and multi-domain on-chip DVFS support for dynamic power management. With our custom-made shared virtual memory programming library, memory sharing among threads is done efficiently via the shared physical memory (SPM) while the library has taken care of the coherence. We conduct an in-depth study on the power and performance characteristics of the Graph500 workloads running on this system with varying system scales and power states. Our experimental results are insightful for the design of energy-efficient many-core systems for data-intensive applications.},
  doi       = {10.1109/CCIOT.2014.7062496},
  keywords  = {cloud computing;parallel processing;software architecture;software performance evaluation;storage management;power analysis;performance analysis;Graph 500 benchmark;single-chip cloud computer;energy awareness;high-performance computing;HPC systems;data-intensive workloads;data-parallel implementation;non-coherent many-core architecture;multidomain on-chip DVFS support;dynamic power management;custom-made shared virtual memory programming library;memory sharing;shared physical memory;SPM;Kernel;Benchmark testing;Computer architecture;Programming;Power measurement;Computational modeling;Runtime;performance analysis;power efficiency;DVFS;Graph 500;data-intensive computing;many-core computing},
}

@InProceedings{7459489,
  author    = {R. d. {Clercq} and R. D. {Keulenaer} and B. {Coppens} and B. {Yang} and P. {Maene} and K. d. {Bosschere} and B. {Preneel} and B. d. {Sutter} and I. {Verbauwhede}},
  title     = {SOFIA: Software and control flow integrity architecture},
  booktitle = {2016 Design, Automation Test in Europe Conference Exhibition (DATE)},
  year      = {2016},
  pages     = {1172-1177},
  month     = {March},
  abstract  = {Microprocessors used in safety-critical systems are extremely sensitive to software vulnerabilities, as their failure can lead to injury, damage to equipment, or environmental catastrophe. This paper proposes a hardware-based security architecture for microprocessors used in safety-critical systems. The proposed architecture provides protection against code injection and code reuse attacks. It has mechanisms to protect software integrity, perform control flow integrity, prevent execution of tampered code, and enforce copyright protection. We are the first to propose a mechanism to enforce control flow integrity at the finest possible granularity. The proposed architectural features were added to the LEON3 open source soft microprocessor, and were evaluated on an FPGA running a software benchmark. The results show that the hardware area is 28.2% larger and the clock is 84.6% slower, while the software benchmark has a cycle overhead of 13.7% and a total execution time overhead of 110% when compared to an unmodified processor.},
  keywords  = {copyright;safety-critical software;security of data;software architecture;SOFIA;control flow integrity architecture;software integrity architecture;hardware-based security architecture;microprocessors;safety-critical systems;code injection protection;code reuse attacks;software integrity protection;tampered code execution prevention;copyright protection;LEON3 open source soft microprocessor;FPGA;Software;Radiation detectors;Computer architecture;Encryption;Microprocessors},
}

@InProceedings{1639593,
  author    = {A. {Kogekar} and D. {Kaul} and A. {Gokhale} and P. {Vandal} and U. {Praphamontripong} and S. {Gokhale} and {Jing Zhang} and {Yuehua Lin} and J. {Gray}},
  title     = {Model-driven generative techniques for scalable performability analysis of distributed systems},
  booktitle = {Proceedings 20th IEEE International Parallel Distributed Processing Symposium},
  year      = {2006},
  pages     = {8 pp.-},
  month     = {April},
  abstract  = {The ever increasing societal demand for the timely availability of newer and feature-rich but highly dependable network-centric applications imposes the need for these applications to be constructed by the composition, assembly and deployment of off-the-shelf infrastructure and domain-specific services building blocks. Service oriented architecture (SOA) is an emerging paradigm to build applications in this manner by defining a choreography of loosely coupled building blocks. However, current research in SOA does not yet address the per for mobility (i.e., performance and dependability) challenges of these modern applications. Our research is developing novel mechanisms to address these challenges. We initially focus on the composition and configuration of the infrastructure hosting the individual services. We illustrate the use of domain-specific modeling languages and model weavers to model infrastructure composition using middleware building blocks, and to enhance these models with the desired performability attributes. We also demonstrate the use of generative tools that synthesize metadata from these models for performability validation using analytical, simulation and empirical benchmarking tools.},
  doi       = {10.1109/IPDPS.2006.1639593},
  keywords  = {simulation languages;middleware;meta data;model-driven generative technique;scalable performability analysis;distributed system;service oriented architecture;domain-specific modeling language;middleware building block;generative tool;metadata;performability validation;analytical benchmarking tool;simulation tool;empirical benchmarking tool;model driven development;generative programming;Performance analysis;Application software;Service oriented architecture;Computer science;Middleware;Programming;Assembly;Information analysis;Computer networks;Distributed computing;Model driven development;Generative programming;Performability},
}

@InProceedings{6657076,
  author    = {A. {Sharma} and J. {Sloan} and L. F. {Wanner} and S. H. {Elmalaki} and M. B. {Srivastava} and P. {Gupta}},
  title     = {Towards analyzing and improving robustness of software applications to intermittent and permanent faults in hardware},
  booktitle = {2013 IEEE 31st International Conference on Computer Design (ICCD)},
  year      = {2013},
  pages     = {435-438},
  month     = {Oct},
  abstract  = {Although a significant fraction of emerging failure and wearout mechanisms result in intermittent or permanent faults in hardware, their impact (as distinct from transient faults) on software applications has not been well studied. In this paper, we develop a distinguishing application characteristic, referred to as similarity from fundamental circuit-level understanding of the failure mechanisms. We present a mathematical definition and a procedure for similarity computation for practical software applications and experimentally verify the relationship between similarity and fault rate. Leveraging dependence of application robustness on the similarity metric, we present example architecture independent code transformations to reduce similarity and thereby the worst-case fault rate with minimal performance degradation. Our experimental results with arithmetic unit faults show as much as 74% improvement in the worst case fault rate on benchmark kernels, with less than 10% runtime penalty.},
  doi       = {10.1109/ICCD.2013.6657076},
  keywords  = {digital arithmetic;program compilers;program diagnostics;software architecture;software fault tolerance;system recovery;robustness analysis;robustness improvement;software application;intermittent hardware fault;permanent hardware fault;failure mechanism;wearout mechanism;transient fault;circuit-level understanding;mathematical definition;similarity computation;architecture independent code transformation;worst-case fault rate;performance degradation;arithmetic unit fault;benchmark kernels;runtime penalty;Circuit faults;Vectors;Hardware;Software;Delays;Conferences;Computer architecture;Permanent fault;code transformation},
}

@Article{6355533,
  author   = {M. {Kayaalp} and M. {Ozsoy} and N. A. {Ghazaleh} and D. {Ponomarev}},
  title    = {Efficiently Securing Systems from Code Reuse Attacks},
  journal  = {IEEE Transactions on Computers},
  year     = {2014},
  volume   = {63},
  number   = {5},
  pages    = {1144-1156},
  month    = {May},
  abstract = {Code reuse attacks (CRAs) are recent security exploits that allow attackers to execute arbitrary code on a compromised machine. CRAs, exemplified by return-oriented and jump-oriented programming approaches, reuse fragments of the library code, thus avoiding the need for explicit injection of attack code on the stack. Since the executed code is reused existing code, CRAs bypass current hardware and software security measures that prevent execution from data or stack regions of memory. While software-based full control flow integrity (CFI) checking can protect against CRAs, it includes significant overhead, involves non-trivial effort of constructing a control flow graph, relies on proprietary tools and has potential vulnerabilities due to the presence of unintended branch instructions in architectures such as x86-those branches are not checked by the software CFI. We propose branch regulation (BR), a lightweight hardware-supported protection mechanism against the CRAs that addresses all limitations of software CFI. BR enforces simple control flow rules in hardware at the function granularity to disallow arbitrary control flow transfers from one function into the middle of another function. This prevents common classes of CRAs without the complexity and run-time overhead of full CFI enforcement. BR incurs a slowdown of about 2% and increases the code footprint by less than 1% on the average for the SPEC 2006 benchmarks.},
  doi      = {10.1109/TC.2012.269},
  keywords = {flow graphs;security of data;software architecture;software libraries;software reusability;code reuse attacks;system security;CRA;security exploits;arbitrary code;compromised machine;return-oriented programming approaches;jump-oriented programming approaches;reuse fragments;library code;attack code;executed code;software security measures;hardware security measures;software-based full control flow integrity checking;nontrivial effort;control flow graph;unintended branch instructions;x86;branch regulation;lightweight hardware-supported protection mechanism;BR;software CFI;SPEC 2006 benchmarks;function granularity;arbitrary control flow transfers;Libraries;Software;Hardware;Security;Programming;Computer architecture;Benchmark testing;Security;microarchitecture;code reuse attacks},
}

@InProceedings{5396149,
  author    = {K. R. {Shetti} and C. L. {Koh} and M. T. {Aung} and T. {Bretschneider}},
  title     = {Development and code partitioning in a software configurable processor},
  booktitle = {TENCON 2009 - 2009 IEEE Region 10 Conference},
  year      = {2009},
  pages     = {1-5},
  month     = {Jan},
  abstract  = {Most reconfigurable processors are not fully controlled by software; they are reconfigured using hardware description languages. By moving the data paths into the processor, the system architect can discard the external control logic, the finite state machines and micro-sequencers. Examples for such a processor are the members of the Stretch family, Software Configurable Processors which have a reconfigurable fabric embedded inside the actual processor. This allows development in a high level programming language like C. The paper provides an overview of the architecture and a case study problem based on the 2D FFT illustrating the issue of software-hardware partitioning. A software only solution is first described and is used as a benchmark to compare two code partitioning schemes proposed later. The designs of the partitions are described in detail along with their time analyses. The primary criterion chosen is the overall execution time along with the frequency at which the reconfigurable fabric can be clocked. By comparing the results it can be concluded as to which partition performs better and the parameters that contribute to reduction in execution time. This paper also provides an overview of the challenges and benefits of software reconfigurable processors.},
  doi       = {10.1109/TENCON.2009.5396149},
  keywords  = {C language;fast Fourier transforms;hardware description languages;program processors;software architecture;code partitioning;software configurable processor;reconfigurable processors;hardware description languages;C language;software-hardware partitioning;2D FFT;Fabrics;Hardware design languages;Control systems;Reconfigurable logic;Logic programming;Automata;Embedded software;Computer languages;Computer architecture;Frequency;Software configurable processors;Code partitioning;Fast Fourier Transform},
}

@InProceedings{7482087,
  author    = {M. {Malik} and A. {Sasan} and R. {Joshi} and S. {Rafatirah} and H. {Homayoun}},
  title     = {Characterizing Hadoop applications on microservers for performance and energy efficiency optimizations},
  booktitle = {2016 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2016},
  pages     = {153-154},
  month     = {April},
  abstract  = {The traditional low-power embedded processors such as Atom and ARM are entering the high-performance server market. At the same time, as the size of data grows, emerging Big Data applications require more and more server computational power that yields challenges to process data energy-efficiently using current high performance server architectures. Furthermore, physical design constraints, such as power and density have become the dominant limiting factor for scaling out servers. Numerous big data applications rely on using the Hadoop MapReduce framework to perform their analysis on large-scale datasets. Since Hadoop configuration parameters as well as architecture parameters directly affect the MapReduce job performance and energy-efficiency, system and architecture level parameters tuning is vital to maximize the energy efficiency. In this work, through methodical investigation of performance and power measurements, we demonstrate how the interplay among various Hadoop configurations and system and architecture level parameters affect the performance and energy-efficiency across various Hadoop applications.},
  doi       = {10.1109/ISPASS.2016.7482087},
  keywords  = {Big Data;file servers;parallel processing;power aware computing;software architecture;software performance evaluation;Hadoop;MapReduce;microserver architecture;performance optimization;energy efficiency optimization;Big Data;Energy efficiency;Big data;Computer architecture;Servers;Tuning;Benchmark testing;Time-frequency analysis},
}

@InProceedings{6164941,
  author    = {M. {Abdur Rouf} and S. {Kim}},
  title     = {Low-cost control flow error protection by exploiting available redundancies in the pipeline},
  booktitle = {17th Asia and South Pacific Design Automation Conference},
  year      = {2012},
  pages     = {175-180},
  month     = {Jan},
  abstract  = {Due to device miniaturization and reducing supply voltage, embedded systems are becoming more susceptible to transient faults. Specifically, faults in control flow can change the execution sequence, which might be catastrophic for safety critical applications. Many techniques are devised using software, hardware or software-hardware co-design for control flow error checking. Software techniques suffer from a significant amount of code size overhead, and hence, negative impact on performance and energy consumption. On the other hand, hardware-based techniques have a significant amount of hardware and area cost. In this research we exploit the available redundancies in the pipeline. The branch target buffer stores target addresses of taken branches, and ALU generates target addresses using the low-order branch displacement bits of branch instructions. To exploit these redundancies in the pipeline, we propose a control flow error checking (CFEC) scheme. It can detect control flow errors and recover from them with negligible energy and performance overhead.},
  doi       = {10.1109/ASPDAC.2012.6164941},
  keywords  = {buffer storage;embedded systems;error correction;error detection;hardware-software codesign;performance evaluation;pipeline processing;redundancy;safety-critical software;software architecture;low cost control flow error protection;device miniaturization;embedded system;transient fault;safety critical application;software-hardware codesign;control flow error checking;software technique;energy consumption;hardware-based technique;low order branch displacement;branch instruction;control flow error detection;hardware design;software design;Registers;Pipelines;Redundancy;Hardware;Software;Transient analysis;Benchmark testing;control flow error checking;transient fault;branch target buffer;low energy},
}

@InProceedings{4032310,
  author    = {B. {Jin} and G. {Li} and L. {Zhang}},
  title     = {Reconstruct the Distributed Transaction Monitor OnceTX},
  booktitle = {2006 Sixth International Conference on Quality Software (QSIC'06)},
  year      = {2006},
  pages     = {393-402},
  month     = {Oct},
  abstract  = {Distributed transaction monitor is the fundamental software for critical transactional applications across network. We concern its quality from the point of view of practicality. This paper presents the key technologies during the reconstruction of the distributed transaction monitor OnceTX1.0 to OnceTX2.0, including scalable architecture, multi-phase concurrency control, dynamic configuration and access for system services, and optimization of message organization and processing. OnceTX2.0 is now applicable to not only small-scale and tight-coupling transaction applications, but also geographically dispersed applications with transaction demands. Its performance has also been enhanced, which is proved by the benchmark results. Moreover, extensibility and maintainability are achieved},
  doi       = {10.1109/QSIC.2006.52},
  keywords  = {concurrency control;distributed processing;software architecture;software quality;system monitoring;transaction processing;distributed transaction monitor;OnceTX;critical transactional applications;scalable architecture;multiphase concurrency control;dynamic configuration;system service access;message organization optimization;processing optimization;geographically dispersed applications;transaction demands;Monitoring;Containers;Application software;Concurrency control;Scalability;Master-slave;Topology;Computer architecture;Switches;Software engineering;Distributed Transaction Monitor;Scalability;Concurrency Control;Dynamic Configuration;Optimization},
}

@Article{7416213,
  author   = {Y. {Lu} and X. {Wang} and W. {Zhang} and H. {Chen} and L. {Peng} and W. {Zhao}},
  title    = {Performance Analysis of Multimedia Retrieval Workloads Running on Multicores},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2016},
  volume   = {27},
  number   = {11},
  pages    = {3323-3337},
  month    = {Nov},
  abstract = {Multimedia data has become a major data type in the Big Data era. The explosive volume of such data and the increasing real-time requirement to retrieve useful information from it have put significant pressure in processing such data in a timely fashion. However, while prior efforts have done in-depth analysis on architectural characteristics of traditional multimedia processing and text-based retrieval algorithms, there has been no systematic study towards the emerging multimedia retrieval applications. This may impede the architecture design and system evaluation of these applications. In this paper, we make the first attempt to construct a multimedia retrieval benchmark suite (MMRBench for short) that can be used to evaluate architectures and system designs for multimedia retrieval applications. MMRBench covers modern multimedia retrieval algorithms with different versions (sequential, parallel and distributed). MMRBench also provides a series of flexible interfaces as well as certain automation tools. With such a flexible design, the algorithms in MMRBench can be used both in individual kernel-level evaluation and in integration to form a complete multimedia data retrieval infrastructure for full system evaluation. Furthermore, we use performance counters to analyze a set of architecture characteristics of multimedia retrieval algorithms in MMRBench, including the characteristics of core level, chip level and inter-chip level. The study shows that micro-architecture design in current processor is inefficient (both in performance and power) for these multimedia retrieval workloads, especially in core resources and memory systems. We then derive some insights into the architecture design and system evaluation for such multimedia retrieval algorithms.},
  doi      = {10.1109/TPDS.2016.2533606},
  keywords = {Big Data;information retrieval;multimedia computing;multiprocessing systems;software architecture;software performance evaluation;text analysis;user interfaces;performance analysis;multimedia retrieval workload;multimedia retrieval benchmark suite;MMRBench;multicore processor;Big Data;text-based retrieval algorithm;user interface;architecture design;system evaluation;Multimedia communication;Algorithm design and analysis;Benchmark testing;Feature extraction;Computer architecture;Streaming media;Multimedia retrieval;benchmarks;architectural characteristics},
}

@InProceedings{6219231,
  author    = {W. {Yuanli} and Y. {Hongqiao} and W. {Hui} and W. {Huaimin}},
  title     = {CBD-OSGi: A distributed OSGi middleware},
  booktitle = {2012 IEEE Symposium on Robotics and Applications (ISRA)},
  year      = {2012},
  pages     = {490-493},
  month     = {June},
  abstract  = {As OSGi technology has been widely used in distributed environment such as pervasive computing and enterprise computing these years, standard centralized OSGi specification needs to be extended to support distributed processing. For the deficiencies of existing works in this domain, this paper proposes a CORBA-based distributed OSGi model, which aims at non-invasiveness, generality and interoperation. Based on this model, this paper designs and implements an OSGi distributed extension middleware - CBD-OSGi, which supports the remote service invocation capability with CORBA dynamic invocation and Java reflection technology. It can deploy the central OSGi applications into distributed environment transparently while keeping the service-oriented programming model and lightweight feature of OSGi. Furthermore, CBD-OSGi supports the interoperation between OSGi and CORBA applications and also has performance advantages over existing projects.},
  doi       = {10.1109/ISRA.2012.6219231},
  keywords  = {distributed object management;formal specification;Java;middleware;service-oriented architecture;CBD-OSGi technology;distributed OSGi middleware;standard centralized OSGi specification;distributed processing;CORBA-based distributed OSGi model;OSGi distributed extension middleware;remote service invocation capability;CORBA dynamic invocation;Java reflection technology;service-oriented programming model;Java;Protocols;Computational modeling;Benchmark testing;Distributed processing;Software;Monitoring;OSGi;Service-Oriented;Distributed Extension;Middleware},
}

@InProceedings{6957193,
  author    = {C. {Engelmann} and T. {Naughton}},
  title     = {Improving the Performance of the Extreme-Scale Simulator},
  booktitle = {2014 IEEE/ACM 18th International Symposium on Distributed Simulation and Real Time Applications},
  year      = {2014},
  pages     = {198-207},
  month     = {Oct},
  abstract  = {Investigating the performance of parallel applications at scale on future high-performance computing (HPC) architectures and the performance impact of different architecture choices is an important component of HPC hardware/software co-design. The Extreme-scale Simulator (xSim) is a simulation-based toolkit for investigating the performance of parallel applications at scale. xSim scales to millions of simulated Message Passing Interface (MPI) processes. The overhead introduced by a simulation tool is an important performance and productivity aspect. This paper documents two improvements to xSim: (1) a new deadlock resolution protocol to reduce the parallel discrete event simulation management overhead and (2) a new simulated MPI message matching algorithm to reduce the oversubscription management overhead. The results clearly show a significant performance improvement, such as by reducing the simulation overhead for running the NAS Parallel Benchmark suite inside the simulator from 1,020% to 238% for the conjugate gradient (CG) benchmark and from 102% to 0% for the embarrassingly parallel (EP) and benchmark, as well as, from 37,511% to 13,808% for CG and from 3,332% to 204% for EP with accurate process failure simulation.},
  doi       = {10.1109/DS-RT.2014.32},
  keywords  = {conjugate gradient methods;hardware-software codesign;message passing;parallel processing;software architecture;extreme-scale simulator;parallel applications;high-performance computing;HPC architectures;hardware/software codesign;xSim;simulation-based toolkit;message passing interface;MPI process;NAS parallel benchmark suite;conjugate gradient benchmark;System recovery;Clocks;Protocols;Computer architecture;Context;Benchmark testing;Computational modeling;Performance Prediction;Message Passing Interface;Parallel Discrete Event Simulation;High-performance Computing},
}

@InProceedings{5069082,
  author    = {S. {Cheng} and D. {Garlan} and B. {Schmerl}},
  title     = {Evaluating the effectiveness of the Rainbow self-adaptive system},
  booktitle = {2009 ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems},
  year      = {2009},
  pages     = {132-141},
  month     = {May},
  abstract  = {Rainbow is a framework for engineering a system with run-time, self-adaptive capabilities to monitor, detect, decide, and act on opportunities for system improvement. We applied Rainbow to a system, Znn.com, and evaluated its effectiveness to self-adapt on three levels: its effectiveness to maintain quality attribute in the face of changing conditions, run-time overheads of adaptation, and the engineering effort to use it to add self-adaptive capabilities to Znn.com. We make Znn.com and the associated evaluation tools available to the community so that other researchers can use it to evaluate their own systems and the community can compare different systems. In this paper, we report on our evaluation experience, reflect on some principles for benchmarking self-adaptive systems, and discuss the suitability of our evaluation tools for this purpose.},
  doi       = {10.1109/SEAMS.2009.5069082},
  keywords  = {software architecture;software maintenance;software performance evaluation;software quality;system monitoring;systems engineering;rainbow self-adaptive system;system monitoring;quality attribute maintenance;run-time overhead;rainbow evaluation tool;system engineering;architecture-based technique;Znn.com experimental platform;Maintenance engineering;Monitoring;Systems engineering and theory;Humans;Automatic control;Utility theory;Face detection;Runtime;Quality of service;Control systems},
}

@InProceedings{4584431,
  author    = {M. {Jiang} and L. {Li} and M. {Hu} and Y. {Ding}},
  title     = {Design and Model Analysis of the E-Commerce Development Platform for 3-Tiered Web Applications},
  booktitle = {2008 International Conference on Advanced Language Processing and Web Information Technology},
  year      = {2008},
  pages     = {581-584},
  month     = {July},
  abstract  = {Many Internet applications employ 3-tier software architecture such as e-commerce system. Starting from analyze the architecture of an e-commerce development platform model based on 3-tiered Web, We present by using queuing network theory, we propose a reusable e-commerce development platform based on 3-tiered Web. Finally, we implement the MVA algorithm with the help of Matlab and test the experimental data in LoadRunner benchmark.},
  doi       = {10.1109/ALPIT.2008.34},
  keywords  = {electronic commerce;queueing theory;software architecture;software reusability;Web services;reusable e-commerce development platform;3-tiered Web application;Internet;3-tier software architecture;queuing network theory;Matlab;mean-value analysis;Throughput;Mathematical model;Business;Time factors;Analytical models;Databases;Load modeling;e-commerce},
}

@InProceedings{1281775,
  author    = {M. H. {Assaf} and S. R. {Das} and E. M. {Petriu} and M. {Sahinoglu}},
  title     = {Enhancing testability in architectural design for the new generation of core-based embedded systems},
  booktitle = {Eighth IEEE International Symposium on High Assurance Systems Engineering, 2004. Proceedings.},
  year      = {2004},
  pages     = {312-313},
  month     = {March},
  abstract  = {This paper proposes testability enhancements in architectural design for embedded cores-based system-on-a-chip (SoC). There exist methods to ensure correct SoC functionality in both hardware and software, but one of the most reliable ways to realize this is through the use of design for testability approaches. Specifically, applications of built-in self-test (BIST) methodology for testing embedded cores are considered in the paper, with specific implementations being targeted towards ISCAS 85 combinational and ISCAS 89 sequential benchmark circuits.},
  doi       = {10.1109/HASE.2004.1281775},
  keywords  = {embedded systems;built-in self test;circuit testing;software architecture;system-on-chip;program testing;software testability;architectural design;core-based embedded systems;system-on-chip;SoC;built-in self-test;BIST;sequential benchmark circuits;System testing;Embedded system;System-on-a-chip;Built-in self-test;Circuit testing;Hardware;Design for testability;Application software;Automatic testing;Sequential analysis},
}

@InProceedings{1419917,
  author    = {F. J. {Seinstra} and C. G. M. {Snoek} and D. {Koelma} and J. M. {Geusebroek} and M. {Worring}},
  title     = {User transparent parallel processing of the 2004 NIST TRECVID data set},
  booktitle = {19th IEEE International Parallel and Distributed Processing Symposium},
  year      = {2005},
  pages     = {8 pp.-},
  month     = {April},
  abstract  = {The Parallel-Horus framework, developed at the University of Amsterdam, is a unique software architecture that allows non-expert parallel programmers to develop fully sequential multimedia applications for efficient execution on homogeneous Beowulf-type commodity clusters. Previously obtained results for realistic, but relatively small-sized applications have shown the feasibility of the Parallel-Horus approach, with parallel performance consistently being found to be optimal with respect to the abstraction level of message passing programs. In this paper we discuss the most serious challenge Parallel-Horus has had to deal with so far: the processing of over 184 hours of video included in the 2004 NIST TRECVID evaluation, i.e. the de facto international standard benchmark for content-based video retrieval. Our results and experiences confirm that Parallel-Horus is a very powerful support-tool for state-of-the-art research and applications in multimedia processing.},
  doi       = {10.1109/IPDPS.2005.443},
  keywords  = {software architecture;parallel programming;message passing;content-based retrieval;multimedia computing;video databases;workstation clusters;Parallel-Horus framework;software architecture;parallel processing;sequential multimedia applications;homogeneous Beowulf-type commodity clusters;message passing programs;content-based video retrieval;NIST TRECVID data set;Parallel processing;NIST;Application software;Programming profession;Message passing;Libraries;Content based retrieval;Informatics;Software architecture;Multimedia computing},
}

@InProceedings{4547825,
  author    = {S. {Rogmans} and J. {Lu} and G. {Lafruit}},
  title     = {A Scalable End-to-End Optimized Real-Time Image-Based Rendering Framework on Graphics Hardware},
  booktitle = {2008 3DTV Conference: The True Vision - Capture, Transmission and Display of 3D Video},
  year      = {2008},
  pages     = {129-132},
  month     = {May},
  abstract  = {This paper presents the system-level overview of a real-time image- based rendering framework performing multiple intermediate view synthesis, completely on the Graphics Processing Unit (GPU). The software design achieves high-performance, yet maintains flexibility and ease of development through a hierarchical layered architecture. The framework implements the intermediate view synthesis by a chain of consecutive processing modules, as an extension to the Middlebury open software structure, allowing it to benchmark quality and execution time of individual modules for end-to-end system performance optimization. The modules can be flexibly coordinated, enabling scalability to run the multiple view synthesis in real-time on both powerful and weak GPUs.},
  doi       = {10.1109/3DTV.2008.4547825},
  keywords  = {computer graphic equipment;image matching;optimisation;rendering (computer graphics);software architecture;stereo image processing;real-time image-based rendering framework;graphics hardware;graphics processing unit;software design;Middlebury open software structure;end-to-end system performance optimization;stereo matching;Rendering (computer graphics);Graphics;Hardware;Real time systems;Software design;Computer architecture;Software performance;Software quality;System performance;Scalability;scalability;real-time;end-to-end system optimization;framework;image-based rendering;GPU;stereo matching},
}

@InProceedings{723290,
  author    = {K. {Wakayama} and S. {Tanabe} and Y. {Takeda}},
  title     = {Distributed Control Architecture Based On In Benchmark},
  booktitle = {Workshop on Intelligent Network},
  year      = {1994},
  pages     = {1_263-1_278},
  month     = {May},
  abstract  = {A new benchmark is introduced to represent the processing power for advanced telecommunication services. Traffic estimation using this benchmark shows the necessity of a distributed control architecture that provides high throughput to satisfy rapid increase of processing power in telecommunication networks. Three software architectures are proposed for a distributed network environment. Model evaluation demonstrates the efficiency of these approaches.},
  doi       = {10.1109/INW.1994.723290},
  keywords  = {Distributed control;Telecommunication services;Transaction databases;Communication system traffic control;Traffic control;Computer architecture;Throughput;Software architecture;Telephony;Computer networks},
}

@InProceedings{5993659,
  author    = {C. {Brandolese} and S. {Corbetta} and W. {Fornaciari}},
  title     = {Software energy estimation based on statistical characterization of intermediate compilation code},
  booktitle = {IEEE/ACM International Symposium on Low Power Electronics and Design},
  year      = {2011},
  pages     = {333-338},
  month     = {Aug},
  abstract  = {Early estimation of embedded software power consumption is a critical issue that can determine the quality and, sometimes, the feasibility of a system. Architecture-specific, cycle-accurate simulators are valuable tools for fine-tuning performance of critical sections of the application but are often too slow for the simulation of entire systems. This paper proposes a fast and statistically accurate methodology to evaluate the energy performance of embedded software and describes the associated toolchain. The methodology is based on a static characterization of the target instruction set to allow estimation on an equivalent, target-independent intermediate code representation.},
  doi       = {10.1109/ISLPED.2011.5993659},
  keywords  = {power aware computing;program compilers;software architecture;software energy estimation;statistical characterization;intermediate compilation code;embedded software power consumption;fine-tuning performance;target-independent intermediate code representation;Mathematical model;Assembly;Estimation;Equations;Instruments;Software;Benchmark testing},
}

@InProceedings{4222615,
  author    = {Y. {Jin} and A. {Tang} and J. {Han} and Y. {Liu}},
  title     = {Performance Evaluation and Prediction for Legacy Information Systems},
  booktitle = {29th International Conference on Software Engineering (ICSE'07)},
  year      = {2007},
  pages     = {540-549},
  month     = {May},
  abstract  = {Database-centric information systems are critical to the operations of large organisations. In particular, they often process a large amount of data with stringent performance requirements. Currently, however, there is a lack of systematic approaches to evaluating and predicting their performance when they are subject to an exorbitant growth of workload. In this paper, we introduce such a systematic approach that combines benchmarking, production system monitoring, and performance modelling (BMM) to address this issue. The approach helps the performance analyst to understand the system's operating environment and quantify its performance characteristics under varying load conditions via monitoring and benchmarking. Based on such realistic measurements, modelling techniques are used to predict the system performance. Our experience of applying BMM to a real-world system demonstrates the capability of BMM in predicting the performance of existing and enhanced software architectures in planning for its capacity growth.},
  doi       = {10.1109/ICSE.2007.64},
  keywords  = {database management systems;software architecture;performance evaluation;legacy information systems;database-centric information systems;production system monitoring;performance modelling;BMM;enhanced software architectures;Information systems;Predictive models;Databases;Production systems;Australia;System performance;Performance analysis;Capacity planning;Software performance;Hardware},
}

@InProceedings{6825693,
  author    = {S. {Harrer} and C. {Röck} and G. {Wirtz}},
  title     = {Automated and Isolated Tests for Complex Middleware Products: The Case of BPEL Engines},
  booktitle = {2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops},
  year      = {2014},
  pages     = {390-398},
  month     = {March},
  abstract  = {Today, a plethora of enterprise middleware solutions are available, leading to the problem of choosing the right tool for a specific use case. Automated tests can support the selection of such software by determining decision relevant metrics, like e.g., throughput or the degree of standard conformance. To avoid side effects between tests, test isolation, i.e., to provide fresh instances of the software for each test execution, is essential. However, middleware suites are inherently complex, provide a large range of configuration options, have tedious or sometimes manual installation procedures, and long startup times. These idiosyncrasies aggravate the creation of fresh instances of such middleware suites, leading to slower turnaround times and increasing the cost for ensuring test isolation. We aim to overcome these issues with methods and tools from the area of virtualization and devops. In this work, we focus on BPEL engines which are common middleware components in Web Service based SOAs. We applied our proposed method to the BPEL Engine Test System (betsy), a conformance test suite and testing tool for BPEL engines. Results reveal that our method a) enables automatic creation of fresh instances of software without manual installation steps, b) reduces the time to create these fresh instance dramatically, and c) introduces only a neglectable performance overhead, therefore, reducing the overall costs of testing complex software.},
  doi       = {10.1109/ICSTW.2014.45},
  keywords  = {conformance testing;cost reduction;middleware;program testing;service-oriented architecture;Web Services Business Process Execution Language;automated test;isolated test;complex middleware product;software selection;test execution;middleware suites;middleware components;Web service based SOA;BPEL engine test system;conformance test suite;testing tool;cost reduction;complex software testing;Engines;Middleware;Virtual machining;Standards;Benchmark testing;test isolation;test automation;virtualization;BPEL engines},
}

@InProceedings{8065752,
  author    = {A. {Mohsin} and S. I. R. {Naqvi} and A. U. {Khan} and T. {Naeem} and M. A. {AsadUllah}},
  title     = {A comprehensive framework to quantify fault tolerance metrics of web centric mobile applications},
  booktitle = {2017 International Conference on Communication Technologies (ComTech)},
  year      = {2017},
  pages     = {65-71},
  month     = {April},
  abstract  = {Applications well-defined with high Fault Tolerance index are considered reliable and trustworthy to drive quality. Fault Tolerance metrics for web mobile applications are neither defined formally nor are they organized at a single place for measurement to researchers and quality engineers. Fault Tolerance metrics determines the quality of web mobile applications provided they are measured accurately. There is a phenomenal gap between Fault Tolerance (FT) metrics development and measurement for web based mobile applications. Unfortunately, existing research has failed in providing a precise and well-organized definition for Fault Tolerance metrics in the domain of web mobile applications. This has led to inadequate Fault Tolerance metrics to perfectly measure and benchmark modern day web mobile applications. This research work has developed a comprehensive Fault Tolerance Metrics Framework, targeting Web and Mobile Applications to measure associated quality attributes by applying GQM technique. Dispersed FT attributes are identified and organized from existing Quality Models and standards. Sub factors have been well defined using standard units in line with web architectures. Particularly, this research applies Systematic Mathematical formulation to quantify Fault Tolerance to target web mobile applications. Further Fault Tolerance is measured and evaluated through an experimental study designed on target web mobile applications.},
  doi       = {10.1109/COMTECH.2017.8065752},
  keywords  = {Internet;mobile computing;software architecture;software fault tolerance;software metrics;software quality;web centric mobile applications;high Fault Tolerance index;Fault Tolerance metrics development;web based mobile applications;comprehensive Fault Tolerance Metrics Framework;target web mobile applications;associated quality attributes;GQM technique;dispersed FT attributes;Web architectures;systematic mathematical formulation;Fault tolerance;Fault tolerant systems;Measurement;Software;Modeling;Software reliability;Fault Tolerance;Web Centric Applications;Software Quality metrics;Reliability;Availability;Product Quality},
}

@InProceedings{8432188,
  author    = {H. {Alipour} and Y. {Liu}},
  title     = {Model Driven Deployment of Auto-Scaling Services on Multiple Clouds},
  booktitle = {2018 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  year      = {2018},
  pages     = {93-96},
  month     = {April},
  abstract  = {Hybrid cloud platforms have been adopted to facilitate different parts of services to deliver functionalities to service consumers. Each cloud platform offers elastic resource allocation, which accommodates fluctuating demands on services by automating the provision/deprovision of resources, referred as auto-scaling. In term of service deployment, auto-scaling is usually not interoperable between multiple cloud platforms. As a result, the service level auto-scaling strategy needs to be configured separately on disparate cloud platforms, which incurs difficulties in tracing the configuration and maintaining consistent deployment. This paper presents a model-driven method to connect a cloud platform independent model of services with cloud specific operations. Through the automated transformation from model to the configuration, we use cloud management tools to deliver auto-scaling deployment across clouds. We demonstrate our method with scaling configuration and deployment of an open source benchmark application - Dell DVD store on two cloud platforms, AWS and Rackspace. The experiment demonstrates our proposed method resolves the vendor lock issues by a model-to-configuration-to-deployment automation. The empirical measurement shows our method reduces the effort of deploying auto-scaling services on cloud platforms.},
  doi       = {10.1109/ICSA-C.2018.00033},
  keywords  = {cloud computing;resource allocation;service level auto-scaling strategy;model-driven method;cloud platform independent model;cloud specific operations;cloud management tools;auto-scaling deployment;model-to-configuration-to-deployment automation;auto-scaling services;model driven deployment;hybrid cloud platforms;elastic resource allocation;service deployment;Dell DVD store;AWS platform;Rackspace platform;Cloud computing;Unified modeling language;Tools;DVD;Manuals;Computational modeling;Benchmark testing;Model-Driven;Auto-scaling;Cloud Computing;DevOps},
}

@Article{4526979,
  author   = {D. Q. {Ren} and T. {Park} and B. {Mirican} and S. {McFee} and D. D. {Giannacopoulos}},
  title    = {A Methodology for Performance Modeling and Simulation Validation of Parallel 3-D Finite Element Mesh Refinement With Tetrahedra},
  journal  = {IEEE Transactions on Magnetics},
  year     = {2008},
  volume   = {44},
  number   = {6},
  pages    = {1406-1409},
  month    = {June},
  abstract = {The design and implementation of parallel finite element methods (FEMs) is a complex and error-prone task that can benefit significantly by simulating models of them first. However, such simulations are useful only if they accurately predict the performance of the parallel system being modeled. The purpose of this contribution is to present a new, practical methodology for validation of a promising modeling and simulation approach for parallel 3-D FEMs. To meet this goal, a parallel 3-D unstructured mesh refinement model is developed and implemented based on a detailed software prototype and parallel system architecture parameters in order to simulate the functionality and runtime behavior of the algorithm. Estimates for key performance measures are derived from these simulations and are validated with benchmark problem computations obtained using the actual parallel system. The results illustrate the potential benefits of the new methodology for designing high performance parallel FEM algorithms.},
  doi      = {10.1109/TMAG.2007.916572},
  keywords = {mesh generation;parallel processing;Petri nets;software architecture;software prototyping;parallel 3D finite element mesh refinement;error-prone task;parallel 3D unstructured mesh refinement model;software prototype;parallel system architecture parameters;runtime behavior;petri nets;simulation approach;Finite element methods;Predictive models;Computational modeling;Software prototyping;Virtual prototyping;Computer architecture;Runtime;Software algorithms;Concurrent computing;Design methodology;Finite element methods;mesh generation;parallel processing;petri nets},
}

@InProceedings{8567366,
  author    = {T. {Goethals} and M. {Sebrechts} and A. {Atrey} and B. {Volckaert} and F. {De Turck}},
  title     = {Unikernels vs Containers: An In-Depth Benchmarking Study in the Context of Microservice Applications},
  booktitle = {2018 IEEE 8th International Symposium on Cloud and Service Computing (SC2)},
  year      = {2018},
  pages     = {1-8},
  month     = {Nov},
  abstract  = {Unikernels are a relatively recent way to create and quickly deploy extremely small virtual machines that do not require as much functional and operational software overhead as containers or virtual machines by leaving out unnecessary parts. This paradigm aims to replace bulky virtual machines on one hand, and to open up new classes of hardware for virtualization and networking applications on the other. In recent years, the tool chains used to create unikernels have grown from proof of concept to platforms that can run both new and existing software written in various programming languages. This paper studies the performance (both execution time and memory footprint) of unikernels versus Docker containers in the context of REST services and heavy processing workloads, written in Java, Go, and Python. With the results of the performance evaluations, predictions can be made about which cases could benefit from the use of unikernels over containers.},
  doi       = {10.1109/SC2.2018.00008},
  keywords  = {Java;Python;virtual machines;virtualisation;virtualization;unikernels;Docker containers;microservice applications;virtual machines;REST services;Java;Go;Python;Containers;Virtual machining;Kernel;Java;Python;Virtual machine monitors;containers;unikernels;microservices;virtualization;IoT},
}

@InProceedings{6973757,
  author    = {J. {Li} and Q. {Wang} and C. A. {Lai} and J. {Park} and D. {Yokoyama} and C. {Pu}},
  title     = {The Impact of Software Resource Allocation on Consolidated n-Tier Applications},
  booktitle = {2014 IEEE 7th International Conference on Cloud Computing},
  year      = {2014},
  pages     = {320-327},
  month     = {June},
  abstract  = {Consolidating several under-utilized user applications together to achieve higher utilization of hardware resources is important for cloud vendors to reduce cost and maximize profit. In this paper, we study the impact of tuning software resources (e.g., server thread pool size or connection pool size) on n-tier web application performance in a consolidated cloud environment. By measuring CPU utilizations and performance of two consolidated n-tier web application benchmark systems running RUBBoS, we found significant differences depending on the amount of soft resources allocated. When the two systems have different soft resource allocations and are fully utilized, the application with more software resources may steal up to 8% CPU from the co-resident application. Further analysis shows that the CPU stealing is due to more threads being scheduled for the system with higher software resources. By limiting the number of runnable active threads for the consolidated VMs, we were able to mitigate the performance interference. More generally, our results show that careful software resource allocation is a significant factor when deploying and tuning n-tier application performance in clouds.},
  doi       = {10.1109/CLOUD.2014.51},
  keywords  = {client-server systems;cloud computing;resource allocation;software architecture;software resource allocation;n-tier Web application performance;cloud environment;thread scheduling;Resource management;Time factors;Servers;Throughput;Software;Virtual machine monitors;Databases;application colocation;consolidation;software resources;n-tier;performance;RUBBoS;sharing},
}

@InProceedings{5686441,
  author    = {C. {Huang} and C. {Chen} and S. {Yu} and S. {Hsu} and C. {Lin}},
  title     = {Accelerate in-line packet processing using fast queue},
  booktitle = {TENCON 2010 - 2010 IEEE Region 10 Conference},
  year      = {2010},
  pages     = {1048-1052},
  month     = {Nov},
  abstract  = {It is common for network researchers and system developers to run packet processing algorithms on UNIX-like operating systems. For the ease of development, complex packet processing algorithms are often implemented at the user-space level. As a result, performance benchmarks for packet processing algorithms often show a great gap when packets are input from different sources. An algorithm that performs well by reading packets from a raw packet trace file may get a worse result when it reads packets directly from a network interface. Such a phenomenon gets much worse when the algorithm is going to process packets in-line. In this paper, we identify the performance bottleneck of existing in-line packet processing implementations in the Linux operating system. Based on the observation, a new software architecture, named Fast Queue, is proposed and implemented to show that the identified bottleneck can be effectively eliminated. Experiments show that the proposed software architecture reduces 30% of CPU utilization. In addition, the overall system throughput can be improved by a factor of 1.6 when it is applied to the well-known snort-inline open source intrusion detection system.},
  doi       = {10.1109/TENCON.2010.5686441},
  keywords  = {Linux;public domain software;security of data;software architecture;accelerate in-line packet processing algorithm;fast queue;UNIX-like operating systems;user-space level;raw packet trace file;network interface;software architecture;snort-inline open source intrusion detection system;Linux operating system;Kernel;Linux;Benchmark testing;Sockets;Network interfaces;Servers;Fast Queue;in-line packet processing;zero copy interface},
}

@InProceedings{7273675,
  author    = {A. L. {Sartor} and A. F. {Lorenzon} and A. C. S. {Beck}},
  title     = {The Impact of Virtual Machines on Embedded Systems},
  booktitle = {2015 IEEE 39th Annual Computer Software and Applications Conference},
  year      = {2015},
  volume    = {2},
  pages     = {626-631},
  month     = {July},
  abstract  = {Embedded systems are becoming increasingly complex and, due to their tight energy requirements, all the available resources must be used in the best possible way. However, Android, the most used software platform for embedded systems, features a virtual machine to run applications. Even though it ensures flexibility so the application can execute on different underlying architectures without the need for recompilation, it burdens the system because of the introduction of an extra software layer. Considering this scenario, through the development of an extension of the Android QEMU emulator and a specific benchmark set, this work evaluates the significance of the virtual machine by comparing applications written in Java and in native language. We show that, given a fixed energy budget, a different amount of applications can be executed depending the way they were implemented. We also demonstrate that this difference varies according to the processor, by executing the applications on all officially supported Android architectures (Intel x86, ARM, and MIPS). Therefore, even though the Virtual Machine provides total transparency to the software developer, he/she must be aware of it and the underlying target micro architecture at early designs stages so as to build a low-energy application.},
  doi       = {10.1109/COMPSAC.2015.90},
  keywords  = {embedded systems;Java;mobile computing;smart phones;software architecture;virtual machines;virtual machine;VM;embedded system;Android QEMU emulator;software architecture;Java;Java;Androids;Humanoid robots;Virtual machining;Energy consumption;Benchmark testing;Smart phones;Virtual machine;Android;Dalvik;JNI},
}

@InProceedings{6641465,
  author    = {Y. {Kashnikov} and P. {de Oliveira Castro} and E. {Oseret} and W. {Jalby}},
  title     = {Evaluating architecture and compiler design through static loop analysis},
  booktitle = {2013 International Conference on High Performance Computing Simulation (HPCS)},
  year      = {2013},
  pages     = {535-544},
  month     = {July},
  abstract  = {Using the MAQAO loop static analyzer, we characterize a corpus of binary loops extracted from common benchmark suits such as SPEC, NAS, etc. and several industrial applications. For each loop, MAQAO extracts low-level assembly features such as: integer and floating-point vectorization ratio, number of registers used and spill-fill, number of concurrent memory streams accessed, etc. The distributions of these features on a large representative code corpus can be used to evaluate compilers and architectures and tune them for the most frequently used assembly patterns. In this paper, we present the MAQAO loop analyzer and a characterization of the 4857 binary loops. We evaluate register allocation and vectorization on two compilers and propose a method to tune loop buffer size and stream prefetcher based on static analysis of benchmarks.},
  doi       = {10.1109/HPCSim.2013.6641465},
  keywords  = {feature extraction;optimising compilers;program control structures;program diagnostics;software architecture;storage management;prefetcher;loop buffer size;vectorization evaluation;register allocation evaluation;assembly patterns;large representative code corpus;low-level assembly feature extraction;benchmark suits;binary loops;MAQAO loop static analyzer;static loop analysis;architecture design evaluation;compiler design evaluation;Registers;Benchmark testing;Vectors;Assembly;Measurement;Computer architecture;Ports (Computers);Benchmarking and Assessment;Software Monitoring and Measurement;HPC Monitoring and Instrumentation;Modeling;Simulation and Evaluation Techniques},
}

@InProceedings{6802585,
  author    = {R. {Bakhshayeshi} and M. K. {Akbari} and M. S. {Javan}},
  title     = {Performance analysis of virtualized environments using HPC Challenge benchmark suite and Analytic Hierarchy Process},
  booktitle = {2014 Iranian Conference on Intelligent Systems (ICIS)},
  year      = {2014},
  pages     = {1-6},
  month     = {Feb},
  abstract  = {Steep improvement of cloud computing in recent years, persuaded experts admit it as a suitable and appropriate substitution for traditional computing methods. Nowadays, more and more organizations are getting used to create private clouds, on the other hand, public clouds must be robust enough to handle scientific-driven computing requests of users in an efficient and cost effective manner. Apart from all these necessities, it is intellectual to improve the cloud infrastructure performance by appropriate choices. In this paper we are going to evaluate the performance of some virtualized environments, including VMware ESXi, KVM, Xen, Oracle VirtualBox, and VMware Workstation using HPC Challenge (HPCC) benchmark suite and Open MPI in order to represent solutions for virtualization layer of cloud computing architecture and then designate the best approach in general using Analytic Hierarchy Process.},
  doi       = {10.1109/IranianCIS.2014.6802585},
  keywords  = {analytic hierarchy process;cloud computing;software architecture;virtualisation;virtualized environments;performance analysis;HPC challenge benchmark suite;analytic hierarchy process;steep improvement;public clouds;scientific driven computing;cloud infrastructure;VMware ESXi;KVM;Xen;Oracle VirtualBox;VMware Workstation;cloud computing architecture;Open MPI;virtualization layer;Benchmark testing;Virtual machine monitors;Cloud computing;Bandwidth;Virtualization;Operating systems;Workstations;cloud computing;virtualization;hypervisor;performance evaluation},
}

@InProceedings{5395113,
  author    = {T. {Baker} and A. {Taleb-Bendiab} and D. {Al-Jumeily} and P. {Miseldine}},
  title     = {Creating On-Demand Service-Oriented Applications from Intentions Model},
  booktitle = {2009 Second International Conference on Developments in eSystems Engineering},
  year      = {2009},
  pages     = {219-226},
  month     = {Dec},
  abstract  = {The recent years have seen a flurry of research inspired by social and biological models to achieve the software autonomy. This has been prompted by the need to automate laborious administration tasks, recovery from unanticipated systems failure, and provide self-protection from security vulnerabilities, whilst guaranteeing predictable autonomic software behavior. However, runtime assured adaptation of software to new requirement is still an outstanding issue for research. This paper presents a supporting language for process-oriented programming of autonomic software. The paper starts by a review of the state-of-the-art into runtime software adaptation. This is followed by an outline of a developed Neptune framework and language support, which is here described via an illustrative example pet shop benchmark. The paper ends with a discussion and some concluding remarks leading to suggested further works.},
  doi       = {10.1109/DeSE.2009.40},
  keywords  = {distributed processing;security of data;software architecture;software fault tolerance;on-demand service-oriented application;intentions model;social models;biological models;software autonomy;systems failure;self-protection;security vulnerabilities;autonomic software behavior;runtime assured adaptation;process-oriented programming;Neptune framework;Runtime;Biological system modeling;Mathematical model;Positron emission tomography;Kernel;Middleware;Biology computing;Security;Software performance;Software agents},
}

@InProceedings{945194,
  author    = {P. A. {Dinda} and T. {Gross} and R. {Karrer} and B. {Lowekamp} and N. {Miller} and P. {Steenkiste} and D. {Sutherland}},
  title     = {The architecture of the Remos system},
  booktitle = {Proceedings 10th IEEE International Symposium on High Performance Distributed Computing},
  year      = {2001},
  pages     = {252-265},
  month     = {Aug},
  abstract  = {Remos provides resource information to distributed applications. Its design goals of scalability, flexibility, and portability are achieved through an architecture that allows components to be positioned across the network, each collecting information about its local network. To collect information from different types of networks and from hosts on those networks, Remos provides several collectors that use different technologies, such as SNMP or benchmarking. By matching the appropriate collector to each particular network environment and by providing an architecture for distributing the output of these collectors across all querying environments, Remos collects appropriately detailed information at each site and distributes this information where needed in a scalable manner. Prediction services are integrated at the user-level, allowing history-based data collected across the network to be used to generate the predictions needed by a particular user. Remos has been implemented and tested in a variety of networks and is in use in a number of different environments.},
  doi       = {10.1109/HPDC.2001.945194},
  keywords  = {software architecture;distributed processing;resource allocation;software portability;application program interfaces;protocols;Remos system;resource information;distributed applications;scalability;flexibility;software portability;software architecture;SNMP;benchmarking;prediction services;Grid Monitoring Architecture;application program interface;Computer science;Scalability;Computer architecture;Educational institutions;System testing;US Government;Condition monitoring;Real time systems;Bandwidth;Delay},
}

@InProceedings{4780733,
  author    = {N. {Laranjeiro} and M. {Vieira} and H. {Madeira}},
  title     = {Timing Failures Detection in Web Services},
  booktitle = {2008 IEEE Asia-Pacific Services Computing Conference},
  year      = {2008},
  pages     = {554-559},
  month     = {Dec},
  abstract  = {Current business critical environments increasingly rely on SOA standards to execute business operations. These operations are frequently based on Web service compositions that use several Web services over the internet and have to fulfill specific timing constraints. In these environments, an operation that does not conclude in due time may have a high cost as it can easily turn into service abandonment with financial and prestige losses to the service provider. In fact, at certain points, carrying on with the execution of an operation may be useless as a timely response will be impossible to obtain. This paper proposes a time-aware programming model for Web services that provides transparent timing failure detection. The paper illustrates the proposed model using a set of services specified by the TPC-App performance benchmark.},
  doi       = {10.1109/APSCC.2008.236},
  keywords  = {business data processing;software architecture;software fault tolerance;Web services;timing failures detection;Web services;business critical environments;SOA standards;business operations;Web service compositions;Internet;timing constraints;service abandonment;financial losses;prestige losses;service provider;time-aware programming model;TPC-App performance benchmark;Timing;Web services;Uncertainty;Programming profession;Informatics;Service oriented architecture;Web and internet services;Costs;Delay effects;Detectors;web services;timing failures;failure detection},
}

@InProceedings{6658270,
  author    = {J. {Lenhard} and G. {Wirtz}},
  title     = {Measuring the Portability of Executable Service-Oriented Processes},
  booktitle = {2013 17th IEEE International Enterprise Distributed Object Computing Conference},
  year      = {2013},
  pages     = {117-126},
  month     = {Sep.},
  abstract  = {A key promise of process languages based on open standards, such as the Web Services Business Process Execution Language, is the avoidance of vendor lock-in through the portability of process definitions among runtime environments. Despite the fact that today, various runtimes claim to support this language, every runtime implements a different subset, thus hampering portability and locking in their users. In this paper, we intend to improve this situation by enabling the measurement of the degree of portability of process definitions. This helps developers to assess their process definitions and to decide if it is feasible to invest in the effort of porting a process definition to another runtime. We define several software quality metrics that quantify the degree of portability a process definition provides from different viewpoints. We validate these metrics theoretically with two validation frameworks and empirically with a large set of process definitions coming from several process libraries.},
  doi       = {10.1109/EDOC.2013.21},
  keywords  = {service-oriented architecture;software metrics;specification languages;Web services;vendor lock-in avoidance;Web services business process execution language;process libraries;software quality metrics;process definition portability degree measurement;executable service-oriented processes portability measurement;Measurement;Runtime;Engines;Software;Benchmark testing;Standards;XML;SOA;BPEL;portability;metrics},
}

@InProceedings{5961417,
  author    = {Z. {Wan}},
  title     = {Many-Core Processor Based High Performance Traffic Multiplier},
  booktitle = {2011 Fifth Asia Modelling Symposium},
  year      = {2011},
  pages     = {198-202},
  month     = {May},
  abstract  = {Performance benchmark of telecommunication core network relies on high throughput traffic generator. The traffic generators are software based or dedicated hardware based. The software based traffic generators are usually running on relatively cheap commercial computer in a flexible, low cost way but suffering from poor performance. The hardware based generators can provide more precisely measured heavy traffic load but for complex traffic generation the replay mode is usually adopted. The replay mode can only perform limited scenarios without real interactive procedures and status checking. In this paper, a many-core processor based traffic multiplier working with real applications is presented to simulate heavy traffic load with normal interactive procedures. The simulated traffic behavior can be dynamically changed to fit the workload diversification in the network. The benchmark of many-core processor based traffic multiplier proofed that it can multiply complex traffic with high fidelity.},
  doi       = {10.1109/AMS.2011.44},
  keywords  = {multiprocessing systems;telecommunication networks;telecommunication traffic;many-core processor based high performance traffic multiplier;software based traffic generators;telecommunication core network;high throughput traffic generator;Generators;Hardware;Throughput;Software;Protocols;Benchmark testing;Software architecture;Traffic generator;Traffic multipiler;Many-core;Multi-service;Core network},
}

@InProceedings{5211006,
  author    = {B. {Anckaert} and M. H. {Jakubowski} and R. {Venkatesan} and C. W. {Saw}},
  title     = {Runtime Protection via Dataflow Flattening},
  booktitle = {2009 Third International Conference on Emerging Security Information, Systems and Technologies},
  year      = {2009},
  pages     = {242-248},
  month     = {June},
  abstract  = {Software running on an open architecture, such as the PC, is vulnerable to inspection and modification. Since software may process valuable or sensitive information, many defenses against data analysis and modification have been proposed. This paper complements existing work and focuses on hiding data location throughout program execution. To achieve this, we combine three techniques: (i) periodic reordering of the heap, (ii) migrating local variables from the stack to the heap and (iii) pointer scrambling. By essentially flattening the dataflow graph of the program, the techniques serve to complicate static dataflow analysis and dynamic data tracking. Our methodology can be viewed as a data-oriented analogue of control-flow flattening techniques. Dataflow flattening is useful in practical scenarios like DRM, information-flow protection, and exploit resistance. Our prototype implementation compiles C programs into a binary for which every access to the heap is redirected through a memory management unit. Stack-based variables may be migrated to the heap, while pointer accesses and arithmetic may be scrambled and redirected. We evaluate our approach experimentally on the SPEC CPU2006 benchmark suite.},
  doi       = {10.1109/SECURWARE.2009.44},
  keywords  = {data encapsulation;data flow analysis;data flow graphs;security of data;software architecture;dataflow flattening;data location hiding;dataflow graph;static dataflow analysis;dynamic data tracking;data-oriented analogue;control-flow flattening techniques;information-flow protection;C-program compiling;memory management unit;stack-based variables;SPEC CPU2006 benchmark suite;Runtime;Software protection;Data security;Computer architecture;Inspection;Data analysis;Memory management;Application software;Automata;Embedded software;software protection;data hiding;obfuscation;security},
}

@InProceedings{6924445,
  author    = {L. {Schor} and I. {Bacivarov} and L. G. {Murillo} and P. S. {Paolucci} and F. {Rousseau} and A. E. {Antably} and R. {Buecs} and N. {Fournel} and R. {Leupers} and D. {Rai} and L. {Thiele} and L. {Tosoratto} and P. {Vicini} and J. {Weinstock}},
  title     = {EURETILE Design Flow: Dynamic and Fault Tolerant Mapping of Multiple Applications Onto Many-Tile Systems},
  booktitle = {2014 IEEE International Symposium on Parallel and Distributed Processing with Applications},
  year      = {2014},
  pages     = {182-189},
  month     = {Aug},
  abstract  = {EURETILE investigates foundational innovations in the design of massively parallel tiled computing systems by introducing a novel parallel programming paradigm and a multi-tile hardware architecture. Each tile includes multiple general-purpose processors, specialized accelerators, and a fault-tolerant distributed network processor, which connects the tile to the inter-tile communication network. This paper focuses on the EURETILE software design flow, which provides a novel programming environment to map multiple dynamic applications onto a many-tile architecture. The elaborated high-level programming model specifies each application as a network of autonomous processes, enabling the automatic generation and optimization of the architecture-specific implementation. Behavioral and architectural dynamism is handled by a hierarchically organized runtime-manager running on top of a lightweight operating system. To evaluate, debug, and profile the generated binaries, a scalable many-tile simulator has been developed. High system dependability is achieved by combining hardware-based fault awareness strategies with software-based fault reactivity strategies. We demonstrate the capability of the design flow to exploit the parallelism of many-tile architectures with various embedded and high performance computing benchmarks targeting the virtual EURETILE platform with up to 192 tiles.},
  doi       = {10.1109/ISPA.2014.32},
  keywords  = {operating systems (computers);parallel programming;program debugging;programming environments;software architecture;software fault tolerance;EURETILE design flow;dynamic mapping;fault tolerant mapping;many-tile systems;parallel tiled computing system design;parallel programming paradigm;multitile hardware architecture;general-purpose processors;specialized accelerators;fault-tolerant distributed network processor;intertile communication network;EURETILE software design flow;programming environment;multiple dynamic application mapping;high-level programming model;autonomous processes;architecture-specific implementation;behavioral dynamism;architectural dynamism;organized runtime-manager;lightweight operating system;debugging;scalable many-tile simulator;system dependability;hardware-based fault awareness strategies;software-based fault reactivity strategies;many-tile architectures;embedded computing benchmark;high performance computing benchmark;virtual EURETILE platform;Computer architecture;Program processors;Hardware;Programming;Optimization;Abstracts},
}

@InProceedings{8115373,
  author    = {P. I. {Martos} and A. {Garrido}},
  title     = {Software patterns for asymmetric multiprocessing devices on embedded systems: a performance assessment},
  booktitle = {2017 Eight Argentine Symposium and Conference on Embedded Systems (CASE)},
  year      = {2017},
  pages     = {1-6},
  month     = {Aug},
  abstract  = {In embedded systems there is a variant of Multicore System on Chip devices (MSoC devices) where not all the computing elements (processor cores) are equal. The differences in the cores of these devices range from different hardware architectures using the same instruction set to completely different processors working together inside the same device. These SoCs are called “Asymmetric Multi Processing Devices” (AMP Devices). In order to help developers to take advantage of the possibilities that these devices may offer in the context of embedded systems, software design patterns have been defined, describing software architectural solutions with known uses. However, there are still no experimental results showing the benefits of these solutions. In this work we measure the performance of a design pattern called Mini Me, applied on an AMP device configuration, and compare it against two Symmetric Multiprocessing Device (SMP Device) configurations. The evaluations show a better than expected computing performance of the AMP Configuration using the design pattern Mini Me.},
  doi       = {10.23919/SASE-CASE.2017.8115373},
  keywords  = {embedded systems;microprocessor chips;multiprocessing systems;software architecture;system-on-chip;completely different processors;asymmetric multiprocessing devices;embedded systems;software design patterns;software architectural solutions;AMP device configuration;MSoC devices;processor cores;hardware architectures;multicore system on chip devices;SMP device;AMP devices;instruction set;Mini Me design pattern;Benchmark testing;Embedded systems;Multicore processing;Switched mode power supplies;Hardware;Embedded Systems Patterns;Asymmetric Multiprocessing Patterns},
}

@InProceedings{7973715,
  author    = {E. {Silvestri} and S. {Economo} and P. D. {Sanzo} and A. {Pellegrini} and F. {Quaglia}},
  title     = {Preemptive Software Transactional Memory},
  booktitle = {2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  year      = {2017},
  pages     = {294-303},
  month     = {May},
  abstract  = {In state-of-the-art Software Transactional Memory (STM) systems, threads carry out the execution of transactions as non-interruptible tasks. Hence, a thread can react to the injection of a higher priority transactional task and take care of its processing only at the end of the currently executed transaction. In this article we pursue a paradigm shift where the execution of an in-memory transaction is carried out as a preemptable task, so that a thread can start processing a higher priority transactional task before finalizing its current transaction. We achieve this goal in an application-transparent manner, by only relying on Operating System facilities we include in our preemptive STM architecture. With our approach we are able to re-evaluate CPU assignment across transactions along a same thread every few tens of microseconds. This is mandatory for an effective priority-aware architecture given the typically finer-grain nature of in-memory transactions compared to their counterpart in database systems. We integrated our preemptive STM architecture with the TinySTM package, and released it as open source. We also provide the results of an experimental assessment of our proposal based on running a port of the TPC-C benchmark to the STM environment.},
  doi       = {10.1109/CCGRID.2017.98},
  keywords  = {concurrency (computers);database management systems;microprocessor chips;multiprocessing systems;multi-threading;operating systems (computers);public domain software;software architecture;storage management;preemptive software transactional memory;software transactional memory architecture;STM architecture;in-memory transaction;operating system;CPU;priority-aware architecture;database systems;TinySTM package;open source;Instruction sets;Context;Computer architecture;Proposals;Operating systems;Delays;Transactional memory;Performance optimization;Operating system support},
}

@Article{5571033,
  author   = {M. {Khalgui} and H. {Hanisch}},
  title    = {Reconfiguration Protocol for Multi-Agent Control Software Architectures},
  journal  = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
  year     = {2011},
  volume   = {41},
  number   = {1},
  pages    = {70-80},
  month    = {Jan},
  abstract = {This paper deals with distributed multi-agent reconfigurable embedded control systems following the International Industrial Standard IEC 61499 in which a Function Block (FB) is an event-triggered software component owning data and a control application is a network of distributed blocks that should satisfy functional and temporal properties according to user requirements. We define an architecture of reconfigurable multi-agent systems in which a Reconfiguration Agent is affected to each device of the execution environment to apply local reconfigurations, and a Coordination Agent is proposed for coordinations between devices in order to guarantee safe and adequate distributed reconfigurations. A communication protocol is proposed to handle coordinations between agents by using well-defined Coordination Matrices. We specify both reconfiguration agents to be modeled by nested state machines, and the Coordination Agent according to the formalism Net Condition-Event Systems (NCES) which is an extension of Petri nets. To validate the whole architecture, we check by applying the model checker SESA in each device functional and temporal properties to be described according to the temporal logic “Computation Tree Logic.” We have also to check all possible coordinations between devices by verifying that whenever a reconfiguration is applied in a device, the Coordination Agent and other concerned devices react as described in user requirements. The paper's contributions are applied to two Benchmark Production Systems available in our research laboratory.},
  doi      = {10.1109/TSMCC.2010.2064163},
  keywords = {control engineering computing;formal verification;multi-agent systems;Petri nets;protocols;software architecture;temporal logic;reconfiguration protocol;multiagent control software architectures;distributed multiagent reconfigurable embedded control systems;International Industrial Standard IEC 61499;function block;event-triggered software component;user requirements;reconfigurable multiagent systems;coordination agent;distributed reconfiguration;communication protocol;coordination matrices;reconfiguration agents;nested state machines;net condition-event systems;Petri nets;model checker;SESA;temporal logic;computation tree logic;Production;Tin;Computer architecture;Lifting equipment;Control systems;IEC standards;Grippers;Adaptive systems;component architectures;embedded control systems;software intelligent agents;system analysis and design},
}

@InProceedings{5195842,
  author    = {M. {Khalgui} and O. {Mosbahi} and H. {Hanisch}},
  title     = {Implementation of agent-based reconfigurable embedded control systems},
  booktitle = {2009 7th IEEE International Conference on Industrial Informatics},
  year      = {2009},
  pages     = {428-433},
  month     = {June},
  abstract  = {The paper deals with the implementation of reconfigurable embedded control systems following the Component-based International Industrial Standard IEC61499, in which a Function Block is an event triggered software component owning data and an application is a composition of blocks. To handle automatic reconfigurations of the whole system, we define an agent-based architecture such that the agent checks and interprets the environment evolution before creates, updates or deletes Function Blocks according to well-defined conditions. It is composed of three modules: the interpreter that interprets the environment evolution, the engine that decides new reconfigurations according to such evolution and the converter that applies agent decisions. A Benchmark Production System is used to explain our contribution, and technical solutions are proposed to implement its different agent modules.},
  doi       = {10.1109/INDIN.2009.5195842},
  keywords  = {control systems;embedded systems;software agents;software architecture;reconfigurable embedded control systems;agent-based architecture;benchmark production system;Control systems;Automatic control;Computer industry;Electrical equipment industry;Industrial control;Software standards;Embedded software;Application software;Computer architecture;Engines;Embedded Systems;IEC61499;Reconfiguration;Agent-based Architecture;Implementation},
}

@InProceedings{8457908,
  author    = {A. A. Z. A. {Ibrahim} and M. U. {Wasim} and S. {Varrette} and P. {Bouvry}},
  title     = {PRESEnCE: Performance Metrics Models for Cloud SaaS Web Services},
  booktitle = {2018 IEEE 11th International Conference on Cloud Computing (CLOUD)},
  year      = {2018},
  pages     = {936-940},
  month     = {July},
  abstract  = {Cloud services are delivered to cloud customers on a pay-per-use model by the Cloud Services Providers (CSPs). CSP is using Service Level Agreements (SLAs) to define the quality of the provided services. Unfortunately, a standard mechanism does not exist to verify and assure that delivered services satisfy the signed SLA agreement. In this context, an automatic framework named PRESENCE was introduced to define a set of Common performance metrics handled by a set of agents within a customized client (called the Auditor) for measuring the behaviour of cloud applications on top of a given CSP. In this paper, one of the components of the PRESENCE framework dedicated to the provision of stochastic models for selected performance metrics is presented. More precisely, 11 generated models are depicted and analysed, out of which 90, 91% accurately represent Web Service (WS) performance metrics for four representatives SaaS components used for the validation of the PRESENCE approach.},
  doi       = {10.1109/CLOUD.2018.00140},
  keywords  = {cloud computing;contracts;service-oriented architecture;software metrics;software performance evaluation;Web services;performance metrics models;Cloud SaaS Web Services;customers;pay-per-use model;Cloud Services Providers;Service Level Agreements;provided services;standard mechanism;delivered services;signed SLA agreement;automatic framework;Common performance metrics;customized client;cloud applications;PRESENCE framework;stochastic models;Web Service performance metrics;representatives SaaS components;PRESENCE approach;CSP;WS;Quality of service;Cloud computing;Adaptation models;Benchmark testing;Web services;Performance evaluation;Cloud Computing;SaaS;Web Services;SLA;Perfromance;Evaluation;Metrics;Modeling},
}

@InProceedings{6274006,
  author    = {M. {Smit} and P. {Pawluk} and B. {Simmons} and M. {Litoiu}},
  title     = {A Web Service for Cloud Metadata},
  booktitle = {2012 IEEE Eighth World Congress on Services},
  year      = {2012},
  pages     = {361-368},
  month     = {June},
  abstract  = {Descriptive information about available cloud services (i.e., metadata) is required in order to make good decisions about which cloud service provider(s) to utilize when deploying an application topology to the cloud. Presently, there are no uniform mechanisms for describing these services. Further, there is no unifying process that aggregates this metadata from the set of cloud providers and makes it available to a user in a programmatic fashion from a single location. This paper presents a methodology for and an implementation of a service-oriented application that provides relevant metadata information describing offered cloud services via a uniform RESTful web service. The data provided by this service is automatically acquired and mapped to a standard ontology. Community members can submit performance benchmarks using a metrics agent that submits metrics via a web service. Several example applications using this API to help users select resources are presented.},
  doi       = {10.1109/SERVICES.2012.13},
  keywords  = {application program interfaces;cloud computing;meta data;ontologies (artificial intelligence);service-oriented architecture;software metrics;software performance evaluation;Web services;cloud metadata;descriptive information;decision making;cloud service provider;application topology;service-oriented application;metadata information;uniform RESTful Web service;standard ontology;performance benchmarks;metrics agent;API;resource selection;Measurement;Cloud computing;Communities;Benchmark testing;HTML;Bandwidth;Databases;web service;RESTful;metrics;instances;intercloud;cloud;metadata},
}

@InProceedings{7975269,
  author    = {J. {Gómez-Luna} and I. E. {Hajj} and L. {Chang} and V. {García-Floreszx} and S. G. {de Gonzalo} and T. B. {Jablin} and A. J. {Peña} and W. {Hwu}},
  title     = {Chai: Collaborative heterogeneous applications for integrated-architectures},
  booktitle = {2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2017},
  pages     = {43-54},
  month     = {April},
  abstract  = {Heterogeneous system architectures are evolving towards tighter integration among devices, with emerging features such as shared virtual memory, memory coherence, and systemwide atomics. Languages, device architectures, system specifications, and applications are rapidly adapting to the challenges and opportunities of tightly integrated heterogeneous platforms. Programming languages such as OpenCL 2.0, CUDA 8.0, and C++ AMP allow programmers to exploit these architectures for productive collaboration between CPU and GPU threads. To evaluate these new architectures and programming languages, and to empower researchers to experiment with new ideas, a suite of benchmarks targeting these architectures with close CPU-GPU collaboration is needed. In this paper, we classify applications that target heterogeneous architectures into generic collaboration patterns including data partitioning, fine-grain task partitioning, and coarse-grain task partitioning. We present Chai, a new suite of 14 benchmarks that cover these patterns and exercise different features of heterogeneous architectures with varying intensity. Each benchmark in Chai has seven different implementations in different programming models such as OpenCL, C++ AMP, and CUDA, and with and without the use of the latest heterogeneous architecture features. We characterize the behavior of each benchmark with respect to varying input sizes and collaboration combinations, and evaluate the impact of using the emerging features of heterogeneous architectures on application performance.},
  doi       = {10.1109/ISPASS.2017.7975269},
  keywords  = {groupware;parallel processing;programming languages;software architecture;software performance evaluation;Chai;collaborative heterogeneous applications;heterogeneous system architectures;programming languages;CPU-GPU collaboration;collaboration patterns;data partitioning;fine-grain task partitioning;coarse-grain task partitioning;application performance;parallel processor;Benchmark testing;Graphics processing units;Collaboration;Computer architecture;Support vector machines;Image edge detection;Histograms},
}

@InProceedings{6906853,
  author    = {M. {Dayarathna} and T. {Suzumura}},
  title     = {Towards Emulation of Large Scale Complex Network Workloads on Graph Databases with XGDBench},
  booktitle = {2014 IEEE International Congress on Big Data},
  year      = {2014},
  pages     = {748-755},
  month     = {June},
  abstract  = {Graph database systems are getting a lot of attention in recent times from the big data management community due to their efficiency in graph data storage andpowerful graph query specification abilities. In this paper we present a methodology for modeling workload spikes in a graph database system using a scalable benchmarking framework called XGDBench. We describe how two main types of workload spikes called data spikes and volume spikes can be implemented in the context of graph databases by considering realworld workload traces and empirical evidence.We implemented these features on XGDBench which we developed using X10. We validated these features by running workloads on Titan which is a popular open source distributed graph database server.We observed the ability of XGDBench in generating realistic workload spikes on Titan. The distributed architecture of XGDBench promotes implementation of such techniques efficiently through utilization of computing power offered by distributed memory compute clusters.},
  doi       = {10.1109/BigData.Congress.2014.140},
  keywords  = {Big Data;distributed databases;formal specification;graph theory;query processing;software architecture;storage management;large scale complex network workload emulation;graph database systems;XGDBench;big data management community;graph data storage;graph query specification abilities;workload spike modeling;scalable benchmarking framework;data spikes;volume spikes;open source distributed graph database server;distributed architecture;Titan;computing power utilization;distributed memory compute clusters;Benchmark testing;Servers;Generators;Distributed databases;Database systems;Emulation;management; Distributed databases;Graph databases; Graph theory; Benchmark testing; Performance},
}

@Article{1247675,
  author   = {A. {Corsaro} and D. C. {Schmidt}},
  title    = {The design and performance of real-time Java middleware},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2003},
  volume   = {14},
  number   = {11},
  pages    = {1155-1167},
  month    = {Nov},
  abstract = {More than 90 percent of all microprocessors are now used for real-time and embedded applications. The behavior of these applications is often constrained by the physical world. It is therefore important to devise higher-level languages and middleware that meet conventional functional requirements, as well as dependably and productively enforce real-time constraints. We provide two contributions to the study of languages and middleware for real-time and embedded applications. We first describe the architecture of jRate, which is an open-source ahead-of-time-compiled implementation of the RTSJ middleware. We then show performance results obtained using RTJPerf, which is an open-source benchmarking suite that systematically compares the performance of RTSJ middleware implementations. We show that, while research remains to be done to make RTSJ a bullet-proof technology, the initial results are promising. The performance and predictability of JRate provides a baseline for what can be achieved by using ahead-of-time compilation. Likewise, RTJPerf enables researchers and practitioners to evaluate the pros and cons of RTSJ middleware systematically as implementations mature.},
  doi      = {10.1109/TPDS.2003.1247675},
  keywords = {middleware;Java;quality of service;real-time systems;software performance evaluation;software architecture;microprocessor;real-time Java middleware;embedded application;higher-level language;jRate architecture;RTSJ middleware;RTJPerf benchmarking;real-time resource management;quality of service;QoS;object-oriented language;performance evaluation;Java;Middleware;Real time systems;Embedded system;Memory management;Control systems;Embedded software;Microprocessors;Open source software;Biology computing},
}

@Article{4629388,
  author   = {S. {Oh} and D. {Lee} and S. R. T. {Kumara}},
  title    = {Effective Web Service Composition in Diverse and Large-Scale Service Networks},
  journal  = {IEEE Transactions on Services Computing},
  year     = {2008},
  volume   = {1},
  number   = {1},
  pages    = {15-32},
  month    = {Jan},
  abstract = {The main research focus of Web services is to achieve the interoperability between distributed and heterogeneous applications. Therefore, flexible composition of Web services to fulfill the given challenging requirements is one of the most important objectives in this research field. However, until now, service composition has been largely an error-prone and tedious process. Furthermore, as the number of available web services increases, finding the right Web services to satisfy the given goal becomes intractable. In this paper, toward these issues, we propose an AI planning-based framework that enables the automatic composition of Web services, and explore the following issues. First, we formulate the Web-service composition problem in terms of AI planning and network optimization problems to investigate its complexity in detail. Second, we analyze publicly available Web service sets using network analysis techniques. Third, we develop a novel Web-service benchmark tool called WSBen. Fourth, we develop a novel AI planning-based heuristic Web-service composition algorithm named WSPR. Finally, we conduct extensive experiments to verify WSPR against state-of-the-art AI planners. It is our hope that both WSPR and WSBen will provide useful insights for researchers to develop Web-service discovery and composition algorithms, and software.},
  doi      = {10.1109/TSC.2008.1},
  keywords = {semantic Web;Web services;Web service composition;service networks;silver bullet;service oriented architecture;Al planning-based framework;complex network;Web services;Planning;Artificial intelligence;Strips;Software;Benchmark testing;Equations;Information networks;Web-based services;Information networks;Web-based services},
}

@InProceedings{8754189,
  author    = {G. {Da Silva Vieira} and F. {Alphonsus A.M.N. Soares} and J. C. {De Lima} and H. A. D. {Do Nascimento} and G. T. {Laureano} and R. {Martins Da Costa} and J. C. {Ferreira} and W. {Galvão Rodrigues}},
  title     = {A Disparity Computation Framework},
  booktitle = {2019 IEEE 43rd Annual Computer Software and Applications Conference (COMPSAC)},
  year      = {2019},
  volume    = {2},
  pages     = {634-639},
  month     = {Jul},
  abstract  = {A disparity map is a key component of stereo vision systems. Autonomous navigation, 3D reconstruction and mobility are examples of areas that use disparity maps as an important element. Although much work has been done in the stereo vision field, it is not easy to build stereo systems with concepts such as reuse and extensible scope. In the present paper, we contribute to reducing this gap by presenting a software architecture that can accommodate different stereo methods through a new standard structure. Firstly, we introduce scenarios that illustrate use cases of disparity maps, and we show a novel architecture that foments code reuse. A Disparity Computation Framework (DCF) is presented and how its components are structured regarding compartmentalization are discussed. Then, we introduce a prototype that closely follows our proposal, and we describe some test cases that were performed. We conclude that the DCF can satisfy different on-demand scenarios and that it can support new stereo methods, functions, and evaluations for different applications without much effort.},
  doi       = {10.1109/COMPSAC.2019.10279},
  keywords  = {computer vision;image reconstruction;stereo image processing;autonomous navigation;disparity map;stereo vision field;stereo systems;foments code reuse;stereo vision systems;disparity computation framework;DCF;Stereo vision;Computer architecture;Benchmark testing;Proposals;Task analysis;Buildings;Measurement;stereo vision, disparity map, software design, software architecture},
}

@Article{6036158,
  author   = {J. {Mattila} and J. {Koikkalainen} and A. {Virkki} and M. {van Gils} and J. {Lötjönen; for the Alzheimer's Disease Neuroimaging Initiative}},
  title    = {Design and Application of a Generic Clinical Decision Support System for Multiscale Data},
  journal  = {IEEE Transactions on Biomedical Engineering},
  year     = {2012},
  volume   = {59},
  number   = {1},
  pages    = {234-240},
  month    = {Jan},
  abstract = {Medical research and clinical practice are currently being redefined by the constantly increasing amounts of multiscale patient data. New methods are needed to translate them into knowledge that is applicable in healthcare. Multiscale modeling has emerged as a way to describe systems that are the source of experimental data. Usually, a multiscale model is built by combining distinct models of several scales, integrating, e.g., genetic, molecular, structural, and neuropsychological models into a composite representation. We present a novel generic clinical decision support system, which models a patient's disease state statistically from heterogeneous multiscale data. Its goal is to aid in diagnostic work by analyzing all available patient data and highlighting the relevant information to the clinician. The system is evaluated by applying it to several medical datasets and demonstrated by implementing a novel clinical decision support tool for early prediction of Alzheimer's disease.},
  doi      = {10.1109/TBME.2011.2170986},
  keywords = {decision support systems;diseases;medical diagnostic computing;physiological models;statistical analysis;generic clinical decision support system;multiscale patient data;medical research;healthcare;multiscale modeling;genetic model;molecular model;structural model;neuropsychological model;diagnostic work;Alzheimer disease;statistical modelling;Libraries;Databases;Benchmark testing;Training;Alzheimer's disease;Accuracy;Clinical diagnosis;decision support systems;software architecture;supervised learning;Alzheimer Disease;Data Mining;Decision Support Systems, Clinical;Diagnosis, Computer-Assisted;Health Records, Personal;Humans;Software;Software Design},
}

@InProceedings{7557407,
  author    = {C. H. G. {Ferreira} and L. H. {Nunes} and L. A. {Pereira} and L. H. V. {Nakamura} and J. C. {Estrella} and S. {Reiff-Marganiec}},
  title     = {PEESOS-Cloud: A Workload-Aware Architecture for Performance Evaluation in Service-Oriented Systems},
  booktitle = {2016 IEEE World Congress on Services (SERVICES)},
  year      = {2016},
  pages     = {118-125},
  month     = {June},
  abstract  = {It is a challenging task to ensure quality in service-oriented systems deployed in cloud computing owing to the dynamicity of its environment. Many approaches have been adopted to identify and evaluate bottlenecks and problems in performance. The most common scenario consists of distributed systems that use a workload capable of enabling clients to exploit the target system in different operational conditions. However, one requirement that tends to be overlooked is to determine how the workload is executed, as software and hardware faults can lead to its mischaracterization. In this paper, a number of problems in the workload generation have been identified and summarized. A new architecture, called PEESOS-Cloud, is proposed which allows these services to be evaluated as well as to improve the ability of the workload so that it conforms with its described characteristics. Experiments in a cloud environment were conducted to show how PEESOS-Cloud works and validate its capabilities. Our experiment also showed that the mischaracterization of the workload leads to poor results, whereas an workload-aware implementation leads to a better performance evaluation.},
  doi       = {10.1109/SERVICES.2016.25},
  keywords  = {cloud computing;service-oriented architecture;software performance evaluation;PEESOS-Cloud;workload-aware architecture;performance evaluation;service-oriented systems;cloud computing;distributed systems;software faults;hardware faults;workload generation;workload mischaracterization;Cloud computing;Quality of service;Measurement;Benchmark testing;Computer architecture;Prototypes;Web Services;Service-oriented Systems;Performance Evaluation;Workload Characterisation},
}

@InProceedings{8091223,
  author    = {R. {Panda} and L. K. {John}},
  title     = {Proxy Benchmarks for Emerging Big-Data Workloads},
  booktitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques (PACT)},
  year      = {2017},
  pages     = {105-116},
  month     = {Sep.},
  abstract  = {Early design-space evaluation of computer-systems is usually performed using performance models such as detailed simulators, RTL-based models etc. Unfortunately, it is very challenging (often impossible) to run many emerging applications on detailed performance models owing to their complex application software-stacks, significantly long run times, system dependencies and the limited speed/potential of early performance models. To overcome these challenges in benchmarking complex, long-running database applications, we propose a fast and efficient proxy generation methodology, PerfProx that can generate miniature proxy benchmarks, which are representative of the performance of real-world database applications and yet, converge to results quickly and do not need any complex software-stack support. Past research on proxy generation utilizes detailed micro-architecture independent metrics derived from detailed functional simulators, which are often difficult to generate for many emerging applications. PerfProx enables fast and efficient proxy generation using performance metrics derived primarily from hardware performance counters. We evaluate the proposed proxy generation approach on three modern, real-world SQL and NoSQL databases, Cassandra, MongoDB and MySQL running both the data-serving and data-analytics class of applications on different hardware platforms and cache/TLB configurations. The proxy benchmarks mimic the performance (IPC) of the original database applications with ~94.2% (avg) accuracy. We further demonstrate that the proxies mimic original application performance across several other key metrics, while significantly reducing the instruction counts.},
  doi       = {10.1109/PACT.2017.44},
  keywords  = {Big Data;cache storage;data analysis;database management systems;NoSQL databases;performance evaluation;power aware computing;software architecture;SQL;design-space evaluation;computer-systems;complex application software-stacks;system dependencies;fast proxy generation;PerfProx;miniature proxy benchmarks;real-world database applications;complex software-stack support;microarchitecture independent metrics;performance metrics;hardware performance counters;proxy generation approach;NoSQL databases;data-serving;performance models;big-data workloads;functional simulators;SQL databases;MySQL;MongoDB;Cassandra;nstruction counts;Benchmark testing;Databases;Hardware;Radiation detectors;Measurement;Computational modeling;Software;Proxy Generation;SQL;NoSQL;Workload Characterization;Databases},
}

@InProceedings{7581272,
  author    = {S. {Kumar} and W. N. {Sumner} and A. {Shriraman}},
  title     = {SPEC-AX and PARSEC-AX: extracting accelerator benchmarks from microprocessor benchmarks},
  booktitle = {2016 IEEE International Symposium on Workload Characterization (IISWC)},
  year      = {2016},
  pages     = {1-11},
  month     = {Sep.},
  abstract  = {The end of Dennard Scaling has necessitated research into the adoption of specialized architectures for offloading specific code regions in applications. Recent works in accelerator architectures have chosen diverse workloads and even diverse code regions (within the same workload) to highlight the efficacy of specific accelerator architectures. However this makes it challenging to evaluate the power/performance benefits of each accelerator. It is unclear in the era of specialization whether it will be feasible to standardize a new set of kernels across different architectural ideas. We present an alternative vision where we identify and prepare "acceleratable" code regions from existing CPU-based benchmark suites that are widely used. We identify acceleratable paths by leveraging program analysis [1] to precisely identify directed acyclic paths that are frequently executed. We reconstruct the paths into a separate function within the original binary and demarcate the accelerator region to enable microarchitecture independent analysis and enable precise profiling when executing the program on an architecture simulator or instrumentation tool (e.g., Intel Pin). We extract "accelerator" offload targets from frequently executed paths for 29 workloads across SPEC2000, SPECCPU2006, PARSEC and PERFECT benchmark suites and demonstrate that characterization along paths is more precise than characterization at coarser granularities in prior work. Overall, we analyze 356K paths across 29 workloads and present statistics for the top 5 paths identified for offload in each application. We have also generated a workload suite with the acceleratable code paths to help computer architecture researchers.},
  doi       = {10.1109/IISWC.2016.7581272},
  keywords  = {directed graphs;microprocessor chips;multiprocessing systems;program diagnostics;software architecture;SPEC-AX;PARSEC-AX;accelerator benchmark extraction;microprocessor benchmarks;Dennard scaling;accelerator architectures;power-performance evaluation;acceleratable code regions;CPU-based benchmark;program analysis;directed acyclic paths;microarchitecture independent analysis;architecture simulator;instrumentation tool;PERFECT benchmark suites;SPECCPU2006;SPEC2000;Benchmark testing;Acceleration;Hardware;Instruments;Computer architecture;Kernel;Microarchitecture},
}

@InProceedings{8029761,
  author    = {D. {Athanasopoulos}},
  title     = {The Aspect of Data Translation in Service Similarity},
  booktitle = {2017 IEEE International Conference on Web Services (ICWS)},
  year      = {2017},
  pages     = {188-195},
  month     = {June},
  abstract  = {To avoid the lock-in problem in service-oriented software, existing interface-decoupling mechanisms focus on identifying high-level service mappings, which are not necessarily applicable on translating actual data. Based on the fundamental data-translation process, its successful outcome is guaranteed if mappings are low-level, i.e. they satisfy schema constraints. The problem is that if similar services have been identified based on high-level mappings, then schema constraints may be violated. To overcome this problem, we propose a proactive approach to identify similar services that satisfy schema constraints. In particular, our approach follows a composite workflow (instead of existing hybrid workflows), which determines schema constraints (service documents do not specify them) and estimates service-translation cost (actual cost is not available) as a function of ensured schema-constraints. We compare our approach against a state-of-the-art service-similarity approach on benchmark services and the results show that high service-similarity does not necessarily imply low service-translation cost, the bidirectional nature of service similarity can be misleading, ensured schema-constraints improve service similarity, and estimated translation-cost is very close to actual cost.},
  doi       = {10.1109/ICWS.2017.32},
  keywords  = {constraint handling;data handling;service-oriented architecture;service-oriented software;interface-decoupling mechanisms;high-level service mappings;data-translation process;schema constraints;high-level mappings;composite workflow;service-translation cost;service-similarity approach;Software;XML;Object oriented modeling;Benchmark testing;Estimation;Electronic mail;Measurement;service interface;high-level mapping;schema constraints;translation cost;composite workflow},
}

@InProceedings{5628551,
  author    = {T. {Hill} and S. {Supakkul} and L. {Chung}},
  title     = {Run-time monitoring of system performance: A goal-oriented and system architecture simulation approach},
  booktitle = {2010 First International Workshop on Requirements@Run.Time},
  year      = {2010},
  pages     = {31-40},
  month     = {Sep.},
  abstract  = {The performance of a software system has been a major concern of the information technology services industry since its inception in the early 1960's. Up to now, simulation has been a key technique for gaining an understanding of the performance characteristics of the system during development time, while software tools, possibly together with manual operations, have been used to monitor some performance characteristics of the system during run-time. Simulation models, by and large, have not been tied closely (not easily traceable) to the requirements; neither have they been tied to the run-time monitoring. In the past, simulation models were constructed and used in design and simply ignored during run-time. In this paper, we propose a goal-oriented framework (implemented as a softgoal interdependency graph to record goals, development alternatives, design rational and linkage to functional requirements) and a system architecture simulation approach to realize and monitor the run-time performance characteristics of the system. This approach treats performance requirements as softgoals: a system architecture is constructed that meets the softgoals, a simulation model is constructed and experiments analyzed to consider varying workloads, resource consumptions, and run-time capacities. A good-enough system architecture is then used to implement the system and help monitor if the run-time behavior of the system indeed matches the performance requirements. Alternative actions, to solve performance miss-matches, can be evaluated using the simulation model. The approach is illustrated using a previously underperforming government tax-payer system.},
  doi       = {10.1109/RERUNTIME.2010.5628551},
  keywords  = {software architecture;software performance evaluation;runtime monitoring;system performance;goal oriented architecture;system architecture simulation approach;software system;information technology services;software tools;softgoal interdependency graph;functional requirements;resource consumptions;Computer architecture;Monitoring;Business;Topology;Benchmark testing;Network topology;Hardware;architecture;architecture simulation model;goal-oriented;information technology services;non-functional requirements;performance;quality-of-service attributes;requirements-to-architecture;requirements-to-run;requirements-to-simulation;run-time monitoring;service level agreement monitoring;simulation;system architecture;softgoal interdependency graph},
}

@InProceedings{4211938,
  author    = {E. {Bondarev} and M. {Chaudron} and P. H. N. {de With}},
  title     = {CARAT: a Toolkit for Design and Performance Analysis of Component-Based Embedded Systems},
  booktitle = {2007 Design, Automation Test in Europe Conference Exhibition},
  year      = {2007},
  pages     = {1-6},
  month     = {April},
  abstract  = {Solid frameworks and toolkits for design and analysis of embedded systems are of high importance, since they enable early reasoning about critical properties of a system. This paper presents a software toolkit that supports the design and performance analysis of real-time component-based software architectures deployed on heterogeneous multiprocessor platforms. The tooling environment contains a set of integrated tools for (a) component storage and retrieval, (b) graphics-based design of software and hardware architectures, (c) performance analysis of the designed architectures and, (d) automated code generation. The cornerstone of the toolkit is a performance analysis framework that automates composition of the individual component models into a system executable model, allows simulation of the system model and gives design-time predictions of key performance properties like response time, data throughput, and usage of hardware resources. The efficiency of this toolkit was illustrated on a car radio navigation benchmark system},
  doi       = {10.1109/DATE.2007.364428},
  keywords  = {automotive electronics;embedded systems;logic CAD;software tools;CARAT;component architecture analysis tool;design analysis toolkit;performance analysis toolkit;component-based embedded systems;software toolkit;real-time component-based software architectures;heterogeneous multiprocessor;component storage;component retrieval;graphics-based design;automated code generation;system executable model;system model simulation;car radio navigation;Performance analysis;Embedded system;Predictive models;Software performance;Software tools;Hardware;Computer architecture;Solids;Software architecture;Storage automation},
}

@Article{4212136,
  author   = {D. {Arora} and S. {Ravi} and A. {Raghunathan} and N. K. {Jha}},
  title    = {Architectural Support for Run-Time Validation of Program Data Properties},
  journal  = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year     = {2007},
  volume   = {15},
  number   = {5},
  pages    = {546-559},
  month    = {May},
  abstract = {As computer systems penetrate deeper into our lives and handle private data, safety-critical applications, and transactions of high monetary value, efforts to breach their security also assume significant dimensions way beyond an amateur hacker's play. Until now, security was always an afterthought. This is evident in regular updates to antivirus software, patches issued by vendors after software bugs are discovered, etc. However, increasingly, we are realizing the need to incorporate security during the design of a system, be it software or hardware. We invoke this philosophy in the design of a hardware-based system to enable protection of a program's data during execution. In this paper, we develop a general framework that provides security assurance against a wide class of security attacks. Our work is based on the observation that a program's normal or permissible behavior with respect to data accesses can be characterized by various properties. We present a hardware/software approach wherein such properties can be encoded as data attributes and enforced as security policies during program execution. These policies may be application- specific (e.g., access control for certain data structures), compiler- generated (e.g., enforcing that variables are accessed only within their scope), or universally applicable to all programs (e.g., disallowing WRITES to unallocated memory). We show how an embedded system architecture can support such policies by: 1) enhancing the memory hierarchy to represent the attributes of each datum as security tags that are linked to it throughout its lifetime and 2) adding a configurable hardware checker that interprets the semantics of the tags and enforces the desired security policies. We evaluated the effectiveness of the proposed architecture in enforcing various security policies for several embedded benchmark applications. Our experiments in the context of the Simplescalar framework demonstrate that the proposed solution ensures run-time validation of application-defined data properties with minimal execution time overheads.},
  doi      = {10.1109/TVLSI.2007.896913},
  keywords = {embedded systems;program debugging;program verification;security of data;software architecture;system monitoring;run-time program data validation;computer systems;antivirus software;software bugs;program data protection;data security assurance;encoded data attributes;security policy;program execution;embedded system architecture;memory hierarchy;application-defined data property;Runtime;Data security;Hardware;Computer hacking;Application software;Computer security;Computer bugs;Software systems;Protection;Access control;Embedded processors;processor architectures;security and protection;special-purpose and application-based systems},
}

@InProceedings{4593171,
  author    = {{Hengye Zhu} and {Guangyao Li} and {Liping Zheng}},
  title     = {Introducing Web Services in HLA-based simulation application},
  booktitle = {2008 7th World Congress on Intelligent Control and Automation},
  year      = {2008},
  pages     = {1677-1682},
  month     = {June},
  abstract  = {High Level Architecture (HLA) conduces to the integration of different models to form a complicated distributed simulation system. However, there exist some limitations in HLA-based simulation application in some aspects. In the paper, aiming at resolving these problems, we focus on the study of introducing Web services in HLA-based simulation application and propose Web Services-Based HLA Simulation Framework (WSHLA). Firstly, the structure of and the Web services in WSHLA are presented. Then, the implementation of every components and the execution flow of the framework are discussed respectively. Experimental results validate the framework and demonstrate that using the framework to introduce web services in HLA-based simulation application can effectively make up for its limitations and ensure its better running at expense of some time.},
  doi       = {10.1109/WCICA.2008.4593171},
  keywords  = {digital simulation;software architecture;Web services;Web service;high level architecture-based simulation application;distributed simulation system;Web services;Solid modeling;Benchmark testing;Fires;Programming;Service oriented architecture;Java;High Level Architecture;web services;Run-Time Infrastructure;simulation application},
}

@InProceedings{7371454,
  author    = {T. {Brummett} and P. {Sheinidashtegol} and D. {Sarkar} and M. {Galloway}},
  title     = {Performance Metrics of Local Cloud Computing Architectures},
  booktitle = {2015 IEEE 2nd International Conference on Cyber Security and Cloud Computing},
  year      = {2015},
  pages     = {25-30},
  month     = {Nov},
  abstract  = {Cloud Computing is a rapidly growing branch of distributed computing. There have been many proposed open source cloud architectures. The purpose of this paper is to look at three of the most popular open source architectures: Eucalyptus, OpenStack, and OpenNebula. These three architectures were compared by their ease of installation and performance. The benchmarks performed calculated CPU performance, Network performance, and I/O performance. The CPU performance was measured by a Python program that calculated Pi to a given digit and returned the time taken to complete the task. Our Network performance benchmark made use of two Virtual Machines (VMs) and the Iperf tool to send data over the network. The results were measured by network throughput between VMs. Finally, I/O performance made use of a MySQL table of 100,000 entries and the sysbench tool to benchmark varying amounts of threads by measuring the number of transactions per second. We also ran these benchmarks on a micro instance of Amazon's Elastic Compute Cloud (EC2) to compare results.},
  doi       = {10.1109/CSCloud.2015.61},
  keywords  = {cloud computing;public domain software;software architecture;SQL;virtual machines;performance metrics;local cloud computing architectures;distributed computing;open source cloud architectures;Eucalyptus;OpenStack;OpenNebula;CPU performance;I/O performance;Python program;Pi;network performance benchmark;virtual machines;VM;Iperf tool;network throughput measurement;MySQL table;sysbench tool;Amazon Elastic Compute Cloud;EC2;Cloud computing;Virtual machining;Computer architecture;Benchmark testing;IP networks;Computational modeling;Operating systems},
}

@InProceedings{4618114,
  author    = {M. {Khalgui} and H. {Hanisch}},
  title     = {Dynamic reconfiguration of two benchmark production systems},
  booktitle = {2008 6th IEEE International Conference on Industrial Informatics},
  year      = {2008},
  pages     = {307-314},
  month     = {July},
  abstract  = {This paper deals with the dynamic reconfiguration of two manufacturing systems following the component-based International Industrial Standard IEC61499. According to this standard, a function block (abbreviated FB) is an event triggered component owning data and a control application is a FB network that has to meet temporal properties according to user requirements. This FB network is totally changed or modified if a dynamic reconfiguration scenario is applied at run-time. To cover all reconfiguration cases, we classify all possible scenarios in three classes and we define an agent-based architecture designed with nested state machines to automatically handle all possible dynamic reconfigurations.},
  doi       = {10.1109/INDIN.2008.4618114},
  keywords  = {formal specification;IEC standards;manufacturing systems;software agents;software architecture;dynamic reconfiguration;production system;manufacturing system;component-based International Industrial Standard IEC61499;function block;event triggered component;user requirement;agent-based architecture},
}

@InProceedings{7975295,
  author    = {S. {Dublish} and V. {Nagarajan} and N. {Topham}},
  title     = {Evaluating and mitigating bandwidth bottlenecks across the memory hierarchy in GPUs},
  booktitle = {2017 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)},
  year      = {2017},
  pages     = {239-248},
  month     = {April},
  abstract  = {GPUs are often limited by off-chip memory bandwidth. With the advent of general-purpose computing on GPUs, a cache hierarchy has been introduced to filter the bandwidth demand to the off-chip memory. However, the cache hierarchy presents its own bandwidth limitations in sustaining such high levels of memory traffic. In this paper, we characterize the bandwidth bottlenecks present across the memory hierarchy in GPUs for general-purpose applications. We quantify the stalls throughout the memory hierarchy and identify the architectural parameters that play a pivotal role in leading to a congested memory system. We explore the architectural design space to mitigate the bandwidth bottlenecks and show that performance improvement achieved by mitigating the bandwidth bottleneck in the cache hierarchy can exceed the speedup obtained by a memory system with a baseline cache hierarchy and High Bandwidth Memory (HBM) DRAM. We also show that addressing the bandwidth bottleneck in isolation at specific levels can be sub-optimal and can even be counter-productive. Therefore, we show that it is imperative to resolve the bandwidth bottlenecks synergistically across different levels of the memory hierarchy. With the insights developed in this paper, we perform a cost-benefit analysis and identify cost-effective configurations of the memory hierarchy that effectively mitigate the bandwidth bottlenecks. We show that our final configuration achieves a performance improvement of 29% on average with a minimal area overhead of 1.6%.},
  doi       = {10.1109/ISPASS.2017.7975295},
  keywords  = {cache storage;graphics processing units;parallel processing;software architecture;software performance evaluation;bandwidth bottlenecks mitigation;bandwidth bottlenecks evaluation;memory hierarchy;GPU;off-chip memory;bandwidth limitations;general-purpose applications;architectural design space;performance improvement;baseline cache hierarchy;high bandwidth memory;HBM DRAM;cost-benefit analysis;cost-effective configurations;parallel computing;Bandwidth;Random access memory;Graphics processing units;Benchmark testing;Computer architecture;Registers;Discrete wavelet transforms},
}

@InProceedings{6032392,
  author    = {X. {Qin} and W. {Zhang} and W. {Wang} and J. {Wei} and H. {Zhong} and T. {Huang}},
  title     = {On-line Cache Strategy Reconfiguration for Elastic Caching Platform: A Machine Learning Approach},
  booktitle = {2011 IEEE 35th Annual Computer Software and Applications Conference},
  year      = {2011},
  pages     = {523-534},
  month     = {July},
  abstract  = {Cloud computing provide scalability and high availability for web applications using such techniques as distributed caching and clustering. As one database offloading strategy, elastic caching platforms (ECPs) are introduced to speed up the performance or handle application state management with fault tolerance. Several cache strategies for ECPs have been proposed, say replicated strategy, partitioned strategy and near strategy. We first evaluate the impact of the three cache strategies using the TPC-W benchmark and find that there is no single cache strategy suitable for all conditions, the selection of the best strategy is related with workload patterns, cluster size and the number of concurrent users. This raises the question of when and how the cache strategy should be reconfigured as the condition varies which has received comparatively less attention. In this paper, we present a machine learning based approach to solving this problem. The key features of the approach are off-line training coupled with on-line system monitoring and robust synchronization process after triggering a reconfiguration, at the same time the performance model is periodically updated. More explicitly, first a rule set used to identify which cache strategy is optimal under the current condition are trained with the system statistics and performance results. We then introduce a framework to switch the cache strategy on-line as the workload varies and keep its overhead to acceptable levels. Finally, we illustrate the advantages of this approach by carrying out a set of experiments.},
  doi       = {10.1109/COMPSAC.2011.73},
  keywords  = {cache storage;cloud computing;fault tolerant computing;learning (artificial intelligence);pattern clustering;software architecture;synchronisation;online cache strategy reconfiguration;elastic caching platform;machine learning approach;cloud computing;distributed caching;distributed clustering;database offloading strategy;application state management;fault tolerance;off-line training;robust synchronization process;Servers;Software;Machine learning;Monitoring;Availability;Scalability;Benchmark testing;Elastic caching platform;cache strategy;scalability;availability;reconfiguration},
}

@InProceedings{6687407,
  author    = {G. {Inggs} and D. {Thomas} and W. {Luk}},
  title     = {A Heterogeneous Computing Framework for Computational Finance},
  booktitle = {2013 42nd International Conference on Parallel Processing},
  year      = {2013},
  pages     = {688-697},
  month     = {Oct},
  abstract  = {This paper presents the Forward Financial Framework (F3), an application framework for describing and implementing forward looking financial computations on high performance, heterogeneous platforms. F3 allows the computational finance problem specification to be captured precisely yet succinctly, then automatically creates efficient implementations for heterogeneous platforms, utilising both multi-core CPUs and FPGAs. The automatic mapping of a high-level problem description to a low-level heterogeneous implementation is possible due to the domain-specific knowledge which is built in F3, along with a software architecture that allows for additional domain knowledge and rules to be added to the framework. Currently the system is able to utilise domain-knowledge of the run-time characteristics of pricing tasks to partition pricing problems and allocate them to appropriate compute resources, and to exploit relationships between financial instruments to balance computation against communication. The versatility of the framework is demonstrated using a benchmark of option pricing problems, where F3 achieves comparable speed and energy efficiency to external manual implementations. Further, the domain-knowledge guided partitioning scheme suggests a partitioning of subtasks that is 13% faster than the average, while exploiting domain dependencies to reduce redundant computations results in an average gain in efficiency of 27%.},
  doi       = {10.1109/ICPP.2013.82},
  keywords  = {field programmable gate arrays;financial data processing;multiprocessing systems;pricing;software architecture;heterogeneous computing framework;forward financial framework;computational finance problem specification;heterogeneous platforms;multicore CPU;FPGA;automatic mapping;high-level problem description;domain-specific knowledge;software architecture;pricing tasks;partition pricing problems;option pricing problems;Pricing;Field programmable gate arrays;Europe;Benchmark testing;Contracts;Multicore processing;Cost accounting;Heterogeneous Computing;Computational Finance;Multicore CPU;FPGA;Application Framework;Domain-Specific},
}

@InProceedings{4291021,
  author    = {Y. {Liu} and I. {Gorton} and L. {Zhu}},
  title     = {Performance Prediction of Service-Oriented Applications based on an Enterprise Service Bus},
  booktitle = {31st Annual International Computer Software and Applications Conference (COMPSAC 2007)},
  year      = {2007},
  volume    = {1},
  pages     = {327-334},
  month     = {July},
  abstract  = {An enterprise service bus (ESB) is a standards-based integration platform that combines messaging, web services, data transformation, and intelligent routing in a highly distributed environment. The ESB has been adopted as a key component of SOA infrastructures. For SOA implementations with large number of users, services, or traffic, maintaining the necessary performance levels of applications integrated using an ESB presents a substantial challenge, both to the architects who design the infrastructure as well as to IT professionals who are responsible for administration. In this paper, we develop a performance model for analyzing and predicting the runtime performance of service applications composed on a COTS ESB platform. Our approach utilizes benchmarking techniques to measure primitive performance overheads of service routing activities in the ESB. The performance characteristics of the ESB and services running on the ESB are modeled in a queuing network, which facilitates the performance prediction of service oriented applications. This model is validated by an example ESB based service application modeled from real world loan broking business application.},
  doi       = {10.1109/COMPSAC.2007.166},
  keywords  = {business data processing;distributed processing;integrated software;Web services;performance prediction;service-oriented applications;enterprise service bus;standards-based integration platform;messaging;Web services;data transformation;intelligent routing;highly distributed environment;SOA infrastructures;performance model;performance overheads;service routing activities;service oriented application;Service oriented architecture;Application software;Performance analysis;Predictive models;Web services;Traffic control;Routing;Instruments;Australia;Runtime},
}

@InProceedings{7395237,
  author    = {D. M. {Rathod} and M. S. {Dahiya} and S. M. {Parikh}},
  title     = {Towards composition of RESTful web services},
  booktitle = {2015 6th International Conference on Computing, Communication and Networking Technologies (ICCCNT)},
  year      = {2015},
  pages     = {1-6},
  month     = {July},
  abstract  = {RESTful HTTP and SOAP/WSDL services are two architectural styles for building web services, Where SOAP/WSDL based services follows an operation centric approach and RESTful HTTP based services follows resource centric approach. Dynamic composition of SOAP web services has been widely studied by researchers as compare to RESTful web services. RESTful web services have gained momentum in the development of distributed application but mostly it is hard wired with client application. We proposed novel approach termed as Web Service Resource Bundle (WSRB) for RESTful web service composition that takes care of selection of targeted web services considering inter-service dependencies and also dynamically bind services with client application. WSRB is bundle of composition of RESTful web service; ready to consume based on key passed by client. We developed hotel booking benchmark application with android mobile client using RESTful and SOAP based web services. We also measured benchmark results in both cases which show that proposed approach WSRB with RESTful web service gives best performance as compare to SOAP based web service.},
  doi       = {10.1109/ICCCNT.2015.7395237},
  keywords  = {resource allocation;software architecture;Web services;WSRB;Web service resource bundle;resource centric approach;architectural style;SOAP/WSDL service;RESTful HTTP service;representational state transfer;RESTful web service composition;Simple object access protocol;Benchmark testing;Ice;Unified modeling language;RESTful;SOAP;Web Service;Composition;WSDL;HTTP},
}

@InProceedings{8529564,
  author    = {R. {Prashnani}},
  title     = {Task Migration Algorithm to Reduce Temperature Imbalance Amongst Cores in Linux Based Multi-Core Processor Systems},
  booktitle = {2018 3rd International Conference for Convergence in Technology (I2CT)},
  year      = {2018},
  pages     = {1-7},
  month     = {April},
  abstract  = {Multi-core architectures have recently emerged as the main design paradigm for current and future processors, however, the addition of multiple cores on the same chip and simultaneous execution of processes on these cores increases the temperature imbalance amongst cores and result in an increase in chip temperature, which in turn reduces the reliability of the processor. In this research project, the problem of temperature imbalance amongst cores in multi-core processor systems is addressed at the operating system level via task migration. The proposed task migration algorithm finds out the thermal contribution of individual task and performs the task migration based on this thermal contribution, in order to maintain temperature balance amongst cores. The proposed task migration algorithm is implemented in Linux operating system by modifying the source code written in C language. Different categories of processes (e.g. CPU bound, IO bound etc.) are executed from Phoronix Test Suite and Sysbench Benchmarks to test the performance of the proposed algorithm. Experimental results for the existing task migration algorithm of Linux and the proposed task migration algorithm show that the proposed algorithm reduces the temperature imbalance amongst cores as compared to existing algorithm, without degrading the performance.},
  doi       = {10.1109/I2CT.2018.8529564},
  keywords  = {benchmark testing;C language;Linux;multiprocessing programs;software architecture;software performance evaluation;source code (software);Linux based multicore processor systems;multicore architectures;chip temperature;Linux operating system;task migration algorithm;source code;C language;Phoronix Test Suite;Sysbench Benchmarks;Task analysis;Multicore processing;Program processors;Measurement;Thermal management;Hardware;Linux;multi-core processor systems;temperature imbalance;task migration},
}

@InProceedings{8605744,
  author    = {S. {Eismann} and J. {Kistowski} and J. {Grohmann} and A. {Bauer} and N. {Schmitt} and N. {Herbst} and S. {Kounev}},
  title     = {TeaStore: A Micro-Service Reference Application for Cloud Researchers},
  booktitle = {2018 IEEE/ACM International Conference on Utility and Cloud Computing Companion (UCC Companion)},
  year      = {2018},
  pages     = {11-12},
  month     = {Dec},
  abstract  = {Researchers propose and employ various methods to analyze, model, optimize and manage modern distributed cloud applications. In order to demonstrate and evaluate these methods in realistic scenarios, researchers rely on reference applications. These applications should offer a range of different behaviors, degrees of freedom allowing for customization and should use a modern and representative technology stack. Existing testing and benchmarking applications are either outdated, designed for specific testing scenarios, or do not offer the necessary degrees of freedom. Further, most cloud reference applications are difficult to deploy and run. In this paper, we present the TeaStore, a micro-service-based test and reference cloud application. TeaStore offers services with various performance characteristics and a high degree of freedom regarding its deployment and configuration to be used as a cloud reference application for researchers. The TeaStore is designed for the evaluation of performance modeling and resource management techniques. We invite cloud researchers to use the TeaStore and provide it open-source, extendable, easily deployable and monitorable.},
  doi       = {10.1109/UCC-Companion.2018.00021},
  keywords  = {cloud computing;program testing;resource allocation;TeaStore;microservice reference application;cloud researchers;reference cloud application;distributed cloud applications;technology stack;microservice-based test;resource management;performance modeling;Software;Cloud computing;Monitoring;Benchmark testing;Containers;Computational modeling;Analytical models;Micro-Service Architectures;Reference Implementation;Cloud Resource Management;Performance Modeling},
}

@InProceedings{6984245,
  author    = {T. {Heyman} and D. {Preuveneers} and W. {Joosen}},
  title     = {Scalability Analysis of the OpenAM Access Control System with the Universal Scalability Law},
  booktitle = {2014 International Conference on Future Internet of Things and Cloud},
  year      = {2014},
  pages     = {505-512},
  month     = {Aug},
  abstract  = {The scalability of a software system is greatly impacted by the scalability of the underlying access control system, which makes analyzing the scalability of that access control system paramount. However, this is not trivial, as contemporary access control systems have a myriad of architectural deployment variations, each of which has a potentially large impact on overall system throughput. There is a need for a systematic approach to map these architectural variations to a reference model which allows to make comparisons and to identify trade-offs. This work provides a piece of the puzzle by demonstrating how this can be achieved by systematically applying the Universal Scalability Law (USL). We illustrate our approach by performing a rigorous scalability analysis of the OpenAM access control system for various deployment alternatives in the domain of authentication. We conclude that the approach is able to provide both qualitative and quantitative results which can be translated into practical operational recommendations for the envisioned types of system deployments.},
  doi       = {10.1109/FiCloud.2014.89},
  keywords  = {authorisation;public domain software;software architecture;scalability analysis;OpenAM access control system;universal scalability law;USL;software system scalability;architectural deployment variations;Scalability;Throughput;Access control;Load management;Authentication;Load modeling;Generators;OpenAM;access control;scalability;benchmarking},
}

@InProceedings{6919192,
  author    = {D. {Pace} and K. {Kavi} and C. {Shelor}},
  title     = {MT-SDF: Scheduled Dataflow Architecture with Mini-threads},
  booktitle = {2013 Data-Flow Execution Models for Extreme Scale Computing},
  year      = {2013},
  pages     = {22-28},
  month     = {Sep.},
  abstract  = {In this paper we show a new execution paradigm based on Decoupled Software Pipelining in the context of Scheduled Dataflow (SDF) architecture. We call the new architecture MT-SDF. We introduce mini-threads to execute loops as a software pipeline. We permit the mini-threads to share registers. We present a qualitative and quantitative comparison of the mini-threads with the original SDF architecture, and out-of-order superscalar architecture. We use several benchmark.},
  doi       = {10.1109/DFM.2013.18},
  keywords  = {data flow computing;multi-threading;program control structures;software architecture;execution paradigm;decoupled software pipelining;scheduled dataflow architecture;MT-SDF architecture;minithreads;loops execution;share registers;out-of-order superscalar architecture;Instruction sets;Registers;Computer architecture;Pipelines;Pipeline processing;Benchmark testing;Dataflow Architecture;Decoupled Software Pipelines;Multithreaded Architecture;and Shared Registers},
}

@InProceedings{6158877,
  author    = {R. A. {Vaishnavi} and R. {Rajalakshmi}},
  title     = {Shype to maintain the ATM system stability},
  booktitle = {2012 International Conference on Computer Communication and Informatics},
  year      = {2012},
  pages     = {1-6},
  month     = {Jan},
  abstract  = {Large applications executing on Grid or cluster architectures consisting of hundreds or thousands of computational Nodes create problems with respect to reliability. The ATM Machine is one of such Application. There is an increasing need for fault tolerance capabilities in logic devices brought about by the scaling of transistors to ever smaller geometrics. This paper presents a secure hypervisor (Shype) based replication approach, Virtual lock step and Processor state finger printing that can be applied to Fault tolerance server to maintain stable execution. These are transparent protocols capable of overcoming problems associated with both, benign faults, i.e., crash faults, and node or subnet volatility. A novel form of processor state fingerprinting can significantly reduce the fault detection latency. This further improves reliability by triggering rollback recovery using Virtual locks when errors are recorded. HyperShield, which is a hypervisor that can be inserted into and removed from a running operating system, for improving security. The low-cost protocols offer the capability of controlling or bounding the overhead. A formal cost model is presented, followed by an experimental evaluation. It is shown that the overhead of the protocol is very small and the Maximum work lost by a crashed process is small and bounded.},
  doi       = {10.1109/ICCCI.2012.6158877},
  keywords  = {automatic teller machines;grid computing;operating systems (computers);security of data;software architecture;software fault tolerance;software maintenance;system recovery;virtualisation;Shype based replication;ATM system stability;grid architecture;cluster architecture;computational nodes;fault tolerance capabilities;logic devices;transistor scaling;secure hypervisor;virtual lock step;processor state fingerprinting;fault tolerance server;benign faults;fault detection latency;reliability improvement;rollback recovery;HyperShield;operating system;security improvement;formal cost model;crash faults;subnet volatility;software-based replication model;Virtual machine monitors;Benchmark testing;Fingerprint recognition;Hardware;Prototypes;Monitoring;Operating systems;Fault tolerant system;Virtual Lock step;Autonomic computing;Virtualization;Secure Hypervisor},
}

@InProceedings{6528649,
  author    = {{Sukyoung Kim} and {Hyunseok Seo} and {EungHa Kim} and {Youngil Choi}},
  title     = {Service-oriented ontology evaluation method by QFD-based approach},
  booktitle = {2012 6th International Conference on New Trends in Information Science, Service Science and Data Mining (ISSDM2012)},
  year      = {2012},
  pages     = {317-324},
  month     = {Oct},
  abstract  = {This paper proposes that quantitative evaluation criteria about roles and functions of the ontology. Currently the ontology is many applied in knowledge base about intelligence / functionalized services paradigm and various application. We are aimed the provision of appropriate services not only the primary application frame but also according to the active needs of people. To apply quantitative ratings, this study suggests that ontology will give helping to justification, efficiency and safety on service-oriented knowledge base using the QFD and HoQ that customer demands are reflected in the design specification about service. In addition, ontology construction is aimed to apply general considerations for the evaluation ontology describing ontology evaluation and analysis for the various research trends such as service-oriented or task-based ontology evaluation, ontology evaluation methods benchmarking and ontology-based evaluation method based.},
  keywords  = {ontologies (artificial intelligence);quality function deployment;service-oriented architecture;software quality;service-oriented ontology evaluation method;quantitative evaluation criteria;intelligence services paradigm;functionalized services paradigm;quantitative ratings;service-oriented knowledge base;QFD;HoQ;customer demands;design specification;ontology construction;task-based ontology evaluation;ontology evaluation methods benchmarking;quality function development;evaluation;QFD;Service-oriented ontology;HoQ Frame of Ontology},
}

@InProceedings{7314155,
  author    = {B. {Xie} and X. {Liu} and J. {Zhan} and Z. {Jia} and Y. {Zhu} and L. {Wang} and L. {Zhang}},
  title     = {Characterizing Data Analytics Workloads on Intel Xeon Phi},
  booktitle = {2015 IEEE International Symposium on Workload Characterization},
  year      = {2015},
  pages     = {114-115},
  month     = {Oct},
  abstract  = {With the growing computation demands of data analytics, heterogeneous architectures become popular for their support of high parallelism. Intel Xeon Phi, a many-core coprocessor originally designed for high performance computing applications, is promising for data analytics workloads. However, to the best of knowledge, there is no prior work systematically characterizing the performance of data analytics workloads on Xeon Phi. It is difficult to design a benchmark suite to represent the behavior of data analytics workloads on Xeon Phi. The main challenge resides in fully exploiting Xeon Phi's features, such as long SIMD instruction, simultaneous multithreading, and complex memory hierarchy. To address this issue, we develop Big Data Bench-Phi, which consists of seven representative data analytics workloads. All of these benchmarks are optimized for Xeon Phi and able to characterize Xeon Phi's support for data analytics workloads. Compared with a 24-core Xeon E5-2620 machine, Big Data Bench-Phi achieves reasonable speedups for most of its benchmarks, ranging from 1.5 to 23.4X. Our experiments show that workloads working on high-dimensional matrices can significantly benefit from instruction- and thread-level parallelism on Xeon Phi.},
  doi       = {10.1109/IISWC.2015.20},
  keywords  = {Big Data;data analysis;matrix algebra;multiprocessing programs;multi-threading;parallel processing;software architecture;data analytics workloads;Intel Xeon Phi;heterogeneous architectures;high parallelism;many-core coprocessor;high performance computing;SIMD instruction;multithreading;complex memory hierarchy;24-core Xeon E5-2620 machine;Big Data Bench-Phi;high-dimensional matrices;Data analysis;Benchmark testing;Computer architecture;Google;Principal component analysis;Scalability;Instruction sets;Xeon Phi;Data Analytics;Characterization},
}

@Article{6936844,
  author   = {Q. {Shi} and H. {Hoffmann} and O. {Khan}},
  title    = {A Cross-Layer Multicore Architecture to Tradeoff Program Accuracy and Resilience Overheads},
  journal  = {IEEE Computer Architecture Letters},
  year     = {2015},
  volume   = {14},
  number   = {2},
  pages    = {85-89},
  month    = {July},
  abstract = {To protect multicores from soft-error perturbations, resiliency schemes have been developed with high coverage but high power/performance overheads (~2x). We observe that not all soft-errors affect program correctness, some soft-errors only affect program accuracy, i.e., the program completes with certain acceptable deviations from soft-error free outcome. Thus, it is practical to improve processor efficiency by trading off resilience overheads with program accuracy. We propose the idea of declarative resilience that selectively applies resilience schemes to both crucial and non-crucial code, while ensuring program correctness. At the application level, crucial and non-crucial code is identified based on its impact on the program outcome. The hardware collaborates with software support to enable efficient resilience with 100 percent soft-error coverage. Only program accuracy is compromised in the worst-case scenario of a soft-error strike during non-crucial code execution. For a set of multithreaded benchmarks, declarative resilience improves completion time by an average of 21 percent over state-of-the-art hardware resilience scheme that protects all executed code. Its performance overhead is ~1.38x over a multicore that does not support resilience.},
  doi      = {10.1109/LCA.2014.2365204},
  keywords = {multi-threading;software architecture;software fault tolerance;multithreaded benchmark;code execution;soft-error perturbation;resilience overhead;program accuracy;multicore architecture;Resilience;Multicore processing;Soft errors;Accuracy;Benchmark testing;Instruction sets;Resilience;soft-errors;program accuracy;multicores;Resilience;soft-errors;program accuracy;multicores},
}

@InProceedings{4721553,
  author    = {M. {Kohler} and A. {Schaad}},
  title     = {ProActive Access Control for Business Process-Driven Environments},
  booktitle = {2008 Annual Computer Security Applications Conference (ACSAC)},
  year      = {2008},
  pages     = {153-162},
  month     = {Dec},
  abstract  = {Users expect that systems react instantly. This is specifically the case for user-centric workflows in business process-driven environments. In today's enterprise systems most actions executed by a user have to be checked against the system's access control policy and require a call to the access control component. Hence, improving the performance of access control decisions will improve the overall performance experienced by the end user significantly. In this paper we propose a caching strategy which pre-computes caching entries by exploiting the fact that the executions of business processes are based on the execution of actions in a predefined order. We propose an accompanying architecture and present the results of our conducted benchmark.},
  doi       = {10.1109/ACSAC.2008.26},
  keywords  = {authorisation;cache storage;software architecture;workflow management software;proactive access control;business process-driven environments;user-centric workflows;enterprise systems;caching strategy;accompanying architecture;Access control;Delay;Computer security;Application software;History;Environmental management;Design optimization;Performance gain;Benchmark testing;Access Control;Business Process;Caching;Workflow},
}

@InProceedings{4907662,
  author    = {J. {Li} and C. {Wu} and W. {Hsu}},
  title     = {An Evaluation of Misaligned Data Access Handling Mechanisms in Dynamic Binary Translation Systems},
  booktitle = {2009 International Symposium on Code Generation and Optimization},
  year      = {2009},
  pages     = {180-189},
  month     = {March},
  abstract  = {Binary translation (BT) has been an important approach to migrate application software across instruction set architectures (ISAs). Some architectures, such as X86, allow misaligned data accesses (MDAs), while most modern architectures have the alignment restriction that requires data to be aligned in memory on natural boundaries. In a binary translation system, where the source ISA allows MDA and the target ISA does not, memory operations must be carefully translated to satisfy the alignment restriction. Naive translation will cause frequent misaligned data access traps to occur at runtime on the target machine, and severely slow down the migrated application.This paper evaluates different approaches in handling MDA in binary translation systems. It also proposes a new mechanism to deal with MDAs. Measurements based on SPEC CPU2000 and CPU2006 benchmark show that the proposed approach can significantly outperform existing methods.},
  doi       = {10.1109/CGO.2009.22},
  keywords  = {data handling;information retrieval;instruction sets;software architecture;software performance evaluation;misaligned data access handling mechanisms;dynamic binary translation systems;naive translation;SPEC CPU2000;CPU2006 benchmark;Computer architecture;Instruction sets;Application software;Runtime;Hardware;Reduced instruction set computing;Laboratories;Computer aided instruction;Computer science;Instruments;optimization;misaligned memory access;binary translation},
}

@InProceedings{5341666,
  author    = {E. {Silva} and L. F. {Pires} and M. v. {Sinderen}},
  title     = {A Framework for the Evaluation of Semantics-Based Service Composition Approaches},
  booktitle = {2009 Seventh IEEE European Conference on Web Services},
  year      = {2009},
  pages     = {66-74},
  month     = {Nov},
  abstract  = {The benefits of service composition are being largely acknowledged in the literature nowadays. However, as the amount of available services increases, it becomes difficult to manage, discover, select and compose them, so that automation is required in these processes. This can be achieved by using semantic information represented in ontologies. Currently there are many different approaches that support semantics-based service composition. However, still little effort has been spent on creating a common methodology to evaluate and compare such approaches. In this paper we present our initial ideas to create an evaluation framework for semantics-based service composition approaches. We use a collection of existing services, and define a set of evaluation metrics, confusion matrix-based and time-based. Furthermore, we present how composition evaluation scenarios are generated from the collection of services and specify the strategy to be used in the evaluation process. We demonstrate the proposed framework through an example. Currently there are mechanisms and initiatives to address the evaluation of the semantics-based service discovery and matchmaking approaches. However, still few efforts have been spent on the creation of comprehensive evaluation mechanisms for semantics-based service composition approaches.},
  doi       = {10.1109/ECOWS.2009.23},
  keywords  = {ontologies (artificial intelligence);semantic Web;Web services;semantics-based service composition;semantic information;ontology;evaluation metrics;composition evaluation scenarios;semantics-based service discovery;Ontologies;Service oriented architecture;Web services;Telematics;Information technology;Automation;Computer networks;Computer industry;Application software;Collaborative software;Semantic Services;Service Composition;Evaluation and Benchmarking},
}

@InProceedings{5161057,
  author    = {S. {Sharkawi} and D. {DeSota} and R. {Panda} and R. {Indukuru} and S. {Stevens} and V. {Taylor} and X. {Wu}},
  title     = {Performance projection of HPC applications using SPEC CFP2006 benchmarks},
  booktitle = {2009 IEEE International Symposium on Parallel Distributed Processing},
  year      = {2009},
  pages     = {1-12},
  month     = {May},
  abstract  = {Performance projections of high performance computing (HPC) applications onto various hardware platforms are important for hardware vendors and HPC users. The projections aid hardware vendors in the design of future systems, enable them to compare the application performance across different existing and future systems, and help HPC users with system procurement and application refinements. In this paper, we present a method for projecting the node level performance of HPC applications using published data of industry standard benchmarks, the SPEC CFP2006, and hardware performance counter data from one base machine. In particular, we project performance of eight HPC applications onto four systems, utilizing processors from different vendors, using data from one base machine, the IBM p575. The projected performance of the eight applications was within 7.2% average difference with respect to measured runtimes for IBM POWER6 systems and standard deviation of 5.3%. For two Intel based systems with different micro-architecture and instruction set architecture (ISA) than the base machine, the average projection difference to measured runtimes was 10.5% with standard deviation of 8.2%.},
  doi       = {10.1109/IPDPS.2009.5161057},
  keywords  = {instruction sets;software architecture;software performance evaluation;performance projection;SPEC CFP2006 benchmarks;high performance computing;hardware platforms;microarchitecture;instruction set architecture;Hardware;Counting circuits;Runtime;Measurement standards;Application software;Procurement;Marketing and sales;Computational modeling;Benchmark testing;Computational fluid dynamics},
}

@InProceedings{840888,
  author    = {H. {Kahn} and A. {Carpenter} and N. {Whitaker}},
  title     = {A Web-based system for assessing and searching for designs},
  booktitle = {Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537)},
  year      = {2000},
  pages     = {755-},
  month     = {March},
  abstract  = {Users need to access design data for a variety of reasons. Designers may be interested in accessing repositories of IP blocks for possible inclusion in their own designs. Alternatively, EDA tool developers and purchasers need a representative set of designs to evaluate or benchmark software. This poster presents a web-based system used both for profiling designs and for searching for designs with specific characteristics. The STEED system summarised here is based on external information models that tailor it to user requirements.},
  doi       = {10.1109/DATE.2000.840888},
  keywords  = {Internet;industrial property;electronic design automation;Web-based system;design data;IP blocks;repositories;EDA tool developers;profiling;STEED system;external information models;user requirements;Software tools;Computer science;Electronic design automation and methodology;Feedback;Control system analysis;Information analysis;Displays;Web server;Computer architecture;Service oriented architecture},
}

@InProceedings{7181860,
  author    = {E. {Diken} and L. {Jóźwiak}},
  title     = {A compilation technique and performance profits for VLIW with heterogeneous vectors},
  booktitle = {2015 4th Mediterranean Conference on Embedded Computing (MECO)},
  year      = {2015},
  pages     = {9-12},
  month     = {June},
  abstract  = {In numerous mobile applications involving complex video, image, signal, communication or security processing, massive parallelism is mainly in the form of data-level parallelism (DLP). However, the sorts and amount of DLP parallelism in applications vary due to different computational characteristics of applications. On the contrary, most of the processors today include single-width SIMD (vector) hardware to exploit DLP. However, single-width SIMD architectures may not be optimal to serve applications with varying DLP and they may cause performance and energy inefficiency. We propose the usage of VLIW processors with multiple native vector-widths to better serve applications with changing DLP. This paper focuses on the short SIMD code generation. More specifically, we target generating 32-bit SIMD code for the native 32-bit wide vector units of our example processor. In this way, we improved the performance of compiler generated SIMD code by reducing the number of overhead operations. Experimental results demonstrated that our methodology implemented in the compiler reduces the number of operations of synthetic benchmarks up to 40%.},
  doi       = {10.1109/MECO.2015.7181860},
  keywords  = {parallel processing;program compilers;software architecture;compilation technique;performance profit;VLIW processor;very long instruction word architecture;heterogeneous vector;data-level parallelism;DLP;single-width SIMD architecture;SIMD code generation;Program processors;Computer architecture;Hardware;VLIW;Parallel processing;Benchmark testing;Mobile communication},
}

@InProceedings{7588952,
  author    = {M. {Tasende}},
  title     = {Generation of the Single Precision BLAS Library for the Parallella Platform, with Epiphany Co-processor Acceleration, Using the BLIS Framework},
  booktitle = {2016 IEEE 14th Intl Conf on Dependable, Autonomic and Secure Computing, 14th Intl Conf on Pervasive Intelligence and Computing, 2nd Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)},
  year      = {2016},
  pages     = {894-897},
  month     = {Aug},
  abstract  = {The Parallella is a hybrid computing platform that came into existence as the result of a Kickstarter project by Adapteva. It is composed of the high performance, energy-efficient, manycore architecture, Epiphany chip (used as co-processor) and one Zynq-7000 series chip, which normally runs a regular Linux OS version, serves as the main processor, and implements "glue logic" in its internal FPGA to communicate with the many interfaces in the Parallella. In this paper an Epiphany-accelerated BLAS library for the Parallella platform was created (which could be suitable, also, for similar hybrid platforms that include the Epiphany chip as a coprocessor). For the actual instantiation of the BLAS, the BLIS framework was used. There have been previous implementations of Matrix-Matrix multiplication, on this platform, that achieved very good performances inside the Epiphany chip (up to 85% of peak), but not so good ones for the complete Parallella platform (due to inter-chip data transfer bandwidth limitations). The main purpose of this work was to get closer to practical Linear Algebra aplications for the entire Parallella platform, with Scientific Computing and, in the long run, Big Data applications, in view.},
  doi       = {10.1109/DASC-PICom-DataCom-CyberSciTec.2016.154},
  keywords  = {coprocessors;field programmable gate arrays;Linux;parallel architectures;software architecture;single precision BLAS library;Parallella platform;Epiphany co-processor acceleration;BLIS framework;hybrid computing platform;high performance computing;manycore architecture;Epiphany chip;Zynq-7000 series chip;regular Linux OS version;FPGA;glue logic;Epiphany-accelerated BLAS library;scientific computing;Kernel;Libraries;Coprocessors;Random access memory;Benchmark testing;Acceleration;Linux;BLAS;Parallella;BLIS;Big Data;High Performance Computing},
}

@InProceedings{748305,
  author    = {M. R. {Brown} and S. D. {Gupta}},
  title     = {Design Of A General Purpose Meta-assembler For Parallel Processor Environment In Isps},
  booktitle = {[1989] Record of Proceedings. The 22nd Annual Simulation Symposium},
  year      = {1989},
  pages     = {105-117},
  month     = {March},
  abstract  = {This paper discusses the design of a general-purpose meta- assembler for both single processor and parallel processor environments to be used as a software tool to improve upon the capability of the ISPS (Instruction Set Processor Specification) simulator. Currently, it is not feasible to simulate multi-microprocessor systems capable of performing parallel processing tasks efficiently in the ISPS language. There remains the problem of running long benchmark programs that have to be written in the machine code of the particular architecture(s) under study. With this in mind run-time results of simulated benchmark programs will be reported for various configurations of parallel processing systems using the meta assembler. We introduce a general purpose assembler for ISPS which will generate the machine code for simulation files. This software package eliminates the inefficiency of writing and testing long benchmark programs in the machine code by providing the user with the capability of writing them in assembly language. This package provides for existing assembly languages and can allow the user to create assembly languages for other possible architecture designs. It is hoped that this software package will make ISPS a more complete and powerful hardware description language.},
  doi       = {10.1109/SIMSYM.1989.748305},
  keywords  = {Registers;Computational modeling;Hardware design languages;Parallel processing;Benchmark testing;Assembly systems;Software packages;Writing;Computer architecture;Service oriented architecture},
}

@Article{7983450,
  author   = {L. {Zheng} and X. {Liao} and H. {Jin} and H. {Liu}},
  title    = {Exploiting the Parallelism Between Conflicting Critical Sections with Partial Reversion},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2017},
  volume   = {28},
  number   = {12},
  pages    = {3443-3457},
  month    = {Dec},
  abstract = {The critical sections with the lock protection greatly limit the concurrency of multi-threaded applications. The prior lock elision based technique is presented to exploit the parallelism between critical sections accessing the disjoint shared data, but still fails to notice and expose a high degree of concurrency between critical sections that contend for the same shared data, i.e., conflicting critical sections (CCS). This paper focuses on exploiting the CCS parallelism. The key insight of this work is that, for each running CCS, a large proportion (>73.4% ) of parallelism between CCSs can be exploited as fully as possible by simply allowing the parallel execution of their first conflict-free code fragment at runtime. We therefore present BSOptimizer, a new microarchitecture, to perform the partial reversion integrated with a series of sophisticated hardware and software strategies for the CCS parallelization. We complement the off-the-shelf cache coherency protocol to perceive the conflict location of CCS, present a predictive checkpoint mechanism to register and predict the concerned conflict point in a lightweight and accurate fashion, and redefine the traditional mutual exclusive semantics with a binary relationship. With these collaborative techniques, each CCS can be scheduled in parallel. Our experimental results on a wide variety of real programs and PARSEC benchmarks show that, compared to the native execution and two state-of-the-art lock elision techniques (including SLE and SLR), BSOptmizer can dramatically improves the performance of programs with a slight (<;0.8% ) energy consumption and (<;3.9% ) extra runtime overhead. Our evaluation on a micro-benchmark with software based optimization also verifies that BSOptimizer can accurately exploit the CCS parallelism as promised.},
  doi      = {10.1109/TPDS.2017.2727485},
  keywords = {checkpointing;concurrency control;multi-threading;software architecture;software fault tolerance;partial reversion;CCS parallelism;parallel execution;conflict-free code fragment;CCS parallelization;conflicting critical sections;lock elision based technique;multithreaded application concurrency;lock protection;disjoint shared data access;BSOptimizer microarchitecture;hardware strategies;software strategies;predictive checkpoint mechanism;conflict point prediction;mutual exclusive semantics;binary relationship;collaborative techniques;CCS scheduling;PARSEC benchmarks;program performance improvement;energy consumption;runtime overhead;software based optimization;cache coherency protocol;CCS conflict location;CCS conflict location;Parallel processing;Runtime;Concurrent computing;Synchronization;Energy consumption;Distance measurement;Lock elision;parallelism;critical section;multi-threaded programs;lock synchronization},
}

@InProceedings{4279595,
  author    = {S. {Lee} and K. D. {Ryu} and K. {Lee} and J. {Choi}},
  title     = {Improving the Performance of Web Services Using Deployment-Time Binding Selection},
  booktitle = {IEEE International Conference on Web Services (ICWS 2007)},
  year      = {2007},
  pages     = {159-167},
  month     = {July},
  abstract  = {In this paper, we present a novel deployment-time binding selection framework for Web services to improve the performance. Using the information about target environments, we determine the best binding based on the availability and the accessibility of a service, and the performance characteristics of the bindings in a target environment. We have implemented the proposed mechanism as part of Eclipse-based development tools. We present an extensive performance evaluation of our methodology using benchmarks that we have created following public Web service interfaces, and emulating several e-business applications including a large scale legacy transaction processing system that runs on a mainframe.},
  doi       = {10.1109/ICWS.2007.106},
  keywords  = {software performance evaluation;Web services;deployment-time binding selection;Eclipse-based development tools;public Web service interfaces;e-business;transaction processing;Web services;Simple object access protocol;Application software;Benchmark testing;Programming;XML;Runtime environment;Service oriented architecture;Computer science;Availability},
}

@InProceedings{6877444,
  author    = {A. {Magni} and C. {Dubach} and M. F. P. {O'Boyle}},
  title     = {A large-scale cross-architecture evaluation of thread-coarsening},
  booktitle = {SC '13: Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis},
  year      = {2013},
  pages     = {1-11},
  month     = {Nov},
  abstract  = {OpenCL has become the de-facto data parallel programming model for parallel devices in today's high-performance supercomputers. OpenCL was designed with the goal of guaranteeing program portability across hardware from different vendors. However, achieving good performance is hard, requiring manual tuning of the program and expert knowledge of each target device. In this paper we consider a data parallel compiler transformation - thread-coarsening - and evaluate its effects across a range of devices by developing a source-to-source OpenCL compiler based on LLVM. We thoroughly evaluate this transformation on 17 benchmarks and five platforms with different coarsening parameters giving over 43,000 different experiments. We achieve speedups over 9x on individual applications and average speedups ranging from 1.15x on the Nvidia Kepler GPU to 1.50x on the AMD Cypress GPU. Finally, we use statistical regression to analyse and explain program performance in terms of hardware-based performance counters.},
  doi       = {10.1145/2503210.2503268},
  keywords  = {graphics processing units;multi-threading;program compilers;regression analysis;software architecture;software performance evaluation;software portability;large-scale cross-architecture evaluation;thread-coarsening parameters;de-facto data parallel programming model;high-performance supercomputers;program portability;data parallel compiler trans- formation;source-to-source OpenCL compiler;LLVM;Nvidia Kepler GPU;AMD Cypress GPU;statistical regression;program performance;hardware-based performance counters;Instruction sets;Performance evaluation;Kernel;Hardware;Benchmark testing;Graphics processing units;Multicore processing;GPU;OpenCL;Thread coarsening;Regression trees},
}

@InProceedings{8107434,
  author    = {J. {Kroß} and H. {Krcmar}},
  title     = {Model-Based Performance Evaluation of Batch and Stream Applications for Big Data},
  booktitle = {2017 IEEE 25th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS)},
  year      = {2017},
  pages     = {80-86},
  month     = {Sep.},
  abstract  = {Batch and stream processing represent the two main approaches implemented by big data systems such as Apache Spark and Apache Flink. Although only stream applications are intended to satisfy real-time requirements, both approaches are required to meet certain response time constraints. In addition, cluster architectures continuously expand and computing resources constitute high investments and expenses for organizations. Therefore, planning required capacities and predicting response times is crucial. In this work, we present a performance modeling and simulation approach by using and extending the Palladio component model. We predict performance metrics of batch and stream applications and its underlying processing systems by the example of Apache Spark on Apache Hadoop. Whereas most related work concentrates on one specific processing technique and focuses on the metric response time, we propose a general approach and consider the utilization of resources as well. In different experiments we evaluated our approach using applications and data workloads of the HiBench benchmark suite. The results indicate accurate predictions for upscaling cluster sizes as well as workloads with errors less than 18%.},
  doi       = {10.1109/MASCOTS.2017.21},
  keywords  = {Big Data;data analysis;data handling;parallel processing;power aware computing;public domain software;software architecture;software performance evaluation;stream applications;stream processing;big data systems;Apache Spark;Apache Flink;real-time requirements;response time constraints;high investments;predicting response times;performance modeling;simulation approach;Palladio component model;performance metrics;Apache Hadoop;specific processing technique;metric response time;data workloads;HiBench benchmark suite;cluster size upscaling;Sparks;Time factors;Computational modeling;Data models;Big Data;big data;performance;modeling;simulation},
}

@InProceedings{7307616,
  author    = {H. {Fujita} and K. {Iskra} and P. {Balaji} and A. A. {Chien}},
  title     = {Empirical Comparison of Three Versioning Architectures},
  booktitle = {2015 IEEE International Conference on Cluster Computing},
  year      = {2015},
  pages     = {456-459},
  month     = {Sep.},
  abstract  = {Future supercomputer systems will face serious reliability challenges. Among failure scenarios, latent errors are some of the most serious and concerning. Preserving multiple versions of critical data is a promising approach to deal with such errors. We are developing the Global View Resilience (GVR) library, with multi-version global arrays as one of the key features. This paper presents three array versioning architectures: flat array, flat array with change tracking, and log-structured array. We use a synthetic workload comparing the three array architectures in terms of runtime performance and memory requirements. The experiments show that the flat array with change tracking is the best architecture in terms of runtime performance, for versioning frequencies of 10-5ops-1or higher matching the second best architecture or beating it by over 8 times, whereas the log-structured array is preferable for low memory usage, since it saves up to 88% of memory compared with a flat array.},
  doi       = {10.1109/CLUSTER.2015.69},
  keywords  = {software architecture;software libraries;software reliability;three array versioning architecture;supercomputer system reliability;global view resilience library;GVR library;Arrays;Memory management;Resilience;Libraries;Runtime;Benchmark testing;Global View Resilience;multi-versioning;distributed array;change tracking;log-based data structures},
}

@InProceedings{4732088,
  author    = {L. {Li} and M. {Jiang} and S. {Ma}},
  title     = {Balanced Job Bounds Calculation for Approximating the E-Commerce Development Platform for 3-Tiered Web Applications: Performance Factors},
  booktitle = {2008 International Symposium on Intelligent Information Technology Application Workshops},
  year      = {2008},
  pages     = {922-926},
  month     = {Dec},
  abstract  = {As the complexity of large-scale enterprise application increases, supplying performance verification staging becomes very important. Starting from analyze the architecture of an e-commerce development platform model based on 3-tiered Web, using queuing network theory, we propose a reusable e-commerce development platform based on 3-tiered Web. Finally, we implement the evaluation algorithm and the calculation of the balanced job bounds with the help of MATLAB and test the experimental data in LoadRunner benchmark.},
  doi       = {10.1109/IITA.Workshops.2008.101},
  keywords  = {electronic commerce;Java;queueing theory;Web services;balanced job bound calculation;e-commerce development platform;3-tiered Web service application;large-scale enterprise application;queuing network theory;J2EE environment;Object oriented modeling;Logic;Object oriented databases;Transaction databases;Computer architecture;Application software;Service oriented architecture;Mathematical model;Web server;Measurement;performance;3-tieres;bounds;web},
}

@InProceedings{4536209,
  author    = {G. S. {Poghosyan} and M. {Kunze}},
  title     = {Monitoring for multi-middleware grid},
  booktitle = {2008 IEEE International Symposium on Parallel and Distributed Processing},
  year      = {2008},
  pages     = {1-6},
  month     = {April},
  abstract  = {Within the framework of the German Grid Computing Initiative (D-Grid), we study the monitoring systems and software suites that are used to collect the information from computational grids working with single or multiple middleware systems. Based on these investigations we build the prototypes of monitoring systems and implement it in the D-Grid infrastructure. A concept of Site Check Center (SCC) suggested to providing a unified interface for access to data from different test-benchmark systems working with more than one middleware software. A Vertical hierarchal architecture for exchange of information and building the network of monitoring systems is suggested and employed. A concept for separation between consumer and resource/service provider related monitoring information is proposed. Furthermore, we study the integration of monitoring components into general computational multi- middleware grid infrastructure developed according to specific community needs.},
  doi       = {10.1109/IPDPS.2008.4536209},
  keywords  = {grid computing;middleware;software architecture;system monitoring;multi-middleware grid;German grid computing initiative;monitoring systems;site check center;vertical hierarchal architecture;Monitoring;Grid computing;Middleware;Software systems;Software prototyping;Prototypes;Software testing;System testing;Computer architecture;Buildings},
}

@InProceedings{4591742,
  author    = {Y. {Magid} and A. {Adi} and M. {Barnea} and D. {Botzer} and E. {Rabinovich}},
  title     = {Application Generation Framework for Real-Time Complex Event Processing},
  booktitle = {2008 32nd Annual IEEE International Computer Software and Applications Conference},
  year      = {2008},
  pages     = {1162-1167},
  month     = {July},
  abstract  = {We propose to develop a framework which provides the ability to apply complex event processing in realtime domains, thus allowing an easier process of developing and maintaining specific solutions for real-time event-based systems, while upholding the real time requirements of the system. Specifically, we propose to develop a framework that includes an integrated development environment for defining rules, and, given a set of rules, generates code for a complex event processing application for which it is able to determine time bounds on the response of this application to a set of supported events. In particular, the tool helps determine a time bound for the execution time of the code corresponding to each rule. Many Service Oriented Architecture (SOA) applications, in domains such as financial services, manufacturing, gaming and military/aerospace, have real-time performance requirements. We present real-life industry use cases from these domains as motivation for the potential benefit in developing real-time complex event processing applications. In support of a feasibility argument for the proposed approach we present some preliminary experimental results obtained on a partially implemented tool.},
  doi       = {10.1109/COMPSAC.2008.146},
  keywords  = {business data processing;formal specification;real-time systems;application generation framework;real-time complex event processing;real-time event-based systems;real time system requirements;integrated development environment;service oriented architecture;Real time systems;Engines;Middleware;Time measurement;Benchmark testing;Industries;Databases;Complex Event Processing;Real-Time Applications},
}

@InProceedings{1281660,
  author    = {C. -. {Luk} and R. {Muth} and {Harish Patil} and R. {Cohn} and G. {Lowney}},
  title     = {Ispike: a post-link optimizer for the Intel/spl reg/ Itanium/spl reg/ architecture},
  booktitle = {International Symposium on Code Generation and Optimization, 2004. CGO 2004.},
  year      = {2004},
  pages     = {15-26},
  month     = {March},
  abstract  = {Ispike is a post-link optimizer developed for the Intel/spl reg/ Itanium Processor Family (IPF) processors. The IPF architecture poses both opportunities and challenges to post-link optimizations. IPF offers a rich set of performance counters to collect detailed profile information at a low cost, which is essential to post-link optimization being practical. At the same time, the predication and bundling features on IPF make post-link code transformation more challenging than on other architectures. In Ispike, we have implemented optimizations like code layout, instruction prefetching, data layout, and data prefetching that exploit the IPF advantages, and strategies that cope with the IPF-specific challenges. Using SPEC CINT2000 as benchmarks, we show that Ispike improves performance by as much as 40% on the ltanium/spl reg/2 processor, with average improvement of 8.5% and 9.9% over executables generated by the Intel/spl reg/ Electron compiler and by the Gcc compiler, respectively. We also demonstrate that statistical profiles collected via IPF performance counters and complete profiles collected via instrumentation produce equal performance benefit, but the profiling overhead is significantly lower for performance counters.},
  doi       = {10.1109/CGO.2004.1281660},
  keywords  = {computer architecture;optimising compilers;storage management;benchmark testing;program control structures;operating systems (computers);instruction sets;Ispike;post-link optimizer;Intel/spl reg/ Itanium/spl reg/ architecture;Itanium Processor Family;performance counters;code transformation;code layout;instruction prefetching;data layout;data prefetching;SPEC CINT2000;Intel/spl reg/ Electron compiler;Gcc compiler;statistical profiles;Service oriented architecture;Counting circuits;Linux;Prefetching;Monitoring;Electrons;Design optimization;Instruments;Computer architecture;Software performance},
}

@Article{1274026,
  author   = {P. {Foglia} and R. {Giorgi} and C. A. {Prete}},
  title    = {Simulation study of memory performance of SMP multiprocessors running a TPC-W workload},
  journal  = {IEE Proceedings - Computers and Digital Techniques},
  year     = {2004},
  volume   = {151},
  number   = {2},
  pages    = {93-109},
  month    = {March},
  abstract = {The infrastructure to support electronic commerce is one of the areas where more processing power is needed. A multiprocessor system can offer advantages for running electronic commerce applications. The memory performance of an electronic commerce server, i.e. a system running electronic commerce applications, is evaluated in the case of shared-bus multiprocessor architecture. The software architecture of this server is based on a three-tier model and the workloads have been setup as specified by the TPC-W benchmark. The hardware configurations are: a single SMP running tiers two and three, and two SMPs each one running a single tier. The influence of memory subsystem on performance and scalability is analysed and several solutions aimed at reducing the latency of memory considered. After initial experiments, which validate the methodology, choices for cache, scheduling algorithm, and coherence protocol are explored to enhance performance and scalability. As in previous studies on shared-bus multiprocessors, it was found that the memory performance is highly influenced by cache parameters. While scaling the machine, the coherence overhead weighs more and more on the memory performance. False sharing in the kernel is among the main causes of this overhead. Unlike previous studies, passive sharing i.e. the useless sharing of the private data of the migrating processes, is shown to be an important factor that influences performance. This is especially true when multiprocessors with a higher number of processors are considered: an increase in the number of processors produces real benefits only if advanced techniques for reducing the coherence overhead are properly adopted. Scheduling techniques limiting process migration may reduce passive sharing, while restructuring techniques of the kernel data may reduce false sharing misses. However, even when process migration is reduced through cache-affinity techniques, standard coherence protocols like MESI protocol don't allow the best performance. Coherence protocols such as PSCR and AMSD produce performance benefits. PSCR, in particular, eliminates coherence overhead due to passive sharing and minimises the number of coherence misses. The adoption of PSCR and cache-affinity scheduling allows the multiprocessor scalability to be extended to 20 processors for a 128-bit shared bus and current values of main-memory-to-processor speed gap.},
  doi      = {10.1049/ip-cdt:20040349},
  keywords = {shared memory systems;resource allocation;electronic commerce;cache storage;processor scheduling;client-server systems;digital simulation;software architecture;transport protocols;SMP multiprocessor memory performance;TPC-W workload;electronic commerce;shared-bus multiprocessor architecture;three-tier model;hardware configuration;memory latency reduction;scheduling algorithm;coherence protocol;cache parameter;false sharing;passive sharing;cache-affinity technique},
}

@InProceedings{7008987,
  author    = {R. {Finker} and I. {del Campo} and J. {Echanobe} and V. {Martínez}},
  title     = {An intelligent embedded system for real-time adaptive extreme learning machine},
  booktitle = {2014 IEEE Symposium on Intelligent Embedded Systems (IES)},
  year      = {2014},
  pages     = {61-69},
  month     = {Dec},
  abstract  = {Extreme learning machine (ELM) is an emerging approach that has attracted the attention of the research community because it outperforms conventional back-propagation feed-forward neural networks and support vector machines (SVM) in some aspects. ELM provides a robust learning algorithm, free of local minima, suitable for high speed computation, and less dependant on human intervention than the above methods. ELM is appropriate for the implementation of intelligent embedded systems with real-time learning capability. Moreover, a number of cutting-edge applications demanding a high performance solution could benefit from this approach. In this work, a scalable hardware/software architecture for ELM is presented, and the details of its implementation on a field programmable gate array (FPGA) are analyzed. The proposed solution provides high speed, small size, low power consumption, autonomy, and true capability for real-time adaptation (i.e. the learning stage is performed on-chip). The developed system is able to deal with highly demanding multiclass classification problems. Two real-world applications are presented, a benchmark problem of the Landsat images database, and a novel driver identification system for smart car applications. Experimental results that validate the proposal are provided.},
  doi       = {10.1109/INTELES.2014.7008987},
  keywords  = {embedded systems;field programmable gate arrays;learning (artificial intelligence);pattern classification;software architecture;intelligent embedded system;real-time adaptive extreme learning machine;multiclass classification;ELM;robust learning algorithm;local minima;real-time learning capability;hardware architecture;software architecture;field programmable gate array;FPGA;Landsat images database;driver identification system;smart car applications;Neurons;Random access memory;Embedded systems;Field programmable gate arrays;Hardware;Digital signal processing;Real-time systems;extreme learning machine;neural networks;multiclass classification;intelligent embedded system;field programmable gate array (FPGA)},
}

@InProceedings{8764873,
  author    = {T. {MOLOABI}},
  title     = {Empirical Assessment of the Effectiveness of Business Intelligence Tools: Case of Free State Government Departments},
  booktitle = {2019 IST-Africa Week Conference (IST-Africa)},
  year      = {2019},
  pages     = {1-10},
  month     = {May},
  abstract  = {In this paper, we employ a multi-pronged analytical approach (qualitative survey and factor analysis) to empirically analyze the usage and effectiveness of a widely used business intelligence tool, namely the Vulindela System (VS) in prominent service-oriented government departments (Provincial Treasury, Health and CoGTA) in the Free State province in South Africa. Using the novel Task Technology Fit theoretical framework proposed by Goodhue (1995) as an evaluation benchmark, we compute two principal component analysis (PCA) models to identify key latent features of the VS technology. We find evidence for both bi-directional and unidirectional links between the operational capacity, usability and functionality of the VS and executed tasks, productivity and decision-making capability of the main users. The empirical result suggest that the strength of VS lies in its ability to perform unstructured tasks, collate information, improve decision making and productivity but the operational capacity and functionality of the system is constrained due to incompatibility to meet user's task profile and inflexibility to execute new task demanded. Based on these findings, the reliability and usability of the VS can be improved by testing the system in different network environment and continuous development of new software, upgrade of operating systems.},
  doi       = {10.23919/ISTAFRICA.2019.8764873},
  keywords  = {competitive intelligence;decision making;government data processing;principal component analysis;service-oriented architecture;software reliability;empirical assessment;business intelligence tools;free state government departments;multipronged analytical approach;Vulindela System;Free State province;principal component analysis models;VS technology;unidirectional links;operational capacity;decision-making capability;operating systems;bi-directional links;service-oriented government departments;task technology fit theoretical framework;Task analysis;Tools;Government;Business intelligence;Decision making;Reliability;Business Intelligence tool;Information Technology;Principal Component Analysis;Task-Technology Fit;Vulindela System;Free State},
}

@InProceedings{7603547,
  author    = {W. {Hussain} and F. {Hussain} and O. {Hussain}},
  title     = {QoS prediction methods to avoid SLA violation in post-interaction time phase},
  booktitle = {2016 IEEE 11th Conference on Industrial Electronics and Applications (ICIEA)},
  year      = {2016},
  pages     = {32-37},
  month     = {June},
  abstract  = {Due to the dynamic nature of cloud computing it is very important for a small to medium scale service providers to optimally assign computing resources and apply accurate prediction methods that enable the best resource management. The choice of an ideal quality of service (QoS) prediction method is one of the key factors in business transactions that help a service provider manage the risk of SLA violations by taking appropriate and immediate action to reduce occurrence, or avoid operations that may cause risk. In this paper we analyze ten prediction methods, including neural network methods, stochastic methods and others to predict time series cloud data and compare their prediction accuracy over five time intervals. We use Cascade Forward Backpropagation, Elman Backpropagation, Generalized Regression, NARX, Simple Exponential Smoothing, Simple Moving Average, Weighted Moving Average, Extrapolation, Holt-Winters Double Exponential Smoothing and ARIMA and predict resource usage at 1, 2, 3, 4 and 5 hours into the future. We use Root Means Square Error and Mean Absolute Deviation as a benchmark for their prediction accuracy. From the prediction results we observed that the ARIMA method provides the most optimal prediction results for all time intervals.},
  doi       = {10.1109/ICIEA.2016.7603547},
  keywords  = {backpropagation;cloud computing;contracts;extrapolation;mean square error methods;neural nets;quality of service;regression analysis;resource allocation;smoothing methods;software architecture;quality of service prediction method;QoS prediction method;service level agreement;SLA violation;postinteraction time phase;cloud computing architecture;resource management;cascade forward backpropagation;Elman backpropagation;generalized regression;NARX;weighted moving average;extrapolation;Holt-Winters double exponential smoothing;ARIMA method;root mean square error;mean absolute deviation;Prediction methods;Quality of service;Backpropagation;Smoothing methods;Hidden Markov models;Cloud computing;Time series analysis;SLA violation prediction;cloud computing;SLA monitoring;QoS prediction methods},
}

@InProceedings{8029759,
  author    = {M. {Daagi} and A. {Ouniy} and M. {Kessentini} and M. M. {Gammoudi} and S. {Bouktif}},
  title     = {Web Service Interface Decomposition Using Formal Concept Analysis},
  booktitle = {2017 IEEE International Conference on Web Services (ICWS)},
  year      = {2017},
  pages     = {172-179},
  month     = {June},
  abstract  = {In the service-oriented paradigm, Web service interfaces are considered contracts between Web service subscribers and providers. The structure of service interfaces has an extremely important role to discover, understand, and reuse Web services. However, it has been shown that service developers tend to pay little care to the design of their interfaces. A common design issue that often appears in real-world Web services is that their interfaces lack cohesion, i.e., they expose several operations that are often semantically unrelated. Such a bad design practice may significantly complicate the comprehension and reuse of the services functionalities and lead to several maintenance and evolution problems. In this paper, we propose a new approach for Web service interface decomposition using a Formal Concept Analysis (FCA) framework. The proposed FCA-based approach aims at identifying the hidden relationships among service operations in order to improve the interface modularity and usability. The relationships between operations are based on cohesion measures including semantic, sequential and communicational cohesion. The identified groups of semantically related operations having common properties are used to define new cohesive and loosely coupled service interfaces. We conducted a quantitative and qualitative empirical study to evaluate our approach on a benchmark of 26 real world Web services provided by Amazon and Yahoo. The obtained results show that our approach can significantly improve Web service interface design quality compared to state-of-the-art approaches.},
  doi       = {10.1109/ICWS.2017.30},
  keywords  = {formal concept analysis;software maintenance;software metrics;user interfaces;Web services;Web service interface decomposition;reuse Web services;service developers;real-world Web services;services functionalities;Formal Concept Analysis framework;interface modularity;cohesive coupled service interfaces;loosely coupled service interfaces;Web service interface design quality;Formal concept analysis;Service-oriented architecture;Cloud computing;Measurement;Quality of service;Semantics;Web service;interface;cohesion;modularization;design},
}

@InProceedings{7509423,
  author    = {N. {Bombieri} and F. {Busato} and F. {Fummi} and M. {Scala}},
  title     = {MIPP: A microbenchmark suite for performance, power, and energy consumption characterization of GPU architectures},
  booktitle = {2016 11th IEEE Symposium on Industrial Embedded Systems (SIES)},
  year      = {2016},
  pages     = {1-6},
  month     = {May},
  abstract  = {GPU-accelerated applications are becoming increasingly common in high-performance computing as well as in low-power heterogeneous embedded systems. Nevertheless, GPU programming is a challenging task, especially if a GPU application has to be tuned to fully take advantage of the GPU architectural configuration. Even more challenging is the application tuning by considering power and energy consumption, which have emerged as first-order design constraints in addition to performance. Solving bottlenecks of a GPU application such as high thread divergence or poor memory coalescing have a different impact on the overall performance, power and energy consumption. Such an impact also depends on the GPU device on which the application is run. This paper presents a suite of microbenchmarks, which are specialized chunks of GPU code that exercise specific device components (e.g., arithmetic instruction units, shared memory, cache, DRAM, etc.) and that provide the actual characteristics of such components in terms of throughput, power, and energy consumption. The suite aims at enriching standard profiler information and guiding the GPU application tuning on a specific GPU architecture by considering all three design constraints (i.e., power, performance, energy consumption). The paper presents the results obtained by applying the proposed suite to characterize two different GPU devices and to understand how application tuning may impact differently on them.},
  doi       = {10.1109/SIES.2016.7509423},
  keywords  = {embedded systems;energy conservation;graphics processing units;parallel processing;power aware computing;power consumption;software architecture;software performance evaluation;MIPP;microbenchmark suite;performance consumption;power consumption;energy consumption;GPU-accelerated applications;high-performance computing;low-power heterogeneous embedded systems;GPU programming;GPU architectural configuration;Graphics processing units;Performance evaluation;Computer architecture;Throughput;Optimization;Benchmark testing;Tuning},
}

@InProceedings{5718800,
  author    = {H. {Amrouch} and J. {Henkel}},
  title     = {Self-Immunity Technique to Improve Register File Integrity Against Soft Errors},
  booktitle = {2011 24th Internatioal Conference on VLSI Design},
  year      = {2011},
  pages     = {189-194},
  month     = {Jan},
  abstract  = {Continuous shrinking in feature size, increasing power density etc. increase the vulnerability of microprocessors against soft errors even in terrestrial applications. The register file is one of the essential architectural components where soft errors can be very mischievous because errors may rapidly spread from there throughout the whole system. Thus, register files are recognized as one of the major concerns when it comes to reliability. This paper introduces Self-Immunity, a technique that improves the integrity of the register file with respect to soft errors. Based on the observation that a certain number of register bits are not always used to represent a value stored in a register. This paper deals with the difficulty to exploit this obvious observation to enhance the register file integrity against soft errors. We show that our technique can reduce the vulnerability of the register file considerably while exhibiting smaller overhead in terms of area and power consumption compared to state-of-the-art in register file protection.},
  doi       = {10.1109/VLSID.2011.68},
  keywords  = {file organisation;microprocessor chips;power aware computing;software architecture;software reliability;self-immunity technique;register file integrity protection;soft error;power density;microprocessors;register bits;Registers;Program processors;Computer architecture;Benchmark testing;Decoding;Silicon;Fault tolerance},
}

@InProceedings{5161086,
  author    = {R. {Couturier} and D. {Laiymani} and S. {Miquee}},
  title     = {High performance computing using ProActive environment and the asynchronous iteration model},
  booktitle = {2009 IEEE International Symposium on Parallel Distributed Processing},
  year      = {2009},
  pages     = {1-7},
  month     = {May},
  abstract  = {This paper presents a new library for the ProActive environment, called AIL-PA (asynchronous iterative library for ProActive). This new library allows to execute programs for solving large scale problems on various architectures. Two models of algorithm can be used: the synchronous iteration model which is efficient on single clusters; the asynchronous iteration model which is more efficient on distributed clusters. Both approaches are tested on both architectures, using Kernel CG of the NAS Parallel Benchmarks on the Grid'5000 platform. These tests also allow us to compare ProActive with AIL-PA and with the Jace programming environment. The results show that the asynchronous iteration model with AIL-PA is more efficient on distributed clusters than the synchronous iteration model. Moreover, these experiments also show that AIL-PA does not involve additional overhead to ProActive.},
  doi       = {10.1109/IPDPS.2009.5161086},
  keywords  = {grid computing;iterative methods;large-scale systems;middleware;software architecture;software libraries;workstation clusters;high performance computing;ProActive environment;asynchronous iteration model;asynchronous iterative library for ProActive;AIL-PA;large scale problems;synchronous iteration model;single clusters;distributed clusters;Kernel CG;NAS Parallel Benchmarks on the Grid 5000 platform;Jace programming;High performance computing;Libraries;Large-scale systems;Computer architecture;Clustering algorithms;Iterative algorithms;Kernel;Character generation;Benchmark testing;Programming environments},
}

@InProceedings{7119353,
  author    = {H. {Riener} and M. {Soeken} and C. {Werther} and G. {Fey} and R. {Drechsler}},
  title     = {MetaSMT: a unified interface to SMT-LIB2},
  booktitle = {Proceedings of the 2014 Forum on Specification and Design Languages (FDL)},
  year      = {2014},
  volume    = {978-2-9530504-9-3},
  pages     = {1-6},
  month     = {Oct},
  abstract  = {Various problems from artificial intelligence and formal methods are solved utilizing Satisfiability Modulo Theories (SMT) solvers. Selecting the best SMT solver for a specific application, however, is a daunting task. In this paper, we present the novel metaSMT TCP server and client architecture which can be used to solve SMT instances expressed in SMT-LIB2 by multiple solver processes in parallel. The metaSMT TCP server provides a unified interface for SMT-LIB2 instances with the capability to either use the API or the file interface of a solver process and thus serves as a highly customizable portfolio solver. We show that the run-time overhead required by the metaSMT TCP server and client architecture is marginal using selected benchmarks from SMT-LIB.},
  doi       = {10.1109/FDL.2014.7119353},
  keywords  = {application program interfaces;client-server systems;computability;high level languages;software architecture;software libraries;unified interface;SMT-LIB2;artificial intelligence;formal methods;satisfiability modulo theory solvers;SMT solvers;metaSMT TCP server architecture;metaSMT TCP client architecture;multiple solver processes;API;file interface;customizable portfolio solver;run-time overhead;Servers;Portfolios;Computer architecture;Benchmark testing;Standards;Command languages;Syntactics},
}

@InProceedings{6903485,
  author    = {S. G. {Sáez} and V. {Andrikopoulos} and F. {Leymann} and S. {Strauch}},
  title     = {Evaluating Caching Strategies for Cloud Data Access Using an Enterprise Service Bus},
  booktitle = {2014 IEEE International Conference on Cloud Engineering},
  year      = {2014},
  pages     = {289-294},
  month     = {March},
  abstract  = {Nowadays different Cloud services enable enterprises to migrate applications to the Cloud. An application can be partially migrated by replacing some of its components with Cloud services, or by migrating one or multiple of its layers to the Cloud. As a result, accessing application data stored off-premise requires mechanisms to mitigate the negative impact on Quality of Service (QoS), e.g. due to network latency. In this work, we propose and realize an approach for transparently accessing data migrated to the Cloud using a multi-tenant open source Enterprise Service Bus (ESB) as the basis. Furthermore, we enhance the ESB with QoS awareness by integrating it with an open source caching solution. For evaluation purposes we generate a representative application workload using data from the TPC-H benchmark. Based on this workload, we then evaluate the optimal caching strategy among multiple eviction algorithms when accessing relational databases located at different Cloud providers.},
  doi       = {10.1109/IC2E.2014.49},
  keywords  = {cache storage;cloud computing;public domain software;quality of service;relational databases;software architecture;caching strategy evaluation;cloud data access;multitenant open source enterprise service bus;cloud services;quality of service;network latency;ESB;QoS awareness;open source caching solution;TPC-H benchmark;multiple eviction algorithms;relational databases;Throughput;Containers;Database systems;Protocols;Quality of service;Relational databases;Enterprise Service Bus (ESB);Cache;Relational Databases;Performance Optimization},
}

@InProceedings{8432182,
  author    = {I. {Gerostathopoulos} and C. {Prehofer} and L. {Bulej} and T. {Bureš} and V. {Horký} and P. {Tuma}},
  title     = {Cost-Aware Stage-Based Experimentation: Challenges and Emerging Results},
  booktitle = {2018 IEEE International Conference on Software Architecture Companion (ICSA-C)},
  year      = {2018},
  pages     = {72-75},
  month     = {April},
  abstract  = {Experimentation at post-deployment phases (in production environments) can be a powerful tool for both learning how a deployed system operates and how it is being used. Though this knowledge is invaluable for optimization of the system, collecting it may require long time and experiments may even worsen the system with negative effects on users and business. This calls for methods for performing experimentation in production environments that balance the profit of experimentation with its cost. In this paper, we describe related challenges and our emerging results towards cost-aware stage-based experimentation. In particular, we aim for performing experiments that optimize towards their profit while making sure that the overall experimentation cost (e.g. total experimentation time) stays within given bounds. First, we illustrate the challenges and needs of such experimentation in two use cases from different domains. Second, we describe the main concepts behind our method in a semi-formal notation. Third, we exemplify the method by applying it in the two use cases and we report interesting first results.},
  doi       = {10.1109/ICSA-C.2018.00027},
  keywords  = {costing;profitability;research and development management;experimentation profit;post-deployment phases;experimentation time;experimentation cost;cost-aware stage-based experimentation;production environments;Benchmark testing;Measurement;Production;Optimization;Urban areas;Automobiles;Navigation;experimentation;cost-aware;optimization},
}

@InProceedings{7840861,
  author    = {P. {Xenopoulos} and J. {Daniel} and M. {Matheson} and S. {Sukumar}},
  title     = {Big data analytics on HPC architectures: Performance and cost},
  booktitle = {2016 IEEE International Conference on Big Data (Big Data)},
  year      = {2016},
  pages     = {2286-2295},
  month     = {Dec},
  abstract  = {Data driven science, accompanied by the explosion of petabytes of data, has called into need dedicated analytics computing resources. Dedicated analytics clusters require large capital outlays due to their expensive hardware requirements. Additionally, if such resources are located far from the data they analyze, they also incur substantial data transfer, which has both cost and latency implications. In this paper, we benchmark a variety of high-performance computing (HPC) architectures for classic data science algorithms, as well as conduct a cost analysis of these architectures. Additionally, we compare algorithms across analytic frameworks, as well as explore hidden costs in the form of queuing mechanisms. We observe that node architectures with large memory and high memory bandwidth are better suited for big data analytics on HPC hardware. We also conclude that cloud computing is more cost effective for small or experimental data workloads, but HPC is more cost effective at scale. Additionally, we quantify the hidden costs of queuing and how it relates to data science workloads. Finally, we observe that software developed for the cloud, such as Spark, performs significantly worse than pbdR when run in HPC environments.},
  doi       = {10.1109/BigData.2016.7840861},
  keywords  = {Big Data;cloud computing;data analysis;parallel processing;queueing theory;software architecture;Big data analytics;HPC architectures;high-performance computing architectures;cloud computing;software development;Computer architecture;Cloud computing;Hardware;Benchmark testing;Sparks;Matrix decomposition;Data science},
}

@InProceedings{1169171,
  author    = {A. {Gorin} and J. {Shoenfelt} and R. {Lewine}},
  title     = {Speech recognition on the DADO/DSP multiprocessor},
  booktitle = {ICASSP '86. IEEE International Conference on Acoustics, Speech, and Signal Processing},
  year      = {1986},
  volume    = {11},
  pages     = {361-364},
  month     = {April},
  abstract  = {The investigation of multiprocessor architectures and parallel algorithms for speech recognition is important. Large vocabulary speech recognition is a computationally intensive problem, which can require orders of magnitude acceleration over uniprocessors to achieve real-time performance. Also, there is still much algorithm development work to be done, which requires a programmable computer rather than a hardware implementation. This paper describes a massively parallel hardware/software architecture that is applicable to accelerating a wide class of large vocabulary speech recognition algorithms. The general principles of applicability supporting this claim will be described. Timing and sizing results obtained by applying these principles to Rabiner's level-building DTW algorithm for connected-word recognition will be given. Finally, a benchmark algorithm is described that demonstrates the programmability and performance of the architecture.},
  doi       = {10.1109/ICASSP.1986.1169171},
  keywords  = {Speech recognition;Digital signal processing;Vocabulary;Acceleration;Hardware;Computer architecture;Parallel algorithms;Software architecture;Software algorithms;Timing},
}

@Article{7938396,
  author   = {J. {Liu} and W. {Lu} and F. {Zhou} and P. {Lu} and Z. {Zhu}},
  title    = {On Dynamic Service Function Chain Deployment and Readjustment},
  journal  = {IEEE Transactions on Network and Service Management},
  year     = {2017},
  volume   = {14},
  number   = {3},
  pages    = {543-553},
  month    = {Sep.},
  abstract = {Network function virtualization (NFV) is a promising technology to decouple the network functions from dedicated hardware elements, leading to the significant cost reduction in network service provisioning. As more and more users are trying to access their services wherever and whenever, we expect the NFV-related service function chains (SFCs) to be dynamic and adaptive, i.e., they can be readjusted to adapt to the service requests' dynamics for better user experience. In this paper, we study how to optimize SFC deployment and readjustment in the dynamic situation. Specifically, we try to jointly optimize the deployment of new users' SFCs and the readjustment of in-service users' SFCs while considering the trade-off between resource consumption and operational overhead. We first formulate an integer linear programming (ILP) model to solve the problem exactly. Then, to reduce the time complexity, we design a column generation (CG) model for the optimization. Simulation results show that the proposed CG-based algorithm can approximate the performance of the ILP and outperform an existing benchmark in terms of the profit from service provisioning.},
  doi      = {10.1109/TNSM.2017.2711610},
  keywords = {computational complexity;integer programming;linear programming;resource allocation;service-oriented architecture;software defined networking;virtualisation;dynamic service function chain deployment;dynamic service function chain readjustment;network function virtualization;NFV;network service provisioning;SFC deployment;SFC readjustment;resource consumption;operational overhead;integer linear programming;ILP model;time complexity;column generation model;Optimization;Bandwidth;Algorithm design and analysis;Hardware;Heuristic algorithms;Cloud computing;Network topology;Network function virtualization (NFV);service function chain (SFC);dynamic support;column generation},
}

@InProceedings{7167206,
  author    = {A. {Tretter} and P. {Kumar} and L. {Thiele}},
  title     = {Interleaved multi-bank scratchpad memories: A probabilistic description of access conflicts},
  booktitle = {2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)},
  year      = {2015},
  pages     = {1-6},
  month     = {June},
  abstract  = {Shared on-chip memory is common on state-of-the-art multi-core platforms. In a number of designs, memory throughput is enhanced by providing multiple independent memory banks and spreading consecutive memory addresses to these (interleaving). This can reduce, but not eliminate, the number of access conflicts. In this paper, we statically analyse the probabilities and frequencies of these access conflicts and calculate the expected throughput for various hardware configurations and software applications. Using two techniques - the classic occupancy distribution and a Markov model - we are able to explain most of the underlying conflict mechanisms and to provide accurate estimations. We present the practical consequences for hardware and software design and establish an intuitive understanding of the characteristics of interleaved memory architectures.},
  doi       = {10.1145/2744769.2744861},
  keywords  = {Markov processes;multiprocessing systems;semiconductor storage;software architecture;interleaved multibank scratchpad memories;probabilistic description;access conflicts;shared on-chip memory;multicore platforms;memory throughput;multiple independent memory banks;consecutive memory addresses;classic occupancy distribution;Markov model;conflict mechanisms;hardware design;software design;interleaved memory architectures;Throughput;Markov processes;Program processors;Analytical models;Benchmark testing;Probability;Computational modeling},
}

@InProceedings{918862,
  author    = {F. {Bonchi} and F. {Giannotti} and G. {Manco} and C. {Renso} and M. {Nanni} and D. {Pedreschi} and S. {Ruggieri}},
  title     = {Data mining for intelligent Web caching},
  booktitle = {Proceedings International Conference on Information Technology: Coding and Computing},
  year      = {2001},
  pages     = {599-603},
  month     = {April},
  abstract  = {Presents a vertical application of data warehousing and data mining technology: intelligent Web caching. We introduce several ways to construct intelligent Web caching algorithms that employ predictive models of Web requests; the general idea is to extend the LRU (least recently used) policy of Web and proxy servers by making it sensible to Web access models extracted from Web log data using data mining techniques. Two approaches have been studied, in particular one based on association rules and another based on decision trees. The experimental results of the new algorithms show substantial improvements over existing LRU-based caching techniques in terms of the hit rate, i.e. the fraction of Web documents directly retrieved in the cache. We designed and developed a prototypical system, which supports data warehousing of Web log data, extraction of data mining models and simulation of the Web caching algorithms, around an architecture that integrates the various phases in the knowledge discovery process. The system supports a systematic evaluation and benchmarking of the proposed algorithms with respect to existing caching strategies.},
  doi       = {10.1109/ITCC.2001.918862},
  keywords  = {data mining;information resources;cache storage;data warehouses;file servers;Internet;decision trees;data mining;intelligent Web caching;vertical application;data warehousing;predictive models;World Wide Web requests;least recently used policy;Web servers;proxy servers;Web access models;Web log data;association rules;decision trees;hit rate;Web document retrieval;algorithm simulation;knowledge discovery;benchmarking;algorithm evaluation;Data mining;Predictive models;Service oriented architecture;Network servers;Electronic mail;Prediction algorithms;Web server;Councils;Computer science;Application software},
}

@InProceedings{4536566,
  author    = {C. {Esposito} and S. {Russo} and D. {Di Crescenzo}},
  title     = {Performance assessment of OMG compliant data distribution middleware},
  booktitle = {2008 IEEE International Symposium on Parallel and Distributed Processing},
  year      = {2008},
  pages     = {1-8},
  month     = {April},
  abstract  = {Event-driven architectures (EDAs) are widely used to make distributed mission critical software systems more- efficient and scalable. In the context of EDAs, data distribution service (DDS) is a recent standard by the object management group that offers a rich support for quality- of-service and balances predictable behavior and implementation efficiency. The DDS specification does not outline how messages are delivered, so several architectures are nowadays available. This paper focuses on performance assessment of OMG DDS-compliant middleware technologies. It provides three contributions to the study of evaluating the performance of DDS implementations: 1) describe the challenges to be addressed; 2) propose possible solutions; 3) define a representative workload scenario for evaluating the performance and scalability of DDS platforms. At the end of the paper, a case study of DDS performance assessment, performed with the proposed benchmark, is presented.},
  doi       = {10.1109/IPDPS.2008.4536566},
  keywords  = {data handling;distributed programming;middleware;quality of service;software architecture;software performance evaluation;performance assessment;compliant data distribution;middleware;event-driven architectures;distributed mission critical software systems;data distribution service;object management group;quality-of-service;Middleware},
}

@InProceedings{7973708,
  author    = {S. {Gugnani} and X. {Lu} and D. K. {Panda}},
  title     = {Swift-X: Accelerating OpenStack Swift with RDMA for Building an Efficient HPC Cloud},
  booktitle = {2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  year      = {2017},
  pages     = {238-247},
  month     = {May},
  abstract  = {Running Big Data applications in the cloud has become extremely popular in recent times. To enable the storage of data for these applications, cloud-based distributed storage solutions are a must. OpenStack Swift is an object storage service which is widely used for such purposes. Swift is one of the main components of the OpenStack software package. Although Swift has become extremely popular in recent times, its proxy server based design limits the overall throughput and scalability of the cluster. Swift is based on the traditional TCP/IP sockets based communication which has known performance issues such as context-switch and buffer copies for each message transfer. Modern high-performance interconnects such as InfiniBand and RoCE offer advanced features such as RDMA and provide high bandwidth and low latency communication. In this paper, we propose two new designs to improve the performance and scalability of Swift. We propose changes to the Swift architecture and operation design. We propose high-performance implementations of network communication and I/O modules based on RDMA to provide the fastest possible object transfer. In addition, we use efficient hashing algorithms to accelerate object verification in Swift. Experimental evaluations with microbenchmarks, Swift stack benchmark (ssbench), and synthetic application workloads reveal up to 2x and 7.3x performance improvement with our two proposed designs for put and get operations. To the best of our knowledge, this is the first work towards accelerating OpenStack Swift with RDMA over high-performance interconnects in the literature.},
  doi       = {10.1109/CCGRID.2017.103},
  keywords  = {Big Data;cloud computing;formal verification;input-output programs;parallel processing;software architecture;software packages;storage management;transport protocols;Swift-X;OpenStack Swift;RDMA;HPC cloud;Big Data;cloud-based distributed storage solutions;object storage service;software package;TCP/IP sockets;message transfer;high-performance interconnects;InfiniBand;RoCE;Swift architecture;I/O modules;hashing algorithms;object verification;Servers;Cloud computing;Computer architecture;Acceleration;Containers;Libraries;Scalability;OpenStack;Swift;RDMA;High-performance interconnects},
}

@InProceedings{5347106,
  author    = {M. {Khalgui} and H. {Hanisch} and A. {Gharbi}},
  title     = {Model-checking for the functional safety of Control Component-based heterogeneous embedded systems},
  booktitle = {2009 IEEE Conference on Emerging Technologies Factory Automation},
  year      = {2009},
  pages     = {1-10},
  month     = {Sep.},
  abstract  = {This paper1 deals with the model checking of Safe Heterogeneous Embedded Control Systems following different component-based technologies or implemented according to different Architecture Description Languages (ADL) used today in industry. The purpose is to reduce their time to market by exploiting various execution environments and different rich libraries. A ¿Control Component¿ is defined in our research work as an event-triggered software unit composed of an interface for any external interactions and an implementation allowing control actions of physical processes. A control system is assumed to be a composition of components with precedence constraints to control the plant according to well-defined execution orders. We define an agent-based architecture where the agent controls the environment evolution and applies automatic reconfigurations when hardware errors occur at run-time to guarantee a functional safety of the whole system. We model the architecture according to the formalism Net Condition/Event Systems (abbr. NCES), and apply the model checker SESA to check functional properties described according to the well-known Computation Tree Logic (abbr. CTL). Our purpose is to check that whenever an error occurs at run-time, the agent behaves as described in user requirements by activating Control Components and deactivating others to guarantee a functional safety of the whole system. A Benchmark Production System is used in this research work to explain our contribution.},
  doi       = {10.1109/ETFA.2009.5347106},
  keywords  = {control engineering computing;embedded systems;formal verification;object-oriented programming;software architecture;trees (mathematics);model checking;functional safety;control component;heterogeneous embedded systems;safe heterogeneous embedded control systems Architecture Description Languages;event-triggered software unit;agent-based architecture;environment evolution;automatic reconfiguration;net condition;event systems;computation tree logic;Safety;Embedded system;Automatic control;Control systems;Computer architecture;Error correction;Runtime;Control system synthesis;Architecture description languages;Electrical equipment industry;Software Component;Functional Safety;Agent-based Architecture;Automatic Reconfiguration;Model Checking},
}

@InProceedings{5372802,
  author    = {X. {Lu} and Y. {Zou} and F. {Xiong} and J. {Lin} and L. {Zha}},
  title     = {ICOMC: Invocation Complexity Of Multi-Language Clients for Classified Web Services and its Impact on Large Scale SOA Applications},
  booktitle = {2009 International Conference on Parallel and Distributed Computing, Applications and Technologies},
  year      = {2009},
  pages     = {186-194},
  month     = {Dec},
  abstract  = {Theoretically, multi-language clients invocating web services is no longer a problem due to XML-based interface descriptions by WSDL, but the reality is not so good. Some implementation level difficulties still exist when invoking web services from clients in different programming languages. These difficulties are caused by involving complex data structures in the service interface, carrying additional information such as WS-security headers in the SOAP messages, missing language features such as Reflection in C/C++ and so on, which make large scale multi-language SOA application development a time-consuming and buggy work. This paper proposes a new complexity ICOMC, short for Invocation Complexity Of Multi-language Clients, to quantify these difficulties, introduces implementation cost and runtime performance metrics for ICOMC, and indentifies three factors dominating the ICOMC: service interface, message context, and language feature. Consequently, the problem is formulated as finding out the correlation of the three factors to ICOMC. To simplify the problem, web services are classified into four categories: SISM, SICM, CISM and CICM according to service interface complexity and message context complexity. Furthermore, micro-benchmark experiments are done in C/C++/Java for all four categories. This paper also takes the GOS System Software of the China National Grid as a real large scale application to implement its C/C++ client APIs and compare them with the original Java APIs. Evaluations based on micro-benchmarks and real application show the correlations between the factors and ICOMC. Our results benefit web service interface designing, appropriate language adoption, and implementation cost / runtime performance estimation.},
  doi       = {10.1109/PDCAT.2009.74},
  keywords  = {C++ language;client-server systems;data structures;Java;programming languages;ubiquitous computing;Web services;XML;ICOMC;multilanguage clients invocation complexity;Web services;large scale SOA applications;XML based interface;WSDL;programming languages;complex data structures;WS security headers;SOAP messages;runtime performance metrics;interface complexity;message context complexity;micro benchmark experiments;C++;Java;cost performance estimation;runtime performance estimation;Web services;Large-scale systems;Service oriented architecture;Costs;Runtime;Context-aware services;Java;Application software;Computer languages;Data structures;ICOMC;SISM;SICM;CISM;CICM;Web Service Classification;Service Interface Complexity;Message Context Complexity;Language Feature Complexity},
}

@InProceedings{7371699,
  author    = {M. {Amaral} and J. {Polo} and D. {Carrera} and I. {Mohomed} and M. {Unuvar} and M. {Steinder}},
  title     = {Performance Evaluation of Microservices Architectures Using Containers},
  booktitle = {2015 IEEE 14th International Symposium on Network Computing and Applications},
  year      = {2015},
  pages     = {27-34},
  month     = {Sep.},
  abstract  = {Micro services architecture has started a new trend for application development for a number of reasons: (1) to reduce complexity by using tiny services, (2) to scale, remove and deploy parts of the system easily, (3) to improve flexibility to use different frameworks and tools, (4) to increase the overall scalability, and (5) to improve the resilience of the system. Containers have empowered the usage of micro services architectures by being lightweight, providing fast start-up times, and having a low overhead. Containers can be used to develop applications based on monolithic architectures where the whole system runs inside a single container or inside a micro services architecture where one or few processes run inside the containers. Two models can be used to implement a micro services architecture using containers: master-slave, or nested-container. The goal of this work is to compare the performance of CPU and network running benchmarks in the two aforementioned models of micro services architecture hence provide a benchmark analysis guidance for system designers.},
  doi       = {10.1109/NCA.2015.49},
  keywords  = {microprocessor chips;performance evaluation;software architecture;system designer;benchmark analysis guidance;network running benchmark;CPU;nested-container;master-slave;monolithic architecture;overall scalability;application development;microservices architecture;performance evaluation;Containers;Linux;Servers;Computer architecture;Virtual machining;Virtualization;Master-slave;Containers;Networking;Performance Evaluation;Microservices},
}

@Article{5871604,
  author   = {A. {Nisar} and W. {Liao} and A. {Choudhary}},
  title    = {Delegation-Based I/O Mechanism for High Performance Computing Systems},
  journal  = {IEEE Transactions on Parallel and Distributed Systems},
  year     = {2012},
  volume   = {23},
  number   = {2},
  pages    = {271-279},
  month    = {Feb},
  abstract = {Massively parallel applications often require periodic data checkpointing for program restart and post-run data analysis. Although high performance computing systems provide massive parallelism and computing power to fulfill the crucial requirements of the scientific applications, the I/O tasks of high-end applications do not scale. Strict data consistency semantics adopted from traditional file systems are inadequate for homogeneous parallel computing platforms. For high performance parallel applications independent I/O is critical, particularly if checkpointing data are dynamically created or irregularly partitioned. In particular, parallel programs generating a large number of unrelated I/O accesses on large-scale systems often face serious I/O serializations introduced by lock contention and conflicts at file system layer. As these applications may not be able to utilize the I/O optimizations requiring process synchronization, they pose a great challenge for parallel I/O architecture and software designs. We propose an I/O mechanism to bridge the gap between scientific applications and parallel storage systems. A static file domain partitioning method is developed to align the I/O requests and produce a client-server mapping that minimizes the file lock acquisition costs and eliminates the lock contention. Our performance evaluations of production application I/O kernels demonstrate scalable performance and achieve high I/O bandwidths.},
  doi      = {10.1109/TPDS.2011.166},
  keywords = {client-server systems;data analysis;data integrity;file organisation;information retrieval;input-output programs;optimisation;parallel architectures;parallel programming;performance evaluation;software architecture;delegation-based I-O mechanism;high performance computing system;periodic data checkpointing;post-run data analysis;high end application;strict data consistency semantics;homogeneous parallel computing platform;high performance parallel application;parallel program;unrelated I-O access;large scale system;I-O serialization;lock contention;file system layer;I-O optimization;process synchronization;parallel I-O architecture;software design;parallel storage system;static file domain partitioning method;I-O request;client-server mapping;file lock acquisition cost;lock contention;performance evaluation;production application I-O kernel;high I-O bandwidth;Servers;Kernel;Bandwidth;Optimization;Benchmark testing;Multicore processing;Performance evaluation;Parallel I/O;I/O delegation;MPI-IO;non collective I/O;collaborative caching;parallel file systems;file locking.},
}

@InProceedings{5599175,
  author    = {L. {Gao} and D. {Zaretsky} and G. {Mittal} and D. {Schonfeld} and P. {Banerjee}},
  title     = {Automatic Generation of Stream Descriptors for Streaming Architectures},
  booktitle = {2010 39th International Conference on Parallel Processing},
  year      = {2010},
  pages     = {307-312},
  month     = {Sep.},
  abstract  = {We describe a novel approach for automatically generating streaming architectures from software programs. While existing systems require user-defined stream models, our method automatically identifies producer-consumer streaming relationships and translates them into streaming architectures. Data streams between producer-consumer kernels are represented using a combination of stream descriptors and CFGs, which are categorized into four stream types. A bridge module is generated based on the stream type in the streaming architecture to facilitate data streaming between each producer-consumer pair. Several optimizations are also developed to improve throughput and parallelism. We demonstrate our results on a FPGA based platform. The automatically generated streaming architectures show 1.5-3x speedups over the non-streaming designs by employing spatial and temporal data independence to increase parallelism.},
  doi       = {10.1109/ICPP.2010.38},
  keywords  = {field programmable gate arrays;software architecture;stream descriptors;automatic generation streaming architectures;software programs;user-defined stream models;producer-consumer streaming relationships;producer-consumer kernels;bridge module;data streaming;FPGA based platform;nonstreaming designs;temporal data independence;Kernel;Computer architecture;Bridges;Streaming media;Random access memory;Benchmark testing;Throughput;Parallel processing;stream descriptor;stream architecture;FPGA},
}

@InProceedings{7139094,
  author    = {E. {Taborsky} and K. {Allen} and A. {Blanton} and A. K. {Jain} and B. F. {Klare}},
  title     = {Annotating Unconstrained Face Imagery: A scalable approach},
  booktitle = {2015 International Conference on Biometrics (ICB)},
  year      = {2015},
  pages     = {264-271},
  month     = {May},
  abstract  = {As unconstrained face recognition datasets progress from containing faces that can be automatically detected by commodity face detectors to face imagery with full pose variations that must instead be manually localized, a significant amount of annotation effort is required for developing benchmark datasets. In this work we describe a systematic approach for annotating fully unconstrained face imagery using crowdsourced labor. For such data preparation, a cascade of crowdsourced tasks are performed, which begins with bounding box annotations on all faces contained in images and videos, followed by identification of the labelled person of interest in such imagery, and, finally, landmark annotation of key facial fiducial points. In order to allow such annotations to scale to large volumes of imagery, a software system architecture is provided which achieves a sustained rate of 30,000 annotations per hour (or 500 manual annotations per minute). While previous crowdsourcing guidance described in the literature generally involved multiple choice questions or text input, our tasks required annotators to provide geometric primitives (rectangles and points) in images. As such, algorithms are provided for combining multiple annotations of an image into a single result, and automatically measuring the quality of a given annotation. Finally, other guidance is provided for improving the accuracy and scalability of crowdsourced image annotation for face detection and recognition.},
  doi       = {10.1109/ICB.2015.7139094},
  keywords  = {face recognition;outsourcing;pose estimation;software architecture;unconstrained face imagery annotation;scalable approach;unconstrained face recognition datasets;commodity face detectors;pose variations;benchmark datasets;systematic approach;labor crowdsourcing;data preparation;crowdsourced tasks;bounding box annotations;labelled person identification;landmark annotation;software system architecture;crowdsourced image annotation;face detection;Face;Servers;Databases;Middleware;Face recognition;Videos;Crowdsourcing},
}

@InProceedings{1657144,
  author    = {K. {Karuri} and R. {Leupers} and G. {Ascheid} and H. {Meyr} and M. {Kedia}},
  title     = {Design and Implementation of a Modular and Portable IEEE 754 Compliant Floating-Point Unit},
  booktitle = {Proceedings of the Design Automation Test in Europe Conference},
  year      = {2006},
  volume    = {2},
  pages     = {1-6},
  month     = {March},
  abstract  = {Multimedia and communication algorithms from embedded system domain often make extensive use of floating-point arithmetic. Due to the complexity and expense of the floating-point hardware, final implementations of these algorithms are usually carried out using floating-point emulation in software, or conversion (manually or automatically) of the floating-point operations to fixed point operations. Such strategies often lead to semi-optimal and imprecise software implementation. This paper presents the design and implementation of a floating-point unit (FPU) for an application specific instruction set processor (ASIP) suitable for embedded systems domain. Using a state-of-the-art architecture description language (ADL) based ASIP design framework, the FPU is implemented in such a modular way that it can be easily adapted to any other RISC like processor. The implemented operations are fully compliant to the IEEE 754 standard which facilitates portable software development. The benchmarking, in terms of energy, area and speed, of the designed FPU highlights the trade-offs of having a hardware FPU w.r.t. software emulation of floating-point operations},
  doi       = {10.1109/DATE.2006.243906},
  keywords  = {application specific integrated circuits;coprocessors;floating point arithmetic;hardware description languages;instruction sets;IEEE 754;floating-point unit;multimedia algorithm;communication algorithm;embedded system;floating-point arithmetic;floating-point emulation software;FPU;application specific instruction set processor;ASIP;architecture description language;ADL;RISC like processor;Embedded system;Hardware;Emulation;Application specific processors;Multimedia communication;Multimedia systems;Floating-point arithmetic;Software algorithms;Application software;Architecture description languages},
}

@InProceedings{5227149,
  author    = {W. {Plishker} and N. {Sane} and S. S. {Bhattacharyya}},
  title     = {Mode grouping for more effective generalized scheduling of dynamic dataflow applications},
  booktitle = {2009 46th ACM/IEEE Design Automation Conference},
  year      = {2009},
  pages     = {923-926},
  month     = {July},
  abstract  = {For a number of years, dataflow concepts have provided designers of digital signal processing systems with environments capable of expressing high-level software architectures as well as low-level, performance-oriented kernels. To apply these proven techniques to new complex, dynamic applications, we identify repetitive sequences of atomic, repeatable actions ("modes") inside dynamic actors to expose more of the static nature of the application. In this work, we propose a mode grouping strategy that aids in the decomposition of a dynamic dataflow graph into a set of static dataflow graphs that interact dynamically. Mode grouping enables the discovery of larger static subgraphs improving scheduling results. We show that grouping modes results in improved schedules with lower memory requirements for implementations by up to 37% including a common imaging benchmark with dynamic behavior: 3D B-spline interpolation.},
  doi       = {10.1145/1629911.1630148},
  keywords  = {data flow graphs;software architecture;mode grouping;dynamic dataflow applications;digital signal processing systems;high-level software architectures;dynamic dataflow graph;static dataflow graphs;Dynamic scheduling;Digital signal processing;Application software;High performance computing;Signal design;Signal processing algorithms;Scheduling algorithm;Permission;Processor scheduling;Data engineering;dataflow;scheduling;mode grouping},
}

@InProceedings{8029804,
  author    = {A. {Ouni} and M. {Daagi} and M. {Kessentini} and S. {Bouktif} and M. M. {Gammoudi}},
  title     = {A Machine Learning-Based Approach to Detect Web Service Design Defects},
  booktitle = {2017 IEEE International Conference on Web Services (ICWS)},
  year      = {2017},
  pages     = {532-539},
  month     = {June},
  abstract  = {Design defects are symptoms of poor design and implementation solutions adopted by developers during the development of their software systems. While the research community devoted a lot of effort to studying and devising approaches for detecting the traditional design defects in object-oriented (OO) applications, little knowledge and support is available for an emerging category of Web service interface design defects. Indeed, it has been shown that service designers and developers tend to pay little attention to their service interfaces design. Such design defects can be subjectively interpreted and hence detected in different ways. In this paper, we propose a novel approach, named WS3D, using machine learning techniques that combines Support Vector Machine (SVM) and Simulated Annealing (SA) to learn from real world examples of service design defects. WS3D has been empirically evaluated on a benchmark of Web services from 14 different application domains. We compared WS3D with the state-of-theart approaches which rely on traditional declarative techniques to detect service design defects by combining metrics and threshold values. Results show that WS3D outperforms the the compared approaches in terms of accuracy with a precision and recall scores of 91% and 94%, respectively.},
  doi       = {10.1109/ICWS.2017.62},
  keywords  = {learning (artificial intelligence);object-oriented methods;simulated annealing;software architecture;support vector machines;user interfaces;Web services;machine learning-based approach;software systems;object-oriented applications;OO;Web service interface design defects;design defects;WS3D;support vector machine;SVM;simulated annealing;SA;traditional declarative techniques;threshold values;Web services;Support vector machines;Measurement;Training;Software systems;Computer bugs;Simulated annealing;Web service design;design defects;Service interface},
}

@InProceedings{1425000,
  author    = {A. {Pocheville} and A. {Kheddar} and K. {Yokoi}},
  title     = {I-TOUCH: a generic multimodal framework for industry virtual prototyping},
  booktitle = {IEEE Conference on Robotics and Automation, 2004. TExCRA Technical Exhibition Based.},
  year      = {2004},
  pages     = {65-66},
  month     = {Nov},
  abstract  = {Simulations based on virtual reality techniques make often special arrangements for haptic rendering. In fact, in most cases, haptic rendering drives the design of the simulation engine. This work proposes alternative software architecture to handle multimodal and human centered interactive rendering with a particular emphasis for the computer haptics problem. Namely, the architecture allows handling both haptic devices requirements, in terms of high refresh rates, and physically-based simulations requirements, in terms of CPU time. The developed I-TOUCH framework is designed to address these issues; in the meantime, it provides an open architecture and powerful tools to benchmark robustness of subsequent algorithms. All undergoing developments are being tested with actual industry virtual prototyping scenarios, the complexity of some of which highlights the extent of the fundamental problems to overcome.},
  doi       = {10.1109/TEXCRA.2004.1425000},
  keywords  = {virtual prototyping;production engineering computing;virtual reality;haptic interfaces;I-TOUCH;generic multimodal framework;industry virtual prototyping;virtual reality techniques;haptic rendering;human centered interactive rendering;computer haptics problem;Virtual prototyping;Haptic interfaces;Computational modeling;Computer architecture;Virtual reality;Engines;Software architecture;Humans;Computer graphics;Algorithm design and analysis},
}

@InProceedings{7818337,
  author    = {N. {Nikoleris} and A. {Sandberg} and E. {Hagersten} and T. E. {Carlson}},
  title     = {CoolSim: Statistical techniques to replace cache warming with efficient, virtualized profiling},
  booktitle = {2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS)},
  year      = {2016},
  pages     = {106-115},
  month     = {July},
  abstract  = {Simulation is an important part of the evaluation of next-generation computing systems. Detailed, cycle-accurate simulation, however, can be very slow when evaluating realistic workloads on modern microarchitectures. Sampled simulation (e.g., SMARTS and SimPoint) improves simulation performance by an order of magnitude or more through the reduction of large workloads into a small but representative sample. Additionally, the execution state just prior to a simulation sample can be stored into checkpoints, allowing for fast restoration and evaluation. Unfortunately, changes in software, architecture or fundamental pieces of the microarchitecture (e.g., hardware-software co-design) require checkpoint regeneration. The end result for co-design degenerates to creating checkpoints for each modification, a task check pointing was designed to eliminate. Therefore, a solution is needed that allows for fast and accurate simulation, without the need for checkpoints. Virtualized fast-forwarding (VFF), an alternative to using checkpoints, allows for execution at near-native speed between simulation points. Warming the micro-architectural state prior to each simulation point, however, requires functional simulation, a costly operation for large caches (e.g., 8 MB). Simulating future systems with caches of many MBs can require warming of billions of instructions, dominating simulation time. This paper proposes CoolSim, an efficient simulation framework that eliminates cache warming. CoolSim uses VFF to advance between simulation points collecting at the same time sparse memory reuse information (MRI). The MRI is collected more than an order of magnitude faster than functional simulation. At the simulation point, detailed simulation with a statistical cache model is used to evaluate the design. The previously acquired MRI is used to estimate whether each memory request hits in the cache. The MRI is an architecturally independent metric and a single profile can be used in simulations of any size cache. We describe a prototype implementation of CoolSim based on KVM and gem5 running 19 × faster than the state-of-the-art sampled simulation, while it estimates the CPI of the SPEC CPU2006 benchmarks with 3.62% error on average, across a wide range of cache sizes.},
  doi       = {10.1109/SAMOS.2016.7818337},
  keywords  = {cache storage;checkpointing;digital simulation;hardware-software codesign;microprocessor chips;statistical analysis;storage management;virtualisation;CoolSim;cache warming;virtualized profiling;cycle-accurate simulation;microarchitecture;checkpoint regeneration;virtualized fast-forwarding;VFF;memory reuse information;MRI;hardware-software codesign;SPEC CPU2006;Magnetic resonance imaging;Microarchitecture;Software;Benchmark testing;Computational modeling;Data models;Electronic mail},
}

@InProceedings{5070883,
  author    = {S. {Subha}},
  title     = {Variable Block Size Architecture for Programs},
  booktitle = {2009 Sixth International Conference on Information Technology: New Generations},
  year      = {2009},
  pages     = {1640-1641},
  month     = {April},
  abstract  = {Cache reconfiguration is well studied topic in recent time. This paper proposes an algorithm to determine variable block size for variables in a program at some predetermined points called decision points based on their access pattern. The whole program is divided into segments by the decision points. Rules to decide the decision points are developed. The algorithm identifies the decision points, formulates optimization function to determine the average memory access time for the variables involved at these decision points. Solving the optimization function with constraints gives the optimal block size. The algorithm is simulated for a chosen program and an improvement of 78% in AMAT is observed over direct mapped cache in SPEC2K benchmark. An improvement of 28% over prefetching was observed for the chosen program using SPEC2K benchmark.},
  doi       = {10.1109/ITNG.2009.88},
  keywords  = {cache storage;data structures;optimisation;software architecture;table lookup;variable block size architecture;cache reconfiguration;optimization function;average memory access time;decision points;block lookup table;Constraint optimization;Information technology;Prefetching;Table lookup;Cache memory;Data structures;Page description languages;AMAT;Block Lookup Table;Variable block size},
}

@InProceedings{1629477,
  author    = {{Chanik Park} and P. {Talawar} and {Daeski Won} and {MyungJin Jung} and {JungBeen Im} and {Suksan Kim} and {Youngjoon Choi}},
  title     = {A High Performance Controller for NAND Flash-based Solid State Disk (NSSD)},
  booktitle = {2006 21st IEEE Non-Volatile Semiconductor Memory Workshop},
  year      = {2006},
  pages     = {17-20},
  month     = {Feb},
  abstract  = {NAND flash memory based solid-state disk (NSSD) has been used for industrial and military use due to its high reliability and shock resistance. With the bit cost reduction of flash memory and the explosive growth of flash market, NSSD is expected to penetrate into diverse applications such as mobile thin clients, car navigation systems and movie players, which prefer low power consumption, high reliability, high performance and so on. This paper mainly focuses on the development of a high performance and cost-efficient controller for NSSD, with the aim of describing both hardware and software architectures. In order to demonstrate the usefulness of the proposed approach, we show performance, power consumption and start-up time evaluation results over magnetic disks using third party benchmark tools},
  doi       = {10.1109/.2006.1629477},
  keywords  = {disc storage;flash memories;NAND circuits;NSSD controller;high performance cost-efficient controller;hardware architecture;power consumption;start-up time;software architecture;NAND flash-based solid state disk;shock resistance;bit cost reduction;flash memory;magnetic disks;Solid state circuits;Power system reliability;Energy consumption;Defense industry;Electrical equipment industry;Electric shock;Costs;Flash memory;Explosives;Application software},
}

@InProceedings{6583495,
  author    = {L. {Bedogni} and L. {Bononi} and M. {Di Felice} and A. {D'Elia} and R. {Mock} and F. {Montori} and F. {Morandi} and L. {Roffia} and S. {Rondelli} and T. S. {Cinotti} and F. {Vergari}},
  title     = {An interoperable architecture for mobile smart services over the internet of energy},
  booktitle = {2013 IEEE 14th International Symposium on "A World of Wireless, Mobile and Multimedia Networks" (WoWMoM)},
  year      = {2013},
  pages     = {1-6},
  month     = {June},
  abstract  = {The Internet of Energy (IoE) for Electric Mobility is an European research project that aims at deploying a communication infrastructure to facilitate and support the operations of Electric Vehicles (EVs). In this paper, we present three research contributions of IoE. First, we describe a software architecture to support the deployment of mobile and smart services over an Electric Mobility (EM) scenario. The proposed architecture relies on an ontology-based data representation, on a shared repository of information (Service Information Broker), and on software modules (called Knowledge Processors -KPs) for standardized data access/management. As a result, information sharing among the different stakeholders of the EM scenario (i.e. EVs, EVSEs, City Services, etc) is enabled, and the interoperability of smart services offered by heterogeneous providers is guaranteed by the common ontology. Second, we rely on the proposed architecture to develop a remote charging reservation system, that runs on top of mobile smarthphones, and allows drivers to monitor the current state-of-charge of their EV, and to reserve a charging slot at a specific EVSE. Finally, we validate our architecture through a benchmark framework, that supports the embedding of mobile EV applications and of real KPs into a simulated vehicular scenario, including realistic traffic, wireless communication and battery models. Evaluation results confirm the scalability of our architecture, and the ability to support EVs charging operations on a large-scale scenario (i.e. the downtown of Bologna).},
  doi       = {10.1109/WoWMoM.2013.6583495},
  keywords  = {computer network management;computer network reliability;data structures;embedded systems;Internet;mobility management (mobile radio);ontologies (artificial intelligence);open systems;smart phones;telecommunication traffic;interoperable architecture;mobile smart service;Internet of Energy;IoE;electric mobility;European research project;electric vehicle;EV;software architecture;EM;ontology-based data representation;shared information repository;service information broker;knowledge processor;KP;software module;data access-management;remote charging reservation system;mobile smart phone;simulated vehicular scenario;wireless communication;scalability;Mobile communication;Cities and towns;Computer architecture;Vehicles;Ontologies;Batteries;Smart phones;Electric Mobility;Mobile Services;Software Architectures;Smart-M3 technology;Performance Evaluation},
}

@InProceedings{4278515,
  author    = {M. {Pedram} and K. {Patel} and W. {Lee}},
  title     = {B2Sim:: a fast micro-architecture simulator based on basic block characterization},
  booktitle = {Proceedings of the 4th International Conference on Hardware/Software Codesign and System Synthesis (CODES+ISSS '06)},
  year      = {2006},
  pages     = {199-204},
  month     = {Oct},
  abstract  = {State-of-the-art architectural simulators support cycle accurate pipeline execution of application programs. However, it takes days and weeks to complete the simulation of even a moderate- size program. During the execution of a program, program behavior does not change randomly but changes over time in a predictable/periodic manner. This behavior provides the opportunity to limit the use of a pipeline simulator. More precisely, this paper presents a hybrid simulation engine, named B2Sim for (cycle-characterized) Basic Block based Simulator, where a fast cache simulator e.g., sim-cache and a slow pipeline simulator e.g., sim-outorder are employed together. B2Sim reduces the runtime of architectural simulation engines by making use of the instruction behavior within executed basic blocks. We have integrated B2Sim into SimpleScalar and have achieved on average a factor of 3.3 times speedup on the SPEC2000 benchmark and Media-bench programs compared to conventional pipeline simulator while maintaining the accuracy of the simulation results with less than 1% CPI error on average.},
  doi       = {10.1145/1176254.1176303},
  keywords  = {digital simulation;pipeline processing;software architecture;B2Sim;micro-architecture simulator;basic block characterization;state-of-the-art architectural simulators;pipeline execution;SimpleScalar;application programs;Pipelines;Phase detection;Analytical models;Engines;Permission;Runtime;Maintenance;Measurement techniques;Power system reliability;Performance analysis;basic block;micro-architecture simulation;program behavior},
}

@InProceedings{8024506,
  author    = {B. {Guler} and O. {Ozkasap}},
  title     = {Analysis of checkpointing algorithms for primary-backup replication},
  booktitle = {2017 IEEE Symposium on Computers and Communications (ISCC)},
  year      = {2017},
  pages     = {64-69},
  month     = {July},
  abstract  = {Replication is useful for supporting fault-tolerance, reliable and recovery oriented distributed systems. Popular application areas include databases, P2P systems, web services and Internet of Things. In this study, we propose utilizing the checkpointing concept for improving the efficiency of the well-known primary-backup replication protocol in distributed systems. We developed a software framework based on an in-memory replicated key-value store to evaluate various checkpointing algorithms. Using the framework over geographically distributed nodes of the PlanetLab platform, we performed extensive experiments and analysis with several different metrics, including blocking time, checkpointing time, checkpoint size and recovery time. Experimental scenarios consist of using the well-known benchmarking tool, YCSB, performing realistic read/update queries through exemplary workloads. Our findings indicate that incremental checkpointing combined with a periodic usage is the most efficient approach with having up to 30-times better system throughput and 50% decrease in average blocking times compared to traditional primary-backup replication and other checkpointing algorithms.},
  doi       = {10.1109/ISCC.2017.8024506},
  keywords  = {back-up procedures;checkpointing;distributed processing;fault tolerant computing;periodic usage;incremental checkpointing;read/update queries;YCSB benchmarking tool;PlanetLab platform;in-memory replicated key-value store;recovery oriented distributed systems;fault-tolerance;primary-backup replication protocol;checkpointing algorithms;Checkpointing;Servers;Fault tolerance;Fault tolerant systems;Conferences;Performance evaluation;Service-oriented architecture;checkpointing algorithms;primary-backup replication;fault tolerance;distributed systems},
}

@InProceedings{8456398,
  author    = {D. N. {Jha} and S. {Garg} and P. P. {Jayaraman} and R. {Buyya} and Z. {Li} and R. {Ranjan}},
  title     = {A Holistic Evaluation of Docker Containers for Interfering Microservices},
  booktitle = {2018 IEEE International Conference on Services Computing (SCC)},
  year      = {2018},
  pages     = {33-40},
  month     = {July},
  abstract  = {Advancement of container technology (e.g. Docker, LXC, etc.) transformed the virtualization concept by providing a lightweight alternative to hypervisors. Docker has emerged as the most popular container management tool. Recent research regarding the comparison of container with hypervisor and bare-metal demonstrates that the container can accomplish bare-metal performance in almost all case. However, the current literature lacks an in-depth study on the experimental evaluation for understanding the performance interference between microservices that are hosted within a single or across multiple containers. In this paper, we have presented the experimental study on the performance evaluation of Docker containers running heterogeneous set of microservices concurrently. We have conducted a comprehensive set of experiments following CEEM (Cloud Evaluation Experiment Methodology) to measure the interference between containers running either competing or independent microservices. We have also considered the effects of constraining the resources of a container by explicitly specifying the cgroups. We have evaluated the performance of containers in terms of inter-container (caused by two concurrent executing containers) and intra-container (caused between two microservices executing inside a container) interference which is almost neglected in the current literature. The evaluation results can be utilized to model the interference effect for smart resource provisioning of microservices in the containerized environment.},
  doi       = {10.1109/SCC.2018.00012},
  keywords  = {cloud computing;concurrency control;software performance evaluation;virtualisation;Docker containers;container technology;bare-metal performance;performance evaluation;Cloud Evaluation Experiment Methodology;inter-container;concurrent executing containers;microservices interference;container management tool;intra-container;virtualization concept;Containers;Interference;Virtualization;Benchmark testing;Cloud computing;Virtual machine monitors;Performance evaluation;Microservice;Container;Docker;Interference;Cloud Service Evaluation},
}

@InProceedings{601344,
  author    = {S. {Goddard}},
  title     = {Analyzing the real-time properties of a dataflow execution paradigm using a synthetic aperture radar application},
  booktitle = {Proceedings Third IEEE Real-Time Technology and Applications Symposium},
  year      = {1997},
  pages     = {60-71},
  month     = {June},
  abstract  = {Real-time signal processing applications are commonly designed using a data flow software architecture. The author attempts to understand fundamental real-time properties of such an architecture-the Navy's coarse-grain processing graph method (PGM). By applying recent results in real-time scheduling theory to the subset of PGM employed by the ARPA RASSP Synthetic Aperture Radar benchmark application, he identifies inherent real-time properties of nodes in a PGM data flow graph, and demonstrates how these properties can be exploited to perform useful and important system-level analyses such as schedulability analysis, end-to-end latency analysis, and memory requirements analysis. More importantly, he develops relationships between properties such as latency and buffer bounds and show how one may be traded-off for the other. The results assume only the existence of a simple EDF scheduler and thus can be easily applied in practice.},
  doi       = {10.1109/RTTAS.1997.601344},
  keywords  = {real-time systems;synthetic aperture radar;radar computing;data flow graphs;radar signal processing;scheduling;radar theory;data flow computing;data flow execution paradigm;real-time signal processing applications;data flow software architecture;Navy coarse-grain processing graph method;real-time scheduling theory;ARPA RASSP Synthetic Aperture Radar benchmark application;inherent real-time properties;data flow graph nodes;system-level analyses;schedulability analysis;end-to-end latency analysis;memory requirements analysis;buffer bounds;Synthetic aperture radar;Delay;Radar signal processing;Signal processing algorithms;Performance analysis;Application software;Real time systems;Upper bound;Costs;Computer science},
}

@Article{1644959,
  author   = {P. {Guazzoni} and F. {Riccio} and S. {Russo} and M. {Sassi} and L. {Zetta}},
  title    = {Proposed object-oriented architecture of a flexible small-scale system for digital pulse shape acquisition},
  journal  = {IEEE Transactions on Nuclear Science},
  year     = {2006},
  volume   = {53},
  number   = {3},
  pages    = {886-892},
  month    = {June},
  abstract = {In order to fully exploit all the possibilities of digital pulse shape acquisition with AZ/4/spl pi/-multidetectors for charged products, we developed a case study of data acquisition system. The proposed object-oriented scalable architecture is distributed over a network of processing and storage units and based on a set of objects, each one devoted to a specific job. It performs on-line real-time acquisition and product identification, using sampling VME-ADCs and proper algorithms for digital signal processing. A PCI-VME bridge is used to interface the ADCs to a gigabit network of personal computers. Preliminary benchmarking tests have been carried out to determine performances and actual possibilities of the system.},
  doi      = {10.1109/TNS.2006.875067},
  keywords = {high energy physics instrumentation computing;data acquisition;scintillation counters;object-oriented programming;analogue-digital conversion;system buses;signal processing;nuclear electronics;peripheral interfaces;object-oriented scalable architecture;flexible small-scale system;digital pulse shape acquisition;AZ/4pi-multidetectors;charged products;data acquisition system;storage unit;on-line real-time acquisition;product identification;sampling VME-ADCs;digital signal processing;PCI-VME bridge;gigabit network;personal computer;system performance;CsI(Tl) detector;Pulse shaping methods;Shape;Data acquisition;Signal sampling;Signal processing algorithms;Digital signal processing;Bridges;Microcomputers;Benchmark testing;System testing;Data acquisition;digital pulse shape acquisition;digital signal processing;multi-threading;object oriented programming;software architecture},
}

@InProceedings{8188557,
  author    = {G. {Creech}},
  title     = {A framework for the evaluation of the theoretical threat coverage provided by intrusion detection systems},
  booktitle = {2017 Military Communications and Information Systems Conference (MilCIS)},
  year      = {2017},
  pages     = {1-8},
  month     = {Nov},
  abstract  = {Intrusion detection systems are a central component of cyber security architecture, and their accuracy is a critical performance metric for any security deployment. Most of the current performance analysis of intrusion detection systems relies on empirical profiling of a given algorithm or implementation against a benchmark dataset. Whilst effective to a point, this traditional evaluation methodology is unable to assess the completeness of threat coverage provided by an intrusion detection system and is consequently a sub-optimal approach if conducted in isolation of other tests. This paper introduces a framework to evaluate the total potential coverage provided by an intrusion detection system as a function of its data sources, extending and complementing the traditional approach.},
  doi       = {10.1109/MilCIS.2017.8188557},
  keywords  = {security of data;software architecture;intrusion detection system;theoretical threat coverage;cyber security architecture;critical performance metric;Benchmark testing;Intrusion detection;Algorithm design and analysis;Cryptography;Computer security;Measurement},
}

@InProceedings{787287,
  author    = {K. K. {Yu} and B. S. {Lee} and M. R. {Olson}},
  title     = {The scalability of an object descriptor architecture OODBMS},
  booktitle = {Proceedings. IDEAS'99. International Database Engineering and Applications Symposium (Cat. No.PR00265)},
  year      = {1999},
  pages     = {370-377},
  month     = {Aug},
  abstract  = {An object database management system (OODBMS) has been often criticized for its alleged insufficient scalability for a large-scale production system. We investigated the scalability issue on a commercial OODBMS with a focus on the scalability with respect to the number of objects. Our approach was a benchmark experiment using the loading and indexing of SGML text documents as an application. The application was characterized by its small granularity of objects, which resulted in a huge number of objects in order to make a large database volume. The OODBMS we used was built in a so-called "object descriptor architecture (ODA)" as opposed to a "virtual memory mapping architecture (VMMA)". The results showed that the OODBMS scaled better than we had anticipated. It required however, algorithmic resolutions to overcome the shortage of object cache space. Three key resolutions were made. First, we created indexes in fragments by committing a loading transaction before the object cache space become full, and subsequently merged the fragments into one master index. Secondly, we had the application release cached object descriptors (CODs) as soon as they became unnecessary. Thirdly, we utilized a query cursor mechanism to fetch the objects returned from a query piece by piece without overflowing the object cache space. Currently we are attempting to push the scalability up to filling up the maximum available hard disk space.},
  doi       = {10.1109/IDEAS.1999.787287},
  keywords  = {object-oriented databases;database indexing;software architecture;query processing;cache storage;object descriptor architecture;object database management system;large-scale production system;scalability;SGML text document loading;SGML text document indexing;small object granularity;large database volume;algorithmic resolutions;loading transaction;object cache space;cached object descriptors;query cursor mechanism;hard disk space;Scalability;Electrical capacitance tomography;SGML;Production systems;Memory architecture;Filling;Database systems;Benchmark testing;XML},
}

@InProceedings{7349907,
  author    = {Y. {Yu} and T. {Lei} and H. {Chen} and B. {Zang}},
  title     = {Open JDK Meets Xeon Phi: A Comprehensive Study of Java HPC on Intel Many-Core Architecture},
  booktitle = {2015 44th International Conference on Parallel Processing Workshops},
  year      = {2015},
  pages     = {156-165},
  month     = {Sep.},
  abstract  = {The increasing demand for performance has stimulated the wide adoption of many-core accelerators like Intel Xeon Phi Coprocessor, which is based on Intel's Many Integrated Core architecture. While many HPC applications running in native mode have been tuned to run efficiently on Xeon Phi, it is still unclear how a managed runtime like JVM performs on such an architecture. In this paper, we present the first measurement study of a set of Java HPC applications on Xeon Phi under JVM. One key obstacle to the study is that there is currently little support of Java for Xeon Phi. This paper presents the result based on the first porting of Open JDK platform to Xeon Phi, in which Hot Spot VM acts as the kernel execution engine. The main difficulty includes the incompatibility between Xeon Phi ISA and the assembly library of Hotspot VM. By evaluating the multithreaded Java Grande benchmark suite, we quantitatively study the performance and scalability issues of JVM on Xeon Phi and draw several conclusions from the study. To fully utilize the vector computing capability, we also present a semi-automatic vectorization in Hot Spot VM. Together with this optimization and tuning, our optimized JVM achieves up to 3.4X speedup with 60 physical cores compared to that on Xeon CPU processor. Our study indicates that it is viable and potentially performance-beneficial to run applications written for managed runtime on Xeon Phi.},
  doi       = {10.1109/ICPPW.2015.25},
  keywords  = {coprocessors;Java;multiprocessing programs;parallel processing;software architecture;Open JDK;Java HPC;Intel many-core architecture;many-core accelerators;Intel Xeon Phi coprocessor;Hot Spot VM;Xeon Phi ISA;semiautomatic vectorization;Java;Coprocessors;Computer architecture;Benchmark testing;Libraries;Instruction sets;Scalability;Many-core;Java;Xeon Phi;HPC},
}

@InProceedings{8465806,
  author    = {R. {Cataldo} and R. {Fernandes} and K. J. M. {Martin} and J. {Sepulveda} and A. {Susin} and C. {Marcon} and J. {Diguet}},
  title     = {Subutai: Distributed Synchronization Primitives in NoC Interfaces for Legacy Parallel-Applications},
  booktitle = {2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)},
  year      = {2018},
  pages     = {1-6},
  month     = {June},
  abstract  = {Parallel applications are essential for efficiently using the computational power of a Multiprocessor System-on-Chip (MPSoC). Unfortunately, these applications do not scale effortlessly with the number of cores because of synchronization operations that take away valuable computational time and restrict the parallelization gains. Moreover, synchronization is also a bottleneck due to sequential access to shared memory. We address this issue and introduce ”Subutai”, a hardware/software (HW/SW) architecture designed to distribute essential synchronization mechanisms over the Network-on-Chip (NoC). It includes Network Interfaces (NIs), drivers and a custom library of a NoC-based MPSoC architecture that speeds up the essential synchronization primitives of any legacy parallel application. Besides, we provide a fast simulation tool for parallel applications and a HW architecture of the NI. Experimental results with PARSEC benchmark show an average application speedup of 2.05 compared to the same architecture running legacy SW solutions for 36% overhead of HW architecture.},
  doi       = {10.1109/DAC.2018.8465806},
  keywords  = {hardware-software codesign;multiprocessing systems;network interfaces;network-on-chip;parallel processing;synchronisation;system-on-chip;essential synchronization primitives;legacy parallel application;parallel applications;HW architecture;average application speedup;legacy SW solutions;Subutai;distributed synchronization primitives;NoC Interfaces;legacy parallel-applications;computational power;Multiprocessor System-on-Chip;synchronization operations;valuable computational time;parallelization gains;essential synchronization mechanisms;Network-on-Chip;drivers;NoC-based MPSoC architecture;hardware-software architecture;Synchronization;Computer architecture;Message systems;Libraries;Hardware;Registers;Software},
}

@InProceedings{6641409,
  author    = {A. C. {Iordan} and M. {Jahre} and L. {Natvig}},
  title     = {On the energy footprint of task based parallel applications},
  booktitle = {2013 International Conference on High Performance Computing Simulation (HPCS)},
  year      = {2013},
  pages     = {164-171},
  month     = {July},
  abstract  = {From HPC systems to embedded devices, energy consumption is becoming a dominant factor in managing costs. With Chip multiprocessors becoming the platform of choice in almost all ICT segments, software developers need to employ parallel programming to fully exploit this architecture. However, parallelization adds a management overhead to the execution of an application. In this paper, we study parallel applications implemented using two TBP libraries, Wool and Intel TBB. We explore both compute and memory bound executions. We also investigate the energy footprint of the parallelization overhead and the effect it has on the energy-efficiency of the executing system. Our study looks into the behavior of some basic parallelization operations like task spawning, task synchronization and task stealing. We encountered situations when the number of task stealing operations grows exponentially with the core count increasing execution time. This behavior drastically reduces the energy-efficiency of those executions. Avoiding such behavior is crucial if future parallel systems are to reach their performance potential. Our results also show that the energy efficiency for compute intensive applications improves with the core count while for memory intensive application that is not the case.},
  doi       = {10.1109/HPCSim.2013.6641409},
  keywords  = {multiprocessing systems;parallel programming;software architecture;task stealing operations;task synchronization;task spawning;parallel programming;chip multiprocessors;embedded devices;HPC systems;task based parallel applications;energy footprint;Yarn;Wool;Libraries;Programming;Synchronization;Benchmark testing;Computational modeling;Task based programming;chip-multiprocessor;parallel execution;energy-footprint},
}

@InProceedings{1271436,
  author    = {L. A. {Amigo} and V. {Puente} and J. A. {Gregorio}},
  title     = {Simulation methodology for decision support workloads},
  booktitle = {12th Euromicro Conference on Parallel, Distributed and Network-Based Processing, 2004. Proceedings.},
  year      = {2004},
  pages     = {120-125},
  month     = {Feb},
  abstract  = {The impact of any new architectural proposal must be evaluated under realistic working conditions. This class of analysis requires trustworthy simulation tools and representative workloads that allow us to know the real effectiveness of the improvement. We propose a methodology that enable the use of an important family of transactional workloads, such as decision support system workloads, in a full system simulator. In contrast to numerical applications, with this type of workload it is not possible to scale down the problem size in order to reduce the computational requirements of the simulation. We will show the stationary behaviour of the workload and how it can be employed to reduce computational requirements without significant loss. Taking into account this fact, we will show how simulating only 3% of the benchmark the maximum error in the main system performance metrics is approximately 5%.},
  doi       = {10.1109/EMPDP.2004.1271436},
  keywords  = {digital simulation;decision support systems;software architecture;architecture;simulation tool;transactional workload;decision support system;benchmarking;system performance;Computational modeling;Proposals;Decision support systems;Hardware;Space exploration;Computer architecture;Employee welfare;Analytical models;System performance;Supercomputers},
}

@InProceedings{5161100,
  author    = {K. {Madduri} and D. {Ediger} and K. {Jiang} and D. A. {Bader} and D. {Chavarria-Miranda}},
  title     = {A faster parallel algorithm and efficient multithreaded implementations for evaluating betweenness centrality on massive datasets},
  booktitle = {2009 IEEE International Symposium on Parallel Distributed Processing},
  year      = {2009},
  pages     = {1-8},
  month     = {May},
  abstract  = {We present a new lock-free parallel algorithm for computing betweenness centrality of massive complex networks that achieves better spatial locality compared with previous approaches. Betweenness centrality is a key kernel in analyzing the importance of vertices (or edges) in applications ranging from social networks, to power grids, to the influence of jazz musicians, and is also incorporated into the DARPA HPCS SSCA#2, a benchmark extensively used to evaluate the performance of emerging high-performance computing architectures for graph analytics. We design an optimized implementation of betweenness centrality for the massively multithreaded Cray XMT system with the Thread-storm processor. For a small-world network of 268 million vertices and 2.147 billion edges, the 16-processor XMT system achieves a TEPS rate (an algorithmic performance count for the number of edges traversed per second) of 160 million per second, which corresponds to more than a 2times performance improvement over the previous parallel implementation. We demonstrate the applicability of our implementation to analyze massive real-world datasets by computing approximate betweenness centrality for the large IMDb movie-actor network.},
  doi       = {10.1109/IPDPS.2009.5161100},
  keywords  = {graph theory;multi-threading;parallel algorithms;software architecture;software performance evaluation;parallel algorithm;multithreaded implementations;massive datasets;betweenness centrality;vertices;performance evaluation;high-performance computing architectures;graph analytics;XMT system;Parallel algorithms;Computer networks;Performance analysis;Concurrent computing;Complex networks;Kernel;Social network services;Power grids;Grid computing;High performance computing},
}

@InProceedings{4625430,
  author    = {{Bo Mao} and {Dan Feng} and {Suzhen Wu} and {Jianxi Chen} and {Lingfang Zeng} and {Lei Tian}},
  title     = {RAID10L: A high performance RAID10 storage architecture based on logging technique},
  booktitle = {2008 13th Asia-Pacific Computer Systems Architecture Conference},
  year      = {2008},
  pages     = {1-8},
  month     = {Aug},
  abstract  = {RAID10 storage system suffers the relatively poor write performance due to the write request must be served by both disks in a mirror set. To address this problem, in this paper we propose a novel RAID10 storage architecture, called RAID10L, which extends the data mirroring redundancy of RAID10 by incorporating a dedicated log disk. The goal of RAID10L is to significantly improve the write performance of RAID10 at some little expense of reliability. In RAID10L both read and write requests are processed in a balance scheme. For every write request, RAID10L keeps two copies of the write data: one in its normal place of data disk chosen by a write balance scheme and the other in the log disk by writing sequentially. The update to another data disk in a mirror set is delayed to the next quiet period between bursts of client activity. Reliability analysis shows that the reliability of RAID10L, in terms of MTTDL (mean time to data loss), is somewhat worse than RAID10 but much better than RAID5. On the other hand, our prototype implementation of RAID10L driven by Iometer benchmark shows that RAID10L outperforms RAID10 by up to 47.1% and RAID0 by 27.3% in terms of average response time. Driven by some real-life traces, RAID10L gains improvement up to 30.7% with an average of 27.7% than RAID10 in terms of average response time.},
  doi       = {10.1109/APCSAC.2008.4625430},
  keywords  = {software architecture;software reliability;system monitoring;RAID10 storage architecture;logging technique;data mirroring redundancy;balance scheme;write balance scheme;mean time to data loss;Arrays;Reliability;Mirrors;Data structures;Time factors;Nonvolatile memory;Redundancy},
}

@Article{7031930,
  author   = {J. {Zhao} and C. {Xue} and J. {Tao} and R. {Ranjan} and J. {Kołodziej} and L. {Wang} and D. {Chen}},
  title    = {Trusted Performance Analysis on Systems With a Shared Memory},
  journal  = {IEEE Systems Journal},
  year     = {2017},
  volume   = {11},
  number   = {1},
  pages    = {272-282},
  month    = {March},
  abstract = {With the increasing complexity of both data structures and computer architectures, the performance of applications needs fine tuning in order to achieve the expected runtime execution time. Performance tuning is traditionally based on the analysis of performance data. The analysis results may not be accurate, depending on the quality of the data and the applied analysis approaches. Therefore, application developers may ask: Can we trust the analysis results? This paper introduces our research work in performance optimization of the memory system, with a focus on the cache locality of a shared memory and the memory locality of a distributed shared memory. The quality of the data analysis is guaranteed by using both real performance data acquired at the runtime while the application is running and well-established data analysis algorithms in the field of bioinformatics and data mining. We verified the quality of the proposed approaches by optimizing a set of benchmark applications. The experimental results show a significant performance gain.},
  doi      = {10.1109/JSYST.2014.2365234},
  keywords = {cache storage;data analysis;data integrity;data structures;distributed shared memory systems;software architecture;software performance evaluation;trusted performance analysis;distributed shared memory system;data structure;computer architecture;performance tuning;data quality;data analysis;performance optimization;cache locality;memory locality;Runtime;Optimization;Monitoring;Radiation detectors;Data mining;Hardware;Decision trees;Code optimization;data analysis;data locality;distributed shared memory;performance tuning},
}

@InProceedings{5529776,
  author    = {{Cik Ku Haroswati Cik Ku Yahaya} and {Mohd Nazri Ismail} and M. {Kassim}},
  title     = {A study on automated, speech and remote temperature monitoring for modeling Web based temperature monitoring system},
  booktitle = {2010 2nd International Conference on Education Technology and Computer},
  year      = {2010},
  volume    = {5},
  pages     = {V5-229-V5-233},
  month     = {June},
  abstract  = {On analyzing and designing a temperature monitoring system, understanding the situations and benchmark of the previous system that similar to the model is very important. The need to evaluate the current model, hardware and software development architecture for the temperature monitoring is important to build new architecture and designing model of a new system. In addition, most of the previous implementations include very complex architecture, broad areas of interest and various programming computing processes. This research was to study the comparison of speech temperature; automated and remote temperature monitoring that is similar to WEB monitoring system. It is to produce the model that allows the user to design continuously for monitoring temperature condition of a room while the data can be monitored anytime and anywhere from the Internet. The Model of designing the Web Based system is presented in this research. As a conclusion, Web-Based Temperature monitoring model then will be used and real system will be design in future project. This study is considered successful and preparation for the hardware and software for the development process of the real web based monitoring temperature is on the run.},
  doi       = {10.1109/ICETC.2010.5529776},
  keywords  = {computerised monitoring;Internet;sensor fusion;software architecture;temperature measurement;temperature sensors;temperature monitoring system;software development architecture;programming computing process;speech temperature;Web monitoring system;Internet;automated system;remote monitoring;Temperature measurement;Temperature sensors;Computerized monitoring;Remote monitoring;Computer architecture;Speech analysis;Hardware;Condition monitoring;Programming;Internet;Temperature Sensor;Web Based;Automated System;Monitoring Application;Modeling;Internet},
}

@Article{7517402,
  author   = {B. {Chen} and X. {Peng} and Y. {Liu} and S. {Song} and J. {Zheng} and W. {Zhao}},
  title    = {Architecture-Based Behavioral Adaptation with Generated Alternatives and Relaxed Constraints},
  journal  = {IEEE Transactions on Services Computing},
  year     = {2019},
  volume   = {12},
  number   = {1},
  pages    = {73-87},
  month    = {Jan},
  abstract = {Software systems are increasingly required to autonomously adapt their architectural structures and/or behaviors to runtime environmental changes. However, existing architecture-based self-adaptation approaches mostly focus on structural adaptations within a predefined space of architectural alternatives (e.g., switching between two alternative services) while merely considering quality constraints (e.g., reliability and performance). In this paper, we propose a new architecture-based self-adaptation approach, which performs behavioral adaptations with automatically generated alternatives and supports relaxed functional constraints from the perspective of business value. Specifically, we propose a technique to automatically generate behavioral alternatives of a software system from the currently-employed architectural behavioral specification. We employ business value to comprehensively evaluate the behavioral alternatives while capturing the trade-offs among relaxed functional and quality constraints. We also introduce a genetic algorithm-based planning technique to efficiently search for the optimal (sometimes a near-optimal) behavioral alternative that can provide the best business value. The experimental study on an online order processing benchmark has shown promising results that the proposed approach can improve adaptation flexibility and business value with acceptable performance overhead.},
  doi      = {10.1109/TSC.2016.2593459},
  keywords = {formal specification;genetic algorithms;planning (artificial intelligence);software architecture;behavioral alternatives;software system;architectural behavioral specification;business value;quality constraints;architecture-based behavioral adaptation;relaxed constraints;architectural structures;architecture-based self-adaptation approach;functional constraints;runtime environmental changes;genetic algorithm-based planning technique;Business;Runtime;Probabilistic logic;Genetic algorithms;Planning;Computer architecture;Connectors;Behavioral adaptation;relaxed constraints;business value;quantitative verification;genetic algorithm},
}

@InProceedings{5724827,
  author    = {K. {Ye} and D. {Huang} and X. {Jiang} and H. {Chen} and S. {Wu}},
  title     = {Virtual Machine Based Energy-Efficient Data Center Architecture for Cloud Computing: A Performance Perspective},
  booktitle = {2010 IEEE/ACM Int'l Conference on Green Computing and Communications Int'l Conference on Cyber, Physical and Social Computing},
  year      = {2010},
  pages     = {171-178},
  month     = {Dec},
  abstract  = {Virtual machine technology is widely applied to modern data center for cloud computing as a key technology to realize energy-efficient operation of servers. Server consolidation achieves energy efficiency by enabling multiple instantiations of operating systems (OSes) to run simultaneously on a single physical machine. While, live migration of virtual machine can transfer the virtual machine workload from one physical machine to another without interrupting service. However, both the two technologies have their own performance overheads. There is a tradeoff between the performance and energy efficiency. In this paper, we study the energy efficiency from the performance perspective. Firstly, we present a virtual machine based energy-efficient data center architecture for cloud computing. Then we investigate the potential performance overheads caused by server consolidation and live migration of virtual machine technology. Experimental results show that both the two technologies can effectively implement energy-saving goals with little performance overheads. Efficient consolidation and migration strategies can improve the energy efficiency.},
  doi       = {10.1109/GreenCom-CPSCom.2010.108},
  keywords  = {cloud computing;computer centres;energy conservation;operating systems (computers);software architecture;virtual machines;virtual machine;energy efficient data center architecture;cloud computing;server consolidation;operating system;physical machine;live migration;Servers;Virtual machining;Cloud computing;Computer architecture;Degradation;Monitoring;Benchmark testing;Virtual Machine;Server Consolidation;Live Migration;Energy Efficiency},
}

@InProceedings{6879994,
  author    = {J. {Jung} and S. {Kiertscher} and S. {Menski} and B. {Schnor}},
  title     = {Self-adapting load balancing for DNS},
  booktitle = {International Symposium on Performance Evaluation of Computer and Telecommunication Systems (SPECTS 2014)},
  year      = {2014},
  pages     = {564-571},
  month     = {July},
  abstract  = {The Domain Name System belongs to the core services of the Internet infrastructure. Hence, DNS availability and performance is essential for the operation of the Internet and replication and load balancing is used for the root and top level name servers. This paper proposes an architecture for credit based server load balancing (SLB) for DNS. Compared to traditional load balancing algorithms like round robin or least connection, the benefit of credit based SLB is that the load balancer can adapt more easily to heterogeneous load requests and heterogeneous back end server capacities. The challenge of this approach is the definition of a suited credit metric. While this was done before for TCP based services like HTTP, the problem was not solved for UDP based services like DNS. This paper presents an approach to define credits also for UDP based services. This UDP/DNS approach is implemented within the credit based SLB implementation salbnet. The presented measurements confirm the benefit of the self-adapting credit based SLB approach.},
  doi       = {10.1109/SPECTS.2014.6879994},
  keywords  = {hypermedia;Internet;resource allocation;software architecture;self-adapting load balancing;domain name system;core services;Internet infrastructure;DNS availability;architecture;credit based server load balancing;HTTP;salbnet;Servers;Measurement;Kernel;Benchmark testing;Load management;Linux;Protocols;Load Balancing;Cluster Computing;Performance Evaluation},
}

@InProceedings{4625432,
  author    = {{Chen Liu} and J. {Gaudiot}},
  title     = {Resource sharing control in Simultaneous MultiThreading microarchitectures},
  booktitle = {2008 13th Asia-Pacific Computer Systems Architecture Conference},
  year      = {2008},
  pages     = {1-8},
  month     = {Aug},
  abstract  = {Simultaneous multithreading (SMT) achieves improved system resource utilization and accordingly higher instruction throughput because it exploits thread-level parallelism (TLP) in addition to conventional instruction-level parallelism (ILP). The key to high-performance SMT is to optimize the distribution of shared system resources among the threads. However, existing dynamic sharing mechanism has no control over the resource distribution, which could cause one thread to grab too many resources and clog the pipeline. Existing fetch policies address the resource distribution problem only indirectly. In this work, we strive to quantitatively determine the balance between controlling resource allocation and dynamic sharing of different system resources with their impact on the performance of SMT processors. We find that controlling the resource sharing of either the instruction fetch queue (IFQ) or the reorder buffer (ROB) is not sufficient if implemented alone. However, controlling the resource sharing of both IFQ and ROB can yield an average performance gain of 38% when compared with dynamic sharing case. The average L1 D-cache miss rate has been reduced by 33%. The average time that the instruction resides in the pipeline has been reduced by 34%. This demonstrates the power of the resource sharing control mechanism we propose.},
  doi       = {10.1109/APCSAC.2008.4625432},
  keywords  = {parallel architectures;resource allocation;software architecture;resource sharing control;simultaneous multithreading microarchitectures;resource utilization;instruction-level parallelism;shared system resources;resource distribution;instruction fetch queue;reorder buffer;Resource management;Pipelines;Benchmark testing;Program processors;Microarchitecture;Radiation detectors;Dynamic scheduling},
}

@InProceedings{6969383,
  author    = {A. {Sistla} and X. {Luo} and M. {Malladi} and M. {Reisner} and R. {Ganduri} and G. {Mehta}},
  title     = {SmartBricks: A Visual Environment to Design and Explore Novel Custom Domain-Specific Architectures},
  booktitle = {2014 IEEE International Parallel Distributed Processing Symposium Workshops},
  year      = {2014},
  pages     = {161-169},
  month     = {May},
  abstract  = {Custom domain-specific architectures are very promising for creating designs that are highly optimized to the needs of a particular application domain. However, it is extremely difficult to find optimal tradeoffs in designing a new architecture, or even to fully understand the design space. Therefore, there is a great need to develop an optimum design framework that allows designers to explore the design space efficiently and identify efficient architectures quickly for an application domain. In this paper, we describe SmartBricks, a highly visual design environment that we have developed for designing and exploring custom domain-specific architectures quickly and efficiently. This game-like design environment will be accessible to a broad community so that even non-engineers and non-scientists can contribute to building and exploring out-of-the-box architectural designs.},
  doi       = {10.1109/IPDPSW.2014.22},
  keywords  = {software architecture;SmartBricks environment;visual environment;custom domain-specific architecture;application domain;architecture design;design space;game-like design environment;Games;Benchmark testing;Visualization;Architecture;Space exploration;Libraries;Routing},
}

@InProceedings{7030769,
  author    = {M. {Khan} and N. {Priyanka} and W. {Ahmed} and N. {Radhika} and M. {Pavithra} and K. {Parimala}},
  title     = {Understanding source-to-source transformations for frequent porting of applications on changing cloud architectures},
  booktitle = {2014 International Conference on Parallel, Distributed and Grid Computing},
  year      = {2014},
  pages     = {350-354},
  month     = {Dec},
  abstract  = {Writing code for heterogeneous architectures with processors and accelerators from multiple vendors from scratch or translating existing serial code, a lot of effort and investment will be required from the application developer. This problem will become more prominent when HPC applications are moved into the Cloud as Cloud providers frequently update their architectures to keep with market trends. In these scenarios, automatic parallelization tools will definitely have an important role to play. An important constituent of these tools would be the ability to perform pertinent domain decomposition of the serial code to maximize utilization of the available computational elements. One of the first steps in this direction is to understand the role of the number and type of computational element in a heterogeneous architecture to the overall performance of an application. This paper presents observations made on architectures with different types and number of computational elements using two case studies on five different architectures with different types and number of computational elements. Results show that the perceived speedup and actual speedup are not related.},
  doi       = {10.1109/PDGC.2014.7030769},
  keywords  = {cloud computing;parallel processing;software architecture;computational elements;serial code;pertinent domain decomposition;automatic parallelization tools;cloud as cloud providers;HPC applications;heterogeneous architectures;changing cloud architectures;application frequent porting;source-to-source transformations;Computer architecture;Graphics processing units;Benchmark testing;Manuals;Equations;Jacobian matrices;Heterogeneous Architectures;Source-to-Source Transformation;Parallel Computing;GPGPU;CUDA},
}

@InProceedings{5644948,
  author    = {R. V. {Kassick} and F. Z. {Boito} and P. O. A. {Navaux}},
  title     = {Impact of I/O Coordination on a NFS-Based Parallel File System with Dynamic Reconfiguration},
  booktitle = {2010 22nd International Symposium on Computer Architecture and High Performance Computing},
  year      = {2010},
  pages     = {199-206},
  month     = {Oct},
  abstract  = {The large gap between processing and I/O speed makes the storage infrastructure of a cluster a great bottleneck for UPC applications. Parallel File Systems propose a solution to this issue by distributing data onto several servers, dividing the load of I/O operations and increasing the available bandwidth. However, most parallel file systems use a fixed number of I/O servers defined during initialization and do not support addition of new resources as applications' demands grow. With the execution of different applications at the same time, the concurrent access to these resources can impact the performance and aggravate the existing bottleneck. The dNFSp File System proposes a reconfiguration mechanism that aims to include new I/O resources as application's demands grow. These resources are standard cluster nodes and are dedicated to a single application. This paper presents a study of the I/O performance of this reconfiguration mechanism under two circumstances: the use of several independent processes on a multi-core system or of a single centralized I/O process that coordinates the requests from all instances on a node. We show that the use of coordination can improve performance of applications with regular intervals between I/O phases. For applications with no such intervals, on the other hand, uncoordinated I/O presents better performance.},
  doi       = {10.1109/SBAC-PAD.2010.32},
  keywords  = {file servers;input-output programs;network operating systems;reconfigurable architectures;software architecture;NFS based parallel file system;UPC application;I/O server;reconfiguration mechanism;dNFSp file system;cluster architecture;parallel architecture;multicore system;Servers;Bandwidth;Benchmark testing;File systems;Computer architecture;Delay;Data models},
}

@InProceedings{7816910,
  author    = {I. {Verbitskiy} and L. {Thamsen} and O. {Kao}},
  title     = {When to Use a Distributed Dataflow Engine: Evaluating the Performance of Apache Flink},
  booktitle = {2016 Intl IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)},
  year      = {2016},
  pages     = {698-705},
  month     = {July},
  abstract  = {With the increasing amount of available data, distributed data processing systems like Apache Flink, Apache Spark have emerged that allow to analyze large-scale datasets. However, such engines introduce significant computational overhead compared to non-distributed implementations. Therefore, the question arises when using a distributed processing approach is actually beneficial. This paper helps to answer this question with an evaluation of the performance of the distributed data processing framework Apache Flink. In particular, we compare Apache Flink executed on up to 50 cluster nodes to single-threaded implementations executed on a typical laptop for three different benchmarks: TPC-H Query 10, Connected Components,, Gradient Descent. The evaluation shows that the performance of Apache Flink is highly problem dependent, varies from early outperformance in case of TPC-H Query 10 to slower runtimes in case of Connected Components. The reported results give hints for which problems, input sizes,, cluster resources using a distributed data processing system like Apache Flink or Apache Spark is sensible.},
  doi       = {10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0114},
  keywords  = {computer architecture;data flow analysis;data flow computing;data handling;pattern clustering;performance evaluation;software architecture;distributed dataflow engine;Apache Flink performance evaluation;distributed data processing systems;Apache Spark;large-scale dataset analysis;distributed data processing framework;single-threaded implementations;TPC-H Query 10;connected components;gradient descent;cluster resources;Benchmark testing;Data processing;Engines;Measurement;Distributed databases;Portable computers;distributed data processing;parallel dataflows;Apache Flink;performance evaluation;COST},
}

@InProceedings{6417279,
  author    = {S. {Khouri} and L. {Bellatreche} and I. {Boukhari} and S. {Bouarar}},
  title     = {More Investment in Conceptual Designers: Think about it!},
  booktitle = {2012 IEEE 15th International Conference on Computational Science and Engineering},
  year      = {2012},
  pages     = {88-93},
  month     = {Dec},
  abstract  = {Developing database (DB) and data warehouse (DW) applications passes through three main modeling phases imposed by the ANSI/SPARC architecture: conceptual, logical and physical. This architecture creates two different actors: (i) conceptual designers and Database Administrators (DBA). The first actor collects user requirements, chooses the relevant diagrams for designing the conceptual model (or the logical model). The DBA ensures the performance, the maintenance and the tuning of the final application. Most of the tasks performed by these two actors are complex and time consuming for a majority of companies. Recently, some academic and industry research efforts are moving towards truly zero-administration of DW by proposing tools (advisors) substituting some tasks of DBA. One of the functionalities of these tools is to propose recommendations in choosing optimization structures such as indexing, materialized views, etc. They do not guarantee robust solutions. In this paper, we propose a revolutionary economical model for DB /DW application. Instead of substituting the DBA by advisors, we propose to delegate some DBA tasks to conceptual designers like selecting optimization structures. First, we propose to connect user requirements to the conceptual model of the target application. Secondly, based on the analysis of requirements, SQL queries are identified and then used to select optimization structures. Finally, a feasibility of our approach is tested through the selection of bitmap join indexes based on user requirements.},
  doi       = {10.1109/ICCSE.2012.22},
  keywords  = {data warehouses;formal specification;query processing;software architecture;SQL;systems analysis;conceptual designers;developing database applications;DB applications;data warehouse applications;DW applications;ANSI/SPARC architecture;conceptual architecture;logical architecture;physical architecture;database administrators;DBA;user requirements;conceptual model design;optimization structures;economical model;requirements analysis;SQL query identification;investment;Optimization;Indexes;Unified modeling language;Data models;Benchmark testing;Amplitude modulation;Requirements Engineering;ANSI/SPARC Ar-chitecture;Designer;Administrator;Data Warehouse;Optimization;Experiments},
}

@InProceedings{7980076,
  author    = {T. {Zhu} and J. {Li} and J. {Kimball} and J. {Park} and C. {Lai} and C. {Pu} and Q. {Wang}},
  title     = {Limitations of Load Balancing Mechanisms for N-Tier Systems in the Presence of Millibottlenecks},
  booktitle = {2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)},
  year      = {2017},
  pages     = {1367-1377},
  month     = {June},
  abstract  = {The scalability of n-tier systems relies on effective load balancing to distribute load among the servers of the same tier. We found that load balancing mechanisms (and some policies) in servers used in typical n-tier systems (e.g., Apache and Tomcat) have issues of instability when very long response time (VLRT) requests appear due to millibottlenecks, very short bottlenecks that last only tens to hundreds of milliseconds. Experiments with standard n-tier benchmarks show that during millibottlenecks, some load balancing policy/mechanism combinations make the mistake of sending new requests to the node(s) suffering from millibottlenecks, instead of the idle nodes as load balancers are supposed to do. Several of these mistakes are due to the implicit assumptions made by load balancing policies and mechanisms on the stability of system state. Our study shows that appropriate remedies at policy and mechanism levels can avoid these mistakes during millibottlenecks and remove the VLRT requests, thus improving the average response time by a factor of 12.},
  doi       = {10.1109/ICDCS.2017.188},
  keywords  = {client-server systems;resource allocation;software architecture;load balancing mechanisms;n-tier systems scalability;millibottlenecks;load distribution;very long response time;VLRT requests;system state stability;Servers;Time factors;Load management;Queueing analysis;Benchmark testing;Transient analysis;Standards;Performance analysis;distributed systems},
}

@InProceedings{6618831,
  author    = {M. H. {Sujon} and R. C. {Whaley} and Q. {Yi}},
  title     = {Vectorization past dependent branches through speculation},
  booktitle = {Proceedings of the 22nd International Conference on Parallel Architectures and Compilation Techniques},
  year      = {2013},
  pages     = {353-362},
  month     = {Sep.},
  abstract  = {Modern architectures increasingly rely on SIMD vectorization to improve performance for floating point intensive scientific applications. However, existing compiler optimization techniques for automatic vectorization are inhibited by the presence of unknown control flow surrounding partially vectorizable computations. In this paper, we present a new approach, speculative vectorization, which speculates past dependent branches to aggressively vectorize computational paths that are expected to be taken frequently at runtime, while simply restarting the calculation using scalar instructions when the speculation fails. We have integrated our technique in an iterative optimizing compiler and have employed empirical tuning to select the profitable paths for speculation. When applied to optimize 9 floating-point benchmarks, our optimizing compiler has achieved up to 6.8X speedup for single precision and 3.4X for double precision kernels using AVX, while vectorizing some operations considered not vectorizable by prior techniques.},
  doi       = {10.1109/PACT.2013.6618831},
  keywords  = {optimisation;parallel processing;program compilers;software architecture;vectors;modern architectures;SIMD vectorization;floating point intensive scientific applications;compiler optimization techniques;floating-point benchmarks;Vectors;Kernel;Optimization;Algorithm design and analysis;Optimizing compilers;Benchmark testing;Safety;SIMD Vectorization;speculation;compiler optimization;iterative compilation;ATLAS;iFKO},
}

@InProceedings{7723674,
  author    = {A. J. {Awan} and M. {Brorsson} and V. {Vlassov} and E. {Ayguade}},
  title     = {Micro-Architectural Characterization of Apache Spark on Batch and Stream Processing Workloads},
  booktitle = {2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom)},
  year      = {2016},
  pages     = {59-66},
  month     = {Oct},
  abstract  = {While cluster computing frameworks are continuously evolving to provide real-time data analysis capabilities, Apache Spark has managed to be at the forefront of big data analytics for being a unified framework for both, batch and stream data processing. However, recent studies on micro-architectural characterization of in-memory data analytics are limited to only batch processing workloads. We compare the micro-architectural performance of batch processing and stream processing workloads in Apache Spark using hardware performance counters on a dual socket server. In our evaluation experiments, we have found that batch processing and stream processing has same micro-architectural behavior in Spark if the difference between two implementations is of micro-batching only. If the input data rates are small, stream processing workloads are front-end bound. However, the front end bound stalls are reduced at larger input data rates and instruction retirement is improved. Moreover, Spark workloads using DataFrames have improved instruction retirement over workloads using RDDs.},
  doi       = {10.1109/BDCloud-SocialCom-SustainCom.2016.20},
  keywords  = {Big Data;data analysis;parallel processing;software architecture;microarchitectural characterization;Apache Spark;batch processing workload;stream processing workload;Big Data analytics;Sparks;Servers;Big data;Data analysis;Clustering algorithms;Batch production systems;Benchmark testing;Spark Streaming;Workload Characterization;Microarchitectural Performance},
}

@InProceedings{5703498,
  author    = {H. {Cheng} and Y. {Hwang} and R. {Chang} and C. {Chen}},
  title     = {Trading Conditional Execution for More Registers on ARM Processors},
  booktitle = {2010 IEEE/IFIP International Conference on Embedded and Ubiquitous Computing},
  year      = {2010},
  pages     = {53-59},
  month     = {Dec},
  abstract  = {Conditional execution is an important ISA feature of the ARM series of processors. Every instruction can be made to execute conditionally, that is, it is treated as a NOP if the condition is not met. The advantage of conditional execution is that it can maintain high performance while reducing hardware complexity since it can avoid introducing pipeline bubbles even when no branch prediction units are needed. However, conditional execution takes up precious instruction space as conditions are encoded into a 4-bit condition code selector on every 32-bit ARM instruction. Besides, only small percentages of instructions are actually conditionalized in modern embedded applications, and conditional execution might not even lead to performance improvement on modern embedded processors. This paper proposes to trade conditional execution for more ISA registers on ARM processors, and the 4-bit condition field will be used to encode the extra registers. GCC has been ported to generate ARM code with the new instruction format and experimental results have shown that performance can be improved by 6% on average for Media Bench II benchmarks when the number of ISA registers is extended from 16 to 32.},
  doi       = {10.1109/EUC.2010.18},
  keywords  = {instruction sets;pipeline processing;software architecture;conditional execution;ARM processors;hardware complexity;pipeline bubbles;ISA registers;instruction set architecture;Registers;Program processors;Benchmark testing;Encoding;Transform coding;Decoding;Pipelines;Conditional Execution;ISA;Instruction Encoding;Registers},
}

@InProceedings{5990035,
  author    = {M. {Rak} and A. {Cuomo} and U. {Villano}},
  title     = {CHASE: An Autonomic Service Engine for Cloud Environments},
  booktitle = {2011 IEEE 20th International Workshops on Enabling Technologies: Infrastructure for Collaborative Enterprises},
  year      = {2011},
  pages     = {116-121},
  month     = {June},
  abstract  = {The rapidly spreading cloud computing paradigm delegates to the network the provision of most resources, even those strictly linked to hardware as storage and CPU time. This approach enables the development of applications which may exploit a variable amount of resources in a flexible way, so as to satisfy the actual load of requests coming from the users. A side effect of such flexibility is that optimization has to be more focused on user-perceived performance indexes than on resource usage. This paper takes a step in this direction, presenting the design and development of CHASE, an autonomic engine designed to optimize the scheduling of virtual machines in a cloud environment. The paper illustrates the CHASE architecture and its application in two different contexts: in PerfCloud, an environment for IaaS provision based on cloud and grid integration, and inside Cloud@Home, a project whose objective is to build a cloud using volunteer-based resources. Some preliminary experimental results based on HPC applications are presented.},
  doi       = {10.1109/WETICE.2011.21},
  keywords  = {cloud computing;grid computing;scheduling;software architecture;virtual machines;CHASE architecture;autonomic service engine;cloud computing paradigm;user-perceived performance indexes;resource usage;virtual machine scheduling;PerfCloud;IaaS provision;grid integration;cloud integration;Cloud@Home;volunteer-based resources;Computer architecture;Unified modeling language;Benchmark testing;Virtual machining;Optimization;Engines;Computational modeling;Scheduling;Cloud Computing;Autonomic Computing;Grid Computing;Performance Prediction},
}

@InProceedings{8342183,
  author    = {D. {Gadioli} and R. {Nobre} and P. {Pinto} and E. {Vitali} and A. H. {Ashouri} and G. {Palermo} and J. {Cardoso} and C. {Silvano}},
  title     = {SOCRATES — A seamless online compiler and system runtime autotuning framework for energy-aware applications},
  booktitle = {2018 Design, Automation Test in Europe Conference Exhibition (DATE)},
  year      = {2018},
  pages     = {1143-1146},
  month     = {March},
  abstract  = {Configuring program parallelism and selecting optimal compiler options according to the underlying platform architecture is a difficult task. Tipically, this task is either assigned to the programmer or done by a standard one-fits-all policy generated by the compiler or runtime system. A runtime selection of the best configuration requires the insertion of a lot of glue code for profiling and runtime selection. This represents a programming wall for application developers. This paper presents a structured approach, called SOCRATES, based on an aspect-oriented language (LARA) and a runtime autotuner (mARGOt) to mitigate this problem. LARA has been used to hide the glue code insertion, thus separating the pure functional application description from extra-functional requirements. mARGOT has been used for the automatic selection of the best configuration according to the runtime evolution of the application.1},
  doi       = {10.23919/DATE.2018.8342183},
  keywords  = {aspect-oriented programming;parallel processing;program compilers;software architecture;runtime selection;LARA;glue code insertion;extra-functional requirements;automatic selection;runtime evolution;seamless online compiler;system runtime autotuning framework;energy-aware applications;program parallelism;SOCRATES;platform architecture;aspect-oriented language;mARGOt;Runtime;Kernel;Optimization;Benchmark testing;Task analysis;Monitoring;Feature extraction},
}

@InProceedings{7014153,
  author    = {A. {Baruchi} and E. T. {Midorikawa} and M. A. S. {Netto}},
  title     = {Improving Virtual Machine live migration via application-level workload analysis},
  booktitle = {10th International Conference on Network and Service Management (CNSM) and Workshop},
  year      = {2014},
  pages     = {163-168},
  month     = {Nov},
  abstract  = {Virtual Machine (VM) live migration is key for implementing resource management policies to optimize metrics such as server utilization, energy consumption, and quality-of-service. A fundamental challenge for VM live migration is its impact on both user and resource provider sides, including service downtime and high network utilization. Several VM live migration studies have been published in the literature. However, they mostly consider only system level metrics such as CPU, memory, and network usage to trigger VM migrations. This paper introduces ALMA, an Application-aware Live Migration Architecture that explores application level information, in addition to the traditional system level metrics, to determine the best time to perform a migration. Based on experiments with three real applications, by considering application characteristics to trigger the VM live migration, we observed a substantial reduction in data transferred over the network of up to 42% and the total live migration time decrease of up to 63%.},
  doi       = {10.1109/CNSM.2014.7014153},
  keywords  = {cloud computing;energy consumption;quality of service;software architecture;virtual machines;virtual machine live migration;application-level workload analysis;resource management policies;server utilization;energy consumption;quality-of-service;application-aware live migration architecture;cloud computing;Measurement;Benchmark testing;Accuracy;Virtual machining;Memory management;Servers;Live Migration;Cloud Computing;Performance Prediction;Virtualization},
}

@InProceedings{5561585,
  author    = {A. R. {bin Ahlan} and M. {bt Mahmud} and Y. {bin Arshad}},
  title     = {Conceptual architecture design and configuration of thin client system for schools in Malaysia: A pilot project},
  booktitle = {2010 International Symposium on Information Technology},
  year      = {2010},
  volume    = {2},
  pages     = {952-955},
  month     = {June},
  abstract  = {Thin client systems have advanced recently with new innovative design extensions such as virtualisation and cloud computing. Client-server architecture is the basis for thin client system to connect remote multi-workstations to administrative servers located in headquarters or data center. This paper first presents a literature review on thin client concept and virtualization. In the next section, we describe the Wyse Technology and Linux thin client for windows-based and open source solution respectively. We then proceed with description of proposed conceptual architecture design and implementation for primary and secondary schools in Malaysia taking into account International Telecommunication Union and other International standards or benchmarks for high quality of service (QoS) delivery. Finally, our recommendation is that each system design and implementation is unique and thus requires different sets of decision-making criteria before designing an architecture. Networked-schools across a country like Malaysia with heavy users requirements pose high specification in architecture and system design planning to IT divisions.},
  doi       = {10.1109/ITSIM.2010.5561585},
  keywords  = {client-server systems;computer centres;educational institutions;Linux;network computers;quality of service;software architecture;virtual reality;Schools;Malaysia;virtualisation;cloud computing;client-server architecture design;remote multiworkstation connection;administrative servers;data center;Wyse technology;Linux thin client systems;open source solution;International Telecommunication Union;international standards;quality of service;QoS;decision making;IT divisions;Linux;Browsers;Peer to peer computing;Benchmark testing;Educational institutions;Google;Thin clients;virtualization;schools;Malaysia;quality of service},
}

@InProceedings{6193474,
  author    = {{Yichao Jin} and {Yonggang Wen} and {Qinghua Chen}},
  title     = {Energy efficiency and server virtualization in data centers: An empirical investigation},
  booktitle = {2012 Proceedings IEEE INFOCOM Workshops},
  year      = {2012},
  pages     = {133-138},
  month     = {March},
  abstract  = {With a growing concern on the considerable energy consumed by data centers, research efforts are targeting toward green data centers with higher energy efficiency. In particular, server virtualization is emerging as the prominent approach to consolidate applications from multiple applications to one server, with an objective to save energy usage. However, little understanding has been obtained about the potential overhead in energy consumption and the throughput reduction for virtualized servers in data centers. In this research, we take the initiative to characterize the energy usage on virtualized servers. An empirical approach is adopted to investigate how server virtualization affects the energy usage in physical servers. Through intensive data collection and analysis, we identify a fundamental trade-off between the energy saving from server consolidation and the detrimental effects (e.g., energy overhead and throughput reduction) from server virtualization. This characterization lays a mathematical foundation for server consolidation in green data center architecture.},
  doi       = {10.1109/INFCOMW.2012.6193474},
  keywords  = {computer centres;data analysis;energy conservation;power aware computing;software architecture;virtual machines;virtualisation;energy efficiency;server virtualization;potential overhead;energy consumption;throughput reduction;data collection;data analysis;energy saving;server consolidation;mathematical foundation;green data center architecture;Servers;Benchmark testing;Power demand;Energy consumption;Virtual machine monitors;Throughput;Resource management},
}

@InProceedings{7167189,
  author    = {K. {Duraisamy} and R. G. {Kim} and W. {Choi} and G. {Liu} and P. P. {Pande} and R. {Marculescu} and D. {Marculescu}},
  title     = {Energy efficient MapReduce with VFI-enabled multicore platforms},
  booktitle = {2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC)},
  year      = {2015},
  pages     = {1-6},
  month     = {June},
  abstract  = {In an era when power constraints and data movement are proving to be significant barriers for high-end computing, multicore architectures offer a low-power and highly scalable platform suitable for both data- and compute-intensive applications. MapReduce is a popular framework to facilitate the management and development of big-data workloads. In this work, we demonstrate that by using a wireless NoC-enabled Voltage Frequency Island (VFI)-based multicore platform it is possible to enhance the energy efficiency of MapReduce implementations without paying significant execution time penalties. Our experimental results show that for the benchmarks considered, the designed VFI system can achieve an average of 33.7% energy-delay product (EDP) savings over the standard baseline non-VFI mesh-based system while paying a maximum of 3.22% execution time penalty.},
  doi       = {10.1145/2744769.2744835},
  keywords  = {Big Data;energy conservation;multiprocessing programs;parallel processing;software architecture;energy efficient;MapReduce;VFI-enabled multicore platforms;power constraints;data movement;high-end computing;multicore architectures;data-intensive applications;compute-intensive applications;Big Data workloads;wireless NoC-enabled voltage frequency island;energy-delay product;Wireless communication;Multicore processing;Principal component analysis;Switches;Energy efficiency;Benchmark testing;NoC;Big data;Wireless;Multicore;VFI;low power},
}

@Article{7112510,
  author   = {J. {Lee} and S. {Kim}},
  title    = {Write Buffer-Oriented Energy Reduction in the L1 Data Cache for Embedded Systems},
  journal  = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year     = {2016},
  volume   = {24},
  number   = {3},
  pages    = {871-883},
  month    = {March},
  abstract = {In resource-constrained embedded systems, on-chip cache memories play an important role in both performance and energy consumption. In contrast to read operations, scant regard has been paid to optimizing write operations even though the energy consumed by write operations in the data cache constitutes a large portion of the total energy consumption. Consequently, this paper proposes a write buffer-oriented (WO) cache architecture that reduces energy consumption in the L1 data cache. Observing that write operations are very likely to be merged in the write buffer because of their high localities, we construct the proposed WO cache architecture to utilize two schemes. First, the write operations update the write buffer but not the L1 data cache, which is updated later by the write buffer after the write operations are merged. Write merging significantly reduces write accesses to the data cache and, consequently, energy consumption. Second, we further reduce energy consumption in the write buffer by filtering out unnecessary read accesses to the write buffer using a read hit predictor. In this paper, we show that the proposed WO cache architecture is applicable to the conventional embedded processors that support both write-through and write-back policies. Further, the experimental results verify that the proposed cache architecture reduces energy consumption in data caches up to 14%.},
  doi      = {10.1109/TVLSI.2015.2429587},
  keywords = {cache storage;embedded systems;power aware computing;resource allocation;software architecture;write buffer-oriented cache architecture;WO cache architecture;energy consumption reduction;L1 data cache;resource-constrained embedded system;on-chip cache memory;read hit predictor;Energy consumption;Merging;Buffer storage;Embedded systems;Benchmark testing;Computer architecture;Program processors;Data cache;low power;write buffer;write buffer-oriented (WO) cache.;Data cache;low power;write buffer;write buffer-oriented (WO) cache},
}

@InProceedings{8433676,
  author    = {A. {Wendt} and S. {Wilker} and M. {Meisel} and T. {Sauter}},
  title     = {A Multi-Agent-Based Middleware for the Development of Complex Architectures},
  booktitle = {2018 IEEE 27th International Symposium on Industrial Electronics (ISIE)},
  year      = {2018},
  pages     = {723-728},
  month     = {June},
  abstract  = {Complex software systems of today consist of several interacting modules introducing the need of a cohesive architecture. The middleware Java JADE provides this functionality. However, there is a need for additional infrastructure to lower the effort of implementing modular systems. This paper presents the Agent-Based Complex Network Architecture framework (ACONA) that extends the Java JADE multi-agent system by adding a layer, taking away the use-case-based repetitive task of creating a common communication infrastructure, enhanced data storage, and unified function access within the agents. The implementation of a cognitive architecture, which is a highly complex control system in the domain of building automation, provides first proof of concept and benchmarking opportunity.},
  doi       = {10.1109/ISIE.2018.8433676},
  keywords  = {Java;middleware;multi-agent systems;software architecture;Agent-Based Complex Network Architecture framework;Java JADE multiagent system;use-case-based repetitive task;cognitive architecture;multiagent-based middleware;Complex software systems;cohesive architecture;middleware Java JADE;Middleware;Microprocessors;Memory;Java;Data structures;Complex networks;multi-agent-system;cognitive architecture;middleware;ACONA;agent-based complex network architecture framework},
}

@InProceedings{7284093,
  author    = {F. {Liu} and H. {Ahn} and S. R. {Beard} and T. {Oh} and D. I. {August}},
  title     = {DynaSpAM: Dynamic spatial architecture mapping using Out of Order instruction schedules},
  booktitle = {2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)},
  year      = {2015},
  pages     = {541-553},
  month     = {June},
  abstract  = {Spatial architectures are more efficient than traditional Out-of-Order (OOO) processors for computationally intensive programs. However, spatial architectures require mapping a program, either statically or dynamically, onto the spatial fabric. Static methods can generate efficient mappings, but they cannot adapt to changing workloads and are not compatible across hardware generations. Current dynamic methods are adaptive and compatible, but do not optimize as well due to their limited use of speculation and small mapping scopes. To overcome the limitations of existing dynamic mapping methods for spatial architectures, while minimizing the inefficiencies inherent in OOO superscalar processors, this paper presents DynaSpAM (Dynamic Spatial Architecture Mapping), a framework that tightly couples a spatial fabric with an OOO pipeline. DynaSpAM coaxes the OOO processor into producing an optimized mapping with a simple modification to the processor's scheduler. The insight behind DynaSpAM is that today's powerful OOO processors do for themselves most of the work necessary to produce a highly optimized mapping for a spatial architecture, including aggressively speculating control and memory dependences, and scheduling instructions using a large window. Evaluation of DynaSpAM shows a geomean speedup of 1.42× for 11 benchmarks from the Rodinia benchmark suite with a geomean 23.9% reduction in energy consumption compared to an 8-issue OOO pipeline.},
  doi       = {10.1145/2749469.2750414},
  keywords  = {program diagnostics;program processors;scheduling;software architecture;DynaSpAM;dynamic spatial architecture mapping;out of order instruction schedules;program mapping;spatial fabric;static methods;hardware generations;OOO superscalar processors;OOO pipeline;OOO processor;optimized mapping;processor scheduler;memory dependences;scheduling instructions;geomean speedup;Rodinia benchmark suite;energy consumption;Fabrics;Pipelines;Lead;Hardware;Registers;Buffer storage},
}

@InProceedings{6691613,
  author    = {F. {Dehne} and Q. {Kong} and A. {Rau-Chaplin} and H. {Zaboli} and R. {Zhou}},
  title     = {A distributed tree data structure for real-time OLAP on cloud architectures},
  booktitle = {2013 IEEE International Conference on Big Data},
  year      = {2013},
  pages     = {499-505},
  month     = {Oct},
  abstract  = {In contrast to queries for on-line transaction processing (OLTP) systems that typically access only a small portion of a database, OLAP queries may need to aggregate large portions of a database which often leads to performance issues. In this paper we introduce CR-OLAP, a Cloud based Real-time OLAP system based on a new distributed index structure for OLAP, the distributed PDCR tree, that utilizes a cloud infrastructure consisting of (m + 1) multi-core processors. With increasing database size, CR-OLAP dynamically increases m to maintain performance. Our distributed PDCR tree data structure supports multiple dimension hierarchies and efficient query processing on the elaborate dimension hierarchies which are so central to OLAP systems. It is particularly efficient for complex OLAP queries that need to aggregate large portions of the data warehouse, such as “report the total sales in all stores located in California and New York during the months February-May of all years”. We evaluated CR-OLAP on the Amazon EC2 cloud, using the TPC-DS benchmark data set. The tests demonstrate that CR-OLAP scales well with increasing number of processors, even for complex queries. For example, on an Amazon EC2 cloud instance with eight processors, for a TPC-DS OLAP query stream on a data warehouse with 80 million tuples where every OLAP query aggregates more than 50% of the database, CR-OLAP achieved a query latency of 0.3 seconds which can be considered a real time response.},
  doi       = {10.1109/BigData.2013.6691613},
  keywords  = {cloud computing;data mining;data warehouses;multiprocessing systems;query processing;software architecture;tree data structures;query latency;TPC-DS OLAP query stream;TPC-DS benchmark data set;Amazon EC2 cloud;New York;California;data warehouses;dimension hierarchies;query processing;multiple dimension hierarchies;distributed PDCR tree data structure;performance maintenance;database size;multicore processors;cloud infrastructure;distributed index structure;cloud-based real-time OLAP system;CR-OLAP;performance issues;online transaction processing systems;cloud architectures;real-time OLAP query aggregation;Program processors;Databases;Vegetation;Real-time systems;Arrays;Data warehouses;Aggregates},
}

@InProceedings{7837042,
  author    = {Y. {Kishore} and N. H. V. {Datta} and K. V. {Subramaniam} and D. {Sitaram}},
  title     = {QoS Aware Resource Management for Apache Cassandra},
  booktitle = {2016 IEEE 23rd International Conference on High Performance Computing Workshops (HiPCW)},
  year      = {2016},
  pages     = {3-10},
  month     = {Dec},
  abstract  = {Apache Cassandra is a distributed database of choice when it comes to big data management with zero downtime, linear scalability, and seamless multiple data center deployment. However, resource allocation for the system during deployment is a major issue which could either lead to bad performance or under-utilization of resources. In this paper we describe a QoS-aware architecture that manages resources for the distributed storage system to proactively and dynamically allocate resources for the distributed storage system to ensure that effective resource utilization and deliver performance according to the specified QoS. The architecture uses machine learning techniques to proactively predict and judge the performance of the system and make decisions for effective resource management for the database. In addition, we propose an adaptive sampling mechanism for classification to ensure that the architecture does not impose an unreasonable overhead on the system. We evaluate and provide results using Yahoo! Cloud Serving Benchmark (YCSB) on the modified Cassandra cluster. Our evaluation shows an increase of the overall utilization of the allocated resources without compromising on the required QoS latency for the overall cluster.},
  doi       = {10.1109/HiPCW.2016.009},
  keywords  = {Big Data;cloud computing;computer centres;distributed databases;learning (artificial intelligence);quality of service;resource allocation;software architecture;QoS aware resource management;Apache Cassandra;distributed database;big data management;linear scalability;seamless multiple data center deployment;resource allocation;QoS-aware architecture;machine learning;Yahoo! cloud serving benchmark;YCSB;Peer-to-peer computing;Quality of service;Resource management;Engines;Computer architecture;Databases;Cloud computing;Apache Cassandra;QoS;Machine Learning;Resource Management;YCSB;Weka},
}

@InProceedings{1530866,
  author    = {Y. {Yan} and Y. {Liang} and X. {Du}},
  title     = {Controlling remote instruments using Web services for online experiment systems},
  booktitle = {IEEE International Conference on Web Services (ICWS'05)},
  year      = {2005},
  pages     = {732},
  month     = {July},
  abstract  = {Online experimentation allows students from anywhere to operate remote instruments at any time. This promising e-learning application is well positioned to use Web services to conduct online experiment systems due to its interoperability and Internet compliance. We present a double client-server architecture for online experiment systems and the methodology to wrap the functions of instruments into Web services. We propose that the instrument Web services should be stateful services and we present the framework to manage the states of the instrument Web services. We benchmark the performance of this system when using SOAP as the wire format for communication and propose solutions to optimize performance.},
  doi       = {10.1109/ICWS.2005.40},
  keywords  = {computer aided instruction;Internet;open systems;client-server systems;student experiments;telecontrol;remote instrument control;Web services;online experiment system;e-learning;interoperability;Internet compliance;client-server architecture;SOAP;student experiments;Control systems;Instruments;Web services;Simple object access protocol;Electronic learning;Application software;Web and internet services;Service oriented architecture;Educational institutions;Remote laboratories},
}

@InProceedings{7434988,
  author    = {V. B. Y. {Kumar} and K. {Dhiman} and M. {Datar} and A. {Pacharne} and H. {Narayanan} and S. B. {Patkar}},
  title     = {Relaxation Based Circuit Simulation Acceleration over CPU-FPGA},
  booktitle = {2016 29th International Conference on VLSI Design and 2016 15th International Conference on Embedded Systems (VLSID)},
  year      = {2016},
  pages     = {409-414},
  month     = {Jan},
  abstract  = {We present a hardware-software architecture for accelerating point-relaxation based approach to simulation of circuits, where the nonlinear device characteristics can be specified as look-up tables. The approach is shown to be particularly suitable, from a computational perspective, for accelerating variation-aware Monte-Carlo simulations. For each simulation time-step, the approach uses Block Gauss-Seidel or Gauss-Jacobi iterations with Newton-Raphson (NR) iterations for computing node-potentials. The NR iterations are executed on an array of lightweight stack-based processors while the look-up table driven model evaluation requests are scheduled on pipelined bezier interpolation units. Tightly integrated or hybrid CPU-FPGA systems are the ideal targets for this application, however, the architecture was implemented for evaluation in Blue spec System Verilog with SceMI co-emulation on Xilinx Virtex-6 FPGA (ML605 board) hosted on an Intel i7 over PCIe. In our preliminary evaluation over some benchmark circuits, we achieve more than 80% utilization of the 100+ floating-point operators used, amounting to more than 4~GFLOPS computational performance even with the hardware running at 50 MHz.},
  doi       = {10.1109/VLSID.2016.84},
  keywords  = {field programmable gate arrays;hardware-software codesign;Monte Carlo methods;Newton-Raphson method;table lookup;relaxation based circuit simulation acceleration;CPU-FPGA;hardware-software architecture;accelerating point-relaxation;look-up tables;variation-aware Monte-Carlo simulations;block Gauss-Seidel iteration;Gauss-Jacobi iteration;Newton-Raphson iterations;pipelined bezier interpolation units;Xilinx Virtex-6 FPGA;Integrated circuit modeling;Table lookup;Hardware;Computational modeling;Interpolation;Monte Carlo methods;Acceleration;Relaxation based circuit simulation;CPU-FPGA;Hybrid-FPGA;Bezier interpolation;Model-Evaluation},
}

@InProceedings{7917625,
  author    = {A. {Dongare} and C. {Hesling} and K. {Bhatia} and A. {Balanuta} and R. L. {Pereira} and B. {Iannucci} and A. {Rowe}},
  title     = {OpenChirp: A Low-Power Wide-Area Networking architecture},
  booktitle = {2017 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)},
  year      = {2017},
  pages     = {569-574},
  month     = {March},
  abstract  = {Infrastructure monitoring applications currently lack a cost-effective and reliable solution for supporting the last communication hop for low-power devices. The use of cellular infrastructure requires contracts and complex radios that are often too power hungry and cost prohibitive for sensing applications that require just a few bits of data each day. New low-power, sub-GHz, long-range radios are an ideal technology to help fill this communication void by providing access points that are able to cover multiple kilometers of urban space with thousands of end-point devices. These new Low-Power Wide-Area Networking (LPWAN) platforms provide a cost-effective and highly deployable option that could piggyback off of existing public and private wireless networks (WiFi, Cellular, etc). In this paper, we present OpenChirp, a prototype end-to-end LPWAN architecture built using LoRa Wide-Area Network (LoRaWAN) with the goal of simplifying the design and deployment of Internet-of-Things (IoT) devices across wide areas like campuses and cities. We present a software architecture that exposes an application layer allowing users to register devices, describe transducer properties, transfer data and retrieve historical values. We define a service model on top of LoRaWAN that acts as a session layer to provide basic encoding and syntax to raw data streams. At the device-level, we introduce and benchmark an open-source hardware platform that uses Bluetooth Low-Energy (BLE) to help provision LoRa clients that can be extended with custom transducers. We evaluate the system in terms of end-node energy consumption, radio penetration into buildings as well as coverage provided by a network currently deployed at Carnegie Mellon University.},
  doi       = {10.1109/PERCOMW.2017.7917625},
  keywords  = {Bluetooth;Internet;Internet of Things;network coding;public domain software;wide area networks;OpenChirp;low-power wide-area networking architecture;infrastructure monitoring application;reliability;cellular infrastructure;complex radio;access point;wireless network;WiFi;prototype end-toend LPWAN architecture;LoRa wide-area network;LoRaWAN;Internet-of-Things;IoT device;software architecture;transducer property;data transfer;encoding;data streaming;open-source hardware platform;Bluetooth low-energy;BLE;end-node energy consumption;Carnegie Mellon University;Logic gates;Servers;Computer architecture;Sensors;Long Term Evolution;Wireless fidelity;Bandwidth},
}

@InProceedings{6926002,
  author    = {J. {Reineke} and J. {Doerfert}},
  title     = {Architecture-parametric timing analysis},
  booktitle = {2014 IEEE 19th Real-Time and Embedded Technology and Applications Symposium (RTAS)},
  year      = {2014},
  pages     = {189-200},
  month     = {April},
  abstract  = {Platforms are families of microarchitectures that implement the same instruction set architecture but that differ in architectural parameters, such as frequency, memory latencies, or memory sizes. The choice of these parameters influences execution time, implementation cost, and energy consumption. In this paper, we introduce the first general framework for architecture-parametric timing analysis (APTA). APTA computes an expression that bounds the worst-case execution time (WCET) of a program in terms of architectural parameters. This enables to configure a platform, at design or even at run time, in a way that is guaranteed to meet all deadlines, while minimizing implementation cost and/or energy consumption. We demonstrate the feasibility of our approach by implementing APTA for a precision-timed (PRET) platform and by evaluating our implementation on Mälardalen benchmarks.},
  doi       = {10.1109/RTAS.2014.6926002},
  keywords  = {instruction sets;program diagnostics;software architecture;architecture-parametric timing analysis;microarchitectures;instruction set architecture;architectural parameter;implementation cost;energy consumption;APTA;worst-case execution time;WCET;precision-timed platform;PRET platform;Mälardalen benchmark;Timing;Cost accounting;Analytical models;Linear programming;Vectors;Microarchitecture;Algorithm design and analysis},
}

@Article{4663955,
  author   = {K. {Schmidt} and E. G. {Schmidt}},
  title    = {Message Scheduling for the FlexRay Protocol: The Static Segment},
  journal  = {IEEE Transactions on Vehicular Technology},
  year     = {2009},
  volume   = {58},
  number   = {5},
  pages    = {2170-2179},
  month    = {Jun},
  abstract = {In recent years, time-triggered communication protocols have been developed to support time-critical applications for in-vehicle communication. In this respect, the FlexRay protocol is likely to become the de facto standard. In this paper, we investigate the scheduling problem of periodic signals in the static segment of FlexRay. We identify and solve two subproblems and introduce associated performance metrics: (1) The signals have to be packed into equal-size messages to obey the restrictions of the FlexRay protocol, while using as little bandwidth as possible. To this end, we formulate a nonlinear integer programming (NIP) problem to maximize bandwidth utilization. Furthermore, we employ the restrictions of the FlexRay protocol to decompose the NIP and compute the optimal message set efficiently. (2) A message schedule has to be determined such that the periodic messages are transmitted with minimum jitter. For this purpose, we propose an appropriate software architecture and derive an integer linear programming (ILP) problem that both minimizes the jitter and the bandwidth allocation. A case study based on a benchmark signal set illustrates our results.},
  doi      = {10.1109/TVT.2008.2008654},
  keywords = {bandwidth allocation;integer programming;mobile radio;protocols;scheduling;message scheduling;flexray protocol;time-triggered communication protocol;in-vehicle communication;de facto standard;nonlinear integer programming;bandwidth allocation;jitter minimization;Protocols;Bandwidth;Jitter;Time factors;Measurement;Signal processing;Linear programming;Processor scheduling;Software architecture;Integer linear programming;FlexRay;integer programming;real time;scheduling;vehicular communication networks},
}

@InProceedings{8115907,
  author    = {F. {Lemic} and V. {Handziski} and I. {Azcarate} and J. {Wawrzynek} and J. {Rabaey} and A. {Wolisz}},
  title     = {SLSR: A flexible middleware localization service architecture},
  booktitle = {2017 International Conference on Indoor Positioning and Indoor Navigation (IPIN)},
  year      = {2017},
  pages     = {1-8},
  month     = {Sep.},
  abstract  = {Location information of mobile devices is a foundational input to location-based services and a valuable source of context information in wireless networks. To maximize the value, we need location information that is accurate, robust, and promptly and seamlessly available. Unfortunately, individual localization services seldom satisfy all these requirements. For achieving that vision, a set of challenges has to be addressed, pertaining to handover, fusion, and integration of different sources of location information. Current approaches for integration of individual localization services are either not specific enough or are limited in scope and lack flexibility. In the following, we provide a detailed design and a prototypical implementation of the Standardized Localization Service (SLSR), a middleware architecture for achieving those goals. We instantiate the service in an office environment and perform exhaustive performance benchmarking in a testbed specifically designed for supporting such experimentation. Our results characterize the effects of different functional components envisioned in the SLSR on its performance. Our results also quantify the accuracy benefits of fusion of representative sources of location information.},
  doi       = {10.1109/IPIN.2017.8115907},
  keywords  = {middleware;mobile computing;software architecture;Web services;individual localization services;location information;Standardized Localization Service;SLSR;middleware architecture;flexible middleware localization service architecture;context information;Mobile handsets;Economic indicators;Middleware;Handover;Power demand;Wireless networks},
}

@InProceedings{8786539,
  author    = {S. {Maabi} and S. {Attarzadeh-Niaki} and M. {Abbaspour}},
  title     = {OMID: Optimized MIcro-Architecture Design for Fog Computing Applications},
  booktitle = {2019 27th Iranian Conference on Electrical Engineering (ICEE)},
  year      = {2019},
  pages     = {2033-2038},
  month     = {April},
  abstract  = {By the progress of Internet of Things (IoT) and the increasing number of connected nodes, new concepts such as edge and fog computing appeared to prohibit unneeded data transfer to reduce delay, bandwidth and power consumption. Although these concepts provide more processing, connectivity and storage capabilities to support different IoT use cases, but they also create new challenges especially in circuit and system domains. They require a hardware with configurable architecture, adequate data processing and network management capabilities which also considers high energy efficiency. Considering these requirements, this paper seeks to present a performance optimum processor architecture based on a specific systematic design methodology. Hence, an algorithm is proposed in this design methodology to design an optimum data-path based on the two proposed architectural solutions. This algorithm along with the micro-architectural optimization techniques, concludes the final optimum design among the feasible data-paths. The implemented benchmark applications on the final optimum architecture represents a significant improvement in performance with an acceptable power consumption and area overhead.},
  doi       = {10.1109/IranianCEE.2019.8786539},
  keywords  = {Internet of Things;power aware computing;software architecture;storage management;energy efficiency;systematic design methodology;data-path;power consumption;IoT;optimized microarchitecture design;Internet of Things;storage capabilities;fog computing applications;OMID;microarchitectural optimization techniques;performance optimum processor architecture;Computer architecture;Edge computing;Design methodology;Feature extraction;Radio frequency;Systematics;Pipelines;processor design;fog computing;edge computing;architecture design methodology},
}

@InProceedings{6169647,
  author    = {J. {Steinbrecher} and W. {Shang}},
  title     = {On Optimizing the Longest Common Subsequence Problem by Loop Unrolling Along Wavefronts},
  booktitle = {2012 20th Euromicro International Conference on Parallel, Distributed and Network-based Processing},
  year      = {2012},
  pages     = {603-611},
  month     = {Feb},
  abstract  = {Loop unrolling is a loop transformation where a few loop iterations are grouped as a super iteration for exploring more independent instructions and to decrease the total loop overhead. This paper characterizes loop unrolling by the unrolling factor, the number of iterations in a super iteration and the unrolling direction, the choice of iterations to be grouped to form the super iteration. We use loop unrolling for maximizing instruction-level parallelism in the longest common subsequence problem. To increase the number of independent instructions in the super iteration, we use a linear schedule to group iterations on the same wave front, a hyper plane in the loop iteration space. Then, the loop is unrolled along the wave front which guarantees all iterations in the same super iteration are independent. The selection of the optimal unrolling factor is based on the assumption that if all the pipelines are saturated, the performance should not be bad. Two necessary conditions and a sufficient condition for optimality are presented and used to find the optimal unrolling factor. The total execution time is expressed as a function of algorithm parameters, architecture parameters and the unrolling factor. A benchmark of the technique scores a 1.475 speed-up over traditional methods.},
  doi       = {10.1109/PDP.2012.49},
  keywords  = {iterative methods;optimisation;parallel programming;program compilers;program control structures;software architecture;longest common subsequence problem optimization;loop unrolling;loop transformation;super iteration;loop overhead;unrolling direction;instruction-level parallelism maximization;linear schedule;loop iteration space hyper plane;optimal unrolling factor;algorithm parameter;architecture parameter;wavefront;Assembly;Schedules;Pipelines;Registers;Vectors;Mathematical model;Parallel processing;Loop unrolling;instruction-level parallelism;uniform dependence algorithm;longest common subsequence problem},
}

@InProceedings{7382338,
  author    = {Y. {Liu} and {Jie Yang} and {Yuan Huang} and {Lixiong Xu}},
  title     = {The performance evaluations for Hadoop in diverse system architectures},
  booktitle = {2015 12th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)},
  year      = {2015},
  pages     = {2448-2452},
  month     = {Aug},
  abstract  = {The utilities of data nowadays have been significantly improved due to discovery of latent data features from big data. However, one critical issue of discovering the knowledge in big data is people have to process large volumes of data but using very limited computing resources. In order to solve the problem, people developed a series of distributed computing technologies among which Hadoop framework is proved to be the most famous one due to its reliability and scalability. Hadoop framework highly relies on Linux operating system for functionalizing its components. However, the latest released versions of the framework only support 32bit Linux operating systems, which may less utilize the resources due to system limitation. Therefore, for enabling Hadoop framework in supporting 64bit operating system and observing its performances, this paper creates a 64 bit Hadoop framework and further evaluates the performances based on a standard benchmark algorithm wordcount in both 32bit and 64bit operating systems.},
  doi       = {10.1109/FSKD.2015.7382338},
  keywords  = {Big Data;data mining;Linux;parallel processing;resource allocation;software architecture;performance evaluation;diverse system architectures;big data;knowledge discovery;latent data feature discovery;distributed computing;Hadoop framework;Linux operating system;resource utilization;Computer architecture;Operating systems;Data processing;Algorithm design and analysis;Computers;Distributed databases;MapReduce;Hadoop compilation;Ganglia},
}

@InProceedings{7794018,
  author    = {F. {Boschi} and C. {Zanetti} and G. {Tavola} and M. {Taisch}},
  title     = {Functional requirements for reconfigurable and flexible cyber-physical system},
  booktitle = {IECON 2016 - 42nd Annual Conference of the IEEE Industrial Electronics Society},
  year      = {2016},
  pages     = {5717-5722},
  month     = {Oct},
  abstract  = {A truly global market characterized by aggressive competition on a global scale and rapid changes in process technology requires creating production systems that are easy to upgrade, being able to readily integrate new technologies and new functions. In these terms, PERFoRM (Production harmonizEd Reconfiguration of Flexible Robots and Machinery) a European funded project, aims at developing an innovative manufacturing system based on a new agile concept introducing the implementation of methods and methodologies for transforming existing production systems into plug-and-produce production ones based on Cyber-Physical Systems technologies. In particular, this paper aims at describing a methodology leading to identification and deployment of general business and strategic requirements needed to implement new plug-and-produce paradigms into traditional production systems. This approach mainly based on the Requirement Engineering methodology (RE) also leads to the identification of an appropriate set of KPIs (technical and business) able to measure and to benchmark collected requirements. Four Industrial Use Cases have been analyzed, taking the CPS-5C architecture as reference model to map their AS-IS and TO-BE situations with respect to their CPS attitude, confirming the possibility to use this approach among different manufacturing sectors, for large companies, SMEs as well as for new and existing small plants.},
  doi       = {10.1109/IECON.2016.7794018},
  keywords  = {cyber-physical systems;formal specification;formal verification;innovation management;manufacturing systems;production engineering computing;software architecture;systems analysis;functional requirements;reconfigurable cyber-physical system;flexible cyber-physical system;global market;aggressive competition;process technology;production systems;PERFoRM;production harmonized reconfiguration of flexible robots and machinery;European funded project;innovative manufacturing system;agile concept;plug-and-produce production;cyber-physical systems;business requirements;strategic requirements;requirement engineering;industrial use cases;CPS-5C architecture;Stakeholders;Production systems;Requirements engineering;Cyber-physical systems;Manufacturing;5C architecture;Cyber-Physical System;Requirements Engineering;Methodology;Plug-and-produce;KPIs},
}

@InProceedings{5610614,
  author    = {T. S. {López} and A. {Brintrup} and D. {McFarlane} and D. {Dwyer}},
  title     = {Selecting a multi-agent system development tool for industrial applications: a case study of self-serving aircraft assets},
  booktitle = {4th IEEE International Conference on Digital Ecosystems and Technologies},
  year      = {2010},
  pages     = {400-405},
  month     = {April},
  abstract  = {Industrialists have few example processes they can benchmark against in order to choose a multi-agent development kit. In this paper we present a review of commercial and academic agent tools with the aim of selecting one for developing an intelligent, self-serving asset architecture. In doing so, we map and enhance relevant assessment criteria found in literature. After a preliminary review of 20 multi-agent platforms, we examine in further detail those of JADE, JACK and Cougaar. Our findings indicate that Cougaar is well suited for our requirements, showing excellent support for criteria such as scalability, persistence, mobility and lightweightness.},
  doi       = {10.1109/DEST.2010.5610614},
  keywords  = {aerospace industry;aircraft;multi-agent systems;software architecture;multiagent system development tool;industrial application;self-serving aircraft asset;multiagent development kit;commercial agent tool;academic agent tool;self-serving asset architecture;assessment criteria;Java;Computer architecture;Standards;Security;Book reviews;Multiagent systems;Conferences},
}

@InProceedings{7498461,
  author    = {S. {Babalou} and M. J. {Kargar} and S. H. {Davarpanah}},
  title     = {Large-scale ontology matching: A review of the literature},
  booktitle = {2016 Second International Conference on Web Research (ICWR)},
  year      = {2016},
  pages     = {158-165},
  month     = {April},
  abstract  = {Ontology as the base of semantic web is used in many applications. Different ontologies in the same domain lead some heterogeneities and ontology matching systems are developed for resolved them. Heterogeneities have arisen owing to the fact that these ontologies have been created by various people through diverse methods. Nowadays, using large-scale ontologies in some applications such as medical fields seems inevitable. By using large-scale ontologies, some problems like the shortage of memory consumption and long duration of execution appeared in ontology matching systems. In this paper, large-scale ontology matching systems are studied and proposed a general architecture for them. Then large-scale ontology matching systems classified based on the partitioning large ontologies into several sub-ontologies, as known as the modularization, decomposition, summarization, clustering, and divide and conquer categories. This new classification will be useful for future research works in this field. In order to find out the efficiency of the ontology matching systems the results of OAEI (Ontology Alignment Evaluation Initiative) for the period 2011 to 2015 are compared. In spite of great progress, increasing accuracy is required in some section such as conference and benchmark sections.},
  doi       = {10.1109/ICWR.2016.7498461},
  keywords  = {ontologies (artificial intelligence);semantic Web;software architecture;string matching;ontology matching system;semantic Web;system architecture;Ontologies;Modulation;Clustering algorithms;Semantic Web;Libraries;Encapsulation;Cognition;Ontology;ontology matching;large-scale ontology matching;literature review},
}

@InProceedings{6338805,
  author    = {S. {Noth} and J. {Edelbrunner} and I. {Iossifidis}},
  title     = {An integrated architecture for the development and assessment of ADAS},
  booktitle = {2012 15th International IEEE Conference on Intelligent Transportation Systems},
  year      = {2012},
  pages     = {347-354},
  month     = {Sep.},
  abstract  = {Advanced Driver Assistant Systems act, by definition in natural, often poorly structured, environments and are supposed to closely interact with human operators. Both, natural environments as well as human behaviour have no inherent metric and can not be modelled/measured in the classical way physically plausibly behaving systems are described. This makes the development of a new methodology and especially the development of an experimental environment necessary, reflecting all constraints of the task to be observed, incorporating the influence of each single component and the generation of variations over scenes and behaviours. In this contribution we focus on the development of an integrated architecture based on a simulated reality framework incorporating simple behavioural models of autonomous acting humans, physical plausible behaving scene objects, a dynamic scene generator and an advanced recording and analysing system, operating on-line on all data streams. The architecture allows to develop, assess and benchmark embedded components, as well as whole ADAS.},
  doi       = {10.1109/ITSC.2012.6338805},
  keywords  = {behavioural sciences computing;driver information systems;software architecture;virtual reality;ADAS;integrated architecture;advanced driver assistant systems act;natural environments;human behaviour;experimental environment;simulated reality framework;simple behavioural models;autonomous acting humans;physical plausible behaving scene objects;advanced recording;Vehicles;Roads;Humans;Computational modeling;Data models;Testing;Wheels},
}

@Article{8768003,
  author   = {D. {Zhang} and F. {Cursi} and G. {Yang}},
  title    = {WSRender: A Workspace Analysis and Visualization Toolbox for Robotic Manipulator Design and Verification},
  journal  = {IEEE Robotics and Automation Letters},
  year     = {2019},
  volume   = {4},
  number   = {4},
  pages    = {3836-3843},
  month    = {Oct},
  abstract = {Workspace analysis is essential for robotic manipulators, which helps researchers study, evaluate, and optimize their designs based on specific criteria with due consideration of ergonomics and usability. Although workspace analysis is a common research topic, current solutions provide design-specific evaluation, and there is a lack of generic software tools for different hardware configurations. This letter presents WSRender, a versatile research-oriented framework for workspace analysis and visualization. It is based on the Orocos Kinematics and Dynamics Library and the Matlab Robotic Toolbox. The software architecture is presented using four use cases for demonstrating its practical use in single robot, dual-arm manipulator performance evaluation, multi-robot interaction analysis, and master–slave mapping. The source code of WSRender1 is made publicly available for the benefit of the research community for the design or evaluation of robotic manipulators.},
  doi      = {10.1109/LRA.2019.2929986},
  keywords = {Manipulators;Robot kinematics;Kinematics;Ellipsoids;Jacobian matrices;Indexes;Performance evaluation and benchmarking;multi-robot systems;telerobotics;and teleoperation},
}

@InProceedings{8742225,
  author    = {E. {Grayver}},
  title     = {Scaling the Fast x86 DVB-S2 Decoder to 1 Gbps},
  booktitle = {2019 IEEE Aerospace Conference},
  year      = {2019},
  pages     = {1-9},
  month     = {March},
  abstract  = {Software implementation of LDPC decoders has been an active area of development for the last 10 years. Researchers have focused on implementing the computationally expensive algorithm on both GPPs and GPUs. A major leap in performance was reported in the groundbreaking paper by Bertrand le Gal [2]. This paper builds on the work in [2] by considering the scaling of that implementation on modern many-core processors. We look at the performance of LDPC code specified in the DVB-S2 standard. The large block size of the DVB-S2 code makes the memory architecture of the processor just as important as the clock rate and instruction set. We present results for two generations of Intel Xeons, an Intel Phi (KNL), the recently released AMD EPYC. The key finding is that performance scaling is limited by the amount of available cache memory rather than the number of cores. We also find that heavily multi-threaded, but deterministic software architecture benefits from explicit allocation of threads to cores vs. allowing the operating system to manage threading. The maximum throughput of 1 Gbps was achieved on a mid-range AMD server - issuing a new era of all-software receivers for very high rate waveforms. We also present the performance of the algorithm ported to a low-power ARM processor and compare that to a low-end Intel Core.},
  doi       = {10.1109/AERO.2019.8742225},
  keywords  = {Instruction sets;Parity check codes;Decoding;Throughput;Benchmark testing;Forward error correction},
}

@Article{4455640,
  author   = {G. {Goth}},
  title    = {"Googling" Test Practices? Web Giant's Culture Encourages Process Improvement},
  journal  = {IEEE Software},
  year     = {2008},
  volume   = {25},
  number   = {2},
  pages    = {92-94},
  month    = {March},
  abstract = {In the wider world, Google has become a common verb as well as a noun; you can "google" any person, place, or thing, and more likely than not obtain some sort of information. But Google might also become a benchmark term for a new wave of improved software-testing practices. Numerous emerging elements, beyond Google's sheer size and cachet as the Web's most-used search engine, could make this possible.},
  doi      = {10.1109/MS.2008.28},
  keywords = {program testing;search engines;software process improvement;process improvement;Google;software-testing practices;search engine;Testing;Service oriented architecture;Search engines;Information services;Web sites;Internet;Guidelines;Application software;Software architecture;Programming profession;Software development;software testing;Google;OpenSocial;Testapalooza},
}

@Article{4799453,
  author   = {E. {Prassler} and K. {Nilson}},
  title    = {1,001 robot architectures for 1,001 robots [Industrial Activities]},
  journal  = {IEEE Robotics Automation Magazine},
  year     = {2009},
  volume   = {16},
  number   = {1},
  pages    = {113-113},
  month    = {March},
  abstract = {With the financial support of the European Commission and the Robot Standards and Reference Architectures (RoSta; grant no. IST 45304) Consortium, the RAS Standing Committee on Standardization Activities has established a study group in 2007 and has organized a series of open workshops and expert meetings to address the questions of measurability, comparability, interoperability, and resusability of architectural concepts and components in robotics. The conclusions of these meetings and the recommendations of the expert group are currently being turned into a white paper, which will be circulated in a larger community of stakeholders for feedback, before being finally published in spring 2009.},
  doi      = {10.1109/MRA.2009.932180},
  keywords = {open systems;robot programming;software architecture;software maintenance;software reusability;robot architecture design;benchmarking;robotic software harmonization;robotic software refactoring;robotic software reusability;mobile robotics;European Commission;interoperability;Service robots;Robotics and automation;Computer architecture;Intelligent robots;Motion planning;Benchmark testing;Standardization;Application software;Mobile robots;Cultural differences},
}

@InProceedings{4026876,
  title     = {IEEE International Conference on Services Computing - Title},
  booktitle = {2006 IEEE International Conference on Services Computing (SCC'06)},
  year      = {2006},
  pages     = {i-iii},
  month     = {Sep.},
  abstract  = {The following topics are dealt with: services computing; business process management; services and semantics; compliance, access control & SLAs; SOA and service infrastructures; SOA programming and benchmark; services orchestration and choreography; services composition and discovery; service-oriented modeling; SLA and services optimization; industry applications; services in Web and mobile environment; QoS, RFID, and grid flow management; application of grid services; and service composition and dynamic service invocation},
  doi       = {10.1109/SCC.2006.56},
  keywords  = {business data processing;grid computing;Internet;software architecture;services computing;business process management;access control;service level architecture;service-oriented architecture;services composition;services discovery;service-oriented modeling;Web services;mobile environment services;QoS;RFID;grid flow management;grid services;service composition;dynamic service invocation},
}

@InProceedings{5470375,
  title     = {Table of contents},
  booktitle = {2010 IEEE International Symposium on Parallel Distributed Processing (IPDPS)},
  year      = {2010},
  pages     = {1-10},
  month     = {April},
  abstract  = {The following topics are dealt with: parallel processing; distributed processing; network management; scientific computing; GPU; data storage system; data memory system; fault tolerance; sorting; performance improvement; scalability improvement; network architecture; benchmarking tools; resource allocation; image processing; data mining; transactional memory; correctness analysis; parallel linear algebra; P2P algorithm; string problem; sequence problem; energy-aware task management; parallel operating system; parallel graph; caching; thread scheduling; automatic tuning; automatic parallelization; architectural support; runtime system; client-server system management; wireless network; and data management.},
  doi       = {10.1109/IPDPS.2010.5470375},
  keywords  = {cache storage;distributed processing;linear algebra;natural sciences computing;operating systems (computers);resource allocation;scheduling;software architecture;software fault tolerance;software performance evaluation;sorting;parallel processing;distributed processing;network management;scientific computing;GPU;data storage system;data memory system;fault tolerance;sorting;performance improvement;scalability improvement;network architecture;benchmarking tools;resource allocation;image processing;data mining;transactional memory;correctness analysis;parallel linear algebra;P2P algorithm;string problem;sequence problem;energy-aware task management;parallel operating system;parallel graph;caching;thread scheduling;automatic tuning;automatic parallelization;architectural support;runtime system;client-server system management;wireless network;data management},
}

@Article{Yu2019,
  author          = {Yu, Y. and Yang, J. and Guo, C. and Zheng, H. and He, J.},
  title           = {Joint optimization of service request routing and instance placement in the microservice system},
  journal         = {Journal of Network and Computer Applications},
  year            = {2019},
  volume          = {147},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Microservice architecture is a promising architectural style. It decomposes monolithic software into a set of loosely coupled containerized microservices and associates them into multiple microservice chains to serve service requests. The new architecture creates flexibility for service provisioning but also introduces increased energy consumption and low service performance. Efficient resource allocation is critical. Unfortunately, existing solutions are designed at a coarse level for virtual machine (VM)-based clouds and not optimized for such chain-oriented service provisioning. In this paper, we study the resource allocation optimization problem for service request routing and microservice instance placement, so as to jointly reduce both resource usage and chains’ end-to-end response time for saving energy and guaranteeing Quality of Service (QoS). We design detailed workload models for microservices and chains and formulate the optimization problem as a bi-criteria optimization problem. To address it, a three-stage scheme is proposed to search and optimize the trade-off decisions, route service requests into instances and deploy instances to servers in a balanced manner. Through numerical evaluations, we show that while assuring the same QoS, our scheme performs significantly better than and faster than benchmarking algorithms on reducing energy consumption and balancing load. © 2019},
  affiliation     = {School of Electronic Information, Wuhan University, Wuhan, 430072, China; Shenzhen R&D Center, Huawei Technologies. Co., Ltd., Shenzhen, 518000, China},
  art_number      = {102441},
  author_keywords = {Bi-criteria optimization; Energy consumption; Load balance; Microservice; Microservice chain; QoS},
  document_type   = {Article},
  doi             = {10.1016/j.jnca.2019.102441},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85072519087&doi=10.1016%2fj.jnca.2019.102441&partnerID=40&md5=12d28f55304bc761583ba7edea990eb5},
}

@Article{Kraemer2019,
  author          = {Krämer, M. and Frese, S. and Kuijper, A.},
  title           = {Implementing secure applications in smart city clouds using microservices},
  journal         = {Future Generation Computer Systems},
  year            = {2019},
  volume          = {99},
  pages           = {308-320},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Smart Cities make use of ICT technology to address the challenges of modern urban management. The cloud provides an efficient and cost-effective platform on which they can manage, store and process data, as well as build applications performing complex computations and analyses. The quickly changing requirements in a Smart City require flexible software architectures that let these applications scale in a distributed environment such as the cloud. Smart Cities have to deal with huge amounts of data including sensitive information about infrastructure and citizens. In order to leverage the benefits of the cloud, in particular in terms of scalability and cost-effectiveness, this data should be stored in a public cloud. However, in such an environment, sensitive data needs to be encrypted to prevent unauthorized access. In this paper, we present a software architecture design that can be used as a template for the implementation of Smart City applications. The design is based on the microservice architectural style, which provides properties that help make Smart City applications scalable and flexible. In addition, we present a hybrid approach to securing sensitive data in the cloud. Our architecture design combines a public cloud with a trusted private environment. To store data in a cost-effective manner in the public cloud, we encrypt metadata items with CP-ABE (Ciphertext-Policy Attribute-Based Encryption)and actual Smart City data with symmetric encryption. This approach allows data to be shared across multiple administrations and makes efficient use of cloud resources. We show the applicability of our design by implementing a web-based application for urban risk management. We evaluate our architecture based on qualitative criteria, benchmark the performance of our security approach, and discuss it regarding honest-but-curious cloud providers as well as attackers trying to access user data through eavesdropping. Our findings indicate that the microservice architectural style fits the requirements of scalable Smart City applications while the proposed security approach helps prevent unauthorized access. © 2019 Elsevier B.V.},
  affiliation     = {Fraunhofer Institute for Computer Graphics Research IGD, Darmstadt, 64283, Germany; wetransform GmbH, Darmstadt, 64283, Germany; Technische Universität Darmstadt, Darmstadt, 64283, Germany},
  author_keywords = {Cloud computing; Geospatial information systems; Security; Software architecture; Urban management},
  document_type   = {Article},
  doi             = {10.1016/j.future.2019.04.042},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85065140115&doi=10.1016%2fj.future.2019.04.042&partnerID=40&md5=4623fff81ec9ddb6ad587028ba10c041},
}

@Article{Ellrott2019,
  author        = {Ellrott, K. and Buchanan, A. and Creason, A. and Mason, M. and Schaffter, T. and Hoff, B. and Eddy, J. and Chilton, J.M. and Yu, T. and Stuart, J.M. and Saez-Rodriguez, J. and Stolovitzky, G. and Boutros, P.C. and Guinney, J.},
  title         = {Reproducible biomedical benchmarking in the cloud: Lessons from crowd-sourced data challenges},
  journal       = {Genome Biology},
  year          = {2019},
  volume        = {20},
  number        = {1},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Challenges are achieving broad acceptance for addressing many biomedical questions and enabling tool assessment. But ensuring that the methods evaluated are reproducible and reusable is complicated by the diversity of software architectures, input and output file formats, and computing environments. To mitigate these problems, some challenges have leveraged new virtualization and compute methods, requiring participants to submit cloud-ready software packages. We review recent data challenges with innovative approaches to model reproducibility and data sharing, and outline key lessons for improving quantitative biomedical data analysis through crowd-sourced benchmarking challenges. © 2019 The Author(s).},
  affiliation   = {Biomedical Engineering, Oregon Health and Science University, Portland, OR 97239, United States; Sage Bionetworks, Seattle, WA, United States; IBM Research, Yorktown Heights, NY, United States; Department of Biochemistry and Molecular Biology, Pennsylvania State University, State College, PA, United States; University of California, Santa Cruz, CA, United States; Institute for Computational Biomedicine, Heidelberg University, Faculty of Medicine, Heidelberg University Hospital, Bioquant Heidelberg, Germany; Joint Research Center for Computational Biomedicine, RWTH Aachen University, Faculty of Medicine, Aachen, Germany; Ontario Institute for Cancer Research, Toronto, Canada; Departments of Medical Biophysics and Pharmacology and Toxicology, University of Toronto, Toronto, Canada; Departments of Human Genetics and Urology, University of California, Los Angeles, CA, United States; Jonsson Comprehensive Cancer Centre, University of California, Los Angeles, CA, United States; Institute for Precision Health, University of California, Los Angeles, CA, United States; Biomedical Informatics and Medical Education, University of Washington, Seattle, WA 98195, United States},
  art_number    = {195},
  document_type = {Article},
  doi           = {10.1186/s13059-019-1794-0},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85072025943&doi=10.1186%2fs13059-019-1794-0&partnerID=40&md5=110ee4b80707e31f79e0f40a0da5f108},
}

@Article{Coppolino2019,
  author          = {Coppolino, L. and D'Antonio, S. and Mazzeo, G. and Romano, L.},
  title           = {A comparative analysis of emerging approaches for securing java software with Intel SGX},
  journal         = {Future Generation Computer Systems},
  year            = {2019},
  volume          = {97},
  pages           = {620-633},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Intel SGX enables developers to protect security critical parts of their application code and data even from privileged software. This type of protection is needed in all cases where applications run on untrusted infrastructures, including public clouds. Since a significant fraction of current applications is written in Java, the research strand on how to fully unleash the potential of SGX in Java is flourishing, and multiple techniques have been proposed. In this paper, we review such techniques, and select the most promising ones – namely SCONE, SGX-LKL, and SGX-JNI Bridge – for an experimental comparison with respect to effort, security, and performance. We use a benchmark application from a real-world case study based on microservices – possibly the most prominent software architecture for current applications – and built on the widely adopted Vert.x development framework. We focus on specific microservices characterized by three different profiles in terms of resource usage – I/O-, CPU-, and Memory-intensive – and assess the trade-offs of the three aforementioned techniques for SGX integration. The results of the analysis can be used as a reference by practitioners willing to identify the best approach for integrating SGX in their Java applications, based on priorities of their particular context. © 2019 Elsevier B.V.},
  affiliation     = {University of Naples ’Parthenope’, Centro Direzionale, Isola C4, Napoli, 80133, Italy},
  author_keywords = {Cloud security; Intel SGX; Java; JVM; Microservice; Trusted execution; Vert.X},
  document_type   = {Article},
  doi             = {10.1016/j.future.2019.03.018},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85063297874&doi=10.1016%2fj.future.2019.03.018&partnerID=40&md5=59e916c389e3078ae0c28b73ef6fc998},
}

@Article{Borg2019,
  author          = {Borg, M. and Chatzipetrou, P. and Wnuk, K. and Alégroth, E. and Gorschek, T. and Papatheocharous, E. and Shah, S.M.A. and Axelsson, J.},
  title           = {Selecting component sourcing options: A survey of software engineering's broader make-or-buy decisions},
  journal         = {Information and Software Technology},
  year            = {2019},
  volume          = {112},
  pages           = {18-34},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Context: Component-based software engineering (CBSE) is a common approach to develop and evolve contemporary software systems. When evolving a system based on components, make-or-buy decisions are frequent, i.e., whether to develop components internally or to acquire them from external sources. In CBSE, several different sourcing options are available: (1) developing software in-house, (2) outsourcing development, (3) buying commercial-off-the-shelf software, and (4) integrating open source software components. Objective: Unfortunately, there is little available research on how organizations select component sourcing options (CSO) in industry practice. In this work, we seek to contribute empirical evidence to CSO selection. Method: We conduct a cross-domain survey on CSO selection in industry, implemented as an online questionnaire. Results: Based on 188 responses, we find that most organizations consider multiple CSOs during software evolution, and that the CSO decisions in industry are dominated by expert judgment. When choosing between candidate components, functional suitability acts as an initial filter, then reliability is the most important quality. Conclusion: We stress that future solution-oriented work on decision support has to account for the dominance of expert judgment in industry. Moreover, we identify considerable variation in CSO decision processes in industry. Finally, we encourage software development organizations to reflect on their decision processes when choosing whether to make or buy components, and we recommend using our survey for a first benchmarking. © 2019},
  affiliation     = {RISE Research Institutes of Sweden AB, Scheelevägen 17, Lund, SE-223 70, Sweden; Blekinge Institute of Technology, Valhallavägen 1, Karlskrona, SE-371 41, Sweden; Örebro University School of Business, Örebro, SE-701 82, Sweden; iZettle, Regeringsgatan 59, Stockholm, SE-111 56, Sweden},
  author_keywords = {Component-based software engineering; Decision making; Software architecture; Sourcing; Survey},
  document_type   = {Article},
  doi             = {10.1016/j.infsof.2019.03.015},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85064013176&doi=10.1016%2fj.infsof.2019.03.015&partnerID=40&md5=8b7b4aca2daf0d9dce2d6679e9cab33b},
}

@Article{Martin-Lopo2019,
  author          = {Martín-Lopo, M.M. and Boal, J. and Sánchez-Miralles, Á.},
  title           = {Transitioning from a meta-simulator to electrical applications: An architecture},
  journal         = {Simulation Modelling Practice and Theory},
  year            = {2019},
  volume          = {94},
  pages           = {177-198},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Nowadays, simulators are used in different applications, such as benchmarking tools, for entertainment and educational purposes, to test scenarios that otherwise would not be possible to analyze (e.g., for security reasons) or to evaluate business and regulatory models. Nevertheless, simulators are usually tailor-made for a specific application. This paper proposes an original approach, an architecture for a software tool capable of simulating any electric power system application, the first energy-oriented meta-simulator. This tool would only require the definition of a set of behavior rules, easing the process of developing new simulators aimed at the energy sector. Its applications range from pure software simulators, that could be used with, for example, benchmarking, decision-making or competition purposes, to applications monitoring and controlling energy assets in real-time, such as hardware devices (e.g., sensors, actuators, or power plants) or digital twins. The proposed architecture resulted by studying previous tools and simulators with the objective of finding common blocks and elements to abstract them. For this reason, the proposed architecture intents to encompass any tool that aims to model the energy sector. Additionally, the proposed architecture is compliant with lightweight Internet of Things (IoT) protocols and smart systems and supports the synchronization with other frameworks at different levels. © 2019 Elsevier B.V.},
  affiliation     = {Instituto de Investigación Tecnológica, Escuela Técnica Superior de Ingeniería ICAI, Universidad Pontificia Comillas, Madrid, Spain},
  author_keywords = {Electricity market; Energy platforms; Internet of Things; Meta-simulator; Software architecture},
  document_type   = {Article},
  doi             = {10.1016/j.simpat.2019.02.007},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85063329948&doi=10.1016%2fj.simpat.2019.02.007&partnerID=40&md5=ee04978af48b4f775580687fd0aabdd0},
}

@Article{Boukharata2019,
  author          = {Boukharata, S. and Ouni, A. and Kessentini, M. and Bouktif, S. and Wang, H.},
  title           = {Improving web service interfaces modularity using multi-objective optimization},
  journal         = {Automated Software Engineering},
  year            = {2019},
  volume          = {26},
  number          = {2},
  pages           = {275-312},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Service interface is a critical component in a service-oriented architecture (SOA). As first-class design artifact, a service interface should be properly designed to provide best practice of third-party reuse. However, a very common bad service design practice in existing SOAs is to place semantically unrelated operations implementing several abstractions in a single interface. Poorly designed service interfaces can have a negative effect on all client applications that use these services. Indeed, services with such poor interface structure tend to be difficult to comprehend, maintain and reuse in business processes, leading to unsuccessful services. Necessarily, then, service designers should “refactor”, i.e., restructure, their service interface into smaller, more cohesive interfaces, each representing a specific abstraction. To address this problem, we introduce a novel approach, namely WSIRem, to support service’s developers in improving the modularization of their service interfaces. WSIRem is based on a multi-objective search-based optimization approach to find the appropriate modularization of a service interface into smaller, more cohesive and loosely coupled interfaces, each implementing a distinct abstraction. WSIRem has been empirically evaluated on a benchmark of 22 real-world Web services provided by Amazon and Yahoo. Results show that the automatically identified interfaces improved the services interface structure. Qualitative evaluation of WSIRem with developers showed the performance of WSIRem in terms of understandability, where the new WSIRem interfaces were recognized as ‘relevant’ from developers point of view with more than 73% of precision and 77% of recall. Overall, the obtained results show that WSIRem outperforms state-of-the-art approaches relying on traditional partitioning techniques. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
  affiliation     = {ETS Montreal, University of Quebec, Montreal, QC, Canada; Computer and Information Science Department, University of Michigan, Ann Arbor, MI, United States; College of Information Technology, UAE University, Al Ain, United Arab Emirates; eBay, San Jose, CA, United States},
  author_keywords = {Modularity; Searchbased software engineering; Service design; SOA; Web service; Web service interface},
  document_type   = {Article},
  doi             = {10.1007/s10515-019-00256-4},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85066014299&doi=10.1007%2fs10515-019-00256-4&partnerID=40&md5=e5270affd4f0b601023f36b8a2e9eb90},
}

@Article{Costanza2019,
  author          = {Costanza, P. and Herzeel, C. and Verachtert, W.},
  title           = {A comparison of three programming languages for a full-fledged next-generation sequencing tool},
  journal         = {BMC Bioinformatics},
  year            = {2019},
  volume          = {20},
  number          = {1},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Background: elPrep is an established multi-threaded framework for preparing SAM and BAM files in sequencing pipelines. To achieve good performance, its software architecture makes only a single pass through a SAM/BAM file for multiple preparation steps, and keeps sequencing data as much as possible in main memory. Similar to other SAM/BAM tools, management of heap memory is a complex task in elPrep, and it became a serious productivity bottleneck in its original implementation language during recent further development of elPrep. We therefore investigated three alternative programming languages: Go and Java using a concurrent, parallel garbage collector on the one hand, and C++17 using reference counting on the other hand for handling large amounts of heap objects. We reimplemented elPrep in all three languages and benchmarked their runtime performance and memory use. Results: The Go implementation performs best, yielding the best balance between runtime performance and memory use. While the Java benchmarks report a somewhat faster runtime than the Go benchmarks, the memory use of the Java runs is significantly higher. The C++17 benchmarks run significantly slower than both Go and Java, while using somewhat more memory than the Go runs. Our analysis shows that concurrent, parallel garbage collection is better at managing a large heap of objects than reference counting in our case. Conclusions: Based on our benchmark results, we selected Go as our new implementation language for elPrep, and recommend considering Go as a good candidate for developing other bioinformatics tools for processing SAM/BAM data as well. © 2019 The Author(s).},
  affiliation     = {Imec, ExaScience Lab, Kapeldreef 75, Leuven, 3001, Belgium},
  art_number      = {301},
  author_keywords = {C++; Garbage collection; Go; Java; Memory usage; Next-generation sequencing; Reference counting; Runtime performance; SAM/BAM files; Sequence analysis},
  document_type   = {Article},
  doi             = {10.1186/s12859-019-2903-5},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85066850960&doi=10.1186%2fs12859-019-2903-5&partnerID=40&md5=331447826ff03d4689c1ca000140b660},
}

@Conference{Wang2019,
  author        = {Wang, Y. and Wang, Y. and Li, H. and Shi, C. and Li, X.},
  title         = {Systolic cube: A spatial 3D CNN accelerator architecture for low power video analysis},
  year          = {2019},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {3D convolutional neural networks (CNN) are gaining popularity in action/activity analysis. Compared to 2D convolutions that share the filters in 2D spatial domain, 3D convolutions further reuse filters in the temporal dimension to capture time-domain features. Prior works on specialized 3D-CNN accelerators employ additional on-chip memories and multi-cluster architecture to reuse data among the process element (PE) arrays, which is too expensive for low-power chips. Instead of harvesting in-memory locality, we propose a 3D systolic cube architecture to exploit the spatial-andtemporal localities of 3D CNNs, which moves the reusable data inbetween PEs connected via a 3D-cube Network-on-Chip. Evaluation shows that systolic-cube contributes to considerable energy-efficiency boost for activity-recognition benchmarks. © 2019 Association for Computing Machinery.},
  affiliation   = {SKLCA, Institute of Computing Technology, Chinese Academy of Sciences, China; University of Chinese Academy of Sciences, China; School of Microelectronics and Communication Engineering, Chongqing University, China; Schepens Eye Research Institute, Massachusetts Eye and Ear, Harvard Medical School, United States},
  art_number    = {a210},
  document_type = {Conference Paper},
  doi           = {10.1145/3316781.3317919},
  journal       = {Proceedings - Design Automation Conference},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85067827411&doi=10.1145%2f3316781.3317919&partnerID=40&md5=7bae7e309437bd53a8d24418367b578c},
}

@Article{Safonov2019,
  author          = {Safonov, A.A.},
  title           = {3D topology optimization of continuous fiber-reinforced structures via natural evolution method},
  journal         = {Composite Structures},
  year            = {2019},
  volume          = {215},
  pages           = {289-297},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {The method to optimize a topology of 3D continuous fiber-reinforced additively manufactured structures is discussed. The proposed method makes it possible to simultaneously search for density distribution and local reinforcement layup in 3D composite structures of transversely isotropic materials. The approach uses a dynamical systems method to find density distribution, combined with the method for rotation of reinforcement direction to align it in the direction of principal stresses with local minimum compliance. The algorithm is implemented as a built-in material model within Abaqus finite element suite. Both the optimal material density distribution and the distribution of fiber orientation vector are determined for three structural elements used as benchmarks: the bending of simply supported 2D beam under central point load, the loading of 3D cube by vertical load, and the bending of 3D cantilever beam. © 2019 Elsevier Ltd},
  affiliation     = {Center for Design, Manufacturing and Materials, Skolkovo Institute of Science and Technology, Moscow, Russian Federation},
  author_keywords = {3D printing; Composite additive manufacturing; Orientation design; SIMP; Topology optimization},
  document_type   = {Article},
  doi             = {10.1016/j.compstruct.2019.02.063},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85061899976&doi=10.1016%2fj.compstruct.2019.02.063&partnerID=40&md5=43726507fd16f74f3f7d7d0c99ddcf1b},
}

@Article{Dongarra2019,
  author          = {Dongarra, J. and Gates, M. and Haidar, A. and Kurzak, J. and Luszczek, P. and Wu, P. and Yamazaki, I. and Yarkhan, A. and Abalenkovs, M. and Bagherpour, N. and Hammarling, S. and Šístek, J. and Stevens, D. and Zounon, M. and Relton, S.D.},
  title           = {Plasma: Parallel linear algebra software for multicore using OpenMP},
  journal         = {ACM Transactions on Mathematical Software},
  year            = {2019},
  volume          = {45},
  number          = {2},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The recent version of the Parallel Linear Algebra Software for Multicore Architectures (PLASMA) library is based on tasks with dependencies from the OpenMP standard. The main functionality of the library is presented. Extensive benchmarks are targeted on three recent multicore and manycore architectures, namely, an Intel Xeon, Intel Xeon Phi, and IBM POWER 8 processors. © 2019 Copyright held by the owner/author(s).},
  affiliation     = {University of Tennessee, Department of Electrical Engineering and Computer Science, 1122 Volunteer Blvd. Suite 203, Knoxville, TN 37996-3450, United States; University of Houston, Department of Computer Science, 3551 Cullen Blvd, Houston, TX 77204, United States; University of Manchester, School of Mathematics, Manchester, M13 9PL, United Kingdom; Institute of Mathematics of the Czech Academy of Sciences, Žitná 25, Prague, 115 67, Czech Republic; Numerical Algorithms Group (NAG), Manchester One, 53 Portland Street, Manchester, M1 3LD, United Kingdom; University of Leeds, Institute of Health Sciences, Leeds, LS2 9LJ, United Kingdom},
  art_number      = {a16},
  author_keywords = {Multicore processors; Numerical linear algebra libraries; OpenMP; PLASMA; Task-based programming; Tile algorithms},
  document_type   = {Article},
  doi             = {10.1145/3264491},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85065709656&doi=10.1145%2f3264491&partnerID=40&md5=94b9b3a194bd770195e3fa7b726677f9},
}

@Conference{Arabnejad2019,
  author          = {Arabnejad, H. and Bispo, J. and Barbosa, J.G. and Cardoso, J.M.P.},
  title           = {An OpenMP based parallelization compiler for C applications},
  year            = {2019},
  pages           = {915-923},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Directive-drive programming models, such as OpenMP, are one solution for exploiting the potential of multicore architectures, and enable developers to accelerate software applications by adding annotations on for-type loops and other code regions. However, manual parallelization of applications is known to be a non trivial and time consuming process, requiring parallel programming skills. Automatic parallelization approaches can reduce the burden on the application development side. This paper presents an OpenMP based automatic parallelization compiler, named AutoPar-Clava, for automatic identification and annotation of loops in C code. By using static analysis, parallelizable regions are detected, and a compilable OpenMP parallel code from the sequential version is produced. In order to reduce the accesses to shared memory by each thread, each variable is categorized into the proper OpenMP scoping. Also, AutoPar-Clava is able to support reduction on arrays, which is available since OpenMP 4.5. The effectiveness of AutoPar-Clava is evaluated by means of the Polyhedral Benchmark suite, and targeting a N-cores x86-based computing platform. The achieved results are very promising and compare favorably with closely related auto-parallelization compilers such as Intel C/C++ Compiler (i.e., icc), ROSE, TRACO, and Cetus. © 2018 IEEE.},
  affiliation     = {Faculty of Engineering, University of Porto, Portugal; LIACC, Faculty of Engineering, University of Porto, Portugal},
  art_number      = {8672334},
  author_keywords = {Automatic Parallelization; Parallel Programming; Source-to-source Compilation; Static Analysis},
  document_type   = {Conference Paper},
  doi             = {10.1109/BDCloud.2018.00135},
  journal         = {Proceedings - 16th IEEE International Symposium on Parallel and Distributed Processing with Applications, 17th IEEE International Conference on Ubiquitous Computing and Communications, 8th IEEE International Conference on Big Data and Cloud Computing, 11th IEEE International Conference on Social Computing and Networking and 8th IEEE International Conference on Sustainable Computing and Communications, ISPA/IUCC/BDCloud/SocialCom/SustainCom 2018},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85063875919&doi=10.1109%2fBDCloud.2018.00135&partnerID=40&md5=c7a71a9f6474463b5c750ed9e581641d},
}

@Article{Atre2019,
  author          = {Atre, R. and Ul-Huda, Z. and Wolf, F. and Jannesari, A.},
  title           = {Dissecting sequential programs for parallelization—An approach based on computational units},
  journal         = {Concurrency Computation},
  year            = {2019},
  volume          = {31},
  number          = {5},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {When trying to parallelize a sequential program, programmers routinely struggle during the first step: finding out which code sections can be made to run in parallel. While identifying such code sections, most of the current parallelism discovery techniques focus on specific language constructs. In contrast, we propose to concentrate on the computations performed by a program. In our approach, a program is treated as a collection of computations communicating with one another using a number of variables. Each computation is represented as a computational unit (CU). A CU contains the inputs and outputs of a computation, and the three phases of a computation are read, compute, and write. Based on the notion of CU, which ensures that the read phase executes before the write phase, we present a unified framework to identify both loop parallelism and task parallelism in sequential programs. We conducted a range of experiments on 23 applications from four different benchmark suites. Our approach accurately identified the parallelization opportunities in benchmark applications based on comparison with their parallel versions. We have also parallelized the opportunities identified by our approach that were not implemented in the parallel versions of the benchmarks and reported the speedup. © 2018 John Wiley & Sons, Ltd.},
  affiliation     = {Technische Universität Darmstadt, Darmstadt, Germany; Iowa State University, Ames, IA, United States},
  art_number      = {e4770},
  author_keywords = {multicore architectures; parallelism discovery; profiling; static analysis; task parallelism},
  document_type   = {Conference Paper},
  doi             = {10.1002/cpe.4770},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85055660665&doi=10.1002%2fcpe.4770&partnerID=40&md5=5b935a5433bb7cc581803e8164891669},
}

@Article{Fettes2019,
  author          = {Fettes, Q. and Clark, M. and Bunescu, R. and Karanth, A. and Louri, A.},
  title           = {Dynamic Voltage and Frequency Scaling in NoCs with Supervised and Reinforcement Learning Techniques},
  journal         = {IEEE Transactions on Computers},
  year            = {2019},
  volume          = {68},
  number          = {3},
  pages           = {375-389},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Network-on-Chips (NoCs) are the de facto choice for designing the interconnect fabric in multicore chips due to their regularity, efficiency, simplicity, and scalability. However, NoC suffers from excessive static power and dynamic energy due to transistor leakage current and data movement between the cores and caches. Power consumption issues are only exacerbated by ever decreasing technology sizes. Dynamic Voltage and Frequency Scaling (DVFS) is one technique that seeks to reduce dynamic energy; however this often occurs at the expense of performance. In this paper, we propose LEAD Learning-enabled Energy-Aware Dynamic voltage/frequency scaling for multicore architectures using both supervised learning and reinforcement learning approaches. LEAD groups the router and its outgoing links into the same V/F domain and implements proactive DVFS mode management strategies that rely on offline trained machine learning models in order to provide optimal V/F mode selection between different voltage/frequency pairs. We present three supervised learning versions of LEAD that are based on buffer utilization, change in buffer utilization and change in energy/throughput, which allow proactive mode selection based on accurate prediction of future network parameters. We then describe a reinforcement learning approach to LEAD that optimizes the DVFS mode selection directly, obviating the need for label and threshold engineering. Simulation results using PARSEC and Splash-2 benchmarks on a 4 × 4 concentrated mesh architecture show that by using supervised learning LEAD can achieve an average dynamic energy savings of 15.4 percent for a loss in throughput of 0.8 percent with no significant impact on latency. When reinforcement learning is used, LEAD increases average dynamic energy savings to 20.3 percent at the cost of a 1.5 percent decrease in throughput and a 1.7 percent increase in latency. Overall, the more flexible reinforcement learning approach enables learning an optimal behavior for a wider range of load environments under any desired energy versus throughput tradeoff. © 2018 IEEE.},
  affiliation     = {Department of Electrical Engineering and Computer Science, Ohio University, Athens, OH 45701, United States; Department of Electrical and Computer Engineering, George Washington University, Washington, DC 20052, United States},
  art_number      = {8489913},
  author_keywords = {Dynamic voltage and frequency scaling (DVFS); machine learning (ML); reinforcement learning; ridge regression},
  document_type   = {Article},
  doi             = {10.1109/TC.2018.2875476},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85054637525&doi=10.1109%2fTC.2018.2875476&partnerID=40&md5=d3d620730e42710907551dcd1f96f85c},
}

@Conference{Langguth2019,
  author        = {Langguth, J. and Cai, X. and Sourouri, M.},
  title         = {Memory Bandwidth Contention: Communication vs Computation Tradeoffs in Supercomputers with Multicore Architectures},
  year          = {2019},
  volume        = {2018-December},
  pages         = {497-506},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {We study the problem of contention for memory bandwidth between computation and communication in supercomputers that feature multicore CPUs. The problem arises when communication and computation are overlapped, and both operations compete for the same memory bandwidth. This contention is most visible at the limits of scalability, when communication and computation take similar amounts of time, and thus must be taken into account in order to reach maximum scalability in memory bandwidth bound applications. Typical examples of codes affected by the memory bandwidth contention problem are sparse matrix-vector computations, graph algorithms, and many machine learning problems, as they typically exhibit a high demand for both memory bandwidth and inter-node communication, while performing a relatively low number of arithmetic operations. The problem is even more relevant in truly heterogeneous computations where CPUs and accelerators are used in concert. In that case it can lead to mispredictions of expected performance and consequently to suboptimal load balancing between CPU and accelerator, which in turn can lead to idling of powerful accelerators and thus to a large decrease in performance. We propose a simple benchmark in order to quantify the loss of performance due to memory bandwidth contention. Based on that, we derive a theoretical model to determine the impact of the phenomenon on parallel memory-bound applications. We test the model on scientific computations, discuss the practical relevance of the problem and suggest possible techniques to remedy it. © 2018 IEEE.},
  affiliation   = {Department of High Performance Computing, Simula Research Laboratory, Oslo, Norway; Acando Norway, Oslo, Norway},
  art_number    = {8644601},
  document_type = {Conference Paper},
  doi           = {10.1109/PADSW.2018.8644601},
  journal       = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85063322250&doi=10.1109%2fPADSW.2018.8644601&partnerID=40&md5=793b97265e09643499931737e7fbaddc},
}

@Article{Elsayed2019,
  author          = {Elsayed, M. and Zulkernine, M.},
  title           = {Offering security diagnosis as a service for cloud SaaS applications},
  journal         = {Journal of Information Security and Applications},
  year            = {2019},
  volume          = {44},
  pages           = {32-48},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the maturity of service-oriented architecture (SOA), microservices architecture, and Web technologies, web services have become critical components of Software as a Service (SaaS) applications in cloud ecosystem environments. Although these technologies promise impressive benefits, they put SaaS applications at risk against novel as well as prevalent attack vectors. This security risk is further magnified by the loss of control and lack of security enforcement over sensitive data manipulated by SaaS applications. We present our solution as Security Diagnosis as a Service (SDaaS) to analyze the security status of SaaS applications and detect potential information flow vulnerabilities. We evaluate the detection accuracy, performance, and scalability of our framework. The experiments are conducted over benchmark applications for assessing vulnerability detection tools and services. We contrast our solution with several tools comprising static code analyzers, penetration testers, and an anomaly detector. The experiments show that the framework is a viable solution to protect against data integrity and confidentiality violations. The evaluation results demonstrate that SDaaS reveals information flow vulnerabilities with not only high accuracy, performance and scalability, but also lightweight footprint on resource utilization. © 2018},
  affiliation     = {School of Computing, Queen's University, Kingston, ON K7L 2N8, Canada},
  author_keywords = {Cloud applications; Information flow control; SaaS security; Security as Services; Static analysis; Vulnerability detection},
  document_type   = {Article},
  doi             = {10.1016/j.jisa.2018.11.006},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85057208685&doi=10.1016%2fj.jisa.2018.11.006&partnerID=40&md5=f84d4148a0960b2e1eb7aa152c36e28e},
}

@Article{Fehn2019,
  author          = {Fehn, N. and Wall, W.A. and Kronbichler, M.},
  title           = {A matrix-free high-order discontinuous Galerkin compressible Navier-Stokes solver: A performance comparison of compressible and incompressible formulations for turbulent incompressible flows},
  journal         = {International Journal for Numerical Methods in Fluids},
  year            = {2019},
  volume          = {89},
  number          = {3},
  pages           = {71-102},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Both compressible and incompressible Navier-Stokes solvers can be used and are used to solve incompressible turbulent flow problems. In the compressible case, the Mach number is then considered as a solver parameter that is set to a small value, M ≈0.1, in order to mimic incompressible flows. This strategy is widely used for high-order discontinuous Galerkin (DG) discretizations of the compressible Navier-Stokes equations. The present work raises the question regarding the computational efficiency of compressible DG solvers as compared to an incompressible formulation. Our contributions to the state of the art are twofold: Firstly, we present a high-performance DG solver for the compressible Navier-Stokes equations based on a highly efficient matrix-free implementation that targets modern cache-based multicore architectures with Flop/Byte ratios significantly larger than 1. The performance results presented in this work focus on the node-level performance, and our results suggest that there is great potential for further performance improvements for current state-of-the-art DG implementations of the compressible Navier-Stokes equations. Secondly, this compressible Navier-Stokes solver is put into perspective by comparing it to an incompressible DG solver that uses the same matrix-free implementation. We discuss algorithmic differences between both solution strategies and present an in-depth numerical investigation of the performance. The considered benchmark test cases are the three-dimensional Taylor-Green vortex problem as a representative of transitional flows and the turbulent channel flow problem as a representative of wall-bounded turbulent flows. The results indicate a clear performance advantage of the incompressible formulation over the compressible one. © 2018 John Wiley & Sons, Ltd.},
  affiliation     = {Institute for Computational Mechanics, Technical University of Munich, Garching, Germany},
  author_keywords = {discontinuous Galerkin; high-order methods; high-performance computing; matrix-free implementation; Navier-Stokes equations; turbulent flows},
  document_type   = {Article},
  doi             = {10.1002/fld.4683},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85054845767&doi=10.1002%2ffld.4683&partnerID=40&md5=5ad3fce922fbb84f2577fbb2609d47a8},
}

@Article{Li2019,
  author          = {Li, H. and De Meulenaere, P. and Mercelis, S. and Hellinckx, P.},
  title           = {Impact of software architecture on execution time: A power window TACLeBench case study},
  journal         = {International Journal of Grid and Utility Computing},
  year            = {2019},
  volume          = {10},
  number          = {2},
  pages           = {132-140},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Timing analysis is used to extract the timing properties of a system. Various timing analysis techniques and tools have been developed over the past decades. However, changes in hardware platform and software architecture introduced new challenges in timing analysis techniques. In our research, we aim to develop a hybrid approach to provide safe and precise timing analysis results. In this approach, we will divide the original code into smaller code blocks, and then construct a timing model based on the information acquired by measuring the execution time of every individual block. This process can introduce changes in the software architecture. In this paper, we use a multi-component benchmark to investigate the impact of software architecture on the timing behaviour of a system. Copyright © 2019 Inderscience Enterprises Ltd.},
  affiliation     = {University of Antwerp, Faculty of Applied Engineering, Flanders Make – CoSys-Lab, imec – IDLab, Groenenborgerlaan 171, Antwerp, 2020, Belgium; University of Antwerp, Faculty of Applied Engineering, Flanders Make – CoSys-Lab, Groenenborgerlaan 171, Antwerp, 2020, Belgium; University of Antwerp – imec, IDLab – Faculty of Applied Engineering, Sint-Pietersvliet 7, Antwerp, 2000, Belgium},
  author_keywords = {COBRA block generator; Embedded systems; Hybrid timing analysis; Power window; TACLEBench; Timing analysis; WCET},
  document_type   = {Article},
  doi             = {10.1504/IJGUC.2019.098216},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85062767756&doi=10.1504%2fIJGUC.2019.098216&partnerID=40&md5=aef4d0bd508a134b73204ffe8b00396d},
}

@Article{Svogor2019,
  author          = {Švogor, I. and Crnković, I. and Vrček, N.},
  title           = {An extensible framework for software configuration optimization on heterogeneous computing systems: Time and energy case study},
  journal         = {Information and Software Technology},
  year            = {2019},
  volume          = {105},
  pages           = {30-42},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Context: Application of component based software engineering methods to heterogeneous computing (HC) enables different software configurations to realize the same function with different non–functional properties (NFP). Finding the best software configuration with respect to multiple NFPs is a non–trivial task. Objective: We propose a Software Component Allocation Framework (SCAF) with the goal to acquire a (sub–) optimal software configuration with respect to multiple NFPs, thus providing performance prediction of a software configuration in its early design phase. We focus on the software configuration optimization for the average energy consumption and average execution time. Method: We validated SCAF through its instantiation on a real–world demonstrator and a simulation. Firstly, we verified the correctness of our model through comparing the performance prediction of six software configurations to the actual performance, obtained through extensive measurements with a confidence interval of 95%. Secondly, to demonstrate how SCAF scales up, we performed software configuration optimization on 55 generated use–cases (with solution spaces ranging from 1030 to 3070) and benchmark the results against best performing random configurations. Results: The performance of a configuration as predicted by our framework matched the configuration implemented and measured on a real–world platform. Furthermore, by applying the genetic algorithm and simulated annealing to the weight function given in SCAF, we obtain sub–optimal software configurations differing in performance at most 7% and 13% from the optimal configuration (respectfully). Conclusion: SCAF is capable of correctly describing a HC platform and reliably predict the performance of software configuration in the early design phase. Automated in the form of an Eclipse plugin, SCAF allows software architects to model architectural constraints and preferences, acting as a multi–criterion software architecture decision support system. In addition to said, we also point out several interesting research directions, to further investigate and improve our approach. © 2018},
  affiliation     = {Polytechnique Montréal, Montréal, QC, Canada; Faculty of Organization and Informatics, University of Zagreb, Varaždin, Croatia; Chalmers University of Technology, Gothenburg, Sweden},
  author_keywords = {Component based software; Cyber–physical systems; Execution time; Heterogeneous computing; Power consumption; Robot experiment; Software components},
  document_type   = {Article},
  doi             = {10.1016/j.infsof.2018.08.003},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85051630181&doi=10.1016%2fj.infsof.2018.08.003&partnerID=40&md5=3c76d50d43f9328ac63f3a69c51f641b},
}

@Article{Ghandeharizadeh2019,
  author          = {Ghandeharizadeh, S. and Nguyen, H.},
  title           = {A comparison of two cache augmented SQL architectures},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2019},
  volume          = {11135 LNCS},
  pages           = {94-109},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Cloud service providers augment a SQL database management system with a cache to enhance system performance for workloads that exhibit a high read to write ratio. These in-memory caches provide a simple programming interface such as get, put, and delete. Using their software architecture, different caching frameworks can be categorized into Client-Server (CS) and Shared Address Space (SAS) systems. Example CS caches are memcached and Redis. Example SAS caches are Java Cache standard and its Google Guava implementation, Terracotta BigMemory and KOSAR. How do CS and SAS architectures compare with one another and what are their tradeoffs? This study quantifies an answer using BG, a benchmark for interactive social networking actions. In general, obtained results show SAS provides a higher performance with write policies playing an important role. © 2019, Springer Nature Switzerland AG.},
  affiliation     = {USC Database Laboratory, Los Angeles, United States},
  author_keywords = {Caching; Performance; Scalability; Write policy},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-030-11404-6_8},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85061401034&doi=10.1007%2f978-3-030-11404-6_8&partnerID=40&md5=6c87c44632f4cff809713cea62e0c94e},
}

@Conference{Yin2019,
  author          = {Yin, Q. and Wang, L. and Li, B.},
  title           = {Identify MVC architectural pattern based on ontology},
  year            = {2019},
  volume          = {2019-July},
  pages           = {612-617},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {MVC architectural pattern is widely used in software architecture design. It helps decouple the processing and the visualization of system data. Identified MVC architectural pattern helps understand how the software is actually implemented based on MVC architectural pattern, and further improve the consistency between design and source code. This paper proposes an ontology-based MVC architectural pattern identification method. Firstly, we use the combination of design patterns to describe the structure of MVC architectural pattern, so as to construct the MVC ontology of concept layer. Then we construct a program dependency graph by extracting the dependencies between entities in the target system, and build the ontology of instance layer. Finally, the MVC architectural pattern ontology of the specific target system is inferred by ontology reasoner in order to obtained MVC architectural pattern and the pattern elements included in each component. We use open source projects as the benchmark, and the experimental results show that our method effectively identify the MVC architectural pattern and the pattern elements in software system. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.},
  affiliation     = {School of Computer Science and Engineering, Southeast University, Nanjing, China},
  author_keywords = {Architectural pattern; MVC; Observer pattern; Ontology; Pattern identification; Strategy pattern},
  document_type   = {Conference Paper},
  doi             = {10.18293/SEKE2019-163},
  journal         = {Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85071387023&doi=10.18293%2fSEKE2019-163&partnerID=40&md5=96b8678c0e17803cf6013450735ef761},
}

@Article{Sehgal2019,
  author          = {Sehgal, R. and Mehrotra, D. and Nagpal, R.},
  title           = {Complexity metrics for component-based software system},
  journal         = {Advances in Intelligent Systems and Computing},
  year            = {2019},
  volume          = {742},
  pages           = {13-22},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Today, softwares are influencing almost every process involved in our day-to-day life. The dependence of our routine processes of software system makes the reliability of these softwares a major concern of industry. Various metrics and benchmark are designed to ensure the smooth design and implementation of software, among which complexity is one. It is always a desire software architect to design software with lesser complexity. In this paper, component-based software is considered and metrics to measure the complexity of the software is proposed. Complexity needs to be measured at component level and its relationship with other components. UML diagram is drawn to propose a new metrics, and dependency of one component to another component is measured. Various complexity metrics namely Method Complexity MCOM, Number of Calls to Other Methods (NCOM), Component Complexity (CCOM) is evaluated, and Emergent System Complexity (ESCOM) and overall complexity of the system are evaluated incorporating the contribution of each. © Springer Nature Singapore Pte Ltd. 2019.},
  affiliation     = {Amity School of Engineering and Technology, Amity University, Noida, Uttar Pradesh, India},
  author_keywords = {CCOM; Complexity; CPDG; MCOM; NCOM; UML},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-981-13-0589-4_2},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85053919860&doi=10.1007%2f978-981-13-0589-4_2&partnerID=40&md5=01bbd2b22dc2a855da6f8ad644e6a929},
}

@Article{BenNoureddine2019,
  author          = {Ben Noureddine, D. and Gharbi, A. and Ben Ahmed, S.},
  title           = {An Agent-Based Planning Method for Distributed Task Allocation},
  journal         = {Communications in Computer and Information Science},
  year            = {2019},
  volume          = {1077},
  pages           = {282-306},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In multi-agent systems, agents should socially cooperate with their neighboring agents in order to solve task allocation problem in open and dynamic network environments. This paper proposes an agent-based architecture to handle different tasks; in particular, we focus on planning and distributed task allocation. In the proposed approach, each agent uses the fuzzy logic technique to select the alternative plans. We also propose an efficient task allocation algorithm that takes into consideration agent architectures and allows neighboring agents to help to perform a task as well as the indirectly related agents in the system. We illustrate our line of thought with a Benchmark Production System used as a running example in order to explain better our contribution. A set of experiments was conducted to demonstrate the efficiency of our planning approach and the performance of our distributed task allocation method. © 2019, Springer Nature Switzerland AG.},
  affiliation     = {LISI, National Institute of Applied Science and Technology, INSAT, University of Carthage, Tunis, Tunisia; FST, University of El Manar, Tunis, Tunisia},
  author_keywords = {Distributed task allocation; Fuzzy logic; Multi-agent system; Planning; Software architecture},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-030-29157-0_13},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85071696337&doi=10.1007%2f978-3-030-29157-0_13&partnerID=40&md5=901ae5dce3c54526aa3a9f67815af86d},
}

@Conference{Noureddine2019,
  author          = {Noureddine, D.B. and Gharbi, A. and Ahmed, S.B.},
  title           = {A social multi-agent cooperation system based on planning and distributed task allocation: Real case study},
  year            = {2019},
  pages           = {449-459},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In multi-agent systems, agents are socially cooperated with their neighboring agents to accomplish their goals. In this paper, we propose an agent-based architecture to handle different services and tasks; in particular, we focus on individual planning and distributed task allocation. We introduce the multi-agent planning in which each agent uses the fuzzy logic technique to select the alternative plans. We also propose an effective task allocation algorithm able to manage loosely coupled distributed environments where agents and tasks are heterogeneous. We illustrate our line of thought with a Benchmark Production System used as a running example in order to explain better our contribution. A set of experiments show the efficiency of our planning approach and the performance of our distributed task allocation method. Copyright © 2018 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved},
  affiliation     = {LISI, INSAT, National Institute of Applied Science and Technology, University of Carthage, Tunis, Tunisia; FST, University of El Manar, Tunis, Tunisia},
  author_keywords = {Distributed Task Allocation; Fuzzy Logic; Multi-agent System; Planning; Software Architecture},
  document_type   = {Conference Paper},
  journal         = {ICSOFT 2018 - Proceedings of the 13th International Conference on Software Technologies},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85071413780&partnerID=40&md5=882bde590bab4371b6c63b9e0e79b384},
}

@Conference{Konak2019,
  author          = {Konak, O. and Da Cruz, H.F. and Thiele, M. and Golla, D. and Schapranow, M.P.},
  title           = {An information and communication platform supporting analytics for elderly care},
  year            = {2019},
  pages           = {197-204},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Germany faces an increase of its elderly population and along with it the number of people reliant on nursing care is also rising. In this context, access to reliable information is key for all actors involved, be they family members or political decision makers. Currently, the country lacks a centralized platform on which such actors can access and exchange relevant information, e.g. as concerns finding a suitable facility or identifying trends on the demand for care spots. Existing solutions are based on regional data silos, which render information exchange time-consuming and error-prone. As a result, nursing care actors lack access to timely, reliable information to support strategic decision-making. In this paper, we introduce a software platform built upon an In-Memory database that meets the information and communication needs of the different user groups involved. The platform establishes the necessary framework for real-time data collection and information exchange, laying the foundation for deriving key performance indicators and enabling data exploration and prognoses. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved},
  affiliation     = {Hasso Plattner Institute, Digital Health Center, Rudolf-Breitscheid-Str. 187, Potsdam, 14482, Germany; Data Experts GmbH, Allee der Kosmonauten 33G, Berlin, 12681, Germany},
  author_keywords = {Decision Support; In-Memory Database; Nursing Care; Software Architecture},
  document_type   = {Conference Paper},
  journal         = {ICT4AWE 2019 - Proceedings of the 5th International Conference on Information and Communication Technologies for Ageing Well and e-Health},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85067467374&partnerID=40&md5=0b8534f9ddbb9ebe8e94d348b782a7a4},
}

@Article{Gamatie2019,
  author          = {Gamatie, A. and Devic, G. and Sassatelli, G. and Bernabovi, S. and Naudin, P. and Chapman, M.},
  title           = {Towards Energy-Efficient Heterogeneous Multicore Architectures for Edge Computing},
  journal         = {IEEE Access},
  year            = {2019},
  volume          = {7},
  pages           = {49474-49491},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In recent years, the edge computing paradigm has been attracting much attention in the Internet-of-Things domain. It aims to push the frontier of computing applications, data, and services away from the usually centralized cloud servers to the boundary of the network. The benefits of this paradigm shift include better reactivity and reliability, reduced data transfer costs toward the centralized cloud servers, and enhanced confidentiality. The design of energy-efficient edge compute nodes requires, among others, low power cores such as microprocessors. Heterogeneous architectures are key solutions to address the crucial energy-efficiency demand in modern systems. They combine various processors providing attractive power and performance trade-offs. Unfortunately, no standard heterogeneous microcontroller-based architecture exists for edge computing. This paper deals with the aforementioned issue by exploring typical low power architectures for edge computing. Various heterogeneous multicore designs are developed and prototyped on FPGA for unbiased evaluation. These designs rely on cost-effective and inherently ultra-low power cores commercialized by Cortus SA, a world-leading semiconductor IP company in the embedded ultra-low power microcontroller domain. Some microarchitecture-level design considerations, e.g., floating point and out-of-order computing capabilities, are taken into account for exploring candidate solutions. In addition, a tailored and flexible multi-task programming model is defined for the proposed architecture paradigm. We analyze the behavior of various application programs on available core configurations. This provides valuable insights on the best architecture setups that match program characteristics, so as to enable increased energy-efficiency. Our experiments on multi-benchmark programs show that on average 22% energy gain can be achieved (up to 45%) compared to a reference system design, i.e., a system with the same execution architecture, but agnostic of the task management insights gained from the comprehensive evaluation carried out in this work. © 2013 IEEE.},
  affiliation     = {LIRMM-CNRS, University of Montpellier, Montpellier, 34095, France; Cortus S.A. Company, Mauguio, 34130, France},
  art_number      = {8689012},
  author_keywords = {Edge computing; embedded systems; energy-efficiency; heterogeneous multicore architectures; programming model},
  document_type   = {Article},
  doi             = {10.1109/ACCESS.2019.2910932},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85065149456&doi=10.1109%2fACCESS.2019.2910932&partnerID=40&md5=2b19c983944e0b23e5f0c6ce93124b90},
}

@Article{Ding2018,
  author          = {Ding, X.F.},
  title           = {GooStats: A GPU-based framework for multi-variate analysis in particle physics},
  journal         = {Journal of Instrumentation},
  year            = {2018},
  volume          = {13},
  number          = {12},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {GooStats is a software framework that provides a flexible environment and common tools to implement multi-variate statistical analysis. The framework is built upon the CERN ROOT, MINUIT and GooFit packages. Running a multi-variate analysis in parallel on graphics processing units yields a huge boost in performance and opens new possibilities. The design and benchmark of GooStats are presented in this article along with illustration of its application to statistical problems. © 2018 IOP Publishing Ltd and Sissa Medialab.},
  affiliation     = {Gran Sasso Science Institute, L'Aquila, 67100, Italy; INFN Laboratori Nazionali Del Gran Sasso, Assergi (AQ), 67010, Italy},
  art_number      = {P12018},
  author_keywords = {Analysis and statistical methods; Pattern recognition, cluster finding, calibration and fitting methods; Software architectures (event data models, frameworks and databases)},
  document_type   = {Article},
  doi             = {10.1088/1748-0221/13/12/P12018},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85059906071&doi=10.1088%2f1748-0221%2f13%2f12%2fP12018&partnerID=40&md5=4f240a081f98f756463cc8729587c1b0},
}

@Conference{Garcia2018,
  author          = {Garcia, A.M. and Schepke, C. and Girardi, A.G. and Da Silva, S.A.},
  title           = {Power consumption of parallel programming interfaces in multicore architectures: A case study},
  year            = {2018},
  pages           = {77-83},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper evaluates the power consumption of different parallel programming interfaces (PPI) in a multicore architecture. These PPIs are: PThreads, OpenMP, MPI-1 and MPI-2 (spawn). We measure the total energy and execution time of 11 applications in a single architecture, varying the number of threads/processes. The goal is to show that these applications can be used as a parallel benchmark to evaluate the power consumption of different PPIs. The results show that PThreads has the lowest power consumption among the interfaces, consuming less than the sequential version for memory-bound applications. © 2018 IEEE.},
  affiliation     = {Graduate Program of Electrical Engineering, Federal University of Pampa, Alegrete, Brazil},
  art_number      = {8748921},
  author_keywords = {Energy; MPI; OpenMP; Parallel benchmark; Parallel programing; POSIX Threads},
  document_type   = {Conference Paper},
  doi             = {10.1109/WSCAD.2018.00021},
  journal         = {Proceedings - 2018 Symposium on High-Performance Computing Systems, WSCAD 2018},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85069878213&doi=10.1109%2fWSCAD.2018.00021&partnerID=40&md5=e2e05295102a64c45c2c44ea0863a540},
}

@Article{SouzaPinto2018,
  author          = {de Souza Pinto, R. and Botazzo Delbem, A.C. and Monaco, F.J.},
  title           = {Characterization of runtime resource usage from analysis of binary executable programs},
  journal         = {Applied Soft Computing Journal},
  year            = {2018},
  volume          = {71},
  pages           = {1133-1152},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper introduces a methodology for characterizing the runtime resource demands of a computer program from the analysis of its binary executable file. Categorization of applications according to the kind of resources required during execution – such as CPU and memory usage – is a sough-after piece of knowledge for the aims of computer system design and management. Conventional techniques available for this purpose include white-box static source code analysis and profile matching based on historical execution data. The former tends to be challenging in face of complex software architectures and requires access to the source code; the latter is dependent on the availability of reliable past data and on the selection of features yielding effective correlations with resource usage. The alternative data mining approach proposed in this paper avoids those difficulties by manipulating binary executable files. The method combines techniques from information theory, complex networks and phylogenetics to produce a hierarchical clustering of a set of executable files, which can be used to infer potential similarities in terms of runtime resource usage. The paper introduces the method's rationales and presents results of its application to characterize CPU and IO usages of benchmark applications executed on a standard PC platform. Essays carried out over a set of 80 programs from varying sources yielded numerically significant evidences that the prediction of resource usage similarity obtained by the approach is consistent with experimentally measured runtime profile. © 2017},
  affiliation     = {Department of Computer Systems, ICMC, University of São Paulo, Av. do Trabalhador São-carlense, 400, Caixa Postal 668, São Carlos, SP 13560-970, Brazil},
  author_keywords = {Computational modeling; Data mining; Data mining; Performance evaluation; Runtime profiling},
  document_type   = {Article},
  doi             = {10.1016/j.asoc.2017.12.040},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85042147458&doi=10.1016%2fj.asoc.2017.12.040&partnerID=40&md5=e6db15d938b837fc0bb156e84cd26d5e},
}

@Article{Taneja2018,
  author          = {Taneja, S. and Zhou, Y. and Qin, X.},
  title           = {Thermal benchmarking and modeling for HPC using big data applications},
  journal         = {Future Generation Computer Systems},
  year            = {2018},
  volume          = {87},
  pages           = {372-381},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Characterizing thermal profiles of cluster nodes is an integral part of any approach that addresses thermal emergencies in a data center. Most existing thermal models make use of CPU utilization to estimate power consumption, which in turn facilitates outlet-temperature predictions. Such utilization-based thermal models may introduce errors due to inaccurate mappings from system utilization to outlet temperatures. To address this concern in the existing models, we eliminate utilization models as a middleman from the thermal model. In this paper, we propose a thermal model, tModel, that projects outlet temperatures from inlet temperatures as well as directly measured multicore temperatures rather than deploying a utilization model. In the first phase of this work, we perform extensive experimentation by varying applications types, their input data sizes, and cluster sizes. Simultaneously, we collect inlet, outlet, and multicore temperatures of cluster nodes running these diverse bigdata applications. The proposed thermal model estimates the outlet air temperature of the nodes to predict cooling costs. We validate the accuracy of our model against data gathered by thermal sensors in our cluster. Our results demonstrate that tModel estimates outlet temperatures of the cluster nodes with much higher accuracy over CPU-utilization based models. We further show that tModel is conducive of estimating the cooling cost of data centers using the predicted outlet temperatures. © 2018 Elsevier B.V.},
  affiliation     = {Department of Computer Science and Software Engineering in Auburn University, Auburn, AL 36849-5347, United States},
  author_keywords = {Benchmarking; BigData; Distributed computing; Hadoop; HPC clusters; MapReduce applications; Multicore architecture; Thermal model; Thermal profiling},
  document_type   = {Article},
  doi             = {10.1016/j.future.2018.05.004},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85047497401&doi=10.1016%2fj.future.2018.05.004&partnerID=40&md5=1ca075fa1d7b0701b5982b8d2defe8bb},
}

@Conference{Carle2018,
  author          = {Carle, T. and Cassé, H.},
  title           = {Reducing timing interferences in real-time applications running on multicore architectures},
  year            = {2018},
  volume          = {63},
  pages           = {31-312},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {We introduce a unified wcet analysis and scheduling framework for real-time applications deployed on multicore architectures. Our method does not follow a particular programming model, meaning that any piece of existing code (in particular legacy) can be re-used, and aims at reducing automatically the worst-case number of timing interferences between tasks. Our method is based on the notion of Time Interest Points (TIPS), which are instructions that can generate and/or suffer from timing interferences. We show how such points can be extracted from the binary code of applications and selected prior to performing the wcet analysis. We then represent real-time tasks as sequences of time intervals separated by TIPS, and schedule those tasks so that the overall makespan (including the potential timing penalties incurred by interferences) is minimized. This scheduling phase is performed using an Integer Linear Programming (ILP) solver. Preliminary results on state-of-the-art benchmarks show promising results and pave the way for future extensions of the model and optimizations. © Thomas Carle and Hugues Cassé.},
  affiliation     = {Université Paul Sabatier, IRIT, CNRS, Toulouse, France},
  author_keywords = {Multicore architecture; Time interest points; WCET},
  document_type   = {Conference Paper},
  doi             = {10.4230/OASIcs.WCET.2018.3},
  journal         = {OpenAccess Series in Informatics},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85054999705&doi=10.4230%2fOASIcs.WCET.2018.3&partnerID=40&md5=b0372f5d91c1ebda2f2afd72733ea21e},
}

@Conference{Schub2018,
  author          = {Schub, M. and Boano, C.A. and Romer, K.},
  title           = {Moving beyond Competitions: Extending D-Cube to Seamlessly Benchmark Low-Power Wireless Systems},
  year            = {2018},
  pages           = {30-35},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance comparisons of low-power wireless systems are often not substantiated by accurate and realistic evaluations, which raises the need of a proper benchmark. In a first attempt towards a rigorous comparison of protocol performance under the exact same settings, we have developed in 2016 a prototype benchmarking infrastructure called D-Cube, and used it to run the first of a series of competitions aiming to quantitatively assess the performance of low-power wireless protocols in specific scenarios. Given the success of the competition among both academia and industry, we have significantly extended the benchmarking infrastructure in the following two editions: D-Cube now also supports, among others, remote experimentation, multiple traffic patterns and loads, a custom description of how to derive performance metrics, and is further able to control the network density as well as the harshness of the RF environment. In this paper we perform a critical analysis of the current capabilities of D-Cube and argue that its main limiting factor is that the traffic patterns and node identities are manually embedded in the source code by developers and cannot be changed automatically. We show that we can overcome this limitation by utilizing a well-known data structure and by having developers describe its memory address using a configuration file that is passed to the benchmarking infrastructure. Following this concept, we extend D-Cube with the ability of building and applying patches to binary files and show that this allows not only to automatically change traffic patterns and node identities, but to also change user-defined protocol parameters. We believe that this extension is one of the last missing stepping stones to make D-Cube a full-fledged benchmarking infrastructure for low-power wireless systems. © 2018 IEEE.},
  affiliation     = {Institute of Technical Informatics, Graz University of Technology, Austria},
  art_number      = {8429499},
  author_keywords = {benchmarking; competition; dependability; IoT; low power wireless; measurement; performance; testbeds},
  document_type   = {Conference Paper},
  doi             = {10.1109/CPSBench.2018.00012},
  journal         = {Proceedings - 2018 1st Workshop on Benchmarking Cyber-Physical Networks and Systems, CPSBench 2018},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85052538956&doi=10.1109%2fCPSBench.2018.00012&partnerID=40&md5=ad0479ec7622f4f5cae2cc95675a653d},
}

@Article{Omar2018,
  author          = {Omar, H. and Shi, Q. and Ahmad, M. and Dogan, H. and Khan, O.},
  title           = {Declarative resilience: A holistic soft-error resilient multicore architecture that trades off program accuracy for efficiency},
  journal         = {ACM Transactions on Embedded Computing Systems},
  year            = {2018},
  volume          = {17},
  number          = {4},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {To protect multicores from soft-error perturbations, research has explored various resiliency schemes that provide high soft-error coverage. However, these schemes incur high performance and energy overheads.We observe that not all soft-error perturbations affect program correctness, and some soft-errors only affect program accuracy, i.e., the program completes with certain acceptable deviations from error free outcome. Thus, it is practical to improve processor efficiency by trading off resiliency overheads with program accuracy. This article proposes the idea of declarative resilience that selectively applies strong resiliency schemes for code regions that are crucial for program correctness (crucial code) and lightweight resiliency for code regions that are susceptible to program accuracy deviations as a result of soft-errors (non-crucial code). At the application level, crucial and non-crucial code is identified based on its impact on the program outcome. A cross-layer architecture enables efficient resilience along with holistic soft-error coverage. Only program accuracy is compromised in the worst-case scenario of a soft-error strike during non-crucial code execution. For a set of machine-learning and graph analytic benchmarks, declarative resilience reduces performance overhead over a state-of-the-art system that applies strong resiliency for all program code regions from ∼ 1.43× to ∼ 1.2×. © 2018 ACM.},
  affiliation     = {Unit 4157, University of Connecticut, 371 Fairfield Way, Storrs, CT 06269, United States},
  art_number      = {76},
  author_keywords = {Graph analytics; Machine learning; Program accuracy; Soft-errors},
  document_type   = {Conference Paper},
  doi             = {10.1145/3210559},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85052683020&doi=10.1145%2f3210559&partnerID=40&md5=f4b1a5e8f6f67bd7ba6fd9b9cb8a1d34},
}

@Conference{Kugele2018,
  author          = {Kugele, S. and Hettler, D. and Peter, J.},
  title           = {Data-Centric Communication and Containerization for Future Automotive Software Architectures},
  year            = {2018},
  pages           = {65-74},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Context: The functional interconnection and data routing in today's automotive electric/electronic architectures has reached a level of complexity which is hardly manageable and error-prone. This circumstance severely hinders short times from development to operation. Aim: The purpose of the study is to evaluate the feasibility of Data Distribution Services in accord with containerization technologies in an agile development process for automotive software. Method: We propose to represent services by means of topics in a data-centric publish-subscribe approach. We conduct performance benchmarks to evaluate its aptitude and present a case study illustrating fail-operational behavior in a setup recreated from highly automated driving. Results: Backed by the results and the case study we show that containerized services, along with data-centric messaging, manage to meet most of our proposed requirements. We furthermore reveal limitations of the used technology stack and discuss remedies to their shortcomings. © 2018 IEEE.},
  affiliation     = {Technical University of Munich, Germany; Ludwig Maximilian University of Munich, Germany},
  art_number      = {8417118},
  author_keywords = {Automotive-SOA; Containerization; Data-centricity},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICSA.2018.00016},
  journal         = {Proceedings - 2018 IEEE 15th International Conference on Software Architecture, ICSA 2018},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85051140144&doi=10.1109%2fICSA.2018.00016&partnerID=40&md5=72ee1456b00233d2f485476a4578f036},
}

@Conference{Bilal2018,
  author          = {Bilal, M. and Alsibyani, H. and Canini, M.},
  title           = {Mitigating network side channel leakage for stream processing systems in trusted execution environments},
  year            = {2018},
  pages           = {16-27},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {A crucial concern regarding cloud computing is the confidentiality of sensitive data being processed in the cloud. Trusted Execution Environments (TEEs), such as Intel Software Guard eXtensions (SGX), allow applications to run securely on an untrusted platform. However, using TEEs alone for stream processing is not enough to ensure privacy as network communication patterns may leak information about the data. This paper introduces two techniques - anycast and multicast - for mitigating leakage at inter-stage communications in streaming applications according to a user-selected mitigation level. These techniques aim to achieve network data obliviousness, i.e., communication patterns should not depend on the data. We implement these techniques in an SGX-based stream processing system. We evaluate the latency and throughput overheads, and the data obliviousness using three benchmark applications. The results show that anycast scales better with input load and mitigation level, and provides better data obliviousness than multicast. © 2018 Copyright held by the owner/author(s).},
  affiliation     = {Université catholique de Louvain, Belgium; KAUST, Saudi Arabia},
  author_keywords = {Intel SGX; Network data obliviousness; Stream processing},
  document_type   = {Conference Paper},
  doi             = {10.1145/3210284.3210286},
  journal         = {DEBS 2018 - Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85050519606&doi=10.1145%2f3210284.3210286&partnerID=40&md5=5237c3179854947d7c18e9e92e9ed9cf},
}

@Conference{Gulisano2018,
  author          = {Gulisano, V. and Strohbach, M. and Jerzak, Z. and Ziekow, H. and Smirnov, P. and Zissis, D.},
  title           = {The DEBS 2018 grand challenge},
  year            = {2018},
  pages           = {191-194},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {The ACM DEBS 2018 Grand Challenge is the eighth in a series of challenges which seek to provide a common ground and evaluation criteria for a competition aimed at both research and industrial event-based systems. The focus of the 2018 Grand Challenge is on the application of machine learning to spatio-temporal streaming data. The goal of the challenge is to make the naval transportation industry more reliable by providing predictions for vessels' destinations and arrival times. This paper describes the specifics of the data streams and queries that define the DEBS 2018 Grand Challenge. It also describes the benchmarking platform that supports testing of corresponding solutions. reliable by providing predictions for vessels' destinations and arrival times. Predicting both correct destinations and arrival times of vessels are relevant problems, that once solved, will boost the efficiency of the overall supply chain management. The challenge is co-organized by MarineTraffic, the BigDataOcean project and the HOBBIT (https://project-hobbit.eu/) project represented by AGT International (http://www.agtinternational.com/). The Grand Challenge data (presented in Section 2) is provided by the MarineTraffic and hosted by the Big Data Ocean while the automated evaluation platform (described in Section 4) is provided by the HOBBIT project. © 2018 Copyright held by the owner/author(s).},
  affiliation     = {Chalmers University of Technology, Gothenburg, Sweden; AGT International Darmstadt, Germany; SAP SE, Berlin, Germany; Furtwangen University, Furtwangen, Germany; University of the Aegean Syros, Greece},
  author_keywords = {Event processing; Maritime transportation; Streaming},
  document_type   = {Conference Paper},
  doi             = {10.1145/3210284.3220510},
  journal         = {DEBS 2018 - Proceedings of the 12th ACM International Conference on Distributed and Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85050548222&doi=10.1145%2f3210284.3220510&partnerID=40&md5=4bc4a04fb5ca3dd4796123cea7515445},
}

@Conference{Wang2018,
  author          = {Wang, X. and Shi, F. and Chen, X. and Yin, F.},
  title           = {An evaluation power model for TriBA based application mapping and memory-on-chip},
  year            = {2018},
  pages           = {2872-2879},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {An application mapping algorithm is proposed for using the multicore architecture TriBA. Meanwhile a power consumption evaluating model based topology of on-chip memory network design is presented after application mapping strategy onto various multicore architecture topologies. This paper focuses on evaluating power of on-chip memory and communication network based the approaches of Kernighan-Lin tri-partitioning algorithm strategy. Utilizing the average hops of links between the components in System-on-Chip, this method computes the average hops which instructions access memory network and communication network on chip. Experimentations with the average hops and established benchmark VOPD show that the proposed method not only can quickly and effectively estimates power consumption but also direct the improvement of Network-on-Chip architecture by using the same application mapping strategy based on topology of multicore architectures. © 2017 IEEE.},
  affiliation     = {School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China},
  author_keywords = {Application mapping; Kernighan-Lin algorithm; Memory network on chip; Power evaluation},
  document_type   = {Conference Paper},
  doi             = {10.1109/FSKD.2017.8393237},
  journal         = {ICNC-FSKD 2017 - 13th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85050229986&doi=10.1109%2fFSKD.2017.8393237&partnerID=40&md5=cf8ddf0660560cfc92de55edac315b2c},
}

@Article{Atmojo2018,
  author          = {Atmojo, U.D. and Salcic, Z. and Wang, K.I.-K.},
  title           = {Dynamic Reconfiguration and Adaptation of Manufacturing Systems Using SOSJ Framework},
  journal         = {IEEE Transactions on Industrial Informatics},
  year            = {2018},
  volume          = {14},
  number          = {6},
  pages           = {2353-2363},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {One of the key challenges in modern manufacturing systems is how to dynamically reconfigure software behaviors that govern machines to reflect changes in physical manufacturing process without completely resetting the entire manufacturing operation. The existing software solutions used to describe software behaviors in manufacturing systems are typically not based on formal semantics and model of computation and have limited capabilities in handling dynamic adaptation/reconfiguration. This paper presents the Service-Oriented SystemJ (SOSJ) framework that supports a new programming paradigm for designing dynamic distributed manufacturing systems. SOSJ combines the system-level language SystemJ and service-oriented architecture (SOA) paradigm to take advantages of both SystemJ's correct-by-construction formal semantics and SOA's dynamic features, respectively. The paper describes the concepts and functionalities of SOSJ, which enable dynamic reconfiguration of a typical manufacturing system. Performance benchmarks are run to compare the capabilities of SOSJ to a multiagent system framework JADE. © 2005-2012 IEEE.},
  affiliation     = {Department of Electrical Engineering and Automation, Aalto University, Espoo, 02150, Finland; Department of Electrical and Computer Engineering, University of Auckland, Auckland, 1010, New Zealand},
  author_keywords = {Dynamic software systems; industrial manufacturing; reconfiguration; service-oriented architecture (SOA)},
  document_type   = {Article},
  doi             = {10.1109/TII.2018.2808270},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85042364416&doi=10.1109%2fTII.2018.2808270&partnerID=40&md5=b71fbd76166b7db37ebfffb51b53ad9a},
}

@Article{Wermelinger2018,
  author          = {Wermelinger, F. and Rasthofer, U. and Hadjidoukas, P.E. and Koumoutsakos, P.},
  title           = {Petascale simulations of compressible flows with interfaces},
  journal         = {Journal of Computational Science},
  year            = {2018},
  volume          = {26},
  pages           = {217-225},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {We demonstrate a high throughput software for the efficient simulation of compressible multicomponent flow on high performance computing platforms. The discrete problem is represented on structured three-dimensional grids with non-uniform resolution. Discontinuous flow features are captured using a diffuse interface method. A distinguishing characteristic of the method is the proper treatment of the interface zone as a mixing region of liquid and gas. The governing equations are discretized by a Godunov-type finite volume method with explicit time stepping using a low-storage Runge-Kutta scheme. The presented flow solver Cubism-MPCF is based on our Cubism library which enables a highly optimized framework for the efficient treatment of stencil based problems on multicore architectures. The framework is general and not limited to applications in fluid dynamics. We validate our solver by classical benchmark examples. Furthermore, we examine a highly-resolved shock-induced bubble collapse and a cloud of O(103) collapsing bubbles, which demonstrate the high potential of the proposed framework and solver. © 2018 Elsevier B.V.},
  affiliation     = {Computational Science and Engineering Laboratory, ETH Zurich, Switzerland},
  author_keywords = {Cloud collapse; Compressible multicomponent flow; High performance computing; Shock-capturing methods; Shock-induced bubble collapse},
  document_type   = {Article},
  doi             = {10.1016/j.jocs.2018.01.008},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85042420326&doi=10.1016%2fj.jocs.2018.01.008&partnerID=40&md5=a4d1abdff2006f6511c4865ecfe3df59},
}

@Conference{Julian-Moreno2018,
  author          = {Julián-Moreno, G. and Leira, R. and De Vergara, J.E.L. and Gómez-Arribas, F.J. and González, I.},
  title           = {On the feasibility of 40 gbps network data capture and retention with general purpose hardware},
  year            = {2018},
  pages           = {970-978},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {New Ethernet standards, such as 40 GbE or 100 GbE, are already being deployed commercially along with their corresponding Network Interface Cards (NICs) for the servers. However, network measurement solutions are lagging behind: while there are several tools available for monitoring 10 or 20 Gbps networks, higher speeds pose a harder challenge that requires more new ideas, different from those applied previously, and so there are less applications available. In this paper, we show a system capable of capturing, timestamping and storing 40 Gbps network traffic using a tailored network driver together with Non-Volatile Memory express (NVMe) technology and the Storage Performance Development Kit (SPDK) framework. Also, we expose core ideas that can be extended for the capture at higher rates: a multicore architecture capable of synchronization with minimal overhead that reduces disordering of the received frames, methods to filter the traffic discarding unwanted frames without being computationally expensive, and the use of an intermediate buffer that allows simultaneous access from several applications to the same data and efficient disk writes. Finally, we show a testbed for a reliable benchmarking of our solution using custom DPDK traffic generators and replayers, which have been made freely available for the network measurement community. © 2018 ACM.},
  affiliation     = {NAUDIT HPCN S.L., Madrid, Spain; Univ. Autónoma de Madrid (UAM), Madrid, Spain; UAM and NAUDIT HPCN S.L., Madrid, Spain},
  author_keywords = {DPDK; Multicore architecture; Network monitoring; NVMe; Off-the-shelf systems; Packet storage; SPDK; Traffic storage},
  document_type   = {Conference Paper},
  doi             = {10.1145/3167132.3167238},
  journal         = {Proceedings of the ACM Symposium on Applied Computing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85050539616&doi=10.1145%2f3167132.3167238&partnerID=40&md5=4d135182a3723f75a8cba5913b09327e},
}

@Conference{Mahfoudhi2018,
  author          = {Mahfoudhi, R.},
  title           = {High performance recursive LU factorization for multicore systems},
  year            = {2018},
  volume          = {2017-October},
  pages           = {668-674},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The LU factorization (LUF) algorithm is an important kernel used in many Linear Algebra applications such as the resolution of systems of linear equations, the inversion of square matrices and the computation of matrix eigenvalues. It is also used in the LINPACK benchmark for ranking the Top 500 most powerful supercomputers. We propose in this paper a parallel recursive algorithm for LUF based on the 'Divide and Conquer' paradigm. A theoretical performance study permits to establish an accurate comparison between the designed algorithm and the PBLAS library. We achieved a series of experiments that permits to validate the contribution and lead to efficient performances obtained for large sized matrices i.e. up to 40% faster than SCALAPACK. © 2017 IEEE.},
  affiliation     = {Université de Tunis, Faculté des Sciences de Tunis, UR13ES38, Algorithmique Parallèl et Optimization2092, Tunisia},
  author_keywords = {Divide and Conquer; LU factorization; Multicore architecture; Parallel algorithm; PBLAS; Recursive algorithm},
  document_type   = {Conference Paper},
  doi             = {10.1109/AICCSA.2017.199},
  journal         = {Proceedings of IEEE/ACS International Conference on Computer Systems and Applications, AICCSA},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85046076407&doi=10.1109%2fAICCSA.2017.199&partnerID=40&md5=cd49a1e6b5c3d948c40833c4d96f6acb},
}

@Article{Zimniewicz2018,
  author          = {Zimniewicz, M. and Kurowski, K. and Węglarz, J.},
  title           = {Scheduling aspects in keyword extraction problem},
  journal         = {International Transactions in Operational Research},
  year            = {2018},
  volume          = {25},
  number          = {2},
  pages           = {507-522},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {The amount of big data collected during human–computer interactions requires natural language processing (NLP) applications to be executed efficiently, especially in parallel computing environments. Scalability and performance are critical in many NLP applications such as search engines or web indexers. However, there is a lack of mathematical models helping users to design and apply scheduling theory for NLP approaches. Moreover, many researchers and software architects reported various difficulties related to common NLP benchmarks. Therefore, this paper aims to introduce and demonstrate how to apply a scheduling model for a class of keyword extraction approaches. Additionally, we propose methods for the overall performance evaluation of different algorithms, which are based on processing time and correctness (quality) of answers. Finally, we present a set of experiments performed in different computing environments together with obtained results that can be used as reference benchmarks for further research in the field. © 2017 The Authors. International Transactions in Operational Research © 2017 International Federation of Operational Research Societies},
  affiliation     = {Poznan Supercomputing and Networking Center, Institute of Bioorganic Chemistry, Polish Academy of Sciences, Poznan, Poland; Institute of Computing Science, Poznan University of Technology, Poznan, Poland},
  author_keywords = {big data; evaluation; keyword extraction; natural language processing; parallel computing; scheduling model},
  document_type   = {Article},
  doi             = {10.1111/itor.12368},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85013070362&doi=10.1111%2fitor.12368&partnerID=40&md5=f757cb3895bf843c2440e726a71ad143},
}

@Article{Saber2018,
  author          = {Saber, T. and Brevet, D. and Botterweck, G. and Ventresque, A.},
  title           = {Is seeding a good strategy in multi-objective feature selection when feature models evolve?},
  journal         = {Information and Software Technology},
  year            = {2018},
  volume          = {95},
  pages           = {266-280},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Context: When software architects or engineers are given a list of all the features and their interactions (i.e., a Feature Model or FM) together with stakeholders’ preferences – their task is to find a set of potential products to suggest the decision makers. Software Product Lines Engineering (SPLE) consists in optimising those large and highly constrained search spaces according to multiple objectives reflecting the preference of the different stakeholders. SPLE is known to be extremely skill- and labour-intensive and it has been a popular topic of research in the past years. Objective: This paper presents the first thorough description and evaluation of the related problem of evolving software product lines. While change and evolution of software systems is the common case in the industry, to the best of our knowledge this element has been overlooked in the literature. In particular, we evaluate whether seeding previous solutions to genetic algorithms (that work well on the general problem) would help them to find better/faster solutions. Method: We describe in this paper a benchmark of large scale evolving FMs, consisting of 5 popular FMs and their evolutions – synthetically generated following an experimental study of FM evolution. We then study the performance of a state-of-the-art algorithm for multi-objective FM selection (SATIBEA) when seeded with former solutions. Results:Our experiments show that we can improve both the execution time and the quality of SATIBEA by feeding it with previous configurations. In particular, SATIBEA with seeds proves to converge an order of magnitude faster than SATIBEA alone. Conclusion: We show in this paper that evolution of FMs is not a trivial task and that seeding previous solutions can be used as a first step in the optimisation - unless the difference between former and current FMs is high, where seeding has a limited impact. © 2017 Elsevier B.V.},
  affiliation     = {Lero@UCD, School of Computer Science, University College Dublin, Dublin 4, Ireland; Institut Supérieur d'Informatique, de Modélisation et de leurs Applications, Clermont-Ferrand, France; Lero@UL, University of Limerick, Ireland},
  author_keywords = {Evolution; Genetic algorithm; Multi-objective; Software product lines},
  document_type   = {Article},
  doi             = {10.1016/j.infsof.2017.08.010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85028369887&doi=10.1016%2fj.infsof.2017.08.010&partnerID=40&md5=c766930554340f62a5f9660c65fcd63b},
}

@Article{Khaleghzadeh2018,
  author          = {Khaleghzadeh, H. and Deldari, H. and Reddy, R. and Lastovetsky, A.},
  title           = {Hierarchical multicore thread mapping via estimation of remote communication},
  journal         = {Journal of Supercomputing},
  year            = {2018},
  volume          = {74},
  number          = {3},
  pages           = {1321-1340},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Affinity-aware thread mapping is a method to effectively exploit cache resources in multicore processors. We propose an affinity- and architecture-aware thread mapping technique which maximizes data reuse and minimizes remote communications and cache coherency costs of multi-threaded applications. It consists of three main components: Data Sharing Estimator, Affine Mapping Finder and Maximum Speedup Predictor. Data Sharing Estimator creates application-specific data dependency signatures used by Affine Mapping Finder to determine the appropriate thread mapping of application for a given architecture. To prevent excessive thread migration, Maximum Speedup Predictor estimates the speedup of the obtained mapping and ignores it if it causes no significant performance improvement. The proposed framework is evaluated using Phoenix benchmark suite on two different multicore architectures. The proposed thread mapping approach gives 25% improvement in performance compared to default Linux scheduler. We also elucidate that affinity-based thread mapping approaches, which only consider the number of shared blocks, are not appropriate enough to accurately estimate data dependency between threads and determine the proper thread mapping. © 2017, Springer Science+Business Media, LLC.},
  affiliation     = {School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; Salman Institute of Higher Education, Mashhad, Iran},
  author_keywords = {Cache hierarchy; Data reuse; Data sharing; Inter- and intra-thread communication cost; Multicore; Thread mapping},
  document_type   = {Article},
  doi             = {10.1007/s11227-017-2176-6},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85032682476&doi=10.1007%2fs11227-017-2176-6&partnerID=40&md5=a3f9a177b2b8461b924e641d5004b6c1},
}

@Article{Zhou2018b,
  author          = {Zhou, K. and Kılınç, M.R. and Chen, X. and Sahinidis, N.V.},
  title           = {An efficient strategy for the activation of MIP relaxations in a multicore global MINLP solver},
  journal         = {Journal of Global Optimization},
  year            = {2018},
  volume          = {70},
  number          = {3},
  pages           = {497-516},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {Solving mixed-integer nonlinear programming (MINLP) problems to optimality is a NP-hard problem, for which many deterministic global optimization algorithms and solvers have been recently developed. MINLPs can be relaxed in various ways, including via mixed-integer linear programming (MIP), nonlinear programming, and linear programming. There is a tradeoff between the quality of the bounds and CPU time requirements of these relaxations. Unfortunately, these tradeoffs are problem-dependent and cannot be predicted beforehand. This paper proposes a new dynamic strategy for activating and deactivating MIP relaxations in various stages of a branch-and-bound algorithm. The primary contribution of the proposed strategy is that it does not use meta-parameters, thus avoiding parameter tuning. Additionally, this paper proposes a strategy that capitalizes on the availability of parallel MIP solver technology to exploit multicore computing hardware while solving MINLPs. Computational tests for various benchmark libraries reveal that our MIP activation strategy works efficiently in single-core and multicore environments. © 2017, Springer Science+Business Media, LLC.},
  affiliation     = {State Key Laboratory of Industrial Control Technology, College of Control Science and Engineering, Zhejiang University, Hangzhou, 310027, China; Department of Chemical Engineering, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA 15213, United States},
  author_keywords = {Global optimization; Mixed-integer linear programming; Mixed-integer nonlinear programming; Multicore architectures; Parallel computing; Portfolios of relaxations},
  document_type   = {Article},
  doi             = {10.1007/s10898-017-0559-0},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85027999187&doi=10.1007%2fs10898-017-0559-0&partnerID=40&md5=d01ae71672363fab067b99960df39ae5},
}

@Article{Jaafar2018,
  author          = {Jaafar, Y. and Bouzoubaa, K.},
  title           = {A new tool for benchmarking and assessing arabic syntactic parsers},
  journal         = {Communications in Computer and Information Science},
  year            = {2018},
  volume          = {782},
  pages           = {230-243},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This work aims to develop a Natural Language Processing (NLP) tool for benchmarking and assessing Arabic syntactic parsers. This tool is integrated within the Software Architecture For Arabic language pRocessing (SAFAR). Indeed, SAFAR contains several ANLP tools from simple preprocessing up to the semantic level. The benchmarking tool will take advantage of the available basic tools in addition to the flexibility and reusability of SAFAR. The benchmark process takes as input an evaluation corpus and one/several syntactic parsers implementations. As a result, it outputs the most common metrics used for evaluation namely: precision, recall, accuracy and F-measure. We introduced also a new metric called Gp-score which takes into account the execution time besides the accuracy. The execution time is very crucial for some tasks such as real-time automatic translators or in the context of processing huge data. This benchmarking solution will help researchers in comparing their parsers against each other; it will help as well other researchers in selecting the appropriate parser to use within their high level projects. Two Arabic syntactic parsers are evaluated to give a concrete example of this tool: The Stanford parser and the ATKS parser. © Springer International Publishing AG 2018.},
  affiliation     = {Mohammadia School of Engineers, Mohammed Vth University, Rabat, Morocco},
  author_keywords = {Arabic NLP; Benchmark; Evaluation; Syntactic parsers},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-73500-9_17},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85041113336&doi=10.1007%2f978-3-319-73500-9_17&partnerID=40&md5=93f7731e071291bcc7cc81ef2910376c},
}

@Article{Lin2018,
  author          = {Lin, J. and Chen, P. and Zheng, Z.},
  title           = {Microscope: Pinpoint performance issues with causal graphs in micro-service environments},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2018},
  volume          = {11236 LNCS},
  pages           = {3-20},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Driven by the emerging business models (e.g., digital sales) and IT technologies (e.g., DevOps and Cloud computing), the architecture of software is shifting from monolithic to microservice rapidly. Benefit from microservice, software development, and delivery processes are accelerated significantly. However, along with many micro services running in the dynamic cloud environment with complex interactions, identifying and locating the abnormal services are extraordinarily difficult. This paper presents a novel system named “Microscope” to identify and locate the abnormal services with a ranked list of possible root causes in Micro-service environments. Without instrumenting the source code of micro services, Microscope can efficiently construct a service causal graph and infer the causes of performance problems in real time. Experimental evaluations in a micro-service benchmark environment show that Microscope achieves a good diagnosis result, i.e., 88% in precision and 80% in recall, which is higher than several state-of-the-art methods. Meanwhile, it has a good scalability to adapt to large-scale micro-service systems. © Springer Nature Switzerland AG 2018.},
  affiliation     = {School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China},
  author_keywords = {Cloud computing; Kubernetes; Microservice; Root cause analytics},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-030-03596-9_1},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85056827510&doi=10.1007%2f978-3-030-03596-9_1&partnerID=40&md5=a98f746f60ccbf5e8bd99a1061722a3a},
}

@Article{2018,
  title         = {5th European Conference on Service-Oriented and Cloud Computing, ESOCC 2016},
  journal       = {Communications in Computer and Information Science},
  year          = {2018},
  volume        = {707},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 22 papers. The special focus in this conference is on Service-Oriented and Cloud Computing. The topics include: Consumer-driven API testing with performance contracts; patterns for workflow engine benchmarking; Patterns in HCI – a discussion of lessons learned; interactive dashboard for workflow engine benchmarks; a distributed cross-layer monitoring system based on QoS metrics models; continuous, trustless, and fair: Changing priorities in services computing; data integration and quality requirements in emergency services; challanges in services research: A software architecture perspective; towards a unified management of applications on heterogeneous clouds; re-powering service provisioning in federated cloud ecosystems: An algorithm combining energy sustainability and cost-saving strategies; deadlock analysis of service-oriented systems with recursion and concurrency; prediction of quality of service of software applications; impact-minimizing runtime adaptation in cloud-based data stream processing; A motivating case study for coordinating deployment of security VNF in federated cloud networks; the big bucket: An IoT cloud solution for smart waste management in smart cities; towards distributed and context-aware human-centric cyber-physical systems; application development and deployment for IoT devices; cloud migration architecture and pricing – Mapping a licensing business model for software vendors to a SaaS business model; A DMN-based approach for dynamic deployment modelling of cloud applications; cloud migration methodologies: Preliminary findings.},
  document_type = {Conference Review},
  page_count    = {285},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85042071565&partnerID=40&md5=451a318c81fbccd6bdc0c3541766f47b},
}

@Article{Cortinas2018,
  author          = {Cortiñas, A. and Luaces, M.R. and Rodeiro, T.V.},
  title           = {Storing and clustering large spatial datasets using big data technologies},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2018},
  volume          = {10819 LNCS},
  pages           = {15-24},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper we present the architecture of a system to store, query and visualize on the web large datasets of geographic information. The architecture includes a component to simulate a large number of drivers that report their position on a regular basis, an ingestion component that is generic and can acommodate three different storage technologies, a query component that aggregates the results in order to reduce the query time and the data transfered, and a web-based map viewer. In addition, we define an evaluation methodology to be used to benchmark and compare different alternatives for some components of the system, and we validate the architecture with experiments using a dataset of 40 million locations of drivers. © Springer International Publishing AG, part of Springer Nature 2018.},
  affiliation     = {Laboratorio de Bases de Datos, Universidade da Coruña, A Coruña, Spain},
  author_keywords = {Software architectures; Spatial big data; Web-based GIS},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-90053-7_3},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85047380432&doi=10.1007%2f978-3-319-90053-7_3&partnerID=40&md5=e1a634abd4d0945f6e9402f28c4ac4ee},
}

@Article{Jayan2018,
  author          = {Jayan, V.K. and Mohamed Husain, A.K.},
  title           = {Sonar data processing using multicore architecture processor and embedded linux},
  journal         = {Lecture Notes in Electrical Engineering},
  year            = {2018},
  volume          = {492},
  pages           = {313-324},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Background: Traditionally, sonar signal processing is performed on dedicated signal processors to meet the real-time processing requirements. Recently, the computational capability in embedded general purpose processors has also multiplied with the introduction of multiple cores and vector processing units. Embedded Linux is the usage of Linux kernel and various open source components in embedded systems. The key advantage of Linux and open source in the embedded system is the ability to reuse components which allows to design and develop complicated products based on existing components and to have full control of the software part of the system. This project is aimed at bringing up a bare multicore power architecture processor board with Embedded Linux, benchmarking it for performance and implementing a digital signal processing application exploiting multiple cores and SIMD units in each core. Methods: The T4240RDB processor is developed using Yocto Project. Yocto is an open embedded project used to build custom-based Linux images. Next, the processor is benchmarked for its processing capabilities and then, signal processing application is done on the processor. Findings: Yocto Project can be used to develop architecture-specific images as well as images can be modified and additional recipes and packages can be added. The T4240RDB can easily do signal processing application. Application: The T4240RDB can be integrated with the sonar subsystem and can be used for real-time data processing. © Springer Nature Singapore Pte Ltd. 2018.},
  affiliation     = {School of Electronics Engineering, VIT University, Chennai, India; NPOL, DRDO, Kochi, India},
  author_keywords = {Embedded linux; PowerPC; T4240RDB; Yocto},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-981-10-8575-8_30},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85045342211&doi=10.1007%2f978-981-10-8575-8_30&partnerID=40&md5=3833fe7926f72b470dc3d53abf406f0a},
}

@Conference{Duenner2018,
  author        = {Dünner, C. and Parnell, T. and Sarigiannis, D. and Ioannou, N. and Anghel, A. and Ravi, G. and Kandasamy, M. and Pozidis, H.},
  title         = {SNaP ML: A hierarchical framework for machine learning},
  year          = {2018},
  volume        = {2018-December},
  pages         = {252-262},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {We describe a new software framework for fast training of generalized linear models. The framework, named Snap Machine Learning (Snap ML), combines recent advances in machine learning systems and algorithms in a nested manner to reflect the hierarchical architecture of modern computing systems. We prove theoretically that such a hierarchical system can accelerate training in distributed environments where intra-node communication is cheaper than inter-node communication. Additionally, we provide a review of the implementation of Snap ML in terms of GPU acceleration, pipelining, communication patterns and software architecture, highlighting aspects that were critical for achieving high performance. We evaluate the performance of Snap ML in both single-node and multi-node environments, quantifying the benefit of the hierarchical scheme and the data streaming functionality, and comparing with other widely-used machine learning software frameworks. Finally, we present a logistic regression benchmark on the Criteo Terabyte Click Logs dataset and show that Snap ML achieves the same test loss an order of magnitude faster than any of the previously reported results, including those obtained using TensorFlow and scikit-learn. © 2018 Curran Associates Inc..All rights reserved.},
  affiliation   = {IBM Research, Zurich, Switzerland; IBM Systems, Bangalore, India},
  document_type = {Conference Paper},
  journal       = {Advances in Neural Information Processing Systems},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85064818595&partnerID=40&md5=4e0b9bf06a672b5697d83fc90d24dd04},
}

@Article{Benitez-Hidalgo2018,
  author          = {Benítez-Hidalgo, A. and Nebro, A.J. and Durillo, J.J. and García-Nieto, J. and López-Camacho, E. and Barba-González, C. and Aldana-Montes, J.F.},
  title           = {About designing an observer pattern-based architecture for a multi-objective metaheuristic optimization framework},
  journal         = {Studies in Computational Intelligence},
  year            = {2018},
  volume          = {798},
  pages           = {50-60},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multi-objective optimization with metaheuristics is an active and popular research field which is supported by the availability of software frameworks providing algorithms, benchmark problems, quality indicators and other related components. Most of these tools follow a monolithic architecture that frequently leads to a lack of flexibility when a user intends to add new features to the included algorithms. In this paper, we explore a different approach by designing a component-based architecture for a multi-objective optimization framework based on the observer pattern. In this architecture, most of the algorithmic components are observable entities that naturally allows to register a number of observers. This way, a metaheuristic is composed of a set of observable and observer elements, which can be easily extended without requiring to modify the algorithm. We have developed a prototype of this architecture and implemented the NSGA-II evolutionary algorithm on top of it as a case study. Our analysis confirms the improvement of flexibility using this architecture, pointing out the requirements it imposes and how performance is affected when adopting it. © 2018, Springer Nature Switzerland AG.},
  affiliation     = {Dept. de Lenguajes y Ciencias de la Computación, University of Malaga, Campus de Teatinos, Malaga, 29071, Spain; Leibniz Supercomputing Centre, Munich, Germany},
  author_keywords = {Metaheuristics; Multi-objective optimization; Observer pattern; Software architecture; Software framework},
  document_type   = {Book Chapter},
  doi             = {10.1007/978-3-319-99626-4_5},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85053437373&doi=10.1007%2f978-3-319-99626-4_5&partnerID=40&md5=67f00a22707a01accfbb0e260e914b15},
}

@Article{2018a,
  title         = {13th International Conference on Knowledge Management in Organizations, KMO 2018},
  journal       = {Communications in Computer and Information Science},
  year          = {2018},
  volume        = {877},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 60 papers. The special focus in this conference is on Knowledge Management in Organizations. The topics include: Knowledge sharing in a virtual community: Business case in China; collaboration and knowledge sharing as a key to success of entrepreneurial ecosystem; model-driven design of a mobile product training and learning system; exploring knowledge transfer in the media industry; automated monitoring of collaborative working environments for supporting open innovation; one design issue – many solutions. Different perspectives of design thinking – case study; a survey of existing evaluation frameworks for service identification methods: Towards a comprehensive evaluation framework; a literature review on service identification challenges in service oriented architecture; design thinking application methodology for pediatric service innovation; using knowledge management in scientific work - time analysis; knowledge hub: A knowledge service platform to facilitate knowledge creation cycle in a university; Economic growth and gross domestic expenditure on R&D in G7 countries with some benchmarking with BRICS countries: Long-run comparative synergy analyses; the impact of knowledge management and change readiness on the effectiveness of Russian private and state-owned organizations; a case study on challenges and obstacles in transforming to a data-driven business model in a financial organisation; an agent-based virtual organization for risk control in large enterprises; crowdworking as a knowledge management mechanism in the elicitation of missional software requirement; how to manage business in collaborative environment – a case study of multinational companies; politics, abusive supervision and perceived organizational support: The influence of work-family conflict and procedural justice.},
  document_type = {Conference Review},
  page_count    = {727},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85051920705&partnerID=40&md5=31feddce5b4efed48f3b8abbdd320857},
}

@Conference{Fauste2018,
  author        = {Fauste, J. and Peñataro, J.R. and Castaño, F.J. and Durá, J.A. and Cerón, J.M.C.},
  title         = {SPOC an 'alien' app in the android world to monitor spacecraft operations},
  year          = {2018},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {We present SPOC, the Satellite Portable Operations Centre. SPOC is a novel Android App developed by GMV, under ESA supervision, within the context of SMOS, one of ESA’s Earth Explorer missions. It provides systems real time access to gauge the status of the ground segment, including the ground stations passes, operations planning monitoring, and the satellite telemetry. It enables an on-call engineer to monitor ground and space segment critical variables, and to effect a timely reaction should an anomaly occur. SPOC has been developed taking into account the currently matured and consolidated state of the satellite ground segment. The tool has been designed to be compatible with existing ground subsystems and infrastructures, which are based on the standard control systems that are utilized in ESA satellite operations, such as SCOS-2000, and MUST (telemetry retrieval and archive scheme). The new App, already fully functional, is being routinely used to support SMOS payload operations outside working hours. Features are currently under expansion: visualization tools for mission planning activities, orbital position, station coverage, and additional alarm services. In this paper we dissect the SPOC Android App, its software architecture and development process, as well as its operational usage. We employ SMOS as a test case to benchmark capabilities, stressing the fact that the App can be integrated in a standard ground segment, and be easily reused in any other future mission. © 2018, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.},
  affiliation   = {European Space Astronomy Centre (ESAC), ESA, Villanueva de la Cañada, Madrid E-28692, Spain; GMV Aerospace and Defence S.A.U, Tres Cantos, Madrid E-28692, Spain; European Space Astronomy Centre (ESAC), ISDEFE, Villanueva de la Cañada, Madrid E-28692, Spain},
  art_number    = {AIAA 2018-2461},
  document_type = {Conference Paper},
  doi           = {10.2514/6.2018-2461},
  journal       = {15th International Conference on Space Operations, 2018},
  page_count    = {15},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85066480319&doi=10.2514%2f6.2018-2461&partnerID=40&md5=40ba2e43c1b03b1493b5a954c999688b},
}

@Article{Kim2018,
  author          = {Kim, M. and Noh, S. and Hyeon, J. and Hong, S.},
  title           = {Fair-share scheduling in single-ISA asymmetric multicore architecture via scaled virtual runtime and load redistribution},
  journal         = {Journal of Parallel and Distributed Computing},
  year            = {2018},
  volume          = {111},
  pages           = {174-186},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance-asymmetric multicore processors have been increasingly adopted in embedded systems due to their architectural benefits in improved performance and power savings. While fair-share scheduling is a crucial kernel service for such applications, it is still at an early stage with respect to performance-asymmetric multicore architecture. In this article, we first propose a new fair-share scheduler by adopting the notion of scaled CPU time that reflects the performance asymmetry between different types of cores. Using the scaled CPU time, we revise the virtual runtime of the completely fair scheduler (CFS) of the Linux kernel, and extend it into the scaled virtual runtime (SVR). In addition, we propose an SVR balancing algorithm that bounds the maximum SVR difference of tasks running on the same core types. The SVR balancing algorithm periodically partitions the tasks in the system into task groups and allocates them to the cores in such a way that tasks with smaller SVR receive larger SVR increments and thus proceed more quickly. We formally show the fairness property of the proposed algorithm. To demonstrate the effectiveness of the proposed approach, we implemented our approach into Linaro's scheduling framework on ARM's Versatile Express TC2 board and performed a series of experiments using the PARSEC benchmarks. The experiments show that the maximum SVR difference is only 4.09 ms in our approach, whereas it diverges indefinitely with time in the original Linaro's scheduling framework. In addition, our approach incurs a run-time overhead of only 0.4% with an increased energy consumption of only 0.69%. © 2017 Elsevier Inc.},
  affiliation     = {Department of Electrical and Computer Engineering, Seoul National University, South Korea; Department of Transdisciplinary Studies, Graduate School of Convergence Science and Technology, Seoul National University, South Korea; Automation and Systems Research Institute, Seoul National University, South Korea; Advanced Institutes of Convergence Technology, South Korea},
  author_keywords = {Fair-share scheduling; Load balancing; Multicore; Performance-asymmetry},
  document_type   = {Article},
  doi             = {10.1016/j.jpdc.2017.08.012},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85030086350&doi=10.1016%2fj.jpdc.2017.08.012&partnerID=40&md5=0adc2ccc3e31dec38dbee6c3981cf9f5},
}

@Article{ValadBeigi2018,
  author          = {ValadBeigi, M. and Safaei, F. and Pourshirazi, B.},
  title           = {Application-aware virtual paths insertion for NOCs},
  journal         = {Microelectronics Journal},
  year            = {2018},
  volume          = {45},
  number          = {4},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Network-on-chip (NoC) has rapidly become a promising alternative for complex system-on-chip architectures including recent multicore architectures. Additionally, optimizing NoC architectures with respect to different design objectives that are suitable for a particular application domain is crucial for achieving high-performance and energy-efficient customized solutions. Despite the fact that many researches have provided various solutions for different aspects of NoCs design, a comprehensive NoCs system solution has not emerged yet. This paper presents a novel methodology to provide a solution for complex on-chip communication problems to reduce power, latency and area overhead. Our proposed NoC communication architecture is based on setting up virtual source-destination paths between selected pairs of NoCs cores so that the packets belonging to distance nodes in the network can bypass intermediate routers while traveling through these virtual paths. In this scheme, the paths are constructed for an application based on its task-graph at the design time. After that, the run time scheduling mechanism is applied to improve the buffer management, virtual channel and switch allocation schemes and hence, the constructed paths are optimized dynamically. Moreover, in our design the router complexity and its overheads are reduced. Additionally, the suggested router has been implemented on Xilinx Virtex-5 FPGA family. The evaluation results captured by SPLASH-2 benchmark suite reveal that in comparison with the conventional NoC router, the proposed router takes 25% and 53% reduction in latency and energy, respectively besides 3.5% area overhead. Indeed, our experimental results demonstrate a significant reduction in the average packet latency and total power consumption with negligible area overhead. © 2014 Elsevier Ltd. All rights reserved.},
  affiliation     = {Faculty of ECE, Shahid Beheshti University G.C., Evin, 1983963113 Tehran, Iran, Iran},
  author_keywords = {Multiprocessor System-on-Chip; Network architecture; Network-on-Chip; On-chip communication},
  document_type   = {Article},
  doi             = {10.1016/j.mejo.2014.02.010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84894253626&doi=10.1016%2fj.mejo.2014.02.010&partnerID=40&md5=7b1fe0ee00cfd8c925db1dbc983e9f58},
}

@Article{Kapoor2017,
  author          = {Kapoor, P. and Arora, D. and Kumar, A.},
  title           = {Implications of discretization towards improving classification accuracy for software defect data},
  journal         = {Journal of Theoretical and Applied Information Technology},
  year            = {2017},
  volume          = {95},
  number          = {24},
  pages           = {6893-6901},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Since the advent of new software architectures, paradigms and technologies the software design and development has developed a cutting edge requirements of being on the right track in terms of software quality and reliability. This leads the prediction of defects in software at its early stages of its development. Implications of machine learning algorithms are now playing a very crucial role in classification and prediction of the possible bugs during the systems design phase. In this research work a discretization method is proposed based on the Object Oriented metrics threshold values in order to gain better classification accuracy on a given data set. For the experimentation purpose, Jedit, Lucene, tomcat, velocity, xalan and xerces software systems from NASA repositories have been considered and classification accuracies have been compared with the existing approaches with the help of open source WEKA tool. For this study, the Object Oriented CK metrics suite has been considered due to its wide applicability in software industry for software quality prediction. After experimentation it is found that Naive Bayes and Voted Perceptron, classifiers are performing well and provide highest accuracy level with the discretized dataset values. The performance of these classifiers are checked and analyzed on different performance measures like ROC, RMSE, Precision, Recall values in this research work. Result shows significant performance improvements towards classification accuracy if used with discrete features of the individual software systems. © 2005 – ongoing JATIT & LLS.},
  affiliation     = {Department of Computer Science & Engineering, Amity School of Engineering & Technology, Amity University, India; IT and Systems, Indian Institute of Management, Lucknow, India},
  author_keywords = {CK metrics; Classification; Discretization; Software defect prediction},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85039907708&partnerID=40&md5=5dea4f4dbd66c2b480f597b65f72048a},
}

@Conference{Mohamedin2017,
  author        = {Mohamedin, M. and Kishi, M.J. and Palmieri, R.},
  title         = {Shield: A middleware to tolerate CPU transient faults in multicore architectures},
  year          = {2017},
  volume        = {2017-January},
  pages         = {1-9},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Multicore architectures are increasingly becoming prone to transient faults. In this paper we present Shield, a middleware to provide transactional applications with resiliency to those faults that can happen anytime during the execution of a processor but do not cause any hardware interruption. Shield is inspired by the state machine replication approach, where computational resources are partitioned, the shared state is fully replicated, and requests are executed by all partitions in the same order. Our results using the Tilera reveal limited overhead with respect to the non-fault-tolerant approaches on most benchmarks, and an average performance gain of 1.54× over traditional byzantine fault tolerance protocols. © 2017 IEEE.},
  affiliation   = {Virginia Tech, United States; Lehigh University, United States},
  document_type = {Conference Paper},
  doi           = {10.1109/NCA.2017.8171345},
  journal       = {2017 IEEE 16th International Symposium on Network Computing and Applications, NCA 2017},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85046478104&doi=10.1109%2fNCA.2017.8171345&partnerID=40&md5=9ab9dabc64d4144aee6fd5701b87e462},
}

@Article{Hussain2017,
  author          = {Hussain, I. and Parveen, A. and Ahmad, A. and Qadri, M.Y. and Qadri, N.N. and Ahmed, J.},
  title           = {NSGA-II-based design space exploration for energy and throughput aware multicore architectures},
  journal         = {Cybernetics and Systems},
  year            = {2017},
  volume          = {48},
  number          = {6-7},
  pages           = {536-550},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures are mainstream due to ever increasing demand of throughput by modern applications. However, the suboptimal utilization of available resources in these architectures may imply an inevitable energy overhead. This energy overhead can only be avoided if the multicore systems support reconfiguration of available resources as per application demand. To achieve the target objectives (i.e., Energy efficiency with Throughput maximization) in multicore systems, many decision variables need to be optimized or analyzed to find the better trade-off. Heuristic-based approaches are aimed to provide a good-enough solution instead of a lengthy exhaustive search. This paper presents an Evolutionary Algorithm (EA)-based approach, i.e., Nondominated Sorting Genetic Algorithm-II (NSGA-II). Three decision variables, i.e., number of cores, cache size and frequency are used to find best solution. The proposed approach is validated over a set of parallel benchmarks using a cycle accurate simulator. The results show a significant amount of energy saving along with minimal impact on the throughput of the system. © 2017 Taylor & Francis Group, LLC.},
  affiliation     = {HITEC University, Taxila, Pakistan; Department of Electrical Engineering, COMSATS Institute of Information Technology, Wah Cantt, Pakistan; University of Essex, Colchester, United Kingdom},
  author_keywords = {Design space exploration; Energy; Multicore; Optimization; Throughput},
  document_type   = {Article},
  doi             = {10.1080/01969722.2017.1402433},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85045948546&doi=10.1080%2f01969722.2017.1402433&partnerID=40&md5=de5e415c34263b59e9e6c1f607a3f792},
}

@Article{GracianoNeto2017,
  author          = {Graciano Neto, V.V. and Barros Paes, C.E. and Garcés, L. and Guessi, M. and Manzano, W. and Oquendo, F. and Nakagawa, E.Y.},
  title           = {Stimuli-SoS: a model-based approach to derive stimuli generators for simulations of systems-of-systems software architectures},
  journal         = {Journal of the Brazilian Computer Society},
  year            = {2017},
  volume          = {23},
  number          = {1},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Background: Systems-of-systems (SoS) are alliances of independent and interoperable software-intensive systems. SoS often support critical domains, being required to exhibit a reliable operation, specially because people’s safety relies on their services. In this direction, simulations enable the validation of different operational scenarios in a controlled environment, allowing a benchmarking of its response as well as revealing possible breaches that could lead to failures. However, simulations are traditionally manual, demanding a high level of human intervention, being costly and error-prone. A stimuli generator could aid in by continuously providing data to trigger a SoS simulation and maintaining its operation. Methods: We established a model-based approach termed Stimuli-SoS to support the creation of stimuli generators to be used in SoS simulations. Stimuli-SoS uses software architecture descriptions for automating the creation of such generators. Specifically, this approach transforms SoSADL, a formal architectural description language for SoS, into dynamic models expressed in DEVS, a simulation formalism. We carried out a case study in which Stimuli-SoS was used to automatically produce stimuli generators for a simulation of a flood monitoring SoS. Results: We run simulations of a SoS architectural configuration with 69 constituent systems, i.e., 42 sensors, 9 crowdsourcing systems, and 18 drones. Stimuli generators were automatically generated for each type of constituent. These stimuli generators were capable of receiving the input data from the database and generating the expected stimuli for the constituents, allowing to simulate constituent systems interoperations into the flood monitoring SoS. Using Stimuli-SoS, we simulated 38 days of flood monitoring in little more than 6 h. Stimuli generators correctly forwarded data to the simulation, which was able to reproduce 29 flood alerts triggered by the SoS during a flooding event. In particular, Stimuli-SoS is almost 65 times more productive than a manual approach to producing data for the same type of simulation. Conclusions: Our approach succeeded in automatically deriving a functional stimuli generator that can reproduce environmental conditions for simulating a SoS. In particular, we presented new contributions regarding productivity and automation for the use of a model-based approach in SoS engineering. © 2017, The Author(s).},
  affiliation     = {University of São Paulo, Av. Trabalhador Sancarlense, 400, São Carlos, 13566-590, Brazil; University of South Brittany, Rue André Lwoff, Vannes, 56000, France; Universidade Federal de Goiás, Alameda das Palmeiras, Goiânia, 74690-900, Brazil; Pontifical University of São Paulo, R. Monte Alegre, São Paulo, 05014-901, Brazil},
  art_number      = {13},
  author_keywords = {Automatic generation; Model transformation; Simulation; Software architecture; Systems-of-systems},
  document_type   = {Article},
  doi             = {10.1186/s13173-017-0062-y},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85032855271&doi=10.1186%2fs13173-017-0062-y&partnerID=40&md5=2e6bb4bd95ffbfaa6d7c42d8ccc776e8},
}

@Article{Bhat2017,
  author          = {Bhat, G. and Singla, G. and Unver, A.K. and Ogras, U.Y.},
  title           = {Algorithmic Optimization of Thermal and Power Management for Heterogeneous Mobile Platforms},
  journal         = {IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
  year            = {2017},
  volume          = {26},
  number          = {3},
  pages           = {544-557},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {State-of-the-art mobile platforms are powered by heterogeneous system-on-chips that integrate multiple CPU cores, a GPU, and many specialized processors. Competitive performance on these platforms comes at the expense of increased power density due to their small form factor. Consequently, the skin temperature, which can degrade the experience, becomes a limiting factor. Since using a fan is not a viable solution for hand-held devices, there is a strong need for dynamic thermal and power management (DTPM) algorithms that can regulate temperature with minimal performance impact. This paper presents a DTPM algorithm, which uses a practical temperature prediction methodology based on system identification. The proposed algorithm dynamically computes a power budget using the predicted temperature. This budget is used to throttle the frequency and number of cores to avoid temperature violations with minimal impact on the system performance. Our experimental measurements on two different octa-core big.LITTLE processors and common Android applications demonstrate that the proposed technique predicts the temperature with less than 5% error across all benchmarks. Using this prediction, the proposed DTPM algorithm successfully regulates the maximum temperature and decreases the temperature violations by one order of magnitude while also reducing the total power consumption on average by 7% compared with the default solution. © 1993-2012 IEEE.},
  affiliation     = {School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ 85287, United States; ARM Holdings, San Jose, CA 95134, United States; Assembly and Test Technology Development, Intel Corporation, Chandler, AZ 85226, United States},
  author_keywords = {Dynamic power management; heterogeneous computing; multicore architectures; multiprocessor systems-on-chip (MPSoCs); thermal management},
  document_type   = {Article},
  doi             = {10.1109/TVLSI.2017.2770163},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85037669017&doi=10.1109%2fTVLSI.2017.2770163&partnerID=40&md5=26d4a5fe2131988004c35ab26ff5114d},
}

@Article{Shukla2017,
  author          = {Shukla, A. and Chaturvedi, S. and Simmhan, Y.},
  title           = {RIoTBench: An IoT benchmark for distributed stream processing systems},
  journal         = {Concurrency Computation},
  year            = {2017},
  volume          = {29},
  number          = {21},
  note            = {cited By 16},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Internet of Things (IoT) is an emerging technology paradigm where millions of sensors and actuators help monitor and manage physical, environmental, and human systems in real time. The inherent closed-loop responsiveness and decision making of IoT applications make them ideal candidates for using low latency and scalable stream processing platforms. Distributed stream processing systems (DSPS) hosted in cloud data centers are becoming the vital engine for real-time data processing and analytics in any IoT software architecture. But the efficacy and performance of contemporary DSPS have not been rigorously studied for IoT applications and data streams. Here, we propose RIoTBench, a real-time IoT benchmark suite, along with performance metrics, to evaluate DSPS for streaming IoT applications. The benchmark includes 27 common IoT tasks classified across various functional categories and implemented as modular microbenchmarks. Further, we define four IoT application benchmarks composed from these tasks based on common patterns of data preprocessing, statistical summarization, and predictive analytics that are intrinsic to the closed-loop IoT decision-making life cycle. These are coupled with four stream workloads sourced from real IoT observations on smart cities and smart health, with peak streams rates that range from 500 to 10 000 messages/second from up to 3 million sensors. We validate the RIoTBench suite for the popular Apache Storm DSPS on the Microsoft Azure public cloud and present empirical observations. This suite can be used by DSPS researchers for performance analysis and resource scheduling, by IoT practitioners to evaluate DSPS platforms, and even reused within IoT solutions. Copyright © 2017 John Wiley & Sons, Ltd.},
  affiliation     = {Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India},
  art_number      = {e4257},
  author_keywords = {benchmark; big data applications; dataflows; distributed stream processing; Internet of Things; performance evaluation},
  document_type   = {Article},
  doi             = {10.1002/cpe.4257},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85030641647&doi=10.1002%2fcpe.4257&partnerID=40&md5=8b8ab5746cc7df7d0a4ef0a56152e02a},
}

@Conference{Sousa2017,
  author          = {Sousa, R.C.F. and Pereira, M.M. and Pereira, F.M.Q. and Araujo, G.},
  title           = {Data Coherence Analysis and Optimization for Heterogeneous Computing},
  year            = {2017},
  pages           = {9-16},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Although heterogeneous computing has enabled impressive program speed-ups, knowledge about the architecture of the target device is still critical to reap full hardware benefits. Programming such architectures is complex and is usually done by means of specialized languages (e.g. CUDA, OpenCL). The cost of moving and keeping host/device data coherent may easily eliminate any performance gains achieved by acceleration. Although this problem has been extensively studied for multicore architectures and was recently tackled in discrete GPUs through CUDA8, no generic solution exists for integrated CPU/GPUs architectures like those found in mobile devices (e.g. ARM Mali). This paper proposes Data Coherence Analysis (DCA), a set of two data-flow analyses that determine how variables are used by host/device at each program point. It also introduces Data Coherence Optimization (DCO), a code optimization technique that uses DCA information to: (a) allocate OpenCL shared buffers between host and devices; and (b) insert appropriate OpenCL function calls into program points so as to minimize the number of data coherence operations. DCO was implemented in AClang LLVM (www.aclang.org) a compiler capable of translating OpenMP 4.X annotated loops to OpenCL kernels, thus hiding the complexity of directly programming in OpenCL. Experimental results using DCA and DCO in AClang to compile programs from the Parboil, Polybench and Rodinia benchmarks reveal performance speed-ups of up to 5.25x on an Exynos 8890 Octacore CPU with ARM Mali-T880 MP12 GPU and up to 2.03x on a 2.4 GHz dual-core Intel Core i5 processor equipped with an Intel Iris GPU unit. © 2017 IEEE.},
  affiliation     = {Institute of Computing, UNICAMP, Campinas, Brazil; Department of Computer Science, UFMG, Belo Horizonte, Brazil},
  art_number      = {8102172},
  author_keywords = {Compilers; Data coherence; Heterogeneous architectures},
  document_type   = {Conference Paper},
  doi             = {10.1109/SBAC-PAD.2017.9},
  journal         = {Proceedings - 29th International Symposium on Computer Architecture and High Performance Computing, SBAC-PAD 2017},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85041219797&doi=10.1109%2fSBAC-PAD.2017.9&partnerID=40&md5=c9868d118d49fed215e59b8d35cc6893},
}

@Article{Wang2017,
  author          = {Wang, C. and Li, X. and Chen, Y. and Zhang, Y. and Diessel, O. and Zhou, X.},
  title           = {Service-Oriented Architecture on FPGA-Based MPSoC},
  journal         = {IEEE Transactions on Parallel and Distributed Systems},
  year            = {2017},
  volume          = {28},
  number          = {10},
  pages           = {2993-3006},
  note            = {cited By 14},
  __markedentry   = {[Nichl:6]},
  abstract        = {The integration of software services-oriented architecture (SOA) and hardware multiprocessor system-on-chip (MPSoC) has been pursued for several years. However, designing and implementing a service-oriented system for diverse applications on a single chip has posed significant challenges due to the heterogeneous architectures, programming interfaces, and software tool chains. To solve the problem, this paper proposes SoSoC, a service-oriented system-on-chip framework that integrates both embedded processors and software defined hardware accelerators s as computing services on a single chip. Modeling and realizing the SOA design principles, SoSoC provides well-defined programming interfaces for programmers to utilize diverse computing resources efficiently. Furthermore, SoSoC can provide task level parallelization and significant speedup to MPSoC chip design paradigms by providing out-of-order execution scheme with hardware accelerators. To evaluate the performance of SoSoC, we implemented a hardware prototype on Xilinx Virtex5 FPGA board with EEMBC benchmarks. Experimental results demonstrate that the service componentization over original version is less than 3 percent, while the speedup for typical software Benchmarks is up to 372x. To show the portability of SoSoC, we implement the convolutional neural network as a case study on both Xilinx Zynq and Altera DE5 FPGA boards. Results show the SoSoC outperforms state-of-the-art literature with great flexibility. © 1990-2012 IEEE.},
  affiliation     = {University of Science and Technology of China, Hefei, Anhui, 230027, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, 100190, China; Department of Computer Science, Tsinghua University, Beijing, 100084, China; University of New South Wales, Sydney, NSW 2052, Australia},
  art_number      = {7920399},
  author_keywords = {multiprocessor; Service-oriented architecture; system on chip},
  document_type   = {Article},
  doi             = {10.1109/TPDS.2017.2701828},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85029953433&doi=10.1109%2fTPDS.2017.2701828&partnerID=40&md5=8832dc5e4dee1c0026d3586ce0173307},
}

@Article{DeSensi2017,
  author          = {De Sensi, D. and De Matteis, T. and Torquati, M. and Mencagli, G. and Danelutto, M.},
  title           = {Bringing parallel patterns out of the corner: The P3ARSEC benchmark suite},
  journal         = {ACM Transactions on Architecture and Code Optimization},
  year            = {2017},
  volume          = {14},
  number          = {4},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {High-level parallel programming is an active research topic aimed at promoting parallel programming methodologies that provide the programmer with high-level abstractions to develop complex parallel software with reduced time to solution. Pattern-based parallel programming is based on a set of composable and customizable parallel patterns used as basic building blocks in parallel applications. In recent years, a considerable effort has been made in empowering this programming model with features able to overcome shortcomings of early approaches concerning flexibility and performance. In this article, we demonstrate that the approach is flexible and efficient enough by applying it on 12 out of 13 PARSEC applications. Our analysis, conducted on three different multicore architectures, demonstrates that pattern-based parallel programming has reached a good level of maturity, providing comparable results in terms of performance with respect to both other parallel programming methodologies based on pragma-based annotations (i.e., OpenMP and OmpSs) and native implementations (i.e., Pthreads). Regarding the programming effort, we also demonstrate a considerable reduction in lines of code and code churn compared to Pthreads and comparable results with respect to other existing implementations. © 2017 ACM.},
  affiliation     = {University of Pisa, Italy},
  art_number      = {33},
  author_keywords = {Algorithmic skeletons; Benchmarking; Multicore programming; Parallel patterns; Parsec},
  document_type   = {Article},
  doi             = {10.1145/3132710},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85032636022&doi=10.1145%2f3132710&partnerID=40&md5=aa002a60907d9460ad8dc3e15b557989},
}

@Conference{Taneja2017,
  author          = {Taneja, S. and Zhou, Y. and Alghamdi, M.I. and Qin, X.},
  title           = {Thermal-Aware Job Scheduling of MapReduce Applications on High Performance Clusters},
  year            = {2017},
  pages           = {261-270},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this study, we develop a thermal-aware job scheduling strategy called tDispatch tailored for MapReduce applications running on Hadoop clusters. The scheduling idea of tDispatch is motivated by a profiling study of CPU-intensive and I/O-intensive jobs from the perspective of thermal efficiency. More specifically, we investigate the thermal behaviors of these two types of jobs running on a Hadoop cluster by stress testing data nodes through extensive experiments. We show that CPU-intensive and I/O-intensive jobs exhibit various thermal and performance impacts on multicore processors and hard drives of Hadoop cluster nodes. After we quantify the thermal behaviors of Hadoop jobs on the master and data nodes of a cluster, we propose our scheduler to alternatively dispatch CPU-intensive and I/O-intensive jobs. We apply our strategy to several MapReduce applications with different resource consumption profiles. Our experimental results show that tDispatch is conducive of creating opportunities to cool down multicore processors and disks in Hadoop clusters deployed in modern data centers. Our findings can be applied in other thermal-efficient job schedulers that are aware of thermal behaviors of CPU-intensive and I/O-intensive applications submitted to Hadoop clusters. © 2017 IEEE.},
  affiliation     = {Department of Computer Science and Software Engineering, Auburn University, Auburn, AL 36849-5347, United States; Department of Computer Science and Software Engineering, Shelby Center for Engineering Technology, Samuel Ginn College of Engineering, Auburn UniversityAL 36849-5347, United States; Department of Computer Science, Al-Baha University, Al-Baha City, Saudi Arabia},
  art_number      = {8026094},
  author_keywords = {Benchmarking; CPU-intensive and I/O-intensive jobs; Data centers; Hadoop; HiBench; MapReduce; Multicore architecture; Task placement; Thermal profiling; Thermal-aware job scheduler},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICPPW.2017.46},
  journal         = {Proceedings of the International Conference on Parallel Processing Workshops},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85030640888&doi=10.1109%2fICPPW.2017.46&partnerID=40&md5=97b0065e9aeee6ff02446130a6de08d2},
}

@Conference{2017,
  title         = {Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security Companion, QRS-C 2017},
  year          = {2017},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 124 papers. The topics discussed include: the failure behaviors of multi-faults programs: an empirical study; context-aware adaptation of mobile applications driven by software quality and user satisfaction; combinatorial methods of feature selection for cell image classification; combinatorial and MC/DC coverage levels of random testing; a parameter free choice function based hyper-heuristic strategy for pairwise test generation; can pairwise testing perform comparably to manually handcrafted testing carried out by industrial engineers?; provenance information-based trust evaluation using cooperation pattern for self-adaptive systems; highly-available applications on unreliable infrastructure: microservice architectures in practice; stochastic comparisons of used coherent system and new system of used components for non-identically distributed and dependent components; an importance based algorithm for reliability-redundancy allocation of phased mission systems; reliability analysis for a degradation system subject to dependent soft and hard failure processes; rolling bearing vibration signal analysis based on dual-entropy, holder coefficient and gray relation theory; automatic modulation recognition of digital signals based on fisherface; recognition method of software defined radio signal based on evidence theory and interval grey relation; a biological image restoration method with independently local dictionary learning; effects of improper ground truth on target tracking performance evaluation in benchmark; and a new recognition method for M-QAM signals in software defined radio.},
  document_type = {Conference Review},
  journal       = {Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security Companion, QRS-C 2017},
  page_count    = {649},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85034429858&partnerID=40&md5=1b813b08ade503e942a11591f1485271},
}

@Article{Ren2017,
  author          = {Ren, X.-G. and Wang, Q. and Xu, L.-Y. and Yang, W.-J. and Xu, X.-H.},
  title           = {HACPar: An efficient parallel multiscale framework for hybrid atomistic-continuum simulation at the micro- and nanoscale},
  journal         = {Advances in Mechanical Engineering},
  year            = {2017},
  volume          = {9},
  number          = {8},
  pages           = {1-13},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {The hybrid atomistic-continuum coupling method based on domain decomposition serves as an important tool for the microfluidic simulation. However, major modifications to existing codes are often required to enable such simulations, which pose significant difficulties. In this article, in order to provide an efficient and easy-to-use software framework for field users, we propose a hybrid atomistic-continuum parallel coupling framework, named HACPar, based on open-source software platforms. We abstract the software architecture of the hybrid atomistic-continuum coupling framework based on geometric decomposition for the first time, demonstrate the detailed implementation of the framework, and present deep research on the coupling-oriented parallel issues which may improve the flexibility and efficiency of other multiscale parallel applications. The benchmark cases verify the correctness and efficiency of our HACPar framework. The benchmark results show that the scalability of the hybrid simulations is reached up to 1536 cores. © SAGE Publications Ltd, unless otherwise noted. Manuscript content on this site is licensed under Creative Commons Licenses.},
  affiliation     = {College of Computer, National University of Defense Technology, Changsha, China; State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, Hunan, 410073, China},
  author_keywords = {easy-to-use; efficient parallel simulation; HACPar; Hybrid atomistic-continuum coupling; microfluidics simulation},
  document_type   = {Article},
  doi             = {10.1177/1687814017714730},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85028324099&doi=10.1177%2f1687814017714730&partnerID=40&md5=0ae537329d1f3125376b04b33ad2d0e1},
}

@Article{An2017,
  author          = {An, N. and Zhao, X.-G. and Hou, Z.-G.},
  title           = {3D tracker-level fusion for robust RGB-D tracking},
  journal         = {IEICE Transactions on Information and Systems},
  year            = {2017},
  volume          = {E100D},
  number          = {8},
  pages           = {1870-1881},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this study, we address the problem of online RGB-D tracking which confronted with various challenges caused by deformation, occlusion, background clutter, and abrupt motion. Various trackers have different strengths and weaknesses, and thus a single tracker can merely perform well in specific scenarios. We propose a 3D tracker-level fusion algorithm (TLF3D) which enhances the strengths of different trackers and suppresses their weaknesses to achieve robust tracking performance in various scenarios. The fusion result is generated from outputs of base trackers by optimizing an energy function considering both the 3D cube attraction and 3D trajectory smoothness. In addition, three complementary base RGB-D trackers with intrinsically different tracking components are proposed for the fusion algorithm. We perform extensive experiments on a large-scale RGB-D benchmark dataset. The evaluation results demonstrate the effectiveness of the proposed fusion algorithm and the superior performance of the proposed TLF3D tracker against state-of-the-art RGB-D trackers. Copyright © 2017 The Institute of Electronics, Information and Communication Engineers.},
  affiliation     = {Institute of Automation, Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China},
  author_keywords = {3D object tracking; Data fusion; Online video processing; RGB-D tracking},
  document_type   = {Conference Paper},
  doi             = {10.1587/transinf.2016EDP7498},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85026532040&doi=10.1587%2ftransinf.2016EDP7498&partnerID=40&md5=fa6565d046b6e9d5514a1dbcb5947f37},
}

@Conference{Marques2017,
  author          = {Marques, L.F. and Correia, R.C.M. and Spadon, G. and Eler, D.M. and Olivete-Jr, C. and Garcia, R.E.},
  title           = {Data bases available through APIs using Restify: Characteristics, programming models, and benchmarks [Banco de Dados disponível por APIs usando Restify: Características, Modelos de Programação e Análise de Desempenho]},
  year            = {2017},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The volume of data exchanged by computer networks is gradually increasing over time, which provides the need for performance and interoperability between different platforms and systems. In this line, there are several studies dedicated to service-oriented software architectures and resource consumption models. However, a few of them are focused on the development of generic tools for the dynamic creation of data provisioning services. This article presents the analysis of a tool called Restify, which is able to dynamically create web services to provide an online database as a service. Restify achieved the system interoperability requirements regarding heterogeneous operations, programming languages, and server infrastructures. As a result, we observed that the performance of this tool was comparable, if not better, than other evaluated web services, such as REST and SOAP. Finally, Restify excels by behaving like an interface tool, allowing the management and integration of multiple online system tools with various relational databases. © 2017 AISTI.},
  affiliation     = {Departamento de Matemática e Computação (DMC), Universidade Estadual Paulista (FCT/UNESP), Presidente Prudente, São Paulo, Brazil},
  art_number      = {7975715},
  author_keywords = {Database as a Service; REST; Web Service},
  document_type   = {Conference Paper},
  doi             = {10.23919/CISTI.2017.7975715},
  journal         = {Iberian Conference on Information Systems and Technologies, CISTI},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85027009850&doi=10.23919%2fCISTI.2017.7975715&partnerID=40&md5=b99d7efe66da00ce8dd20809f8d2e62d},
}

@Conference{2017a,
  title         = {Proceedings - 2017 IEEE/ACM 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering, ECASE 2017},
  year          = {2017},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 5 papers. The topics discussed include: towards a platform for empirical software design studies; benchmark requirements for microservices architecture research; Ripple: a test-aware architecture modeling framework; towards ontology-based software architecture representations; musings on the holy grail of reproducibility; and copper: bringing flexible components to the .NET framework.},
  document_type = {Conference Review},
  journal       = {Proceedings - 2017 IEEE/ACM 1st International Workshop on Establishing the Community-Wide Infrastructure for Architecture-Based Software Engineering, ECASE 2017},
  page_count    = {40},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85027465990&partnerID=40&md5=e7f509772913727fe1f244510ebdab51},
}

@Article{Yun2017,
  author          = {Yun, H. and Ali, W. and Gondi, S. and Biswas, S.},
  title           = {BWLOCK: A dynamic memory access control framework for soft real-time applications on multicore platforms},
  journal         = {IEEE Transactions on Computers},
  year            = {2017},
  volume          = {66},
  number          = {7},
  pages           = {1247-1252},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {Soft real-time applications often show bursty memory access patterns - requiring high memory bandwidth for a short duration of time - that are often critical for timely data processing and performance. We call the code sections that exhibit such characteristics as Memory-Performance Critical Sections (MPCSs). Unfortunately, in multicore architectures, non-real-time applications on different cores may also demand high memory bandwidth at the same time. Resulting bandwidth contention can substantially increase the time spent on MPCSs of soft real-time applications, which in turn could result in missed deadlines. In this paper, we present a memory access control framework called BWLOCK, which is designed to protect MPCSs of soft real-time applications. BWLOCK consists of a user-level libarary and a kernel-level memory bandwidth control mechanism. The user-level library provides a lock-like API to declare MPCSs for real-time applications. When a real-time application enters a MPCS, the kernel-level bandwidth control mechanism dynamically throttles memory bandwidth of the rest of the cores to protect the MPCS, until it is completed. We evaluate BWLOCK using CortexSuite benchmarks. By selectively applying BWLOCK, based on the memory intensity of the code blocks in each benchmark, we achieve significant performance improvements, up to 150 percent reduction in slowdown, at a controllable throughput impact to non real-time applications. © 2016 IEEE.},
  affiliation     = {University of KansasKS 66045, United States; Bose Corporation, Framingham, MA 01701, United States; Walmart, David Glass Technology Center, Bentonville, AR 72712, United States},
  art_number      = {7784697},
  author_keywords = {Memory bandwidth; Multicore; Operating systems; Real-time systems},
  document_type   = {Article},
  doi             = {10.1109/TC.2016.2640961},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85025589900&doi=10.1109%2fTC.2016.2640961&partnerID=40&md5=86e2147d949354f7721a6acdc54e8493},
}

@Conference{Gesvindr2017,
  author          = {Gesvindr, D. and Michalkova, J. and Buhnova, B.},
  title           = {System for collection and processing of smart home sensor data},
  year            = {2017},
  pages           = {247-250},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper reports on our experience with software architecture design of a PaaS cloud application for processing and collection of sensor data from smart homes. In this paper we discuss the design decisions and cloud architectural tactics that were employed to ensure that the application fulfills given quality criteria, especially high scalability and very low operation costs. As an integral part of this work, we have used a set of performance benchmarks to evaluate the effect of our decisions on the targeted quality criteria. © 2017 IEEE.},
  affiliation     = {1Faculty of Informatics, Masaryk University, Brno, Czech Republic; TapHome, Bratislava, Slovakia},
  art_number      = {7958497},
  author_keywords = {Architectural Tactics; Microsoft Azure; PaaS Cloud; Smart Home},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICSAW.2017.23},
  journal         = {Proceedings - 2017 IEEE International Conference on Software Architecture Workshops, ICSAW 2017: Side Track Proceedings},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85025591341&doi=10.1109%2fICSAW.2017.23&partnerID=40&md5=211fab58474ab7dcefc307cb326fa6b9},
}

@Conference{Hesse2017,
  author          = {Hesse, G. and Matthies, C. and Reissaus, B. and Uflacker, M.},
  title           = {Doctoral symposium: A new application benchmark for data stream processing architectures in an enterprise context},
  year            = {2017},
  pages           = {359-362},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Against the backdrop of ever-growing data volumes and trends like the Internet of Things (IoT) or Industry 4.0, Data Stream Processing Systems (DSPSs) or data stream processing architectures in general receive a greater interest. Continuously analyzing streams of data allows immediate responses to environmental changes. A challenging task in that context is assessing and comparing data stream processing architectures in order to identify the most suitable one for certain settings. The present paper provides an overview about performance benchmarks that can be used for analyzing data stream processing applications. By describing shortcomings of these benchmarks, the need for a new application benchmark in this area, especially for a benchmark covering enterprise architectures, is highlighted. A key role in such an enterprise context is the combination of streaming data and business data, which is barely covered in current data stream processing benchmarks. Furthermore, first ideas towards the development of a solution, i.e., a new application benchmark that is able to fill the existing gap, are depicted. © 2017 Copyright held by the owner/author(s).},
  affiliation     = {Hasso Plattner Institute, August-Bebel-Str. 88, Potsdam, 14482, Germany},
  author_keywords = {Benchmark development; Data stream processing; Internet of Things; Performance benchmarking; Stream processing},
  document_type   = {Conference Paper},
  doi             = {10.1145/3093742.3093902},
  journal         = {DEBS 2017 - Proceedings of the 11th ACM International Conference on Distributed Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85022230511&doi=10.1145%2f3093742.3093902&partnerID=40&md5=7448d8a0ea3d0dc70b13627fc57f44d2},
}

@Conference{Gulisano2017,
  author          = {Gulisano, V. and Jerzak, Z. and Katerinenko, R. and Strohbach, M. and Ziekow, H.},
  title           = {The DEBS 2017 grand challenge},
  year            = {2017},
  pages           = {271-273},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {The ACM DEBS 2017 Grand Challenge is the seventh in a series of challenges which seek to provide a common ground and evaluation criteria for a competition aimed at both research and industrial event-based systems. The focus of the 2017 Grand Challenge is on the analysis of the RDF streaming data generated by digital and analogue sensors embedded within manufacturing equipment. The analysis aims at the detection of anomalies in the behavior of such manufacturing equipment. This paper describes the specifics of the data streams and continuous queries that define the DEBS 2017 Grand Challenge. It also describes the benchmarking platform that supports testing of corresponding solutions. © 2017 Copyright held by the owner/author(s).},
  affiliation     = {Chalmers University of Technology, Hörsalsvägen 11, Gothenburg, 41296, Sweden; SAP SE, Münzstraße 15, Berlin, 10178, Germany; AGT International, Hilpertstrasse 35, Darmstadt, 64295, Germany; Hochschule Furtwangen, Robert-Gerwig-Platz 1, Furtwangen, 78120, Germany},
  author_keywords = {Event processing; Manufacturing; Streaming},
  document_type   = {Conference Paper},
  doi             = {10.1145/3093742.3096342},
  journal         = {DEBS 2017 - Proceedings of the 11th ACM International Conference on Distributed Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85023163075&doi=10.1145%2f3093742.3096342&partnerID=40&md5=8b28aa9dfb697968e4803528c849b0e7},
}

@Conference{Akram2017,
  author          = {Akram, N. and Siriwardene, S. and Jayasinghe, M. and Dayarathna, M. and Perera, I. and Fernando, S. and Perera, S. and Bandara, U. and Suhothayan, S.},
  title           = {Grand challenge: Anomaly detection of manufacturing equipment via high performance RDF data stream processing},
  year            = {2017},
  pages           = {280-285},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The ACM DEBS Grand Challenge 2017 focuses on anomaly detection of manufacturing equipment. The goal of the challenge is to detect abnormal behavior of a manufacturing machine based on the observations of the stream of measurements provided. The data produced by each sensor is clustered and the state transitions between the observed clusters are modeled as a Markov chain. In this paper we present how we used WSO2 Data Analytics Server (DAS), an open source, comprehensive enterprise data analytics platform, to solve the problem. On the HOBBIT (Holistic Benchmarking of Big Linked Data) platform our solution processed 35 megabytes/second with an end-to-end mean latency of 7.5 ms at an input rate of 1 ms, while the events spent only 1 ms time on average within our grand challenge solution. The paper describes the solution we propose, the experiments' results and presents how we optimized the performance of our solution. © 2017 ACM.},
  affiliation     = {WSO2, Inc., 787, Castro Street, Mountain View, CA 94041, United States},
  author_keywords = {Complex event processing; Data clustering; Data stream processing; Machine learning; Markov model; Software performance},
  document_type   = {Conference Paper},
  doi             = {10.1145/3093742.3095100},
  journal         = {DEBS 2017 - Proceedings of the 11th ACM International Conference on Distributed Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85023183735&doi=10.1145%2f3093742.3095100&partnerID=40&md5=f8332a9d15e1261a061bc589ce4ed74a},
}

@Conference{Voudouris2017,
  author        = {Voudouris, P. and Stenström, P. and Pathan, R.},
  title         = {Timing-anomaly free dynamic scheduling of task-based parallel applications},
  year          = {2017},
  pages         = {365-376},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Multicore architectures can provide high predictable performance through parallel processing. Unfortunately, computing the makespan of parallel applications is overly pessimistic either due to load imbalance issues plaguing static scheduling methods or due to timing anomalies plaguing dynamic scheduling methods. This paper contributes with an anomaly-free dynamic scheduling method, called Lazy, which is non-preemptive and non-greedy in the sense that some ready tasks may not be dispatched for execution even if some processors are idle. Assuming parallel applications using contemporary taskbased parallel programming models, such as OpenMP, the general idea of Lazy is to avoid timing anomalies by assigning fixed priorities to the tasks and then dispatch selective highestpriority ready tasks for execution at each scheduling point. We formally prove that Lazy is timing-anomaly free. Unlike all the commonly-used dynamic schedulers like breadth-first and depth-first schedulers (e.g., CilkPlus) that rely on analytical approaches to determine an upper bound on the makespan of parallel application, a safe makespan of a parallel application is computed by simulating Lazy. Our experimental results show that the makespan computed by simulating Lazy is much tighter and scales better as demonstrated by four parallel benchmarks from a task-parallel benchmark suite in comparison to the state-of-the-art. © 2017 IEEE.},
  affiliation   = {Chalmers University of Technology, Sweden},
  art_number    = {7939054},
  document_type = {Conference Paper},
  doi           = {10.1109/RTAS.2017.2},
  journal       = {Proceedings of the IEEE Real-Time and Embedded Technology and Applications Symposium, RTAS},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85021832988&doi=10.1109%2fRTAS.2017.2&partnerID=40&md5=d26a4674a549698c3f26596687d6db67},
}

@Article{Barcelona2017,
  author          = {Barcelona, M.A. and García-Borgoñón, L. and López-Nicolás, G.},
  title           = {Practical experiences in the usage of MIDAS in the logistics domain},
  journal         = {International Journal on Software Tools for Technology Transfer},
  year            = {2017},
  volume          = {19},
  number          = {3},
  pages           = {325-339},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, we present the experience in the usage of MIDAS, an integrated framework for Service Oriented Architecture (SOA) testing automation that is available as Software as a Service (SaaS) in a cloud infrastructure, to test a GS1 Logistics Interoperability Model (GS1 LIM) compliant service architecture for the logistics domain. Activities performed, results achieved and the evaluation of success factors and key performance indicators (KPIs) are detailed as well as other insights: (1) 25 % of companies would pay for model-based testing (MBT), (2) GS1 LIM should be certifiable, and (3) companies identify as a major barrier how to calculate the MBT return on investment (ROI). © 2016, Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Aragón Institute of Technology, c/ María de Luna 7, Zaragoza, 50018, Spain},
  author_keywords = {Case study; Logistics domain; Model based testing; Testing automation},
  document_type   = {Article},
  doi             = {10.1007/s10009-016-0430-5},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84975528887&doi=10.1007%2fs10009-016-0430-5&partnerID=40&md5=0730ea4344a608b543303efac0ca8223},
}

@Article{Lastra-Diaz2017,
  author          = {Lastra-Díaz, J.J. and García-Serrano, A. and Batet, M. and Fernández, M. and Chirigati, F.},
  title           = {HESML: A scalable ontology-based semantic similarity measures library with a set of reproducible experiments and a replication dataset},
  journal         = {Information Systems},
  year            = {2017},
  volume          = {66},
  pages           = {97-118},
  note            = {cited By 16},
  __markedentry   = {[Nichl:6]},
  abstract        = {This work is a detailed companion reproducibility paper of the methods and experiments proposed by Lastra-Díaz and García-Serrano in (2015, 2016) [56–58], which introduces the following contributions: (1) a new and efficient representation model for taxonomies, called PosetHERep, which is an adaptation of the half-edge data structure commonly used to represent discrete manifolds and planar graphs; (2) a new Java software library called the Half-Edge Semantic Measures Library (HESML) based on PosetHERep, which implements most ontology-based semantic similarity measures and Information Content (IC) models reported in the literature; (3) a set of reproducible experiments on word similarity based on HESML and ReproZip with the aim of exactly reproducing the experimental surveys in the three aforementioned works; (4) a replication framework and dataset, called WNSimRep v1, whose aim is to assist the exact replication of most methods reported in the literature; and finally, (5) a set of scalability and performance benchmarks for semantic measures libraries. PosetHERep and HESML are motivated by several drawbacks in the current semantic measures libraries, especially the performance and scalability, as well as the evaluation of new methods and the replication of most previous methods. The reproducible experiments introduced herein are encouraged by the lack of a set of large, self-contained and easily reproducible experiments with the aim of replicating and confirming previously reported results. Likewise, the WNSimRep v1 dataset is motivated by the discovery of several contradictory results and difficulties in reproducing previously reported methods and experiments. PosetHERep proposes a memory-efficient representation for taxonomies which linearly scales with the size of the taxonomy and provides an efficient implementation of most taxonomy-based algorithms used by the semantic measures and IC models, whilst HESML provides an open framework to aid research into the area by providing a simpler and more efficient software architecture than the current software libraries. Finally, we prove the outperformance of HESML on the state-of-the-art libraries, as well as the possibility of significantly improving their performance and scalability without caching using PosetHERep. © 2017 The Authors},
  affiliation     = {NLP & IR Research Group E.T.S.I. Informática, Universidad Nacional de Educación a Distancia (UNED), C/Juan del Rosal 16, 28040 Madrid, Spain; Internet Interdisciplinary Institute (IN3), Universitat Oberta de Catalunya, Av. Carl Friedrich Gauss, 5, Castelldefels, 08860, Spain; Knowledge Media Institute, The Open University, Walton Hall. Mlton Keynes. MK7 6AA, United Kingdom; Department of Computer Science and Engineering, New York University, New York City, NY, United States},
  author_keywords = {HESML; Intrinsic and corpus-based Information Content models; Ontology-based semantic similarity measures; PosetHERep; Reproducible experiments on word similarity; ReproZip; Semantic measures library; WNSimRep v1 dataset; WordNet-based semantic similarity measures},
  document_type   = {Article},
  doi             = {10.1016/j.is.2017.02.002},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85015963191&doi=10.1016%2fj.is.2017.02.002&partnerID=40&md5=88e42bd0256f79d1029c1302b8121754},
}

@Conference{Doyle2017,
  author        = {Doyle, N.C. and Matthews, E. and Holland, G. and Fedorova, A. and Shannon, L.},
  title         = {Performance impacts and limitations of hardware memory access trace collection},
  year          = {2017},
  pages         = {506-511},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {In today's multicore architectures, complex interactions between applications in the memory system can have a significant and highly variable impact on application execution time. System designers typically use hardware counters to profile execution behaviours and diagnose performance problems. However, hardware counters are not always sufficient and some problems are best identified with full memory access traces. Collecting these traces in software is very expensive; our work explores using dedicated hardware for memory-access trace collection. We analyze the limitations of this approach and its impacts on application performance. Our study is performed on actual hardware using two very different CPU platforms: 1) the PolyBlaze multicore soft processor and 2) the ARM Cortex-A9. In both cases, the data collection is implemented on an FPGA. Using micro-benchmarks designed to test the bounds of memory access behaviour, we illustrate the operational regions of data collection and the impact on system performance. By examining the bandwidth bottlenecks that limit the rate of data collection, as well as hardware architecture choices that can aggravate the impact on application performance, we provide guidelines that can be used to extrapolate our analysis to other systems and processor architectures. © 2017 IEEE.},
  affiliation   = {School of Engineering Science, Simon Fraser University, Burnaby, Canada; Dept. of Electrical and Computer Engineering, University of British Columbia, Vancouver, Canada},
  art_number    = {7927041},
  document_type = {Conference Paper},
  doi           = {10.23919/DATE.2017.7927041},
  journal       = {Proceedings of the 2017 Design, Automation and Test in Europe, DATE 2017},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85020233900&doi=10.23919%2fDATE.2017.7927041&partnerID=40&md5=bc328dd1bab2ebbe5b017ee015cba784},
}

@Conference{Wang2017a,
  author        = {Wang, Y. and Kent, K.B.},
  title         = {A Region-Based Approach to Pipeline Parallelism in Java Programs on Multicores},
  year          = {2017},
  pages         = {124-131},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {As multicore architectures dominate mainstream computing platforms, migrating legacy applications into their parallel representation becomes a viable approach to reaping the benefits of multicore computing. In this paper we present a dataflow analysis tool that assists programmers to exploit the coarse-grained pipeline parallelism in stream-like Java applications on multicores. With this tool, programmers can partition a source Java program into a set of regions, which as pipeline stages, are connected via data channels to execute on multicores. To this end, we propose a simple yet effective framework that leverages JVMTI (JVM Tool Interface) and Javaagent techniques to track the data communication patterns among different regions, whereby a stream graph of the program is constructed. The graph is further used by the framework and programmers to re-factor the Java application into a pipelined program so that the potential of the multicores can be fully utilized. This procedure can be repeated in several rounds to progressively improve the performance. By applying this tool to several selected benchmarks, we demonstrate the effectiveness of the approach in terms of the performance improvements of some stream-like Java applications. © 2017 IEEE.},
  affiliation   = {Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, China; CAS-Atlantic, Faculty of Computer Science, University of New Brunswick, Fredericton, Canada},
  art_number    = {7912634},
  document_type = {Conference Paper},
  doi           = {10.1109/PDP.2017.69},
  journal       = {Proceedings - 2017 25th Euromicro International Conference on Parallel, Distributed and Network-Based Processing, PDP 2017},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85019594575&doi=10.1109%2fPDP.2017.69&partnerID=40&md5=e1fc79d2200603463ec88e3316149db2},
}

@Book{Bermbach2017,
  title         = {Cloud service benchmarking: Measuring quality of cloud services from a client perspective},
  year          = {2017},
  author        = {Bermbach, D. and Wittern, E. and Tai, S.},
  note          = {cited By 16},
  __markedentry = {[Nichl:6]},
  abstract      = {Cloud service benchmarking can provide important, sometimes surprising insights into the quality of services and leads to a more quality-driven design and engineering of complex software architectures that use such services. Starting with a broad introduction to the field, this book guides readers step-by-step through the process of designing, implementing and executing a cloud service benchmark, as well as understanding and dealing with its results. It covers all aspects of cloud service benchmarking, i.e., both benchmarking the cloud and benchmarking in the cloud, at a basic level. The book is divided into five parts: Part I discusses what cloud benchmarking is, provides an overview of cloud services and their key properties, and describes the notion of a cloud system and cloud-service quality. It also addresses the benchmarking lifecycle and the motivations behind running benchmarks in particular phases of an application lifecycle. Part II then focuses on benchmark design by discussing key objectives (e.g., repeatability, fairness, or understandability) and defining metrics and measurement methods, and by giving advice on developing own measurement methods and metrics. Next, Part III explores benchmark execution and implementation challenges and objectives as well as aspects like runtime monitoring and result collection. Subsequently, Part IV addresses benchmark results, covering topics such as an abstract process for turning data into insights, data preprocessing, and basic data analysis methods. Lastly, Part V concludes the book with a summary, suggestions for further reading and pointers to benchmarking tools available on the Web. The book is intended for researchers and graduate students of computer science and related subjects looking for an introduction to benchmarking cloud services, but also for industry practitioners who are interested in evaluating the quality of cloud services or who want to assess key qualities of their own implementations through cloud-based experiments. © 2017 Springer International Publishing AG. All Rights Reserved.},
  affiliation   = {Information Systems Engineering, Research Group, Technische Universität Berlin, Berlin, Germany; IBM T.J. Watson Research Center, New York, NY, United States},
  document_type = {Book},
  doi           = {10.1007/978-3-319-55483-9},
  journal       = {Cloud Service Benchmarking: Measuring Quality of Cloud Services from a Client Perspective},
  pages         = {1-167},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85020498620&doi=10.1007%2f978-3-319-55483-9&partnerID=40&md5=347876bb4a93e556cdeaccefa66576a7},
}

@Article{Pei2017,
  author          = {Pei, X.-F. and Yuan, Q.-P. and Huang, Y. and Luo, Z.-P. and Xiao, B.-J.},
  title           = {Integration of real-time GPU parallel equilibrium reconstruction in EAST plasma control system},
  journal         = {Hejubian Yu Dengliziti Wuli/Nuclear Fusion and Plasma Physics},
  year            = {2017},
  volume          = {37},
  number          = {1},
  pages           = {105-112},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {In order to improve the accuracy and efficiency of plasma real time equilibrium reconstruction, a faster parallel code named P-EFIT has taken the place of RT-EFIT. After briefly outlining the hardware and software architecture of the EAST PCS, the integration of the parallel plasma equilibrium reconstruction code P-EFIT into EAST PCS is described. Benchmark tests are carried out to verify the integrated system's correctness using history experimental data. P-EFIT was applied for plasma shape control in EAST operation campaign and the control performance is satisfied. © 2017, Editorial Board of Journal of Nuclear Fusion and Plasma Physics. All right reserved.},
  affiliation     = {Institute of Plasma Physics, Chinese Academy of Science, Hefei, 230031, China},
  author_keywords = {EAST PCS; Graphic processing units; Parallel equilibrium reconstruction; Plasma shape control},
  document_type   = {Article},
  doi             = {10.16568/j.0254-6086.201701019},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85018398422&doi=10.16568%2fj.0254-6086.201701019&partnerID=40&md5=8e7105d651e2793ae1e7011b638ac919},
}

@Article{Ozturk2017,
  author          = {Ozturk, O. and Orhan, U. and Wei, D. and Yedlapalli, P. and Kandemir, M.T.},
  title           = {Cache Hierarchy-Aware Query Mapping on Emerging Multicore Architectures},
  journal         = {IEEE Transactions on Computers},
  year            = {2017},
  volume          = {66},
  number          = {3},
  pages           = {403-415},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {One of the important characteristics of emerging multicores/manycores is the existence of 'shared on-chip caches,' through which different threads/processes can share data (help each other) or displace each other's data (hurt each other). Most of current commercial multicore systems on the market have on-chip cache hierarchies with multiple layers (typically, in the form of L1, L2 and L3, the last two being either fully or partially shared). In the context of database workloads, exploiting full potential of these caches can be critical. Motivated by this observation, our main contribution in this work is to present and experimentally evaluate a cache hierarchy-aware query mapping scheme targeting workloads that consist of batch queries to be executed on emerging multicores. Our proposed scheme distributes a given batch of queries across the cores of a target multicore architecture based on the affinity relations among the queries. The primary goal behind this scheme is to maximize the utilization of the underlying on-chip cache hierarchy while keeping the load nearly balanced across domain affinities. Each domain affinity in this context corresponds to a cache structure bounded by a particular level of the cache hierarchy. A graph partitioning-based method is employed to distribute queries across cores, and an integer linear programming (ILP) formulation is used to address locality and load balancing concerns. We evaluate our scheme using the TPC-H benchmarks on an Intel Xeon based multicore. Our solution achieves up to 25 percent improvement in individual query execution times and 15-19 percent improvement in throughput over the default Linux-based process scheduler. © 1968-2012 IEEE.},
  affiliation     = {Bilkent University, Bilkent, Ankara, 06800, Turkey; Amazon Inc., Seattle, WA 98109-5210, United States; Qualcomm Innovation Center Inc., San Diego, CA 92121, United States; VMware Inc., Palo Alto, CA 94304, United States; Pennsylvania State University, State College, PA 16801, United States},
  art_number      = {7559783},
  author_keywords = {architecture; cache; multicore; Query; schedule},
  document_type   = {Conference Paper},
  doi             = {10.1109/TC.2016.2605682},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85027353711&doi=10.1109%2fTC.2016.2605682&partnerID=40&md5=56d3e344b6a5fd33e71643014412f227},
}

@Article{Mondragon-Gonzalez2017,
  author        = {Mondragón-González, S.L. and Burguière, E.},
  title         = {Bio-inspired benchmark generator for extracellular multi-unit recordings},
  journal       = {Scientific Reports},
  year          = {2017},
  volume        = {7},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The analysis of multi-unit extracellular recordings of brain activity has led to the development of numerous tools, ranging from signal processing algorithms to electronic devices and applications. Currently, the evaluation and optimisation of these tools are hampered by the lack of ground-truth databases of neural signals. These databases must be parameterisable, easy to generate and bio-inspired, i.e. containing features encountered in real electrophysiological recording sessions. Towards that end, this article introduces an original computational approach to create fully annotated and parameterised benchmark datasets, generated from the summation of three components: neural signals from compartmental models and recorded extracellular spikes, non-stationary slow oscillations, and a variety of different types of artefacts. We present three application examples. (1) We reproduced in-vivo extracellular hippocampal multi-unit recordings from either tetrode or polytrode designs. (2) We simulated recordings in two different experimental conditions: anaesthetised and awake subjects. (3) Last, we also conducted a series of simulations to study the impact of different level of artefacts on extracellular recordings and their influence in the frequency domain. Beyond the results presented here, such a benchmark dataset generator has many applications such as calibration, evaluation and development of both hardware and software architectures.},
  affiliation   = {Sorbonne Universités, UPMC Univ Paris 06, CNRS, INSERM, Institut du Cerveau et de la Moelle Épinière (ICM), Paris, F-75013, France},
  art_number    = {43253},
  document_type = {Article},
  doi           = {10.1038/srep43253},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85013832150&doi=10.1038%2fsrep43253&partnerID=40&md5=f28359be2311be9f666900de5aea7acf},
}

@Article{Pruijt2017,
  author          = {Pruijt, L. and Köppe, C. and van der Werf, J.M. and Brinkkemper, S.},
  title           = {The accuracy of dependency analysis in static architecture compliance checking},
  journal         = {Software - Practice and Experience},
  year            = {2017},
  volume          = {47},
  number          = {2},
  pages           = {273-309},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Architecture compliance checking (ACC) is an approach to verify conformance of implemented program code to high-level models of architectural design. Static ACC focuses on the modular software architecture and on the existence of rule violating dependencies between modules. Accurate tool support is essential for effective and efficient ACC. This paper presents a study on the accuracy of ACC tools regarding dependency analysis and violation reporting. Ten tools were tested and compared by means of a custom-made benchmark. The Java code of the benchmark testware contains 34 different types of dependencies, which are based on an inventory of dependency types in object oriented program code. In a second test, the code of open source system FreeMind was used to compare the 10 tools on the number of reported rule violating dependencies and the exactness of the dependency and violation messages. On the average, 77% of the dependencies in our custom-made test software were reported, while 72% of the dependencies within a module of FreeMind were reported. The results show that all tools in the test could improve the accuracy of the reported dependencies and violations, though large differences between the 10 tools were observed. We have identified 10 hard-to-detect types of dependencies and four challenges in dependency detection. The relevance of our findings is substantiated by means of a frequency analysis of the hard-to-detect types of dependencies in five open source systems. © 2016 The Authors. Software: Practice and Experience Published by John Wiley & Sons, Ltd. © 2016 The Authors. Software: Practice and Experience Published by John Wiley & Sons, Ltd.},
  affiliation     = {HU University of Applied Sciences, Utrecht, Netherlands; HAN University of Applied Sciences, Arnhem, Netherlands; University Utrecht, Utrecht, Netherlands},
  author_keywords = {architecture compliance; benchmark test; dependency analysis; Software architecture},
  document_type   = {Conference Paper},
  doi             = {10.1002/spe.2421},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84978280514&doi=10.1002%2fspe.2421&partnerID=40&md5=fbedc2bcff57d6d07c992957ad9aaf9f},
}

@Conference{Wang2017b,
  author          = {Wang, Y. and An, H. and Liu, Z. and Liu, T. and Zhao, D.},
  title           = {Parallelizing back propagation neural network on speculative multicores},
  year            = {2017},
  pages           = {902-907},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Applications typically exhibit extremely different performance characteristics depending on the accelerator. Back propagation neural network (BPNN) has been parallelized into different platforms. However, it has not yet been explored on speculative multicore architecture thoroughly. This paper presents a study of parallelizing BPNN on a speculative multicore architecture, including its speculative execution model, hardware design and programming model. The implementation was analyzed with seven well-known benchmark data sets. Furthermore, it trades off several important design factors in coming speculative multicore architecture. The experimental results show that: (1) the BPNN performs well on speculative multicore platform. It can achieve similar speedup (17.7x to 57.4x) compared with graphics processors (GPU) while provides a more friendly programmability. (2) 64 cores' computing resources can be used efficiently and 4k is the proper speculative buffer capacity in the model. © 2016 IEEE.},
  affiliation     = {Department of Computer Science and Technology, Southwest University of Science and Technology, MianYang, 621010, China; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, 230027, China},
  art_number      = {7823836},
  author_keywords = {Back propagation; Multicore; Parallel programming; Thread level speculation},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICPADS.2016.0121},
  journal         = {Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85018506260&doi=10.1109%2fICPADS.2016.0121&partnerID=40&md5=93cb38b4127d321e317c783f81e2f443},
}

@Conference{Letrache2017,
  author          = {Letrache, K. and El Beggar, O. and Ramdani, M.},
  title           = {Modeling and creating KPIs in MDA approach},
  year            = {2017},
  pages           = {222-227},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {The main purpose of OLAP projects is to consolidate and aggregate operational system data, allowing thus measuring, analyzing and help in decision-making. Nevertheless, such systems are rarely linked to their business context, goals or objectives that enterprise wishes to achieve, and metrics such as Key Performance Indicators (KPIs) to evaluate the degree of their satisfaction. Indeed, KPIs are measurements that gauge the business performance over time and assess the degree of achieving strategic goals. To deal with this drawback, we propose a KPI modeling and code generation solution based on the MDA approach. For that, we provide an extension of the OIM Metamodel to design KPIs and then generate the MDX code to create them. The proposal can be integrated in end-user applications to guide and allow decision maker defining and sharing their KPIs on real time without need of BI developer skills. © 2016 IEEE.},
  affiliation     = {Informatics Department, LIM Laboratory, Faculty of Sciences and Techniques of Mohammedia, University Hassan II Casablanca, Morocco},
  art_number      = {7805046},
  author_keywords = {Data warehouse; KPI; MDA; MDX; OIM; OLAP},
  document_type   = {Conference Paper},
  doi             = {10.1109/CIST.2016.7805046},
  journal         = {Colloquium in Information Science and Technology, CIST},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85010192550&doi=10.1109%2fCIST.2016.7805046&partnerID=40&md5=b6797532538366ab37f1988886a57fe8},
}

@Article{Kratzke2017,
  author          = {Kratzke, N. and Quint, P.-C.},
  title           = {Investigation of impacts on network performance in the advance of a microservice design},
  journal         = {Communications in Computer and Information Science},
  year            = {2017},
  volume          = {740},
  pages           = {187-208},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Due to REST-based protocols, microservice architectures are inherently horizontally scalable. That might be why the microservice architectural style is getting more and more attention for cloud-native application engineering. Corresponding microservice architectures often rely on a complex technology stack which includes containers, elastic platforms and software defined networks. Astonishingly, there are almost no specialized tools to figure out performance impacts (coming along with this microservice architectural style) in the upfront of a microservice design. Therefore, we propose a benchmarking solution intentionally designed for this upfront design phase. Furthermore, we evaluate our benchmark and present some performance data to reflect some often heard cloud-native application performance rules (or myths). © Springer International Publishing AG 2017.},
  affiliation     = {Center of Excellence for Communication, Systems and Applications, (CoSA), Lübeck University of Applied Sciences, Lübeck, 23562, Germany},
  author_keywords = {Benchmark; Cloud-native application; Cluster; Container; Elastic platform; Microservice; Network; Performance; Reference; REST; SDN; Software-defined network},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-62594-2_10},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85024389666&doi=10.1007%2f978-3-319-62594-2_10&partnerID=40&md5=70cc2c3ab9969f2013b5c80d8e4239d6},
}

@Article{DElia2017,
  author          = {D'Elia, A. and Viola, F. and Roffia, L. and Azzoni, P. and Cinotti, T.S.},
  title           = {Enabling interoperability in the internet of things: A OSGi semantic information broker implementation},
  journal         = {International Journal on Semantic Web and Information Systems},
  year            = {2017},
  volume          = {13},
  number          = {1},
  pages           = {147-167},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {Semantic Web technologies act as an interoperability glue among different formats, protocols and platforms, providing a uniform vision of heterogeneous devices and services in the Internet of Things (IoT). Semantic Web technologies can be applied to a broad range of application contexts (i.e., industrial automation, automotive, health care, defense, finance, smart cities) involving heterogeneous actors (i.e., end users, communities, public authorities, enterprises). Smart-M3 is a semantic publish-subscribe software architecture conceived to merge the Semantic Web and the IoT domains. It is based on a core component (SIB, Semantic Information Broker) where data is stored as RDF graphs, and software agents using SPARQL to update, retrieve and subscribe to changes in the data store. This article describes a OSGi SIB implementation extended with a new persistent SPARQL update primitive. The OSGi SIB performance has been evaluated and compared with the reference C implementation. Eventually, a first porting on Android is presented. Copyright © 2017, IGI Global.},
  affiliation     = {ARCES, University of Bologna, Bologna, Italy; DISI, University of Bologna, Bologna, Italy; Eurotech Group, Trento, Italy},
  author_keywords = {Android; Benchmark; OWL; Performance; Publish-subscribe; RDF; Smart-M3; SPARQL},
  document_type   = {Article},
  doi             = {10.4018/IJSWIS.2017010109},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85000415188&doi=10.4018%2fIJSWIS.2017010109&partnerID=40&md5=eaaf052b541b8b246ddccf77f85179e7},
}

@Article{Stier2017,
  author        = {Stier, C. and Werle, D. and Koziolek, A.},
  title         = {Deriving power models for architecture-level energy efficiency analyses},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2017},
  volume        = {10497 LNCS},
  pages         = {214-229},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {In early design phases and during software evolution, design-time energy efficiency analyses enable software architects to reason on the effect of design decisions on energy efficiency. Energy efficiency analyses rely on accurate power models to estimate power consumption. Deriving power models that are both accurate and usable for design time predictions requires extensive measurements and manual analysis. Existing approaches that aim to automate the extraction of power models focus on the construction of models for runtime estimation of power consumption. Power models constructed by these approaches do not allow users to identify the central set of system metrics that impact energy efficiency prediction accuracy. The identification of these central metrics is important for design time analyses, as an accurate prediction of each metric incurs modeling effort. We propose a methodology for the automated construction of multi-metric power models using systematic experimentation. Our approach enables the automated training and selection of power models for the design time prediction of power consumption. We validate our approach by evaluating the prediction accuracy of derived power models for a set of enterprise and data-intensive application benchmarks. © Springer International Publishing AG 2017.},
  affiliation   = {FZI Research Center for Information Technology, Karlsruhe, Germany; Karlsruhe Institute of Technology, Karlsruhe, Germany},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-319-66583-2_14},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85029409208&doi=10.1007%2f978-3-319-66583-2_14&partnerID=40&md5=87aabcd45cd6fafbc4a7fb9599a33272},
}

@Conference{2017b,
  title         = {ICEIS 2017 - Proceedings of the 19th International Conference on Enterprise Information Systems},
  year          = {2017},
  volume        = {3},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 213 papers. The topics discussed include: we need to discuss the relationship - an analysis of facilitators and barriers of software ecosystempartnerships; the influence of software product quality attributes on open source projects: a characterization study; a TOSCA-based programming model for interacting components of automatically deployed cloud and IoT applications; an ontology-based approach to analyzing the occurrence of code smells in software; an integrated inspection system for belt conveyor rollers - advancing in an enterprise architecture; when agile meets waterfall - investigating risks and problems on the interface between agile and traditional software development in a hybrid development organization; development of an electronic health record application using a multiple view service oriented architecture; a characterization of cloud computing adoption based on literature evidence; contact deduplication in mobile devices using textual similarity and machine learning; guidelines of data quality issues for data integration in the context of the TPC-DI benchmark; extensions, analysis and experimental assessment of a probabilistic ensemble-learning framework for detecting deviances in business process instances; audio description on Instagram: evaluating and comparing two ways of describing images for visually impaired; facial expression recognition improvement through an appearance features combination; on the development of serious games in the health sector - a case study of a serious game tool to improve life management skills in the young; and software ecosystems governance - a systematic literature review and research agenda.},
  document_type = {Conference Review},
  journal       = {ICEIS 2017 - Proceedings of the 19th International Conference on Enterprise Information Systems},
  page_count    = {2005},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85023159533&partnerID=40&md5=32fdb8d4811c11eb3a69d481064c9ddb},
}

@Conference{Beckmann2017,
  author        = {Beckmann, P.},
  title         = {Multicore SoCs for automotive audio: Performance, analysis and optimization},
  year          = {2017},
  volume        = {2017-September},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Automotive OEMs are (finally) starting to consider using centralized processing to implement all features of their infotainment systems, using a single System on a Chip (SoC) that integrates all audio processing in a single unit rather than spreading it out among multiple components. They are taking a cue from consumer products, such as mobile and PCs, which have been using this approach for years. OEMs are motivated to make the change primarily by cost savings and reduced development cost. It also leads to a simpler and more modern software architecture, which lends itself more readily to over-the-air updates. These SoCs are still relatively new, and many engineers are unsure about their performance capabilities, especially when implementing premium and high-end audio systems with more features and more sophisticated audio processing. This paper benchmarks the audio processing capabilities of several current automotive SoCs. We also consider multicore architectures and variability in cycle timing due to caches and process scheduling.},
  affiliation   = {DSP Concepts, 1800 Wyatt Dr., Santa Clara, CA 95054, United States},
  document_type = {Conference Paper},
  journal       = {Proceedings of the AES International Conference},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85034222805&partnerID=40&md5=ec6d26b9b7f727b2028b9f5a9fc6043a},
}

@Article{Zhu2017,
  author          = {Zhu, S. and Chandrasekaran, S. and Sun, P. and Chapman, B. and Winter, M. and Schuele, T.},
  title           = {Exploring task parallelism for heterogeneous systems using multicore task management API},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2017},
  volume          = {10104 LNCS},
  pages           = {697-708},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Current trends in multicore platform design indicate that heterogeneous systems are here to stay. Such systems include processors with specialized accelerators supporting different instruction sets and different types of memory spaces among several other features. These features increase the programming effort to port applications to target platforms. We need effective programming strategies that can exploit the rich feature set of such heterogeneous multicore architectures and yet not require increased learning curve to apply these strategies. To distribute workload effectively across such systems that have different cores running at different speed, we have explored task-based programming models in this paper. This model allows decomposition of a problem into a set of tasks for simultaneous execution. We present a task-based approach that employs the Multicore Association’s (MCA) Task Management API (MTAPI), a robust, cross-platform, scalable API that avoids unnecessary synchronization thus offering a tiered and flexible approach and distributing workload efficiently across processors of varying types. For evaluation purposes, we use an NVIDIA Jetson TK1 board (ARM + GPU) as our test bed. As applications, we employ codes from benchmark suites such as Rodinia and BOTS. © Springer International Publishing AG 2017.},
  affiliation     = {Department of Computer Science, University of Houston, Houston, United States; Department of Computer and Information Sciences, University of Delaware, Newark, United States; Siemens Corporate Technology, Princeton, United States},
  author_keywords = {Accelerators; Heterogeneity; MTAPI; Multicore systems; Runtime},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-58943-5_56},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85020452804&doi=10.1007%2f978-3-319-58943-5_56&partnerID=40&md5=56c4a34c2d98a41cd9ce245a1ec9152d},
}

@Article{Negele2017,
  author          = {Negele, F. and Friedrich, F. and Oh, S. and Egger, B.},
  title           = {On the design and implementation of an efficient lock-free scheduler},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2017},
  volume          = {10353 LNCS},
  pages           = {22-45},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Schedulers for symmetric multiprocessing (SMP) machines use sophisticated algorithms to schedule processes onto the available processor cores. Hardware-dependent code and the use of locks to protect shared data structures from simultaneous access lead to poor portability, the difficulty to prove correctness, and a myriad of problems associated with locking such as limiting the available parallelism, deadlocks, starvation, interrupt handling, and so on. In this work we explore what can be achieved in terms of portability and simplicity in an SMP scheduler that achieves similar performance to state-of-the-art schedulers. By strictly limiting ourselves to only lock-free data structures in the scheduler, the problems associated with locking vanish altogether. We show that by employing implicit cooperative scheduling, additional guarantees can be made that allow novel and very efficient implementations of memory-efficient unbounded lock-free queues. Cooperative multitasking has the additional benefit that it provides an extensive hardware independence. It even allows the scheduler to be used as a runtime library for applications running on top of standard operating systems. In a comparison against Windows Server and Linux running on up to 64 cores we analyze the performance of the lock-free scheduler and show that it matches or even outperforms the performance of these two state-of-the-art schedulers in a variety of benchmarks. © Springer International Publishing AG 2017.},
  affiliation     = {Department of Computer Science, ETH Zürich, Zürich, Switzerland; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea},
  author_keywords = {Cooperative multitasking; Lock-free scheduling; Multicore architectures; Run-time environments},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-61756-5_2},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85029429474&doi=10.1007%2f978-3-319-61756-5_2&partnerID=40&md5=746a8f016b0148032a659f3d01f270fe},
}

@Article{Ilic2017,
  author          = {Ilic, A. and Pratas, F. and Sousa, L.},
  title           = {Beyond the roofline: Cache-aware power and energy-efficiency modeling for multi-cores},
  journal         = {IEEE Transactions on Computers},
  year            = {2017},
  volume          = {66},
  number          = {1},
  pages           = {52-58},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {To foster the energy-efficiency in current and future multi-core processors, the benefits and trade-offs of a large set of optimization solutions must be evaluated. For this purpose, it is often crucial to consider how key micro-architecture aspects, such as accessing different memory levels and functional units, affect the attainable power and energy consumption. To ease this process, we propose a set of insightful cache-aware models to characterize the upper-bounds for power, energy and energy-efficiency of modern multi-cores in three different domains of the processor chip: cores, uncore and package. The practical importance of the proposed models is illustrated when optimizing matrix multiplication and deriving a set of power envelopes and energy-efficiency ranges of the micro-architecture for different operating frequencies. The proposed models are experimentally validated on a computing platform with a quad-core Intel 3770K processor by using hardware counters, on-chip power monitoring facilities and assembly micro-benchmarks. © 1968-2012 IEEE.},
  affiliation     = {INESC-ID, IST, Universidade de Lisboa, Lisbon, 1000-029, Portugal; Imagination Technologies Limited, London, EC4V 3BJ, United Kingdom},
  art_number      = {7493653},
  author_keywords = {modeling and simulation; Multicore architectures; performance evaluation and measurement},
  document_type   = {Article},
  doi             = {10.1109/TC.2016.2582151},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85007004930&doi=10.1109%2fTC.2016.2582151&partnerID=40&md5=5949f0e5d447abc9a4474416d95fd08f},
}

@Article{Du2017,
  author        = {Du, Z. and Ge, R. and Lee, V.W. and Vuduc, R. and Bader, D.A. and He, L.},
  title         = {Modeling the Power Variability of Core Speed Scaling on Homogeneous Multicore Systems},
  journal       = {Scientific Programming},
  year          = {2017},
  volume        = {2017},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {We describe a family of power models that can capture the nonuniform power effects of speed scaling among homogeneous cores on multicore processors. These models depart from traditional ones, which assume that individual cores contribute to power consumption as independent entities. In our approach, we remove this independence assumption and employ statistical variables of core speed (average speed and the dispersion of the core speeds) to capture the comprehensive heterogeneous impact of subtle interactions among the underlying hardware. We systematically explore the model family, deriving basic and refined models that give progressively better fits, and analyze them in detail. The proposed methodology provides an easy way to build power models to reflect the realistic workings of current multicore processors more accurately. Moreover, unlike the existing lower-level power models that require knowledge of microarchitectural details of the CPU cores and the last level cache to capture core interdependency, ours are easier to use and scalable to emerging and future multicore architectures with more cores. These attributes make the models particularly useful to system users or algorithm designers who need a quick way to estimate power consumption. We evaluate the family of models on contemporary x86 multicore processors using the SPEC2006 benchmarks. Our best model yields an average predicted error as low as 5%. © 2017 Zhihui Du et al.},
  affiliation   = {Department of Computer, Science and Technology, Tsinghua University, Beijing, China; School of Computing, Clemson University, Clemson, SC, United States; Intel Corporation, Santa Clara, CA, United States; School of Computational, Science and Engineering, Georgia Institute of Technology, Atlanta, GA, United States; Department of Computer Science, University of Warwick, Coventry, United Kingdom},
  art_number    = {8686971},
  document_type = {Article},
  doi           = {10.1155/2017/8686971},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85042611524&doi=10.1155%2f2017%2f8686971&partnerID=40&md5=7c9adea48ab6806e0813a0952c207f7a},
}

@Article{2017e,
  title         = {11th International Conference on Simulated Evolution and Learning, SEAL 2017},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2017},
  volume        = {10593 LNCS},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 85 papers. The special focus in this conference is on Simulated Evolution and Learning. The topics include: On the use of dynamic reference points in HypE; multi-factorial evolutionary algorithm based on M2M decomposition; an efficient local search algorithm for minimum weighted vertex cover on massive graphs; interactive genetic algorithm with group intelligence articulated possibilistic condition preference model; GP-based approach to comprehensive quality-aware automated semantic web service composition; matrix factorization based benchmark set analysis: A case study on HyFlex; learning to describe collective search behavior of evolutionary algorithms in solution space; a hierarchical decomposition-based evolutionary many-objective algorithm; adjusting parallel coordinates for investigating multi-objective search; evolutionary game network reconstruction by memetic algorithm with l 1/2 regularization; An elite archive-based MOEA/D algorithm; a constraint partitioning method based on minimax strategy for constrained multiobjective optimization problems; a fast objective reduction algorithm based on dominance structure for many objective optimization; a memetic algorithm based on decomposition and extended search for multi-objective capacitated arc routing problem; improvement of reference points for decomposition based multi-objective evolutionary algorithms; multi-objective evolutionary optimization for autonomous intersection management; Study of an adaptive control of aggregate functions in MOEA/D; use of inverted triangular weight vectors in decomposition-based many-objective algorithms; surrogate model assisted multi-objective differential evolution algorithm for performance optimization at software architecture level; normalized ranking based particle swarm optimizer for many objective optimization; a simple brain storm optimization algorithm via visualizing confidence intervals; a study on pre-training deep neural networks using particle swarm optimisation; multivariant optimization algorithm with bimodal-gauss.},
  document_type = {Conference Review},
  page_count    = {1038},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85034246582&partnerID=40&md5=518c52e81f4565f0bc1d4f95b4f965e3},
}

@Article{Farhat2016,
  author          = {Farhat, I. and Qadri, M.Y. and Qadri, N.N. and Ahmed, J.},
  title           = {Fuzzy Logic-Based DSE Engine: Reconfiguration for Optimization of Multicore Architectures},
  journal         = {Journal of Circuits, Systems and Computers},
  year            = {2016},
  volume          = {25},
  number          = {12},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Moore's law has been one of the reason behind the evolution of multicore architectures. Modern multicore architectures offer great amount of parallelism and on-chip resources that remain underutilized. This is partly due to inefficient resource allocation by operating system or application being executed. Consequently the poor resource utilization results in greater energy consumption and less throughput. This paper presents a fuzzy logic-based design space exploration (DSE) approach to reconfigure a multicore architecture according to workload requirements. The target design space is explored for L1 and L2 cache size and associativity, operating frequency, and number of cores, while the impact of various configurations of these parameters is analyzed on throughput, miss ratios for L1 and L2 cache and energy consumption. MARSSx86, a cycle accurate simulator, running various SPALSH-2 benchmark applications has been used to evaluate the architecture. The proposed fuzzy logic-based DSE approach resulted in reduction in energy consumption along with an overall improved throughput of the system. © 2016 World Scientific Publishing Company.},
  affiliation     = {Muhammad Nawaz Sharif University of Engineering and Technology, Multan, Pakistan; University of Esssex, Colchester, United Kingdom; COMSATS Institute of Information Technology, Wah Cantt., Pakistan; HITEC University, Taxila, Pakistan},
  art_number      = {1650160},
  author_keywords = {design space exploration; energy; fuzzy logic; Multicore architecture; throughput},
  document_type   = {Article},
  doi             = {10.1142/S0218126616501607},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84985028944&doi=10.1142%2fS0218126616501607&partnerID=40&md5=7efaecfdc6ecb521bca5e6b13f0a0a37},
}

@Article{Javed2016,
  author          = {Javed, A. and Qamar, B. and Jameel, M. and Shafi, A. and Carpenter, B.},
  title           = {Towards Scalable Java HPC with Hybrid and Native Communication Devices in MPJ Express},
  journal         = {International Journal of Parallel Programming},
  year            = {2016},
  volume          = {44},
  number          = {6},
  pages           = {1142-1172},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {MPJ Express is a messaging system that allows application developers to parallelize their compute-intensive sequential Java codes on High Performance Computing clusters and multicore processors. In this paper, we extend MPJ Express software to provide two new communication devices. The first device—called hybrid—enables MPJ Express to exploit hybrid parallelism on cluster of multicore processors by sitting on top of existing shared memory and network communication devices. The second device—called native—uses JNI wrappers in interfacing MPJ Express to native MPI implementations like MPICH and Open MPI. We evaluate performance of these devices on a range of interconnects including 1G/10G Ethernet, 10G Myrinet and 40G InfiniBand. In addition, we analyze and evaluate the cost of MPJ Express buffering layer and compare it with the performance numbers of other Java MPI libraries. Our performance evaluation reveals that the native device allows MPJ Express to achieve comparable performance to native MPI libraries—for latency and bandwidth of point-to-point and collective communications—which is a significant gain in performance compared to existing communication devices. The hybrid communication device—without any modifications at application level—also helps parallel applications achieve better speedups and scalability by exploiting multicore architecture. Our performance evaluation quantifies the cost incurred by buffering and its impact on overall performance of software. We witnessed comparative performance as both new devices improve application performance and achieve upto 90 % of the theoretical bandwidth available without application rewriting effort—including NAS Parallel Benchmarks, point-to-point and collective communication. © 2015, Springer Science+Business Media New York.},
  affiliation     = {Mohammad Ali Jinnah University (MAJU), Islamabad, Pakistan; SEECS, National University of Sciences and Technology (NUST), Islamabad, Pakistan; Information Systems and Machine Learning Lab (ISMLL), University of Hildesheim, Hildesheim, Germany; School of Computing, University of Portsmouth, Portsmouth, United Kingdom},
  author_keywords = {High Performance Computing in Java; Hybrid parallelism in Java; MPJ Express; Native communication device for Java; Parallel computing},
  document_type   = {Article},
  doi             = {10.1007/s10766-015-0375-4},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84940833796&doi=10.1007%2fs10766-015-0375-4&partnerID=40&md5=5531e4eff0cb48df7a276ed456ce1dc8},
}

@Article{Fonseca2016,
  author          = {Fonseca, A. and Cabral, B. and Rafael, J. and Correia, I.},
  title           = {Automatic Parallelization: Executing Sequential Programs on a Task-Based Parallel Runtime},
  journal         = {International Journal of Parallel Programming},
  year            = {2016},
  volume          = {44},
  number          = {6},
  pages           = {1337-1358},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {There are billions of lines of sequential code inside nowadays’ software which do not benefit from the parallelism available in modern multicore architectures. Automatically parallelizing sequential code, to promote an efficient use of the available parallelism, has been a research goal for some time now. This work proposes a new approach for achieving such goal. We created a new parallelizing compiler that analyses the read and write instructions, and control-flow modifications in programs to identify a set of dependencies between the instructions in the program. Afterwards, the compiler, based on the generated dependencies graph, rewrites and organizes the program in a task-oriented structure. Parallel tasks are composed by instructions that cannot be executed in parallel. A work-stealing-based parallel runtime is responsible for scheduling and managing the granularity of the generated tasks. Furthermore, a compile-time granularity control mechanism also avoids creating unnecessary data-structures. This work focuses on the Java language, but the techniques are general enough to be applied to other programming languages. We have evaluated our approach on 8 benchmark programs against OoOJava, achieving higher speedups. In some cases, values were close to those of a manual parallelization. The resulting parallel code also has the advantage of being readable and easily configured to improve further its performance manually. © 2016, Springer Science+Business Media New York.},
  affiliation     = {Department of Informatics Engineering, Universidade de Coimbra, Coimbra, Portugal},
  author_keywords = {Automatic parallelization; Symbolic analysis; Task-based runtime},
  document_type   = {Article},
  doi             = {10.1007/s10766-016-0426-5},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84963738908&doi=10.1007%2fs10766-016-0426-5&partnerID=40&md5=9abc3a9c8a09184c03e0484c8277d4ba},
}

@Article{Scolari2016,
  author          = {Scolari, A. and Bartolini, D.B. and Santambrogio, M.D.},
  title           = {A software cache partitioning system for hash-based caches},
  journal         = {ACM Transactions on Architecture and Code Optimization},
  year            = {2016},
  volume          = {13},
  number          = {4},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Contention on the shared Last-Level Cache (LLC) can have a fundamental negative impact on the performance of applications executed on modern multicores. An interesting software approach to address LLC contention issues is based on page coloring, which is a software technique that attempts to achieve performance isolation by partitioning a shared cache through careful memory management. The key assumption of traditional page coloring is that the cache is physically addressed. However, recent multicore architectures (e.g., Intel Sandy Bridge and later) switched from a physical addressing scheme to a more complex scheme that involves a hash function. Traditional page coloring is ineffective on these recent architectures. In this article, we extend page coloring to work on these recent architectures by proposing a mechanism able to handle their hash-based LLC addressing scheme. Just as for traditional page coloring, the goal of this new mechanism is to deliver performance isolation by avoiding contention on the LLC, thus enabling predictable performance. We implement this mechanism in the Linux kernel, and evaluate it using several benchmarks from the SPEC CPU2006 and PARSEC 3.0 suites. Our results show that our solution is able to deliver performance isolation to concurrently running applications by enforcing partitioning of a Sandy Bridge LLC, which traditional page coloring techniques are not able to handle. © 2016 ACM.},
  affiliation     = {Dipartimento di Elettronica, Informazione e Bioingegneria (DEIB), Politecnico di Milano, Via Ponzio, 34/5, Milan, 20133, Italy; Oracle Software (Schweiz) GmbH, Täfernstrasse 4, Baden-Dättwil, 5405, Switzerland},
  art_number      = {57},
  author_keywords = {Hash-based cache; Last-level cache; Linux; Operating system; Page coloring},
  document_type   = {Article},
  doi             = {10.1145/3018113},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85008146706&doi=10.1145%2f3018113&partnerID=40&md5=cf97df8380fdea50594bb68ef7969100},
}

@Conference{Majumdar2016,
  author          = {Majumdar, S. and Chatterjee, N. and Sahoo, S.R. and Das, P.P.},
  title           = {D-Cube: Tool for Dynamic Design Discovery from Multi-Threaded Applications Using PIN},
  year            = {2016},
  pages           = {25-32},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Program comprehension is a major challenge for system maintenance. Reverse engineering has been employed for control-flow analysis of applications but not much work has been done for comprehending concurrent non-deterministic behavior of multi-Threaded applications. We present D-CUBE, built using dynamic instrumentation APIs, which plugs in during execution and infers various thread models like concurrency, safety, data access, thread-pool state, exception model etc. for multi-Threaded applications at runtime. We extract run-Time events traced according to pre-specified logic and feed them to decision trees for inference. We use 3 benchmark suites (LOC: 50-3200)-CDAC Pthreads benchmark [1] (18 Cases), Open POSIX Test-Suites [2] (21 Cases) and PARSEC 3.0 benchmarks [3] (3 Cases) for accuracy and volume testing and validate our approach by comparing the documented behavior of test-suites with D-CUBE's output models. We achieve over 90% accuracy. D-CUBE produces graphical event-Traces with every inference for quick and effective comprehension of large code. © 2016 IEEE.},
  affiliation     = {Advanced Technology Development, Centre Indian Institute of Technology, Kharagpur, 721302, India; A.K.Choudhury School of Information Technology, U.C.S.T. Rajabazar, University of Calcutta, Kolkata, 700073, India; School of Information, Technology Indian Institute of Technology, Kharagpur, 721302, India; Department of Computer Science and Engineering, Indian Institute of Technology, Kharagpur, 721302, India},
  art_number      = {7589781},
  author_keywords = {Concurrency Models; Dynamic Instrumentation; Execution Patterns; Multi-Threaded Program Analysis; Program Comprehension; Reverse-Engineering},
  document_type   = {Conference Paper},
  doi             = {10.1109/QRS.2016.13},
  journal         = {Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84995467259&doi=10.1109%2fQRS.2016.13&partnerID=40&md5=ec60f9b8ccf7ba9d877dbadada9bee59},
}

@Article{Gruener2016,
  author          = {Grüner, S. and Pfrommer, J. and Palm, F.},
  title           = {RESTful Industrial Communication with OPC UA},
  journal         = {IEEE Transactions on Industrial Informatics},
  year            = {2016},
  volume          = {12},
  number          = {5},
  pages           = {1832-1841},
  note            = {cited By 35},
  __markedentry   = {[Nichl:6]},
  abstract        = {Representational state transfer (REST) is a wide-spread architecture style for decentralized applications. REST proposes the use of a fixed set of service interfaces to transfer heterogeneous resource representations instead of defining custom interfaces for individual applications. This paper explores the advantages of RESTful architectures, i.e., service-oriented software architectures comprised RESTful services, in industrial settings. These include communication advantages such as reduced communication overhead and the possibility to introduce caching layers, and system design advantages including stable service interfaces across applications and the use of resource-oriented information models in cyber-physical systems. Additionally, a RESTful extension to the open platform communications (OPC) unified architecture (OPC UA) binary protocol is proposed in order to leverage these advantages. It requires only minimal modifications to the existing OPC UA stacks and is fully backward compatible with the standard protocol. Performance benchmarks on industrial hardware show a throughput increase up to a factor of eight for short-lived interactions. This reduction of overhead is especially relevant for the use of OPC UA in the emerging Industrial Internet of Things. © 2016 IEEE.},
  affiliation     = {Department of Process Control Engineering, RWTH Aaachen University, Aachen, 52064, Germany; Fraunhofer Institute of Optronics, System Technologies, and Image Exploitation (IOSB), Karlsruhe, 76131, Germany},
  art_number      = {7407396},
  author_keywords = {Industrial communication; internet of things; machine-to-machine communications; manufacturing automation; transport protocols},
  document_type   = {Article},
  doi             = {10.1109/TII.2016.2530404},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85012057836&doi=10.1109%2fTII.2016.2530404&partnerID=40&md5=61a2ccc551ee3c33be937ea8f2695ac4},
}

@Article{Tuncali2016,
  author          = {Tuncali, C.E. and Fainekos, G. and Lee, Y.-H.},
  title           = {Automatic parallelization of multirate block diagrams of control systems on multicore platforms},
  journal         = {ACM Transactions on Embedded Computing Systems},
  year            = {2016},
  volume          = {16},
  number          = {1},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article addresses the problem of parallelizing model block diagrams for real-time embedded applications on multicore architectures. We describe a Mixed Integer Linear Programming formulation for finding a feasible mapping of the blocks to different CPU cores. For single-rate models, we use an objective function that minimizes the overall worst-case execution time. We introduce a set of heuristics to solve the problem for large models in a reasonable time. For multirate models, we solve the feasibility problem for finding a valid mapping. We study the scalability and efficiency of our approach with synthetic benchmarks and an engine controller from Toyota. © 2016 ACM.},
  affiliation     = {Arizona State University, Centerpoint Bldg. STE 203, 660 S. Mill Ave., Tempe, AZ 85281, United States},
  art_number      = {15},
  author_keywords = {Embedded control systems; Model-based development; Multicore platforms; Multirate; Optimization; Scheduling; Simulink; Task allocation},
  document_type   = {Article},
  doi             = {10.1145/2950055},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84992178569&doi=10.1145%2f2950055&partnerID=40&md5=6c3e30997ea9fec3c6e75e19f7de2fdd},
}

@Conference{Lagraviere2016,
  author        = {Lagraviere, J. and Langguth, J. and Sourouri, M. and Ha, P.H. and Cai, X.},
  title         = {On the performance and energy efficiency of the PGAS programming model on multicore architectures},
  year          = {2016},
  pages         = {800-807},
  note          = {cited By 2},
  __markedentry = {[Nichl:6]},
  abstract      = {Using large-scale multicore systems to get the maximum performance and energy efficiency with manageable programmability is a major challenge. The partitioned global address space (PGAS) programming model enhances programmability by providing a global address space over large-scale computing systems. However, so far the performance and energy efficiency of the PGAS model on multicore-based parallel architectures have not been investigated thoroughly. In this paper we use a set of selected kernels from the well-known NAS Parallel Benchmarks to evaluate the performance and energy efficiency of the UPC programming language, which is a widely used implementation of the PGAS model. In addition, the MPI and OpenMP versions of the same parallel kernels are used for comparison with their UPC counterparts. The investigated hardware platforms are based on multicore CPUs, both within a single 16-core node and across multiple nodes involving up to 1024 physical cores. On the multi-node platform we used the hardware measurement solution called High definition Energy Efficiency Monitoring tool in order to measure energy. On the single-node system we used the hybrid measurement solution to make an effort into understanding the observed performance differences, we use the Intel Performance Counter Monitor to quantify in detail the communication time, cache hit/miss ratio and memory usage. Our experiments show that UPC is competitive with OpenMP and MPI on single and multiple nodes, with respect to both the performance and energy efficiency. © 2016 IEEE.},
  affiliation   = {Simula Research Laboratory, Fornebu, NO-1364, Norway; NTNU, Norwegian University of Science and Technology, Trondheim, NO-7491, Norway; Arctic University of Norway, Tromso, NO-9037, Norway},
  art_number    = {7568416},
  document_type = {Conference Paper},
  doi           = {10.1109/HPCSim.2016.7568416},
  journal       = {2016 International Conference on High Performance Computing and Simulation, HPCS 2016},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84991706003&doi=10.1109%2fHPCSim.2016.7568416&partnerID=40&md5=492197e0538ecdcd2fafb46b89fd1fb8},
}

@Conference{Horak2016,
  author        = {Horak, D. and Riha, L. and Sojka, R. and Kruzik, J. and Beseda, M.},
  title         = {Energy consumption optimization of the Total-FETI solver and BLAS routines by changing the CPU frequency},
  year          = {2016},
  pages         = {1031-1032},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {The energy consumption of supercomputers is one of the critical problems for the upcoming Exascale supercomputing era. The awareness of power an energy consumption is required on both software and hardware side. This poster deals with the energy consumption evaluation of the Total-Finite Element Tearing and Interconnect (TFETI) based solvers [2] of linear systems implemented in PERMON toolbox [1], which is an established method for solving real-world engineering problems, and with the energy consumption evaluation of the BLAS routines. The experiments performed in the poster deal with CPU frequency. This work is performed in the scope of the READEX project (Runtime Exploitation of Application Dynamism for Energy-efficient eXascale computing) [6]. The measurements were performed on the Intel Xeon E5-2680 (Intel Haswell micro-architecture) based Taurus system installed at TU Dresden. The system contains over 1400 nodes that have an FPGA-based power instrumentation called HDEEM (High Definition Energy Efficiency Monitoring), that allows for fine-grained and more accurate power and energy measurements. The measurements can be accessed through the HDEEM library, allowing developers to take energy measurements before and after the region of interest. We have evaluated the effect of the CPU frequency on the energy consumption of the TFETI solver for a linear elasticity 3D cube synthetic benchmark. On the dualized problem MPFX=MPd, we have evaluated the effect of frequency tuning on the energy consumption of the essential processing kernels of the TFETI method. There are two main phases in TFETI - preprocessing and solve. In preprocessing it is necessary to regularize the stiffness matrix K and factorize it and to assemble the G and GGT matrices and the second one to factorize. Both operations belong to the most time and also energy consuming operations. The solve employs the Preconditioned Conjugate Gradient (PCG) algorithm, which consists of sparse matrix-vector multiplications (by F, P, ML, MD matrices) and vector dot products and AXPY functions. In each iteration, we need to apply the direct solver twice, i.e., for forward and backward solves for the pseudoinverse K+ action and for the coarse problem solution, the (GGT)-1 action. The multiplication by the dense Schur complement matrix adds an additional operator with different computational characteristics, potentially increasing the exploitable dynamism. The poster provides results for two types of frequency tuning: (1) static tuning and (2) dynamic tuning. For static tuning experiments, the frequency is set before execution and kept constant during the runtime. For dynamic tuning, the frequency is changed during the program execution to adapt the system to the actual needs of the application. The poster shows that static tuning brings up 11.84% energy savings when compared to default CPU settings (the highest clock rate). The dynamic tuning improves this further by up to 2.68%. In total, the approach presented in this paper shows the potential to save up to 14.52% of energy for TFETI based solvers, see Table1. Another energy consumption evaluations were done with selected Sparse and Dense BLAS Level 1, 2 and 3 routines. For benchmarking we have used a set of matrices from University Florida collection [4]. We have employed AXPY, Sparse Matrix-Vector, Sparse MatrixMatrix, Dense Matrix-Vector, Dense Matrix-Matrix and Sparse Matrix-Dense Matrix multiplication routines from Intel Math Kernel Library (MKL) [3]. The measured characteristics illustrate the different energy consumption of BLAS routines, as some operations are memory-bounded and others are compute-bounded. Based on our recommendations one can explore dynamic frequency switching to achieve significant energy savings up to 23%, for more details see Table 2. © 2016 IEEE.},
  affiliation   = {IT4Innovations National Supercomputing Center, VSB-Technical University of Ostrava, 17. listopadu 15, Ostrava, 70833, Czech Republic},
  art_number    = {7568453},
  document_type = {Conference Paper},
  doi           = {10.1109/HPCSim.2016.7568453},
  journal       = {2016 International Conference on High Performance Computing and Simulation, HPCS 2016},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84991665786&doi=10.1109%2fHPCSim.2016.7568453&partnerID=40&md5=47db53e590852d1763dab8b45e1f4c55},
}

@Conference{Viola2016,
  author        = {Viola, F. and D'Elia, A. and Roffia, L. and Cinotti, T.S.},
  title         = {A modular lightweight implementation of the Smart-M3 semantic information broker},
  year          = {2016},
  volume        = {2016-September},
  pages         = {370-377},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {Interoperability among heterogeneous devices is one of the main topics investigated nowadays to realize the Ubiquitous Computing vision. Smart-M3 is a software architecture born to provide interoperability through the Semantic Web technologies and reactivity thanks to the publish-subscribe paradigm. In this paper we present a new implementation in Python of the central component of the Smart-M3 architecture: the Semantic Information Broker (SIB). The new component, named pySIB, has been specifically designed for embedded or resource constrained devices. pySIB represents a new open source lightweight and portable SIB implementation, but also introduces new features and interesting performances. JSON has been introduced as the default information encoding notation as it offers the flexibility of XML with minor bandwidth requirements. Memory allocation on disk and at runtime is in the order of Kilobytes i.e. minimal, if compared with the other reference implementations. Performance tests on existing (SP2B) and ad-hoc benchmarks point out possible improvements but also encouraging data such as the best insertion time among the existing SIB implementations. © 2016 FRUCT.},
  affiliation   = {ARCES - Advanced Research Center on Electronic Systems, University of Bologna, Bologna, Italy; DISI - Department of Computer Science and Engineering, University of Bologna, Bologna, Italy},
  art_number    = {7561552},
  document_type = {Conference Paper},
  doi           = {10.1109/FRUCT-ISPIT.2016.7561552},
  journal       = {Conference of Open Innovation Association, FRUCT},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84989170116&doi=10.1109%2fFRUCT-ISPIT.2016.7561552&partnerID=40&md5=dddf82b48eb28d93653702b18aed7b75},
}

@Article{Riha2016,
  author          = {Říha, L. and Brzobohatý, T. and Markopoulos, A. and Jarošová, M. and Kozubek, T. and Horák, D. and Hapla, V.},
  title           = {Implementation of the efficient communication layer for the highly parallel total FETI and hybrid total FETI solvers},
  journal         = {Parallel Computing},
  year            = {2016},
  volume          = {57},
  pages           = {154-166},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper describes the implementation, performance, and scalability of our communication layer developed for Total FETI (TFETI) and Hybrid Total FETI (HTFETI) solvers. HTFETI is based on our variant of the Finite Element Tearing and Interconnecting (FETI) type domain decomposition method. In this approach a small number of neighboring subdomains is aggregated into clusters, which results in a smaller coarse problem. To solve the original problem TFETI method is applied twice: to the clusters and then to the subdomains in each cluster. The current implementation of the solver is focused on the performance optimization of the main CG iteration loop, including: implementation of communication hiding and avoiding techniques for global communications; optimization of the nearest neighbor communication - multiplication with a global gluing matrix; and optimization of the parallel CG algorithm to iterate over local Lagrange multipliers only. The performance is demonstrated on a linear elasticity 3D cube and real world benchmarks. © 2016 Elsevier B.V.},
  affiliation     = {IT4Innovations National Supercomputing Center, VŠB-Technical University of Ostrava, Ostrava, Czech Republic},
  author_keywords = {Domain decomposition; FETI; HPC; Hybrid total FETI; Scalability; Total FETI},
  document_type   = {Article},
  doi             = {10.1016/j.parco.2016.05.002},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84973649739&doi=10.1016%2fj.parco.2016.05.002&partnerID=40&md5=7b68d660502b9538212e17460d68a009},
}

@Article{RomeroPerez2016,
  author          = {Romero Pérez, J.A. and Sanchis Llopis, R.},
  title           = {A new method for tuning PI controllers with symmetric send-on-delta sampling strategy},
  journal         = {ISA Transactions},
  year            = {2016},
  volume          = {64},
  pages           = {161-173},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper we present a new method for tuning PI controllers with symmetric send-on-delta (SSOD) sampling strategy. First we analyze the conditions that produce oscillations in event based systems considering SSOD sampling strategy. The Describing Function is the tool used to address the problem. Once the conditions for oscillations are established, a new robustness to oscillation performance measure is introduced which entails with the concept of phase margin, one of the most traditional measures of relative stability in closed-loop control systems. Therefore, the application of the proposed robustness measure is easy and intuitive. The method is tested by both simulations and experiments. Additionally, a Java application has been developed to aid in the design according to the results presented in the paper. © 2016 ISA},
  affiliation     = {Departament d׳Enginyeria de Sistemes Industrials i Disseny, Universitat Jaume I, Campus de Riu Sec, Castelló, Spain},
  author_keywords = {Event-based system; PID controller; Tuning method},
  document_type   = {Article},
  doi             = {10.1016/j.isatra.2016.05.011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84992221603&doi=10.1016%2fj.isatra.2016.05.011&partnerID=40&md5=ab8d6badd4b38cacc54fa670364cbc41},
}

@Article{Leotardi2016,
  author          = {Leotardi, C. and Serani, A. and Iemma, U. and Campana, E.F. and Diez, M.},
  title           = {A variable-accuracy metamodel-based architecture for global MDO under uncertainty},
  journal         = {Structural and Multidisciplinary Optimization},
  year            = {2016},
  volume          = {54},
  number          = {3},
  pages           = {573-593},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {A method for simulation-based multidisciplinary robust design optimization (MRDO) of problems affected by uncertainty is presented. The challenging aspects of simulation-based MRDO are both algorithmic and computational, since the solution of a MRDO problem typically requires simulation-based multidisciplinary analyses (MDA), uncertainty quantification (UQ) and optimization. Herein, the identification of the optimal design is achieved by a variable-accuracy, metamodel-based optimization, following a multidisciplinary feasible (MDF) architecture. The approach encompasses a variable (i) density of the design of experiments for the metamodel training, (ii) sample size for the UQ analysis by quasi Monte Carlo simulation and (iii) tolerance for the multidisciplinary consistency in MDA. The focus is on two-way steady fluid-structure interaction problem, assessed by partitioned solvers for the hydrodynamic and the structural analysis. Two analytical test problems are shown, along with the design of a racing-sailboat keel fin subject to the stochastic variation of the yaw angle. The method is validated versus a standard MDF approach to MRDO, taken as a benchmark and solved by fully coupled MDA, fully converged UQ, without metamodels. The method is evaluated in terms of optimal design performances and number of simulations required to achieve the optimal solution. For the current application, the optimal configuration shows performances very close to the benchmark solution. The convergence analysis to the optimum shows a promising reduction of the computational cost. © 2016, Springer-Verlag Berlin Heidelberg.},
  affiliation     = {CNR-INSEAN, National Research Council-Marine Technology Research Institute, Rome, Italy; Department of Engineering, Roma Tre University, Rome, Italy},
  author_keywords = {Global optimization; Metamodel-based optimization; Multidisciplinary robust design optimization (MRDO); Simulation-based design; Uncertainty quantification (UQ)},
  document_type   = {Article},
  doi             = {10.1007/s00158-016-1423-4},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84962034582&doi=10.1007%2fs00158-016-1423-4&partnerID=40&md5=1724ed8114db755a8613b786dccd4d00},
}

@Article{De Matteis2016,
  author          = {De Matteis, T. and Di Girolamo, S. and Mencagli, G.},
  title           = {Continuous skyline queries on multicore architectures},
  journal         = {Concurrency Computation},
  year            = {2016},
  volume          = {28},
  number          = {12},
  pages           = {3503-3522},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {The emergence of real-time decision-making applications in domains like high-frequency trading, emergency management, and service level analysis in communication networks has led to the definition of new classes of queries. Skyline queries are a notable example. Their results consist of all the tuples whose attribute vector is not dominated (in the Pareto sense) by one of any other tuple. Because of their popularity, skyline queries have been studied in terms of both sequential algorithms and parallel implementations for multiprocessors and clusters. Within the Data Stream Processing paradigm, traditional database queries on static relations have been revised in order to operate on continuous data streams. Most of the past papers propose sequential algorithms for continuous skyline queries, whereas there exist very few works targeting implementations on parallel machines. This paper contributes to fill this gap by proposing a parallel implementation for multicore architectures. We propose (i) a parallelization of the eager algorithm based on the notion of Skyline Influence Time, (ii) optimizations of the reduce phase and load-balancing strategies to achieve near-optimal speedup, and (iii) a set of experiments with both synthetic benchmarks and a real dataset in order to show our implementation effectiveness. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.},
  affiliation     = {Department of Computer Science, University of Pisa, Largo B. Pontecorvo 3, Pisa, I-56127, Italy},
  author_keywords = {data stream processing; multicore programming; skyline queries; sliding windows},
  document_type   = {Article},
  doi             = {10.1002/cpe.3866},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84971301407&doi=10.1002%2fcpe.3866&partnerID=40&md5=39177df262299113e3b6e7df1cf8e852},
}

@Article{Oz2016,
  author          = {Oz, I.},
  title           = {Analyzing fault behavior of shared data in parallel applications},
  journal         = {Microprocessors and Microsystems},
  year            = {2016},
  volume          = {45},
  pages           = {67-80},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures are becoming the most promising computing platforms thanks to their high performance. The soft error rate in multicore systems increases by the trend in the transistor sizes and the reduction of the voltage of the transistors. Evaluating the impact of soft errors on parallel applications is critical to understand the fault characteristics and to decide the fault tolerance strategies for the reliable execution. In this paper, we examine the soft error vulnerabilities of shared data in parallel Java applications. To analyze fault behavior of shared data in parallel programs, we design and implement a bytecode instrumentation based analysis and fault injection framework. We evaluate the fault behavior of shared data fields on a set of parallel applications from NAS benchmark suite. Our experimental evaluation demonstrates data type and access characteristics of the shared fields, and shows that shared data structures of parallel applications are more vulnerable to soft errors. While error rates for unshared local data stay around 20% in our target applications, the rate for shared data exceeds above 30% for some applications. We further discuss potential directions of our results and how shared data analysis can be employed to apply partial fault tolerance techniques. © 2016 Elsevier B.V.},
  affiliation     = {Computer Engineering Department, Marmara University, Istanbul, 34722, Turkey},
  author_keywords = {Fault injection; Multicores; Reliability},
  document_type   = {Article},
  doi             = {10.1016/j.micpro.2016.03.014},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84964402234&doi=10.1016%2fj.micpro.2016.03.014&partnerID=40&md5=d810fac3cb60539c433134ee560e1d7b},
}

@Article{Hassan2016,
  author          = {Hassan, A. and Palmieri, R. and Ravindran, B.},
  title           = {Remote transaction commit: Centralizing software transactional memory commits},
  journal         = {IEEE Transactions on Computers},
  year            = {2016},
  volume          = {65},
  number          = {7},
  pages           = {2228-2240},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Software Transactional Memory (STM) has recently emerged as a promising synchronization abstraction for multicore architectures. State-of-the-art STM algorithms, however, suffer from performance challenges due to contention and spinning on locks during the transaction commit phase. In this paper, we introduce Remote Transaction Commit (or RTC), a mechanism for executing commit phases of STM transactions. RTC dedicates server cores to execute transactional commit phases on behalf of application threads. This approach has two major benefits. First, it decreases the overheads of spinning on locks during commit, such as the number of cache misses, blocking of lock holders, and CAS operations. Second, it enables exploiting the benefits of coarse-grained locking algorithms (simple and fast lock acquisition, reduced false conflicts) and bloom filter-based algorithms (concurrent execution of independent transactions). Our experimental study on a 64-core machine with four sockets shows that RTC solves the problem of performance degradation due to spin locking on both micro-benchmarks (red-black trees), and macro-benchmarks (STAMP), especially when the commit phase is relatively long and when thread contention increases. © 1968-2012 IEEE.},
  affiliation     = {Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA 24060, United States},
  art_number      = {7210162},
  author_keywords = {Remote Commit; Software Transactional Memory; Transactions dependency},
  document_type   = {Article},
  doi             = {10.1109/TC.2015.2470245},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84976344670&doi=10.1109%2fTC.2015.2470245&partnerID=40&md5=38dc580864f5c2f88a5adcb26234b0cc},
}

@Conference{Ritter2016,
  author          = {Ritter, D. and May, N. and Sachs, K. and Rinderle-Ma, S.},
  title           = {Industry paper: Benchmarking integration pattern implementations},
  year            = {2016},
  pages           = {125-136},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {The integration of a growing number of distributed, heterogeneous applications is one of the main challenges of enterprise data management. Through the advent of cloud and mobile application integration, higher volumes of messages have to be processed, compared to common enterprise computing scenarios, while guaranteeing high throughput. However, no previous study has analyzed the impact on message throughput for Enterprise Integration Patterns (ElPs) (e. g., channel creation, routing and transformation). Acknowledging this void, we propose EIPBench, a comprehensive micro-benchmark design for evaluating the message throughput of frequently implemented ElPs and message delivery semantics in productive cloud scenarios. For that, these scenarios are collected and described in a process- driven, TPC-C-like taxonomy, from which the most relevant patterns, message formats, and scale factors are derived as foundation for the benchmark. To prove its applicability, we describe an EIPBench reference implementation and discuss the results of its application to an open source integration system that implements the selected patterns. © 2016 ACM.},
  affiliation     = {SAP SE, Dietmar-Hopp-Allee 16, Walldorf, Germany; University of Vienna, Währingerstrasse 29, Vienna, Austria},
  author_keywords = {Benchmark; Data-aware processing; Integration patterns},
  document_type   = {Conference Paper},
  doi             = {10.1145/2933267.2933269},
  journal         = {DEBS 2016 - Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84978706224&doi=10.1145%2f2933267.2933269&partnerID=40&md5=d3b75a0206b0d47d99aa6d38e50557b9},
}

@Conference{Jayasinghe2016,
  author          = {Jayasinghe, M. and Jayawardena, A. and Rupasinghe, B. and Dayarathna, M. and Perera, S. and Suhothayan, S. and Perera, I.},
  title           = {Grand challenge: Continuous analytics on graph data streams using WSO2 complex event processor},
  year            = {2016},
  pages           = {301-308},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {The ACM DEBS Grand Challenge 2016 focuses on analysing the properties of a time evolving social-network graph generated using LDBC (Linked Data Benchmark Council) Social Network Benchmark. In this paper we present how we used WSO2 CEP, an open source, commercially available Complex Event Processing Engine, to solve the problem. On a 4-core/8 GB virtual machine, our solution processed 90,000 events per second with a mean latency of 6 ms for query 1. For query 2 it processed 210,000 events per second with a mean latency of only 0.3 ms. The paper describes the solution we propose, the experiments' results, and presents how we optimized the performance of our solution. © 2016 ACM.},
  affiliation     = {WSO2 Inc., Mountain View, CA, United States},
  author_keywords = {Benchmarking; Complex event processing; Event processing languages; Events; Graph data streams; Social networks; Time evolving graphs},
  document_type   = {Conference Paper},
  doi             = {10.1145/2933267.2933508},
  journal         = {DEBS 2016 - Proceedings of the 10th ACM International Conference on Distributed and Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84978656510&doi=10.1145%2f2933267.2933508&partnerID=40&md5=d6160917c783fdfaffd9500b24136214},
}

@Conference{Eizenberg2016,
  author          = {Eizenberg, A. and Hu, S. and Pokam, G. and Devietti, J.},
  title           = {REMIX: Online detection and repair of cache contention for the JVM},
  year            = {2016},
  volume          = {13-17-June-2016},
  pages           = {251-265},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {As ever more computation shifts onto multicore architectures, it is increasingly critical to find effective ways of dealing with multithreaded performance bugs like true and false sharing. Previous approaches to fixing false sharing in unmanaged languages have employed highly-invasive runtime program modifications. We observe that managed language runtimes, with garbage collection and JIT code compilation, present unique opportunities to repair such bugs directly, mirroring the techniques used in manual repairs. We present REMIX, a modified version of the Oracle HotSpot JVM which can detect cache contention bugs and repair false sharing at runtime. REMIX's detection mechanism leverages recent performance counter improvements on Intel platforms, which allow for precise, unobtrusive monitoring of cache contention at the hardware level. REMIX can detect and repair known false sharing issues in the LMAX Disruptor high-performance inter-thread messaging library and the Spring Reactor event-processing framework, automatically providing 1.5-2x speedups over unoptimized code and matching the performance of handoptimization. REMIX also finds a new false sharing bug in SPECjvm2008, and uncovers a true sharing bug in the HotSpot JVM that, when fixed, improves the performance of three NAS Parallel Benchmarks by 7-25x. REMIX incurs no statistically-significant performance overhead on other benchmarks that do not exhibit cache contention, making REMIX practical for always-on use. © 2016 ACM.},
  affiliation     = {University of Pennsylvania, United States; Intel Corporation, United States},
  author_keywords = {Cache coherence; False sharing; Java},
  document_type   = {Conference Paper},
  doi             = {10.1145/2908080.2908090},
  journal         = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84975842765&doi=10.1145%2f2908080.2908090&partnerID=40&md5=d91ecf43b79404d22aa70dfaa381b3e6},
}

@Article{Zheng2016,
  author          = {Zheng, Z. and Zhai, J. and Li, Y. and Chen, W.},
  title           = {Workload analysis for typical GPU programs using CUPTI interface},
  journal         = {Jisuanji Yanjiu yu Fazhan/Computer Research and Development},
  year            = {2016},
  volume          = {53},
  number          = {6},
  pages           = {1249-1262},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {GPU-based high performance computers have become an important trend in the area of high performance computing. However, developing efficient parallel programs on current GPU devices is very complex because of the complex memory hierarchy and thread hierarchy. To address this problem, we summarize five kinds of key metrics that reflect the performance of programs according to the hardware and software architecture. Then we design and implement a performance analysis tool based on underlying CUPTI interfaces provided by NVIDIA, which can collect key metrics automatically without modifying the source code. The tool can analyze the performance behaviors of GPU programs effectively with very little impact on the execution of programs. Finally, we analyze 17 programs in Rodinia benchmark, which is a famous benchmark for GPU programs, and a real application using our tool. By analyzing the value of key metrics, we find the performance bottlenecks of each program and map the bottlenecks back to source code. These analysis results can be used to guide the optimization of CUDA programs and GPU architecture. Result shows that most bottlenecks come from inefficient memory access, and include unreasonable global memory and shared memory access pattern, and low concurrency for these programs. We summarize the common reasons for typical performance bottlenecks and give some high-level suggestions for developing efficient GPU programs. © 2016, Science Press. All right reserved.},
  affiliation     = {Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China},
  author_keywords = {Graphics processing unit (GPU); Performance counter; Performance metric; Rodinia; Workload analysis},
  document_type   = {Article},
  doi             = {10.7544/issn1000-1239.2016.20148354},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84977274457&doi=10.7544%2fissn1000-1239.2016.20148354&partnerID=40&md5=22a4f385c07c1b4a735076e0da0dbe88},
}

@Conference{Lu2016,
  author        = {Lu, C.-H. and Chao, H.-L. and Song, Y.-C. and Hsiung, P.-A.},
  title         = {Interference-aware Batch Memory Scheduling in heterogeneous multicore architecture},
  year          = {2016},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {In recent years, integrating Central Processing Units (CPUs) and Graphics Processing Units (GPUs) on the same chip has become a major trend. When the number of processors increases, a large number of memory requests are generated causing memory access conflicts. Due to the heterogeneity of processors, the issue has become more serious in the heterogeneous multicore architecture. In this work, we propose a three stages memory scheduling algorithm called Interference-aware Batch Memory Scheduling Algorithm (IBM) to address this issue. We compare IBM against the First-Come First-Serve (FCFS) and Staged Memory Scheduling (SMS). Our evaluations show that IBM reduces at most 4.2% memory access latency and 24.37% CPU access latency in real-world benchmarks. © 2016 IEEE.},
  affiliation   = {Department of Computer Science and Information Engineering, National Chung Cheng University, Chiayi, 621, Taiwan},
  art_number    = {7482559},
  document_type = {Conference Paper},
  doi           = {10.1109/VLSI-DAT.2016.7482559},
  journal       = {2016 International Symposium on VLSI Design, Automation and Test, VLSI-DAT 2016},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84978418226&doi=10.1109%2fVLSI-DAT.2016.7482559&partnerID=40&md5=0c23d327f7aa2e55c7aaba3ee92e490a},
}

@Article{Azarian2016,
  author          = {Azarian, A. and Cardoso, J.M.P.},
  title           = {Pipelining data-dependent tasks in FPGA-based multicore architectures},
  journal         = {Microprocessors and Microsystems},
  year            = {2016},
  volume          = {42},
  pages           = {165-179},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In recent years, there has been increasing interest in using task-level pipelining to accelerate the overall execution of applications mainly consisting of producer/consumer tasks. This paper proposes fine- and coarse-grained data synchronization approaches to achieve pipelining execution of producer/consumer tasks in FPGA-based multicore architectures. Our approaches are able to speedup the overall execution of successive, data-dependent tasks, by using multiple cores and specific customization features provided by FPGAs. An important component of our approach is the use of customized inter-stage buffer schemes to communicate data and to synchronize the cores associated with the producer/consumer tasks. We propose techniques to reduce the number of accesses to external memory in our fine-grained data synchronization approach. The experimental results show the feasibility of the approach in both in-order and out-of-order producer/consumer tasks. Moreover, the results using our approach reveal noticeable performance improvements for a number of benchmarks over a single core implementation without using task-level pipelining. © 2016 Elsevier B.V. All rights reserved.},
  affiliation     = {Faculty of Engineering, University of Porto, INESC-TEC, Rua Dr. Roberto Frias, Porto, Portugal},
  author_keywords = {FPGA; Multicore architectures; Producer/consumer data communication; Task-level pipelining},
  document_type   = {Article},
  doi             = {10.1016/j.micpro.2016.02.008},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84975783862&doi=10.1016%2fj.micpro.2016.02.008&partnerID=40&md5=284c9f735c7a8766bf6cad3e7efce733},
}

@Article{Bondhugula2016,
  author          = {Bondhugula, U. and Acharya, A. and Cohen, A.},
  title           = {The pluto+algorithm: A practical approach for parallelization and locality optimization of affine loop nests},
  journal         = {ACM Transactions on Programming Languages and Systems},
  year            = {2016},
  volume          = {38},
  number          = {3},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {Affine transformations have proven to be powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multidimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks such as the Pluto algorithm, which include a cost function for modern multicore architectures for which coarse-grained parallelism and locality are crucial, consider only a subspace of transformations to avoid a combinatorial explosion in finding transformations. The ensuing practical trade-offs lead to the exclusion of certain useful transformations: in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In addition, there is currently no proof that the algorithm successfully finds a tree of permutable loop bands for all affine loop nests. In this article, we propose an approach to address these two issues (1) bymodeling a much larger space of practically useful affine transformations in conjunction with the existing cost function of Pluto, and (2) by extending the Pluto algorithm in a way that allows a proof for its soundness and completeness for all affine loop nests. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance for any benchmark from Polybench. For the Lattice Boltzmann Method (LBM) simulations with periodic boundary conditions, it provides a mean speedup of 1.33× over Pluto. We also show that Pluto+ does not increase compilation time significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time by only 15%. In cases in which it improves execution time significantly, it increased polyhedral optimization time by only 2.04×. © 2016 ACM.},
  affiliation     = {Department of Computer Science and Automation, Indian Institute of Science, Bangalore, 560012, India; INRIA and ENS, 45 rue d'Ulm, Paris, 75005, France},
  art_number      = {12},
  author_keywords = {Affine transformations; Automatic parallelization; Locality optimization; Loop transformations; Polyhedral model; Tiling},
  document_type   = {Article},
  doi             = {10.1145/2896389},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84966312140&doi=10.1145%2f2896389&partnerID=40&md5=c56e8092aa98ba2559cec5b087d3594e},
}

@Conference{Takaki2016,
  author        = {Takaki, K. and Kurihara, T. and Li, Y.},
  title         = {On the Performance Improvement of an Architecture towards Sharing FPUs across Cores for the Design of Multithreading Multicore CPUs},
  year          = {2016},
  pages         = {408-411},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The multithreading and multicore techniques are widely adopted in the design of the modern high-performance CPUs. Multithreading technique allows multiple threads to share the functional units (FUs) within a core for the better utilization of the FUs. Thus there will be confliction on the use of some FUs, the floating-point unit (FPU) for instance. In such a case, some floating-point instructions will be suspended until the FPU is available for use. Multicore technique implements a small-scale multiprocessor on a chip. A thread that runs on one core cannot use the FUs of other cores. This results in poor utilization of the FPU in some cores if the threads running on those cores do not contain floating-point instructions at all, although in other cores, the threads are straggling to complete for the FPU. Different from the traditional multiprocessors that are implemented with multiple CPU chips, because the multicore CPUs implement multiprocessors on the same chip, it becomes possible to let the threads in a core group share all the FPUs in the group. When a conflict on the use of FPU occurs, some floating-point operations can be redirected to the cores of the same group in which the FPUs are in idle state, so that the overall performance of the multicore CPU will be improved. This paper investigates such a group architecture and gives the performance improvement of the proposed architecture to that of the traditional multicore architecture. Our experimental results show that, on average for the floating-point benchmarks, 53.2%, 71.0%, and 72.5% performance improvements can be achieved by redirecting the floating-point operations to other cores within the group with the group sizes of two, four, and eight, respectively. © 2015 IEEE.},
  affiliation   = {Graduate School of CIS, Hosei University, Tokyo, 184-8584, Japan; Faculty of Computer and Information Sciences, Hosei University, Tokyo, 184-8584, Japan},
  art_number    = {7424748},
  document_type = {Conference Paper},
  doi           = {10.1109/CANDAR.2015.48},
  journal       = {Proceedings - 2015 3rd International Symposium on Computing and Networking, CANDAR 2015},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84964789015&doi=10.1109%2fCANDAR.2015.48&partnerID=40&md5=92f51013bf12b2f5c7ddad2e4be37057},
}

@Article{Corre2016,
  author          = {Corre, Y. and Diguet, J.-P. and Heller, D. and Blouin, D. and Lagadec, L.},
  title           = {TBES: Template-based exploration and synthesis of heterogeneous multiprocessor architectures on FPGA},
  journal         = {ACM Transactions on Embedded Computing Systems},
  year            = {2016},
  volume          = {15},
  number          = {1},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article describes TBES, a software end-to-end environment for synthesizing multitask applications on FPGAs. The implementation follows a template-based approach for creating heterogeneous multiprocessor architectures. Heterogeneity stems from the use of general-purpose processors along with custom accelerators. Experimental results demonstrate substantial speedup for several classes of applications. Furthermore, this work allows for reducing development costs and saving development time for the software architect, the domain expert, and the optimization expert. This work provides a framework to bring together various existing tools and optimisation algorithms. The advantages are manifold: modularity and flexibility, easy customization for best-fit algorithm selection, durability and evolution over time, and legacy preservation including domain experts' know-how. In addition to the use of architecture templates for the overall system, a second contribution lies in using high-level synthesis for promoting exploration of hardware IPs. The domain expert, who best knows which tasks are good candidates for hardware implementation, selects parts of the initial application to be potentially synthesized as dedicated accelerators. As a consequence, the HLS general problem turns into a constrained and more tractable issue, and automation capabilities eliminate the need for tedious and error-prone manual processes during domain space exploration. The automation only takes place once the application has been broken down into concurrent tasks by the designer, who can then drive the synthesis process with a set of parameters provided by TBES to balance tradeoffs between optimization efforts and quality of results. The approach is demonstrated step by step up to FPGA implementations and executions with an MJPEG benchmark and a complex Viola-Jones face detection application. We show that TBES allows one to achieve results with up to 10 times speedup to reduce development times and to widen design space exploration. © 2016 ACM.},
  affiliation     = {Lab-STICC, Université de Bretagne-Sud, Centre de Recherche, BP 92116, Lorient, 56321, France; Lab-STICC, CNRS, Lorient, France; Lab-STICC, ENSTA Bretagne, 2 rue François Verny, Brest Cedex 9, 29806, France; Hasso-Plattner-Institute, University of Potsdam, Prof-Dr-Helmert-Str. 2-3, Potsdam, 14482, Germany},
  art_number      = {9},
  author_keywords = {Electronic system level; High-level synthesis; Multiprocessor; System-on-chip},
  document_type   = {Article},
  doi             = {10.1145/2816817},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84964868775&doi=10.1145%2f2816817&partnerID=40&md5=d37887e1728ba03ee81dfed6e46487b6},
}

@Article{Yao2016,
  author          = {Yao, G. and Yun, H. and Wu, Z.P. and Pellizzoni, R. and Caccamo, M. and Sha, L.},
  title           = {Schedulability analysis for memory bandwidth regulated multicore real-time systems},
  journal         = {IEEE Transactions on Computers},
  year            = {2016},
  volume          = {65},
  number          = {2},
  pages           = {601-614},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architecture brings a significant challenge in designing critical real-time systems because of timing variability caused by concurrent accesses to shared memory. We propose a memory bandwidth regulated system architecture and a novel analysis method to address this challenge. In the proposed architecture, each core's memory access rate is regulated in a globally coordinated manner. The architecture allows system designers to control the system to satisfy desired real-time performance. The proposed analysis method provides a way to calculate worst case response time of each real-time task independently from other activities on other cores; it only depends on the task under analysis, the assigned bandwidth, and the number of cores in the system. We believe this independence is critical to enable modular certification of critical real-time systems. We implement the proposed system model on the gem5 architecture simulator. We evaluate the proposed analysis method by comparing the computed runtime with the measured runtime on the modified simulator. We show that the analysis method provides reasonable upper-bounds based on the SPEC2006 benchmark suite. © 1968-2012 IEEE.},
  affiliation     = {Department of Computer Science, University of Illinois at Urbana-Champaign, 201 North Goodwin Avenue, Urbana, IL 61801, United States; Department of Electrical Engineering and Computer Science, University of Kansas, Lawrence, KS, United States; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada},
  art_number      = {7093140},
  author_keywords = {Gem5; Memory access control; Real-time system; Schedulability analysis; Scheduling},
  document_type   = {Article},
  doi             = {10.1109/TC.2015.2425874},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84962052761&doi=10.1109%2fTC.2015.2425874&partnerID=40&md5=0ed4dab61bb3e5f070feae97da2274a8},
}

@Conference{Kratzke2016,
  author          = {Kratzke, N. and Quint, P.-C.},
  title           = {Ppbench a visualizing network benchmark for microservices},
  year            = {2016},
  volume          = {2},
  pages           = {223-231},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Companies like Netflix, Google, Amazon, Twitter successfully exemplified elastic and scalable microservice architectures for very large systems. Microservice architectures are often realized in a way to deploy services as containers on container clusters. Containerized microservices often use lightweight and REST-based mechanisms. However, this lightweight communication is often routed by container clusters through heavyweight software defined networks (SDN). Services are often implemented in different programming languages adding additional complexity to a system, which might end in decreased performance. Astonishingly it is quite complex to figure out these impacts in the upfront of a microservice design process due to missing and specialized benchmarks. This contribution proposes a benchmark intentionally designed for this microservice setting. We advocate that it is more useful to reflect fundamental design decisions and their performance impacts in the upfront of a microservice architecture development and not in the aftermath. We present some findings regarding performance impacts of some TIOBE TOP 50 programming languages (Go, Java, Ruby, Dart), containers (Docker as type representative) and SDN solutions (Weave as type representative). Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
  affiliation     = {Lübeck University of Applied Sciences, Center of Excellence COSA, Lübeck, Germany},
  author_keywords = {Benchmark; Cluster; Container; Docker; Microservice; Network; Performance; Reference; REST; SDN},
  document_type   = {Conference Paper},
  journal         = {CLOSER 2016 - Proceedings of the 6th International Conference on Cloud Computing and Services Science},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84979570928&partnerID=40&md5=c7f606596322298cbf6e919ed19abf0f},
}

@Article{Sangroya2016,
  author        = {Sangroya, A. and Bouchenak, S.},
  title         = {A reusable architecture for dependability and performance benchmarking of cloud services},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2016},
  volume        = {9586},
  pages         = {207-218},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {With the increasing demand and benefits of cloud computing services, new solutions are needed to benchmark the dependability and performance of these services. Designing a dependability and performance benchmark that covers a variety of fault and execution scenarios, poses various architectural challenges. In this paper, we present a generic software architecture for dependability and performance benchmarking for cloud computing services. We provide the details of this generic architecture i.e. various components and modules, that are responsible for injecting faults in cloud services in addition to the components responsible for measuring the performance and dependability. We make use of this architecture to build two software prototypes: MRBS and MemDB. These prototypes are used to benchmark two popular cloud services: MapReduce and Memcached. The case studies with the use of software prototypes demonstrates the benefits of building a generic architecture. © Springer-Verlag Berlin Heidelberg 2016.},
  affiliation   = {TCS Innovation Labs, Mumbai, India; INSA de Lyon, Villeurbanne, France},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-662-50539-7_17},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84964808107&doi=10.1007%2f978-3-662-50539-7_17&partnerID=40&md5=0de81a4999fb11e97c38c195800d901c},
}

@Article{Suarez2016,
  author          = {Suárez, J.M. and Gutiérrez, L.E.},
  title           = {Domain classification requirements for application of architectural patterns [Tipificación de Dominios de Requerimientos para la Aplicación de Patrones Arquitectónicos]},
  journal         = {Informacion Tecnologica},
  year            = {2016},
  volume          = {27},
  number          = {4},
  pages           = {193-202},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article describes the results of a research developed to support the design phase of software architecture of a project, as the main product a domain requirement that groups the common elements in web development projects that promote the integration of platforms was obtained and digital ecosystems. This domain was used as an input to relate architectural patterns that are currently used and thus define a resource that could be useful in any development project for the selection of the most suitable pattern. For project implementation activities were carried out: selection of software development projects; identifying functional and nonfunctional requirements; selection of architectural benchmarks and domain validation requirements to substantiate that the architectural patterns associated, represented an appropriate option for the requirement.},
  affiliation     = {Unidades Tecnológicas de Santander, Calle de los Estudiantes No 9-82 Ciudadela Real de Minas, Santander-Bucaramanga, Colombia; Universidad Santo Tomás, Seccional Tunja, Sede Centro: Calle. 19 No 11 - 64, Tunja-Boyacá, Colombia},
  author_keywords = {Architectural pattern; Domain requirements; Software architecture; Software development},
  document_type   = {Article},
  doi             = {10.4067/S0718-07642016000400021},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84983354555&doi=10.4067%2fS0718-07642016000400021&partnerID=40&md5=f62902a97bd684b3e3d0c5fc96d0a85c},
}

@Article{2016,
  title         = {13th International Conference on Service-Oriented Computing, ICSOC 2015 and Workshops on WESOA, RMSOC, ISC, DISCO, WESE, BSCI, FOR-MOVES, 2015},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2016},
  volume        = {9586},
  pages         = {1-254},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 20 papers. The special focus in this conference is on Engineering Service-Oriented Applications and Resource Management in Service-Oriented Computing. The topics include: From choreography diagrams to RESTful interactions; estimating the complexity of software services using an entropy based metric; establishing distributed governance infrastructures for enacting cross-organization collaborations; revisiting industrial practice in services computing; distributed service co-evolution based on domain objects; a web services infrastructure for the management of mashup interfaces; a CPS service contract framework for composition; towards RAM-based variant generation of business process models; extending generic BPM with computer vision capabilities; expressive equivalence and succinctness of parametrized automata with respect to finite memory automata; toward the formalization of BPEL; context-aware personalization for smart mobile cloud services; information governance requirements for architectural solutions supporting dynamic business networking; an evolutionary multiobjective approach for the dynamic multilevel component selection problem; a reusable architecture for dependability and performance benchmarking of cloud services; safe configurations of replica voting processes in fault-resilient data collection services; on composition of checkpoint and recovery protocols for distributed systems and a proactive solution to manage web service unavailability in service oriented software systems.},
  document_type = {Conference Review},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84964837024&partnerID=40&md5=06f9521724b837a830a79842e41ee697},
}

@Article{2016a,
  title         = {15th International Conference on Software Reuse, ICSR 2016},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2016},
  volume        = {9679},
  pages         = {1-411},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 29 papers. The special focus in this conference is on Software Product Lines, Business Aspects of Software Reuse, Component-Based Reuse, Reuse-Based Software Engineering and Software Reuse Tools. The topics include: Applying incremental model slicing to product-line regression testing; automated composition of service mashups through software product line engineering; feature location in model-based software product lines through a genetic algorithm; carrying ideas from knowledge-based configuration to software product lines; a method to support the adoption of reuse technology in large software organizations; a practical use case modeling approach to specify crosscutting concerns; an approach for prioritizing software features based on node centrality in probability network; rage reusable game software components and their integration into serious game engines; reusable secure connectors for secure software architecture; concept-based engineering of situation-specific migration methods; leveraging feature location to extract the clone-and-own relationships of a family of software products; an architecture to improve software reuse; pragmatic software reuse in bioinformatics; feature location benchmark for software families using eclipse community releases; java extensions for design pattern instantiation; towards a semantic search engine for open source software; detecting similar programs via the weisfeiler-leman graph kernel; a semi automatic maintenance of OCL constraints; reverse-engineering reusable language modules from legacy domain-specific languages; a framework for enhancing the retrieval of UML diagrams; a tool for analyzing and extracting specification clones in DSLS and a tool to support decision making for component reuse through profiling with ontologies.},
  document_type = {Conference Review},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84977492303&partnerID=40&md5=347f932969ae9dfb1c178ad3143fa5af},
}

@Article{Chen2016,
  author          = {Chen, G. and Xu, Y. and Hu, X. and Guo, X. and Ma, J. and Hu, Y. and Xie, Y.},
  title           = {TSocket: Thermal sustainable power budgeting},
  journal         = {ACM Transactions on Design Automation of Electronic Systems},
  year            = {2016},
  volume          = {21},
  number          = {2},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {As technology scales, thermal management for multicore architectures becomes a critical challenge due to increasing power density. Existing power budgeting techniques focus on maximizing performance under a given power budget by optimizing the core configurations. In multicore era, a chip-wide power budget, however, is not sufficient to ensure thermal constraints because the thermal sustainable power capacity varies with different threading strategies and core configurations. In this article, we propose two models to dynamically estimate the thermal sustainable power capacity in homogeneous multicore systems: uniform power model and nonuniform power model. These two models convert the thermal effect of threading strategies and core configurations into power capacity, which provide a context-based core power capacity for power budgeting. Based on these models, we introduce a power budgeting framework aiming to improve the performance within thermal constraints, named as TSocket. Compared to the chip-wide power budgeting solution, TSocket shows 19% average performance improvement for the PARSEC benchmarks in single program scenario and up to 11% performance improvement in multiprogram scenario. The performance improvement is achieved by reducing thermal violations and exploring thermal headrooms. © 2016 ACM.},
  affiliation     = {AMD Research China Lab, Advanced Micro Devices, Inc., Beijing, China; Space Science Institute, Macau University of Science and Technology, Macau, Macau; Shannon Laboratory, Huawei Technologies Co., Ltd, Shenzhen, China; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, United States; Bureau of Geophysical Prospecting, China National Petroleum Corporation, Beijing, China; Institute of Computing Technology, Chinese Academy of Science, Beijing, China; Department of Electrical and Computer Engineering, University of California at Santa Barbara, Santa Barbara, CA, United States},
  art_number      = {2837023},
  author_keywords = {Multicore system; Performance optimization; Power budgeting; Thermal modeling},
  document_type   = {Article},
  doi             = {10.1145/2837023},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84957105168&doi=10.1145%2f2837023&partnerID=40&md5=34cb1bdc51380481f29d37b80ad6903f},
}

@Conference{Abe2016,
  author          = {Abe, T. and Sugimoto, H. and Arai, S.},
  title           = {Multi-core processors architecture design method for next generation automotive controller},
  year            = {2016},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {A new multi-core processors architecture design method is proposed for next generation automotive controller. This method is an accurate performance prediction technology which enables a whole case study of the automotive electronic system by simulating the process performance of next generation multi-core processors architecture with parallel processing software. In this paper, a detailed analysis of the proposed method is carried out. By means of the proposed method, it has been possible to evaluate the influence of various parameters on next generation microprocessor architecture with parallel benchmark software. In particular, by applying it to general purpose multi-core processors architecture configurations, it has been derived that the performance of dual-core processors in a cluster memory type architecture is 70% more efficient that the same architecture with a single-core processor. © 2016, FISITA. All rights reserved.},
  affiliation     = {DENSO CORPORATION, Japan},
  author_keywords = {Architecture Design Method; Multi-core Processors; Performance Analysis; Performance Prediction; Software Architecture},
  document_type   = {Conference Paper},
  journal         = {FISITA 2016 World Automotive Congress - Proceedings},
  page_count      = {10},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85018407775&partnerID=40&md5=d7e938603ac1042acccc5a45cc3b6fbb},
}

@Article{Rupp2016,
  author          = {Rupp, K. and Tillet, P. and Rudolf, F. and Weinbub, J. and Morhammer, A. and Grasser, T. and Jüngel, A. and Selberherr, S.},
  title           = {Viennacl - Linear algebra library for multi- and many-core architectures},
  journal         = {SIAM Journal on Scientific Computing},
  year            = {2016},
  volume          = {38},
  number          = {5},
  pages           = {S412-S439},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {CUDA, OpenCL, and OpenMP are popular programming models for the multicore architectures of CPUs and many-core architectures of GPUs or Xeon Phis. At the same time, computational scientists face the question of which programming model to use to obtain their scientific results. We present the linear algebra library ViennaCL, which is built on top of all three programming models, thus enabling computational scientists to interface to a single library, yet obtain high performance for all three hardware types. Since the respective compute back end can be selected at runtime, one can seamlessly switch between different hardware types without the need for error-prone and time-consuming recompilation steps. We present new benchmark results for sparse linear algebra operations in ViennaCL, complementing results for the dense linear algebra operations in ViennaCL reported in earlier work. Comparisons with vendor libraries show that ViennaCL provides better overall performance for sparse matrix-vector and sparse matrix-matrix products. Additional benchmark results for pipelined iterative solvers with kernel fusion and preconditioners identify the respective sweet spots for CPUs, Xeon Phis, and GPUs. © 2016 Society for Industrial and Applied Mathematics.},
  affiliation     = {Institute for Microelectronics, TU Wien, Gußhausstraße 27-29/E360, Wien, A-1040, Austria; Institute for Analysis and Scientific Computing, TU Wien, Wiedner Hauptstraße 8-10/E101, Wien, A-1040, Austria; School of Engineering and Applied Sciences, Harvard University, 29 Oxford Street, Cambridge, MA 02138, United States},
  author_keywords = {CPU; CUDA; GPU; Iterative solvers; OpenCL; OpenMP; ViennaCL; Xeon Phi},
  document_type   = {Article},
  doi             = {10.1137/15M1026419},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84994188937&doi=10.1137%2f15M1026419&partnerID=40&md5=e19cea0713a17da540476cda1115432e},
}

@Article{Lozi2016,
  author          = {Lozi, J.-P. and David, F. and Thomas, G. and Lawall, J. and Muller, G.},
  title           = {Fast and portable locking for multicore architectures},
  journal         = {ACM Transactions on Computer Systems},
  year            = {2016},
  volume          = {33},
  number          = {4},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {The scalability of multithreaded applications on current multicore systems is hampered by the performance of lock algorithms, due to the costs of access contention and cache misses. The main contribution presented in this article is a new locking technique, Remote Core Locking (RCL), that aims to accelerate the execution of critical sections in legacy applications on multicore architectures. The idea of RCL is to replace lock acquisitions by optimized remote procedure calls to a dedicated server hardware thread. RCL limits the performance collapse observed with other lock algorithms when many threads try to acquire a lock concurrently and removes the need to transfer lock-protected shared data to the hardware thread acquiring the lock, because such data can typically remain in the server's cache. Other contributions presented in this article include a profiler that identifies the locks that are the bottlenecks in multithreaded applications and that can thus benefit from RCL, and a reengineering tool that transforms POSIX lock acquisitions into RCL locks. Eighteen applications were used to evaluate RCL: the nine applications of the SPLASH-2 benchmark suite, the seven applications of the Phoenix 2 benchmark suite, Memcached, and Berkeley DB with a TPC-C client. Eight of these applications are unable to scale because of locks and benefit from RCL on an ×86 machine with four AMD Opteron processors and 48 hardware threads. By using RCL instead of Linux POSIX locks, performance is improved by up to 2.5 times on Memcached, and up to 11.6 times on Berkeley DB with the TPC-C client. On a SPARC machine with two Sun Ultrasparc T2+ processors and 128 hardware threads, three applications benefit from RCL. In particular, performance is improved by up to 1.3 times with respect to Solaris POSIX locks on Memcached, and up to 7.9 times on Berkeley DB with the TPC-C client. © 2016 ACM.},
  affiliation     = {Université Nice Sophia Antipolis, CNRS, I3S, 930 route des Colles, BP 145, Sophia Antipolis Cedex, 06903, France; Sorbonne Universités, Inria, CNRS, UPMC, LIP6, Couloir 25-26, Etage 3, Bureau 331, 4 place Jussieu, Paris Cedex 05, 75252, France; Département D'informatique, SAMOVAR, CNRS, Télécom ParisSud, Université Paris-Saclay, Bureau B306, Telecom SudParis, 9, rue Charles Fourier, Evry Cedex, F-91011, France; Sorbonne Universités, Inria, CNRS, UPMC, LIP6, Boite courrier 169, Etage 3, Bureau 318, 4 place Jussieu, Paris Cedex 05, 75252, France; Sorbonne Universités, Inria, CNRS, UPMC, LIP6, Boite courrier 318, Etage 3, Bureau 318, 4 place Jussieu, Paris Cedex 05, 75252, France},
  art_number      = {2845079},
  author_keywords = {Busy-waiting; Locality; Locks; Memory contention; Multicore; Profiling; Reengineering; RPC; Synchronization},
  document_type   = {Article},
  doi             = {10.1145/2845079},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84954089641&doi=10.1145%2f2845079&partnerID=40&md5=28e08b02b2fe188d561eeab77876108a},
}

@Article{Jeon2016,
  author        = {Jeon, S. and Jang, M. and Lee, D. and Cho, Y.-J. and Kim, J. and Lee, J.},
  title         = {Multiple robots task allocation for cleaning and delivery},
  journal       = {Studies in Computational Intelligence},
  year          = {2016},
  volume        = {650},
  pages         = {195-214},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper presents a mathematical formulation of the problem of cleaning and delivery task in a large public space with multiple robots, along with a procedural solution based on task reallocation. The task in the cleaning problem is the cleaning zone. A group of robots are assigned to each cleaning zones according to the environmental parameters. Resource constraintsmake cleaning robots stop operation periodically, which can incur a mission failure or deterioration of the mission performance. In our solution approach, continuous operation is assured by replacing robots having resource problems with standby robots by task reallocation. Two resource constraints are considered in our formulation: the battery capacity and the garbage bin size. This paper describes and compares the performance of three task reallocation strategies: All-At-Once, Optimal-Vector, and Performance-Maximization. The performance measures include remaining garbage volume, cleaning quality, and cleaning time. Task allocation algorithms are tested by simulation in an area composed of 4 cleaning zones, and the Performance-Maximization strategy marked the best performance. Hence, a delivery task is added to the cleaning task. The delivery request operates as a new perturbation factor for the reallocation. The task allocation procedure for the delivery task includes the switching of tasks of the delivery robot itself as well as exchanging among cleaning robots to meet the balance of the cleaning performance. The experiment was conducted with 9 robots with the software architecture that enables multi-functional of a robot and they performed both pseudo-clean and delivery task successfully. © Springer International Publishing Switzerland 2016.},
  affiliation   = {Human Robot Interaction Research Team, Electronics and Telecommunications Research Institute (ETRI), 218 Gajeongro, Yuseonggu, Daejeon, 34129, South Korea},
  document_type = {Article},
  doi           = {10.1007/978-3-319-33386-1_10},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84991810160&doi=10.1007%2f978-3-319-33386-1_10&partnerID=40&md5=4bcc5c9c320da2766331988b804b27cb},
}

@Article{Lourenco2015,
  author          = {Lourenço, J.R. and Cabral, B. and Carreiro, P. and Vieira, M. and Bernardino, J.},
  title           = {Choosing the right NoSQL database for the job: a quality attribute evaluation},
  journal         = {Journal of Big Data},
  year            = {2015},
  volume          = {2},
  number          = {1},
  note            = {cited By 43},
  __markedentry   = {[Nichl:6]},
  abstract        = {For over forty years, relational databases have been the leading model for data storage, retrieval and management. However, due to increasing needs for scalability and performance, alternative systems have emerged, namely NoSQL technology. The rising interest in NoSQL technology, as well as the growth in the number of use case scenarios, over the last few years resulted in an increasing number of evaluations and comparisons among competing NoSQL technologies. While most research work mostly focuses on performance evaluation using standard benchmarks, it is important to notice that the architecture of real world systems is not only driven by performance requirements, but has to comprehensively include many other quality attribute requirements. Software quality attributes form the basis from which software engineers and architects develop software and make design decisions. Yet, there has been no quality attribute focused survey or classification of NoSQL databases where databases are compared with regards to their suitability for quality attributes common on the design of enterprise systems. To fill this gap, and aid software engineers and architects, in this article, we survey and create a concise and up-to-date comparison of NoSQL engines, identifying their most beneficial use case scenarios from the software engineer point of view and the quality attributes that each of them is most suited to. © 2015, Lourenço et al.; licensee Springer.},
  affiliation     = {CISUC, Department of Informatics Engineering, University of Coimbra, Pólo II – Pinhal de Marrocos, Coimbra, 3030-290, Portugal; Critical Software, Parque Industrial de Taveiro, lote 49, Coimbra, 3045-504, Portugal; ISEC – Superior Institute of Engineering of Coimbra, Polytechnic Institute of Coimbra, Coimbra, 3030-190, Portugal},
  art_number      = {18},
  author_keywords = {Columnar; Document store; Graph; Key-value; NoSQL databases; Quality attributes; Software architecture; Software engineering},
  document_type   = {Article},
  doi             = {10.1186/s40537-015-0025-0},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85013940863&doi=10.1186%2fs40537-015-0025-0&partnerID=40&md5=7dec0f6b329ddf27797719aeeaaaf4da},
}

@Article{Martinez-Millana2015,
  author          = {Martinez-Millana, A. and Fico, G. and Fernández-Llatas, C. and Traver, V.},
  title           = {Performance assessment of a closed-loop system for diabetes management},
  journal         = {Medical and Biological Engineering and Computing},
  year            = {2015},
  volume          = {53},
  number          = {12},
  pages           = {1295-1303},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {Telemedicine systems can play an important role in the management of diabetes, a chronic condition that is increasing worldwide. Evaluations on the consistency of information across these systems and on their performance in a real situation are still missing. This paper presents a remote monitoring system for diabetes management based on physiological sensors, mobile technologies and patient/doctor applications over a service-oriented architecture that has been evaluated in an international trial (83,905 operation records). The proposed system integrates three types of running environments and data engines in a single service-oriented architecture. This feature is used to assess key performance indicators comparing them with other type of architectures. Data sustainability across the applications has been evaluated showing better outcomes for full integrated sensors. At the same time, runtime performance of clients has been assessed spotting no differences regarding the operative environment. © 2015, International Federation for Medical and Biological Engineering.},
  affiliation     = {ITACA, Health and Wellbeing Technologies, Universidad Politécnica de Valencia, Camino de Vera s/n, Valencia, 46022, Spain; Life Supporting Technologies, Universidad Politécnica de Madrid, Madrid, Spain},
  author_keywords = {Diabetes; KPI; mHealth; Performance; Sensors; SOA; Telemonitoring},
  document_type   = {Article},
  doi             = {10.1007/s11517-015-1245-3},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84948714669&doi=10.1007%2fs11517-015-1245-3&partnerID=40&md5=19056f9d32738ec223a8a99128800691},
}

@Article{Chaturvedi2015,
  author          = {Chaturvedi, N. and Subramaniyan, A. and Gurunarayanan, S.},
  title           = {An adaptive migration–replication scheme (AMR) for shared cache in chip multiprocessors},
  journal         = {Journal of Supercomputing},
  year            = {2015},
  volume          = {71},
  number          = {10},
  pages           = {3904-3933},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Most of today’s chip multiprocessors implement last-level shared caches as non-uniform cache architectures. A major problem faced by such multicore architectures is cache line placement, especially in scenarios where multiple cores compete for line usage in the single non-uniform shared L2 cache. Block migration has been suggested to overcome the problem of optimum placement of cache blocks. Previous research, however, shows that an uncontrolled block migration scheme leads to scenarios where a cache line ‘ping-pongs’ between two requesting cores resulting in higher access latency for both the requestors and greater power dissipation. To address this problem, this paper first proposes a mechanism to dynamically profile data block usage from different cores on the chip. We then propose an adaptive migration–replication scheme for shared last-level non-uniform cache architectures that adapts between selectively replicating frequently used cache lines near the requesting cores and cache line migration towards the requesting core in case of fewer requests. AMR eliminates ‘ping-ponging’ of cache lines between the banks of the requesting cores. However, any mechanism that dynamically adapts between migration and replication at runtime is bound to have a complex search scheme to locate data blocks. To simplify the data lookup policy, this work also presents an efficient data access mechanism for non-uniform cache architectures. Our proposal relies on low overhead and highly accurate in-hardware pointers to keep track of the on-chip location of the cache block. We show that our proposed scheme reduces the completion time by on average 12.25, 8.1 and 3 % and energy consumption by 11.65, 8.5 and 2.1 % when compared to state-of-the-art last-level cache management schemes S-NUCA, D-NUCA and HK-NUCA, respectively. SPEC and PARSEC benchmarks were used to thoroughly evaluate our proposal. © 2015, Springer Science+Business Media New York.},
  affiliation     = {Electrical Electronics Engineering Department, Birla Institute of Technology and Science, Pilani, Pilani, India},
  author_keywords = {Chip multiprocessors (CMP); Non-uniform cache architecture (NUCA)},
  document_type   = {Article},
  doi             = {10.1007/s11227-015-1482-0},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84942369149&doi=10.1007%2fs11227-015-1482-0&partnerID=40&md5=2ef5b55b21d2af4263fe8c03ccc127b7},
}

@Article{Lampka2015,
  author          = {Lampka, K. and Forsberg, B. and Spiliopoulos, V.},
  title           = {Keep it cool and in time: With runtime monitoring to thermal-aware execution speeds for deadline constrained systems},
  journal         = {Journal of Parallel and Distributed Computing},
  year            = {2015},
  volume          = {95},
  pages           = {79-91},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Dynamic Power and Thermal Management (DPTM) system of Dynamic Voltage Frequency Scaling (DVFS) enabled processors compensates peak temperatures by slowing or even powering parts of the system down. While ensuring the integrity of computations, this comes with the drawback of losing performance. In the context of hard real-time systems, such unpredictable losses in performance are unacceptable, as they may lead to deadline misses which may yet compromise the integrity of the system. To safely execute hard real-time workloads on such systems, this article presents an online scheme for assigning speeds in such a way that (a) the system executes at low clock speed as often as possible, while (b) deadline violations are strictly ruled out. The proposed scheme is compared with an offline scheme which has complete knowledge about arrival times and execution demands of the workload. The benchmarking shows that for a workload which is always very close to the modelled maximum, our approach performs on-par with the offline scheme. In case of a workload which diverges from the modelled maximum more often, the speed assignments produced by our scheme become more pessimistic, as to ensure that all deadlines are met. © 2016 Elsevier Inc.},
  affiliation     = {Department of Information Technology, Uppsala University, Sweden},
  author_keywords = {Dynamic power and temperature management; Dynamic Voltage Frequency Scaling; Multicore architectures; Online real-time scheduling; Real-time computing; Run-time monitoring},
  document_type   = {Article},
  doi             = {10.1016/j.jpdc.2016.03.002},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84994667725&doi=10.1016%2fj.jpdc.2016.03.002&partnerID=40&md5=cc275348a2e56a76f38432b3b6457cfe},
}

@Article{Lorenzon2015,
  author          = {Lorenzon, A.F. and Cera, M.C. and Beck, A.C.S.},
  title           = {Investigating different general-purpose and embedded multicores to achieve optimal trade-offs between performance and energy},
  journal         = {Journal of Parallel and Distributed Computing},
  year            = {2015},
  volume          = {95},
  pages           = {107-123},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {Thread-level parallelism (TLP) is being widely exploited in embedded and general-purpose multicore processors (GPPs) to increase performance. However, parallelizing an application involves extra executed instructions and accesses to the shared memory, to communicate and synchronize. The overhead of accessing the shared memory, which is very costly in terms of delay and energy because it is at the bottom of the hierarchy, varies depending on the communication model and level of data exchange/synchronization of the application. On top of that, multicore processors are implemented using different architectures, organizations and memory subsystems. In this complex scenario, we evaluate 14 parallel benchmarks implemented with 4 different parallel programming interfaces (PPIs), with distinct communication rates and TLP, running on five representative multicore processors targeted to general-purpose and embedded systems. We show that while the former presents the best performance and the latter will be the most energy efficient, there is no single option that offers the best result for both. We also demonstrate that in applications with low levels of communication, what matters is the communication model, not a specific PPI. On the other hand, applications with high communication demands have a huge search space that can be explored. For those, Pthreads is the most efficient PPI for Intel Processors, while OpenMP is the best for ARM ones. MPI is the worst choice in almost any scenario, and gets very inefficient as the TLP increases. We also evaluate energy delayxproduct (EDxP), weighting performance towards energy by varying the value of x. In a representative case where energy is the most important, three different processors can be the best alternative for different values of x. Finally, we explore how static power influences total energy consumption, showing that its increase brings benefits to ARM multiprocessors, with the opposite effect for Intel ones. © 2016 Elsevier Inc.},
  affiliation     = {Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Av. Bento Gonçalves, 9500–Campus do Vale, Porto Alegre, 91501-970, Brazil; Campus Alegrete, Federal University of Pampa (UNIPAMPA), Av. Tiaraju, Alegrete, 810–97546-550, Brazil},
  author_keywords = {Embedded and general-purpose processors; Energy; Energy-delay product; Multicore architectures; Performance; Thread-level parallelism exploitation},
  document_type   = {Article},
  doi             = {10.1016/j.jpdc.2016.04.003},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84994667744&doi=10.1016%2fj.jpdc.2016.04.003&partnerID=40&md5=beb0782d77838a52d14657a1ee061f89},
}

@Article{Fernandez2015,
  author          = {Fernández, V. and Montaño, J. and Recupero, C. and Kerr, M. and Rosa, P. and Boada-Bauxell, J. and Dalbies, L.},
  title           = {A tool for industrial verification and benchmarking of FDD/FTC designs},
  journal         = {IFAC-PapersOnLine},
  year            = {2015},
  volume          = {28},
  number          = {21},
  pages           = {1006-1011},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {The RECONFIGURE Functional Engineering Simulator (FES) is a simulation software tool based on the MATLAB™/Simulink™ modeling &amp; simulation environment, specifically designed to support the industrial verification and benchmarking of the Fault Detection and Diagnosis (FDD) and Fault Tolerant Control (FTC) algorithm prototypes designed by the partners of the RECONFIGURE project. The FES includes the benchmark scenarios defined by Airbus for the evaluation of the FDD/FTC designs, namely sensor faults, actuator faults and icing conditions. Although based on a reusable genericpurpose simulation infrastructure layer, the FES has been customized for the verification campaign to be executed with the Airbus benchmark model. Although the development of the FES has not concluded at the time of writing this article, we can describe the software architecture, design and simulation capabilities of this tool within the context of an industrial verification &amp; validation process. © 2015, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.},
  affiliation     = {Elecnor Deimos, Ronda de Poniente 19, Tres Cantos, Madrid, 28760, Spain; Deimos Engenharia, Av. Dom João II 2, Lisboa, 1998-023, Portugal; Airbus Operations SAS, Aircraft Control, 316, route de Bayonne, Toulouse Cedex 09, 31060, France},
  author_keywords = {Benchmark examples; Fault Detection; Fault diagnosis; Fault tolerant systems; Simulators},
  document_type   = {Conference Paper},
  doi             = {10.1016/j.ifacol.2015.09.658},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84992502842&doi=10.1016%2fj.ifacol.2015.09.658&partnerID=40&md5=72b4fb218c6945d3042516ec0db6d2c5},
}

@Conference{2015,
  title         = {Proceedings - 2015 IEEE/ACM 8th International Symposium on Software and Systems Traceability, SST 2015},
  year          = {2015},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 10 papers. The topics discussed include: establishing and navigating trace links between elements of informal diagram sketches; tagging in assisted tracing; adaptive user feedback for IR-based traceability recovery; traceability recovery for innovation processes; the parameterized safety requirements templates; toward actionable software architecture traceability; managing security control assumptions using causal traceability; leveraging traceability to reveal the tapestry of quality concerns in source code; using traceability for incremental construction and evolution of software product portfolios; and guidelines for benchmarking automated software traceability techniques.},
  document_type = {Conference Review},
  journal       = {Proceedings - 2015 IEEE/ACM 8th International Symposium on Software and Systems Traceability, SST 2015},
  page_count    = {76},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84955501135&partnerID=40&md5=cedf827b53a18389449e807d81fa2732},
}

@Article{Herzeel2015,
  author        = {Herzeel, C. and Costanza, P. and Decap, D. and Fostier, J. and Reumers, J.},
  title         = {ElPrep: High-performance preparation of sequence alignment/map files for variant calling},
  journal       = {PLoS ONE},
  year          = {2015},
  volume        = {10},
  number        = {7},
  note          = {cited By 10},
  __markedentry = {[Nichl:6]},
  abstract      = {ElPrep is a high-performance tool for preparing sequence alignment/map files for variant calling in sequencing pipelines. It can be used as a replacement for SAMtools and Picard for preparation steps such as filtering, sorting, marking duplicates, reordering contigs, and so on, while producing identical results. What sets elPrep apart is its software architecture that allows executing preparation pipelines by making only a single pass through the data, no matter how many preparation steps are used in the pipeline. elPrep is designed as a multithreaded application that runs entirely in memory, avoids repeated file I/O, and merges the computation of several preparation steps to significantly speed up the execution time. For example, for a preparation pipeline of five steps on a whole-exome BAM file (NA12878), we reduce the execution time from about 1:40 hours, when using a combination of SAMtools and Picard, to about 15 minutes when using elPrep, while utilising the same server resources, here 48 threads and 23GB of RAM. For the same pipeline on whole-genome data (NA12878), elPrep reduces the runtime from 24 hours to less than 5 hours. As a typical clinical study may contain sequencing data for hundreds of patients, elPrep can remove several hundreds of hours of computing time, and thus substantially reduce analysis time and cost. Copyright: © 2015 Herzeel et al.},
  affiliation   = {Imec, Leuven, Belgium; Intel Corporation, Leuven, Belgium; Department of Information Technology, Ghent University, IMinds, Ghent, Belgium; Janssen Research and Development, A Division of Janssen Pharmaceutica NV, Beerse, Belgium; ExaScience Life Lab, Leuven, Belgium},
  art_number    = {e0132868},
  document_type = {Article},
  doi           = {10.1371/journal.pone.0132868},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84941357399&doi=10.1371%2fjournal.pone.0132868&partnerID=40&md5=0e3c1a65e275d01e23a29f56f0a1d1c4},
}

@Article{Chaturvedi2015a,
  author          = {Chaturvedi, N. and Gurunarayanan, S.},
  title           = {An efficient adaptive block pinning for multicore architectures},
  journal         = {Microprocessors and Microsystems},
  year            = {2015},
  volume          = {39},
  number          = {3},
  pages           = {181-188},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Most of today's multi-core processors feature last level shared L2 caches. A major problem faced by such multi-core architectures is cache contention, where multiple cores compete for usage of the single shared L2 cache. Previous research shows that uncontrolled sharing leads to scenarios where one core evicts useful L2 cache content belonging to another core. To address this problem, the paper first presents a cache miss classification scheme - CII: Compulsory, Inter-processor and Intra-processor misses - for CMPs with shared caches and its comparison to the 3C miss classification for a traditional uniprocessor, to provide a better understanding of the interactions between memory references of different processors at the level of shared cache in a CMP. We then propose a novel approach, called block pinning for eliminating inter-processor misses and reducing intra-processor misses in a shared cache. Furthermore, we show that an adaptive block pinning scheme improves over the benefits obtained by the block pinning and set pinning scheme by significantly reducing the number of off-chip accesses. This work also proposes two different schemes of relinquishing the ownership of a block to avoid domination of ownership by a few active cores in the multi-core system which results in performance degradation. Extensive analysis of these approaches with SPEC and PARSEC benchmarks are performed using a full system simulator. © 2015 Elsevier B.V. All rights reserved.},
  affiliation     = {Electrical Electronics Engineering Department, Birla Institute of Technology and Science, Pilani, India},
  author_keywords = {(3C); Capacity miss; Chip Multiprocessors (CMP); Compulsory miss; Conflict miss; Processor Owned Private (POP) cache},
  document_type   = {Article},
  doi             = {10.1016/j.micpro.2015.02.006},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84925864054&doi=10.1016%2fj.micpro.2015.02.006&partnerID=40&md5=f46bd7ea9ac432e9bd233f4030c81349},
}

@Article{Patel2015,
  author          = {Patel, H.J. and Temple, M.A. and Baldwin, R.O.},
  title           = {Improving ZigBee device network authentication using ensemble decision tree classifiers with radio frequency distinct native attribute fingerprinting},
  journal         = {IEEE Transactions on Reliability},
  year            = {2015},
  volume          = {64},
  number          = {1},
  pages           = {221-233},
  note            = {cited By 39},
  __markedentry   = {[Nichl:6]},
  abstract        = {The popularity of ZigBee devices continues to grow in home automation, transportation, traffic management, and Industrial Control System (ICS) applications given their low-cost and low-power. However, the decentralized architecture of ZigBee ad-hoc networks creates unique security challenges for network intrusion detection and prevention. In the past, ZigBee device authentication reliability was enhanced by Radio Frequency-Distinct Native Attribute (RF-DNA) fingerprinting using a Fisher-based Multiple Discriminant Analysis and Maximum Likelihood (MDA-ML) classification process to distinguish between devices in low Signal-to-Noise Ratio (SNR) environments. However, MDA-ML performance inherently degrades when RF-DNA features do not satisfy Gaussian normality conditions, which often occurs in real-world scenarios where radio frequency (RF) multipath and interference from other devices is present. We introduce non-parametric Random Forest (RndF) and Multi-Class AdaBoost (MCA) ensemble classifiers into the RF-DNA fingerprinting arena, and demonstrate improved ZigBee device authentication. Results are compared with parametric MDA-ML and Generalized Relevance Learning Vector Quantization-Improved (GRLVQI) classifier results using identical input feature sets. Fingerprint dimensional reduction is examined using three methods, namely a pre-classification Kolmogorov-Smirnoff Test (KS-Test), a post-classification RndF feature relevance ranking, and a GRLVQI feature relevance ranking. Using the ensemble methods, an SNR = 18.0 dB improvement over MDA-ML processing is realized at an arbitrary correct classification rate (%C) benchmark of %C = 90%; for all SNR ∈ [0, 30] dB considered, %C improvement over MDA-ML ranged from 9% to 24%. Relative to GRLVQI processing, ensemble methods again provided improvement for all SNR, with a best improvement of %C = 10% achieved at the lowest tested SNR = 0.0 dB. Network penetration, measured using rogue ZigBee devices, show that at the SNR = 12.0 dB (%C = 90%) the ensemble methods correctly reject 31 of 36 rogue access attempts based on Receiver Operating Characteristic (ROC) curve analysis and an arbitrary Rogue Accept Rate of RAR < 10%. This performance is better than MDA-ML, and GRLVQI which rejected 25/36, and 28/36 rogue access attempts respectively. The key benefit of ensemble method processing is improved rogue rejection in noisier environments; gains of 6.0 dB, and 18.0 dB are realized over GRLVQI, and MDA-ML, respectively. Collectively considering the demonstrated %C and rogue rejection capability, the use of ensemble methods improves ZigBee network authentication, and enhances anti-spoofing protection afforded by RF-DNA fingerprinting. © 1963-2012 IEEE.},
  affiliation     = {Department of Electrical and Computer Engineering, US Air Force Institute of Technology (AFIT), Wright-Patterson AFB, Dayton, OH, United States; Cyber Center of Excellence, Riverside Research Organization, BeavercreekOH 45431, United States},
  art_number      = {6981992},
  author_keywords = {AdaBoost; generalized relevance learning vector quantization-improved; multiple discriminant analysis and maximum likelihood; Radio frequency-distinct native attribute fingerprinting; random forest; security; ZigBee},
  document_type   = {Article},
  doi             = {10.1109/TR.2014.2372432},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85027926822&doi=10.1109%2fTR.2014.2372432&partnerID=40&md5=6241ef7dbc832f65c424ab763a6c6a10},
}

@Conference{Zhao2015,
  author          = {Zhao, B. and Li, Z. and Jannesari, A. and Wolf, F. and Wu, W.},
  title           = {Dependence-based code transformation for coarse-grained parallelism},
  year            = {2015},
  volume          = {08-February-2015},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures are becoming more common today. Many software products implemented sequentially have failed to exploit the potential parallelism of multicore architectures. Significant re-engineering and refactoring of existing software is needed to support the use of new hardware features. Due to the high cost of manual transformation, an automated approach to transforming existing software and taking advantage of multicore architectures would be highly beneficial. We propose a novel auto-parallelization approach, which integrates data-dependence profiling, task parallelism extraction and source-to-source transformation. Coarse-grained task parallelism is detected based on a concept called Computational Unit(CU). We use dynamic profiling information to gather control- and data-dependences among tasks and generate a task graph. In addition, we develop a source-to-source transformation tool based on LLVM, which can perform high-level code restructuring. It transforms the generated task graph with loop parallelism and task parallelism of sequential code into parallel code using Intel Threading Building Blocks (TBB). We have evaluated NAS Parallel Benchmark applications, three applications from PARSEC benchmark suite, and real world applications. The obtained results confirm that our approach is able to achieve promising performance with minor user interference. The average speedups of loop parallelization and task parallelization are 3.12x and 9.92x respectively. Copyright 2015 ACM.},
  affiliation     = {Xi'an Jiaotong University, Xi'an, China; German Research School for Simulation Sciences, Aachen, Germany; RWTH Aachen University, Aachen, Germany},
  art_number      = {2723777},
  author_keywords = {Coarse-grained parallelism; Code transformation; Data dependence analysis; Source-to-source; TBB},
  document_type   = {Conference Paper},
  doi             = {10.1145/2723772.2723777},
  journal         = {ACM International Conference Proceeding Series},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84986575884&doi=10.1145%2f2723772.2723777&partnerID=40&md5=2c006f96f1624d99e46b81cca7aff848},
}

@Article{Lee2015,
  author          = {Lee, W.-T. and Hsu, K.-H. and Ma, S.-P.},
  title           = {Designing software architecture with service components using design structure matrix},
  journal         = {International Journal of Computational Science and Engineering},
  year            = {2015},
  volume          = {10},
  number          = {1},
  pages           = {82-89},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Use-case-driven and architecture-centric approaches have been widely used to develop software systems, which also impose a great demand for a systematic approach to derive software architectures from requirements. As service-oriented computing (SOC) gains wide acceptance, the need to design an architecture that integrates web services becomes inevitable. To establish software architectures of web service systems from goals and use cases, we propose, in this work, a software architecture design approach with use case blocks and service components based on design structure matrix (DSM). Relations between goals and use cases are identified using DSM. Further, goals and use cases are partitioned into blocks by DSM partitioning mechanism to form an initial system architecture that includes subsystems or high level components. The candidate service components, reused service components or new reusable service components, among all subsystems or high level components are identified to provide the required services based on the dependency relations between use case blocks. The scenarios of the use cases are used to design classes/services of the corresponding subsystems/components. The proposed approach is illustrated by a benchmark problem domain of a meeting scheduler system. Copyright © 2015 Inderscience Enterprises Ltd.},
  affiliation     = {Department of Software Engineering, National Kaohsiung Normal University, No. 62, Shenjhong Rd., Yanchao District, Kaohsiung, Taiwan; Department of Computer Science, National Taichung University of Education, 140 Min-Shen Rd., Taichung, Taiwan; Department of Computer Science and Engineering, National Taiwan Ocean University, 2 Pei-Ning Road, Keelung, Taiwan},
  author_keywords = {Design structure matrix; DSM; Service component; Software architecture; Use case block; Web service},
  document_type   = {Article},
  doi             = {10.1504/IJCSE.2015.067059},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84921918777&doi=10.1504%2fIJCSE.2015.067059&partnerID=40&md5=b791751294844f5cb0697edc34cf4ec5},
}

@Conference{2015a,
  title         = {Procedia Computer Science},
  year          = {2015},
  volume        = {43},
  number        = {C},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 20 papers. The topics discussed include: models for implementation of software configuration management; software engineering competence evaluation portal; software engineering competence evaluation portal; service oriented solution for managing smartlets; software architecture and detailed design evaluation; assessment of name based algorithms for land administration ontology matching; the tourism service consumption model for the sustainability of the special protection areas; applying theory of diffusion of innovations to evaluate technology acceptance and sustainability; system architectures for real-time bee colony temperature monitoring; holistic benchmarking of the bio-economy in protected landscape areas; and automated learning support system to provide sustainable cooperation between adult education institutions and enterprises.},
  document_type = {Conference Review},
  journal       = {Procedia Computer Science},
  page_count    = {162},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84938602668&partnerID=40&md5=3df3d29b5f86ee2c49719cae98a151e2},
}

@Article{2015c,
  title         = {6th International Conference on Fundamentals of Software Engineering, FSEN 2015},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2015},
  volume        = {9392},
  pages         = {1-320},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 21 papers. The special focus in this conference is on Fundamentals of Software Engineering. The topics include: Towards smart systems of systems; automated integration of service-oriented software systems; software architecture modeling and evaluation based on stochastic activity networks; modeling and efficient verification of broadcasting actors; a theory of integrating tamper evidence with stabilization; a safe stopping protocol to enable reliable reconfiguration for component-based distributed systems; efficient architecture-level configuration of large-scale embedded software systems; benchmarks for parity games; incremental realization of safety requirements; analyzing mutable checkpointing via invariants; high performance computing applications using parallel data processing units; improved iterative methods for verifying markov decision processes; a pre-congruence format for xy-simulation; tooled process for early validation of sysML models using modelica simulation; painless support for static and runtime verification of component-based applications; linear evolution of domain architecture in service-oriented software product lines; an interval-based approach to modelling time in event-B and from event-B models to dafny code contracts.},
  document_type = {Conference Review},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84950324286&partnerID=40&md5=849417d8e9cd231e23467aaf0870fabb},
}

@Article{Uddin2015,
  author          = {Uddin, M.K. and Puttonen, J. and Martinez Lastra, J.L.},
  title           = {Context-sensitive optimisation of the key performance indicators for FMS},
  journal         = {International Journal of Computer Integrated Manufacturing},
  year            = {2015},
  volume          = {28},
  number          = {9},
  pages           = {958-971},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article presents a context-sensitive optimisation approach for flexible manufacturing systems (FMSs), considering dynamic machine utilisation rate and overall equipment effectiveness (OEE) as the key performance indicators (KPIs). Run-time contextual entities are used to monitor KPIs continuously to update an ontology-based context model and subsequently convert it into business-relevant information via context management. The delivered high-level knowledge is further utilised by an optimisation support system (OSS) to infer optimal job (re)scheduling and dispatching, keeping a higher machine utilisation rate at run-time. The reference architecture is presented as add-on functionality for FMS control, where a modular development of the overall approach provides the solution generic and extendable across other domains. The key components are functionally implemented to a practical FMS use-case within service-oriented architecture-based control architecture. Test runs are performed in a simulated environment provided by the use-case control software, and the results are analysed, which indicates an improvement of the dynamic machine utilisation rate and the enhancement of the OEE. © 2014 Taylor and Francis.},
  affiliation     = {Department of Production Engineering, Tampere University of Technology, Tampere, FI-33101, Finland},
  author_keywords = {Context; flexible manufacturing systems (FMS); key performance indicator (KPI); optimisation; service-oriented architecture (SOA); web ontology language (OWL); web service (WS)},
  document_type   = {Article},
  doi             = {10.1080/0951192X.2014.941403},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84930989347&doi=10.1080%2f0951192X.2014.941403&partnerID=40&md5=0a5554d228b39b848416a2df5d7d7165},
}

@Article{Lo2015,
  author          = {Lo, Y.J. and Williams, S. and Van Straalen, B. and Ligocki, T.J. and Cordery, M.J. and Wright, N.J. and Hall, M.W. and Oliker, L.},
  title           = {Roofline model toolkit: A practical tool for architectural and program analysis},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2015},
  volume          = {8966},
  pages           = {129-148},
  note            = {cited By 27},
  __markedentry   = {[Nichl:6]},
  abstract        = {We present preliminary results of theRooflineToolkit formulticore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with MessagePassing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture. © Springer International Publishing Switzerland 2015.},
  affiliation     = {Lawerence Berkeley National Laboratory, University of Utah, Salt Lake City, United States},
  author_keywords = {CUDA unified memory; Memory bandwidth; Roofline},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-319-17248-4_7},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84942513489&doi=10.1007%2f978-3-319-17248-4_7&partnerID=40&md5=b79e8a897f5225a94298c19cbfe3d0ef},
}

@Book{Toegl2015,
  title         = {Programming interfaces for the TPM},
  year          = {2015},
  author        = {Toegl, R. and Winter, J. and Gissing, M. and Winkler, T. and Nauman, M. and Hong, T.W.},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The paradigm of Trusted Computing promises a new approach to improve the security of embedded and mobile systems. The core functionality, based on a hardware component known as Trusted Platform Module (TPM), is widely available. However, integration and application in embedded systems remains limited at present, simply because of the extremely steep learning curve involved in using the programmer–facing interfaces. In this chapter, we describe the current state of the Trusted Computing Group's software architecture and present previous approaches to improve usability. We report on a novel design of a high–level API for Trusted Computing for Java which has been optimized for ease–of–use and clear abstraction of Trusted Computing concepts. We derive requirements and design goals and outline the API design. Finally, we show the application and benchmarks in embedded systems. The result of this effort has been standardized as Java Specification Request 321. © Springer International Publishing Switzerland 2015.},
  affiliation   = {Institute for Applied Information Processing and Communications (IAIK), Graz University of Technology, Inffeldgasse 16a, Graz, A-8010, Austria; Pervasive Computing Group/Institute of Networked and Embedded Systems (NES), Alpen–Adria Universitaet Klagenfurt, Lakeside Park B02b, Klagenfurt, A-9020, Austria; Computer Science Research and Development Unit, Peshawar, Pakistan; University of Cambridge Computer Laboratory, William Gates Building, 15 J.J. Thomson Ave, Cambridge, CB3 0FD, United Kingdom},
  document_type = {Book Chapter},
  doi           = {10.1007/978-3-319-09420-5_1},
  journal       = {Trusted Computing for Embedded Systems},
  pages         = {3-32},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84944593509&doi=10.1007%2f978-3-319-09420-5_1&partnerID=40&md5=bb872ec947c0ac4ae637f880dcdec5bf},
}

@Article{Sarwar2015,
  author          = {Sarwar, A. and Rizos, C. and Glennon, E.},
  title           = {Performance of the High Sensitivity Open Source Multi-GNSS Assisted GNSS Reference Server: Benchmarking the OSGRSv2 in Indoor and Diverse Environments},
  journal         = {Journal of Applied Geodesy},
  year            = {2015},
  volume          = {9},
  number          = {2},
  pages           = {81-100},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Open Source GNSS Reference Server (OSGRS) exploits the GNSS Reference Interface Protocol (GRIP) to provide assistance data to GPS receivers. Assistance can be in terms of signal acquisition and in the processing of the measurement data. The data transfer protocol is based on Extensible Mark-up Language (XML) schema. The first version of the OSGRS required a direct hardware connection to a GPS device to acquire the data necessary to generate the appropriate assistance. Scenarios of interest for the OSGRS users are weak signal strength indoors, obstructed outdoors or heavy multipath environments. This paper describes an improved version of OSGRS that provides alternative assistance support from a number of Global Navigation Satellite Systems (GNSS). The underlying protocol to transfer GNSS assistance data from global casters is the Networked Transport of RTCM (Radio Technical Commission for Maritime Services) over Internet Protocol (NTRIP), and/or the RINEX (Receiver Independent Exchange) format. This expands the assistance and support model of the OSGRS to globally available GNSS data servers connected via internet casters. A variety of formats and versions of RINEX and RTCM streams become available, which strengthens the assistance provisioning capability of the OSGRS platform. The prime motivation for this work was to enhance the system architecture of the OSGRS to take advantage of globally available GNSS data sources. Open source software architectures and assistance models provide acquisition and data processing assistance for GNSS receivers operating in weak signal environments. This paper describes test scenarios to benchmark the OSGRSv2 performance against other Assisted-GNSS solutions. Benchmarking devices include the SPOT satellite messenger, MS-Based & MS-Assisted GNSS, HSGNSS (SiRFstar-III) and Wireless Sensor Networks Assisted-GNSS. Benchmarked parameters include the number of tracked satellites, the Time to Fix First (TTFF), navigation availability and accuracy. Three different configurations of Multi-GNSS assistance servers were used, namely Cloud-Client-Server, the Demilitarized Zone (DMZ) Client-Server and PC-Client-Server; with respect to the connectivity location of client and server. The impact on the performance based on server and/or client initiation, hardware capability, network latency, processing delay and computation times with their storage, scalability, processing and load sharing capabilities, were analysed. The performance of the OSGRS is compared against commercial GNSS, Assisted-GNSS and WSN-enabled GNSS devices. The OSGRS system demonstrated lower TTFF and higher availability. © 2015 Walter de Gruyter GmbH, Berlin/Boston.},
  affiliation     = {School of Civil and Environmental Engineering, University of New South Wales, Australia; Australian Centre for Space Engineering Research, University of New South Wales, Australia},
  author_keywords = {Accuracy; Assisted GNSS; Availability; Multi-GNSS; OSGRSv2; TTFF},
  document_type   = {Article},
  doi             = {10.1515/jag-2014-0022},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84936756272&doi=10.1515%2fjag-2014-0022&partnerID=40&md5=3f359ba22f50daff1add107a44db29ae},
}

@Article{Jiang2015,
  author          = {Jiang, J. and Jiang, L. and Kotsovinos, P. and Zhang, J. and Usmani, A. and McKenna, F. and Li, G.-Q.},
  title           = {OpenSees software architecture for the analysis of structures in fire},
  journal         = {Journal of Computing in Civil Engineering},
  year            = {2015},
  volume          = {29},
  number          = {1},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {Computational modeling of structures subjected to extreme static and dynamic loads (such as snow, wind, impact, and earthquake) using finite-element software are part of mainstream structural engineering curricula in universities (at least at graduate level), and many experts can be found in industry who routinely undertake such analyses. However, only a handful or institutions around the world teach structural response to fire (at any level) and only a few of the top consulting engineers in the world truly specialize in this niche area. Among the reasons for this are the lack of cheap and easily accessible software to carry out such analyses and the highly tedious nature of modeling the full (often coupled) sequence of a realistic fire scenario, heat transfer to structure and structural response (currently impossible using a single software). The authors in this paper describe how finite-element software can be extended to include the modeling of structures under fire load. The added advantage of extending existing finite-element codes, as opposed to creating fire-specific applications, is due to ability to perform multihazard type analysis, e.g., fire following earthquake. Due to its open source nature and object-oriented design, the OpenSees software framework is used for this purpose. In this work, the OpenSees framework, which was initially designed for the earthquake analysis of structures, is extended by the addition of new concrete classes for thermal loads, temperature distributions across element cross sections, and material laws based on Eurocodes. Through class and sequence diagrams, this paper shows the interaction of these classes with the existing classes in the OpenSees framework. The performance of this development is tested using benchmark solutions of a single beam with finite stiffness boundary conditions and a steel frame test. The results from OpenSees agree well with analytical solutions for the benchmark problem chosen and provide reasonable agreement with the test. The experience with OpenSees so far suggests that it has excellent potential to be the basis of a unified software framework for enabling computational modeling of realistic fires, and further work is continuing towards the achievement of this goal. The extensions made to OpenSees described in this work, in keeping with the open source ideals of the framework, have been included in the current OpenSees code and are available for researchers and practicing engineers to test, develop, and use for their own purposes. © 2014 American Society of Civil Engineers.},
  affiliation     = {College of Civil Engineering, Tongji Univ., No. 1239 Siping Rd., Shanghai, 200092, China; Dept. of Civil and Environmental Engineering, Univ. of California, Berkeley, CA 94720, United States; China State Key Laboratory for Disaster Reduction in Civil Engineering, Tongji Univ., No. 1239 Siping Rd., Shanghai, 200092, China},
  art_number      = {04014030},
  author_keywords = {Class diagram; Computational modeling; OpenSees; Sequence diagram; Software architecture; Thermomechanical analysis},
  document_type   = {Article},
  doi             = {10.1061/(ASCE)CP.1943-5487.0000305},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84920861972&doi=10.1061%2f%28ASCE%29CP.1943-5487.0000305&partnerID=40&md5=8f1d4cfef49d433c7194ed12c1932ecd},
}

@Conference{Azarian2015,
  author        = {Azarian, A. and Cardoso, J.M.P.},
  title         = {Reducing misses to external memory accesses in task-level pipelining},
  year          = {2015},
  volume        = {2015-July},
  pages         = {1422-1425},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Recently, researchers have shown an increased interest in using task-level pipelining to accelerate the overall execution of applications mainly consisting of producer-consumer tasks. This paper proposes optimization techniques for enhancing our approach to pipeline the execution of producer-consumer tasks in FPGA-based multicore architectures with reductions in the number of accesses to external memory. Our approach is able to speedup the overall execution of successive, data-dependent tasks, by using multiple cores and specific customization features provided by FPGAS. We evaluate the impact in the performance of task-level pipelining when using different hash functions and optimization schemes in the inter stage buffer (ISB). The optimizations proposed in this paper were evaluated with FPGA implementations. The experimental results show the efficiency of a simple scheme to reduce external memory accesses and the suitability of the hash function being used. Furthermore, the results reveal noticeable performance improvements for the set of benchmarks being used. © 2015 IEEE.},
  affiliation   = {Faculty of Engineering, University of Porto and INESC-TEC, Porto, Portugal},
  art_number    = {7168910},
  document_type = {Conference Paper},
  doi           = {10.1109/ISCAS.2015.7168910},
  journal       = {Proceedings - IEEE International Symposium on Circuits and Systems},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84946222134&doi=10.1109%2fISCAS.2015.7168910&partnerID=40&md5=b810233d594841dc07b06f8d16e34a40},
}

@Article{Papagiannopoulou2015,
  author          = {Papagiannopoulou, D. and Capodanno, G. and Moreshet, T. and Herlihy, M. and Bahar, R.I.},
  title           = {Energy-efficient and high-performance lock speculation hardware for embedded multicore systems},
  journal         = {ACM Transactions on Embedded Computing Systems},
  year            = {2015},
  volume          = {14},
  number          = {3},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Embedded systems are becoming increasingly common in everyday life and like their general-purpose counterparts, they have shifted towards shared memory multicore architectures. However, they are much more resource constrained, and as they often run on batteries, energy efficiency becomes critically important. In such systems, achieving high concurrency is a key demand for delivering satisfactory performance at low energy cost. In order to achieve this high concurrency, consistency across the shared memory hierarchy must be accomplished in a cost-effective manner in terms of performance, energy, and implementation complexity. In this article, we propose EMBEDDED-SPEC, a hardware solution for supporting transparent lock speculation, without the requirement for special supporting instructions. Using this approach, we evaluate the energy consumption and performance of a suite of benchmarks, exploring a range of contention management and retry policies. We conclude that for resource-constrained platforms, lock speculation can provide real benefits in terms of improved concurrency and energy efficiency, as long as the underlying hardware support is carefully configured. © 2015.},
  affiliation     = {Brown University, School of Engineering, 184 Hope Street, Providence, RI 02912, United States; Brown University, Computer Science Department, 115 Waterman Street, Providence, RI 02912, United States; Boston University, Department of Electrical and Computer Engineering, 8 Saint Mary's Street, Boston, MA 02215, United States},
  art_number      = {51},
  author_keywords = {energy-efficiency; lock elision; lock removal; low-power; Transactional memory},
  document_type   = {Article},
  doi             = {10.1145/2700097},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84930673122&doi=10.1145%2f2700097&partnerID=40&md5=5fd93e52f4c944d47a6a867aadfaa099},
}

@Conference{Carpen-Amarie2015,
  author          = {Carpen-Amarie, M. and Marlier, P. and Felber, P. and Thomas, G.},
  title           = {A performance study of Java Garbage Collectors on multicore architectures},
  year            = {2015},
  pages           = {20-29},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {In the last few years, managed runtime environments such as the Java Virtual Machine (JVM) are increasingly used on large-scale multicore servers. The garbage collector (GC) represents a critical component of the JVM and has a significant inuence on the overall performance and efficiency of the running application. We perform a study on all available Java GCs, both in an academic environment (set of bench- marks), as well as in a simulated real-life situation (client- server application). We mainly focus on the three most widely used collectors: ParallelOld, ConcurrentMarkSweep and G1. We find that they exhibit different behaviours in the two tested environments. In particular, the default Java GC, ParallelOld, proves to be stable and adequate in the first situation, while in the real-life scenario its use results in unacceptable pauses for the application threads. We believe that this is partly due to the memory requirements of the multicore server. G1 GC performs notably bad on the benchmarks when forced to have a full collection between the iterations of the application. Moreover, even though G1 and ConcurrentMarkSweep GCs introduce significantly lower pauses than ParallelOld in the client-server environment, they can still seriously impact the response time on the client. Pauses of around 3 seconds can make a real-time system unusable and may disrupt the communication between nodes in the case of large-scale distributed systems. Copyright © 2015 ACM.},
  affiliation     = {Université de Neuchâtel, Neuchâtel, Switzerland; Telecom SudParis, Évry, France},
  author_keywords = {Garbage collection; Multicore; Performance analysis},
  document_type   = {Conference Paper},
  doi             = {10.1145/2712386.2712404},
  journal         = {Proceedings of the 6th International Workshop on Programming Models and Applications for Multicores and Manycores, PMAM 2015},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84942417374&doi=10.1145%2f2712386.2712404&partnerID=40&md5=ea80cb1799f369b8a3f800aee12c0dc2},
}

@Article{Muddukrishna2015,
  author        = {Muddukrishna, A. and Jonsson, P.A. and Brorsson, M.},
  title         = {Locality-aware task scheduling and data distribution for OpenMP programs on NUMA systems and manycore processors},
  journal       = {Scientific Programming},
  year          = {2015},
  volume        = {2015},
  note          = {cited By 8},
  __markedentry = {[Nichl:6]},
  abstract      = {Performance degradation due to nonuniform data access latencies has worsened on NUMA systems and can now be felt on-chip in manycore processors. Distributing data across NUMA nodes and manycore processor caches is necessary to reduce the impact of nonuniform latencies. However, techniques for distributing data are error-prone and fragile and require low-level architectural knowledge. Existing task scheduling policies favor quick load-balancing at the expense of locality and ignore NUMA node/manycore cache access latencies while scheduling. Locality-aware scheduling, in conjunction with or as a replacement for existing scheduling, is necessary to minimize NUMA effects and sustain performance. We present a data distribution and locality-aware scheduling technique for task-based OpenMP programs executing on NUMA systems and manycore processors. Our technique relieves the programmer from thinking of NUMA system/manycore processor architecture details by delegating data distribution to the runtime system and uses task data dependence information to guide the scheduling of OpenMP tasks to reduce data stall times. We demonstrate our technique on a four-socket AMD Opteron machine with eight NUMA nodes and on the TILEPro64 processor and identify that data distribution and locality-aware task scheduling improve performance up to 69% for scientific benchmarks compared to default policies and yet provide an architecture-oblivious approach for programmers. Copyright © 2015 Ananya Muddukrishna et al.},
  affiliation   = {KTH Royal Institute of Technology, School of Information and Communication Technology, Electrum 229, Kista, 164 40, Sweden; SICS Swedish ICT AB, Box 1263, Kista, 164 29, Sweden},
  art_number    = {981759},
  document_type = {Article},
  doi           = {10.1155/2015/981759},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84947272497&doi=10.1155%2f2015%2f981759&partnerID=40&md5=ef6b07ce1d8814f08ac93ae5c04ebf52},
}

@Conference{Acharya2015,
  author          = {Acharya, A. and Bondhugula, U.},
  title           = {PLUTO+: Near-complete modeling of affine transformations for parallelism and locality},
  year            = {2015},
  volume          = {2015-January},
  pages           = {54-64},
  note            = {cited By 12},
  __markedentry   = {[Nichl:6]},
  abstract        = {Affine transformations have proven to be very powerful for loop restructuring due to their ability to model a very wide range of transformations. A single multi-dimensional affine function can represent a long and complex sequence of simpler transformations. Existing affine transformation frameworks like the Pluto algorithm, that include a cost function for modern multicore architectures where coarse-grained parallelism and locality are crucial, consider only a sub-space of transformations to avoid a combinatorial explosion in finding the transformations. The ensuing practical tradeoffs lead to the exclusion of certain useful transformations, in particular, transformation compositions involving loop reversals and loop skewing by negative factors. In this paper, we propose an approach to address this limitation by modeling a much larger space of affine transformations in conjunction with the Pluto algorithm's cost function. We perform an experimental evaluation of both, the effect on compilation time, and performance of generated codes. The evaluation shows that our new framework, Pluto+, provides no degradation in performance in any of the Polybench benchmarks. For Lattice Boltzmann Method (LBM) codes with periodic boundary conditions, it provides a mean speedup of 1.33× over Pluto. We also show that Pluto+ does not increase compile times significantly. Experimental results on Polybench show that Pluto+ increases overall polyhedral source-to-source optimization time only by 15%. In cases where it improves execution time significantly, it increased polyhedral optimization time only by 2.04×.},
  affiliation     = {Department of Computer Science and Automation, Indian Institute of Science, Bangalore, 560012, India},
  author_keywords = {Affine scheduling; Affine transformations; Automatic parallelization; Polyhedral model; Stencil computations; Tiling},
  document_type   = {Conference Paper},
  doi             = {10.1145/2688500.2688512},
  journal         = {Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84930963208&doi=10.1145%2f2688500.2688512&partnerID=40&md5=cd9a1652e83b4fc2f8dd5a805febf977},
}

@Conference{Zhang2015,
  author          = {Zhang, X. and Johnson, C.},
  title           = {Reliability metrics and key performance indicators for cloud-based virtualization applications},
  year            = {2015},
  pages           = {1-6},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Cloud computing is an evolving new technology for complex systems to share computing resources. Cloud computing solutions rely on virtualized compute, memory, storage and networking resources to provide services to end users. These solutions will be different from traditional ones with dedicated hardware and software architectures. This paper discusses the challenges associated with telecommunication network virtualization. Methodologies for reliability and availability assessment for cloudbased telecom applications are proposed. Metrics such as equivalent virtualization solution level Mean Time Between Failures (vMTBF) are introduced. This metric can be used to directly compare to field outage rate at the solution-level. Field outages due to hardware failures, virtual machine failures, application software failures, database failures and network failures, etc. will need to be tracked. The overall outage rate can be compared to the solution level failure rate, which can be inversed from vMTBF. These failures are major factors that reduce service retainability and accessibility, two very critical key performance indicators. In this paper, we illustrate how Markov models can be used to estimate the solution level vMTBF. An example case study is used to illustrate the method. Service accessibility and retainability due to system failure are analyzed as functions of the solution-level failure rate.},
  affiliation     = {ATandT, Middletown, NJ 07748, United States},
  author_keywords = {Absorbing-state markov models; Cloud based solutions; Service accessibility; Service retainability; Virtual function reliability; Virtualization solution-level MTBF},
  document_type   = {Conference Paper},
  journal         = {Proceedings - 21st ISSAT International Conference on Reliability and Quality in Design},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84945175753&partnerID=40&md5=78a5fefdc98e77b073e6044dbb4308e3},
}

@Conference{Salami2015,
  author          = {Salamí, E. and Soler, J.A. and Cuadrado, R. and Barrado, C. and Pastor, E.},
  title           = {Virtualizing super-computation on-board UAS},
  year            = {2015},
  volume          = {40},
  number          = {7W3},
  pages           = {1291-1298},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Unmanned aerial systems (UAS, also known as UAV, RPAS or drones) have a great potential to support a wide variety of aerial remote sensing applications. Most UAS work by acquiring data using on-board sensors for later post-processing. Some require the data gathered to be downlinked to the ground in real-time. However, depending on the volume of data and the cost of the communications, this later option is not sustainable in the long term. This paper develops the concept of virtualizing super-computation on-board UAS, as a method to ease the operation by facilitating the downlink of high-level information products instead of raw data. Exploiting recent developments in miniaturized multi-core devices is the way to speed-up on-board computation. This hardware shall satisfy size, power and weight constraints. Several technologies are appearing with promising results for high performance computing on unmanned platforms, such as the 36 cores of the TILE-G ×36 by Tilera (now EZchip) or the 64 cores of the Epiphany-IV by Adapteva. The strategy for virtualizing super-computation on-board includes the benchmarking for hardware selection, the software architecture and the communications aware design. A parallelization strategy is given for the 36-core TILE-G×36 for a UAS in a fire mission or in similar target-detection applications. The results are obtained for payload image processing algorithms and determine in real-time the data snapshot to gather and transfer to ground according to the needs of the mission, the processing time, and consumed watts.},
  affiliation     = {Technical University of Catalonia (UPC), Computer Architecture Department, Castelldefels, 08860, Spain},
  author_keywords = {Benchmarking; Image processing; Parallelization; Remote sensing; Super-computing; UAS; Virtualization},
  document_type   = {Conference Paper},
  doi             = {10.5194/isprsarchives-XL-7-W3-1291-2015},
  journal         = {International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84930406671&doi=10.5194%2fisprsarchives-XL-7-W3-1291-2015&partnerID=40&md5=77210d22d6d6ebc3adbc5d86d3f11737},
}

@Conference{Gracioli2015,
  author          = {Gracioli, G. and Fröhlich, A.A.},
  title           = {On the Influence of Shared Memory Contention in Real-Time Multicore Applications},
  year            = {2015},
  volume          = {2015-April},
  pages           = {25-30},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {The continuous evolution of processor technology has allowed the utilization of multicore architectures in the embedded system domain. A major part of embedded systems, however, are inherently real-time (soft and hard) and the use of multicores in this domain is not straightforward due to their unpredictability in bounding worst-case execution scenarios. One of the main factors for unpredictability is the coherence through memory hierarchy. This paper characterizes the influence of contention for shared data memory in the context of embedded real-time applications. By using a benchmark, we have measured the impact of excessive shared memory invalidations on five processors with three different cache-coherence protocols (MESI, MOESI, and MESIF) and two memory organizations (UMA and ccNUMA). Results have shown that the execution time of an application is affected by the contention for shared memory (up to 3.8 times slower). We also provide an analysis on Hardware Performance Counters (HPCs) and propose to use them in order to monitor and detect excessive memory invalidations at run-time. © 2014 IEEE.},
  affiliation     = {Hardware Software Integration Lab (LISHA), Center for Mobility Engineering (CEM), Federal University of Santa Catarina (UFSC), Joinville, Santa Catarina, Brazil; Hardware Software Integration Lab (LISHA), Computer Science Department (INE), Federal University of Santa Catarina (UFSC), Florianópolis, Santa Catarina, Brazil},
  art_number      = {7091161},
  author_keywords = {cache coherence; contention for shared memory; hardware performance counters; real-time multicore systems},
  document_type   = {Conference Paper},
  doi             = {10.1109/SBESC.2014.8},
  journal         = {Brazilian Symposium on Computing System Engineering, SBESC},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84946709193&doi=10.1109%2fSBESC.2014.8&partnerID=40&md5=b806df95ee24cde2030d3afea3bdcb96},
}

@Conference{Kim2015,
  author          = {Kim, M. and Noh, S. and Huh, S. and Hong, S.},
  title           = {Fair-share scheduling for performance-asymmetric multicore architecture via scaled virtual runtime},
  year            = {2015},
  pages           = {60-69},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {As users begin to demand applications with superior user experience and high service quality, asymmetric multicore processors are increasingly adopted in embedded systems due to their architectural benefits in improved performance and power savings. While fair-share scheduling is a crucial kernel service for such applications, it is still in an early stage when it comes to performance-asymmetric multicore architecture. In this paper, we propose a new fair-share scheduler by adopting the notion of scaled CPU time which reflects performance asymmetry between different types of cores. Our scheduler can work with kernel's dynamic resource control mechanisms since it makes use of a varying performance ratio between cores and thus captures dynamic performance asymmetry such as a core's changing operating frequency. We develop our approach on top of ARM's big.LITTLE architecture which runs Linaro's scheduling framework. Since Linaro's relies on the completely fair scheduler (CFS) of the Linux kernel and CFS is virtual runtime based, we revise the notion of virtual runtime using the scaled CPU time and incorporate it into the proposed approach. As a result, our approach achieves fair-share scheduling by simply balancing tasks' virtual runtimes. To demonstrate its effectiveness, we have implemented the proposed scheduler and performed a series of experiments on ARM's Versatile Express TC2 board. We ran the SPEC CPU2006 and PARSEC benchmarks for three minutes and measured tasks' virtual runtimes. We observed that the maximum virtual runtime difference was only 0.69 seconds in our approach while the original CFS yielded the maximum difference of 8.35 seconds. © 2015 IEEE.},
  affiliation     = {Department of Electrical and Computer Engineering, United States; Department of Transdisciplinary Studies, Graduate School of Convergence Science and Technology, United States; Automation and Systems Research Institute, Seoul National University, 1 Gwanak-ro, Gwanak-gu, Seoul, South Korea; Advanced Institutes of Convergence Technology, Institute for Smart System, 145 Gwanggyo-ro, Yeongtong-gu, Suwon-si, Gyeonggi-do, 443-270, South Korea},
  art_number      = {7299846},
  author_keywords = {Fairness; Linux; Multicore; Performance asymmetry; Task scheduling},
  document_type   = {Conference Paper},
  doi             = {10.1109/RTCSA.2015.10},
  journal         = {Proceedings - IEEE 21st International Conference on Embedded and Real-Time Computing Systems and Applications, RTCSA 2015},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84962920639&doi=10.1109%2fRTCSA.2015.10&partnerID=40&md5=e10c85194c1bb3aec124f1861eedb744},
}

@Article{Ashari2015,
  author          = {Ashari, A. and Riasetiawan, M.},
  title           = {High performance computing on cluster and multicore architecture},
  journal         = {Telkomnika (Telecommunication Computing Electronics and Control)},
  year            = {2015},
  volume          = {13},
  number          = {4},
  pages           = {1408-1413},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Computing needs that is growing rapidly and more and more the need to make extensive computing resources commensurate. High computing needs can be met by using cluster and high speed processor technology. This study analyzes and compares the performance between cluster and processor technology to determine the high performance computer architecture that can support the process of computation data. Research using Raspberry Pi devices that run with the model cluster then be tested to get the value of the performance, FLOPS, CPU Time and Score. FLOPS value obtained then made equivalent to the load carried by the cluster computing Raspberry Pi. Research is also doing the same thing on the i5 and i7 processor architecture. The research use himeno98 and himeno16Large to analysis the processor and the memory allocation. The test is run on 1000x1000 matrix then benchmark with OpenMP. The analysis focuses on CPU Time in FLOPS and every architecture score. The result shows on raspberry cluster architecture have 2576.07 sec in CPU Time, 86.96 MLPOS, and 2.69 score. The result on Core i5 architecture has 55.57 sec in CPU time, 76.30 MLOPS, and 0.92 score. The result in Core i7 architecture has 59.56 sec CPU Time, 1427.61 MLOPS, and 17.23 score. The cluster and multicore architecture result shows that the architecture models effect to the computing process. The comparison showed the computing performance is strongly influenced by the architecture of the processor power source indicated on the i5 and i7 performance is getting better. Research also shows that both models of cluster and core i5 and i7 alike can process the data to complete. © 2015 Universitas Ahmad Dahlan.},
  affiliation     = {Department of Computer Science and Electronics, Faculty of Mathematic and Natural Sciences, Universitas Gadjah Mada, Indonesia},
  author_keywords = {Cluster; High performance computing; Memory; Multicore; Processor},
  document_type   = {Article},
  doi             = {10.12928/TELKOMNIKA.v13i4.2156},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84979737434&doi=10.12928%2fTELKOMNIKA.v13i4.2156&partnerID=40&md5=ad42a69870034ffae61d03a79a3fef2c},
}

@Article{Paulino2014,
  author          = {Paulino, N. and Ferreira, J.C. and Cardoso, J.M.P.},
  title           = {A reconfigurable architecture for binary acceleration of loops with memory accesses},
  journal         = {ACM Transactions on Reconfigurable Technology and Systems},
  year            = {2014},
  volume          = {7},
  number          = {4},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article presents a reconfigurable hardware/software architecture for binary acceleration of embedded applications. A Reconfigurable Processing Unit (RPU) is used as a coprocessor of the General Purpose Processor (GPP) to accelerate the execution of repetitive instruction sequences called Megablocks. A toolchain detects Megablocks from instruction traces and generates customized RPU implementations. The implementation of Megablocks with memory accesses uses a memory-sharing mechanism to support concurrent accesses to the entire address space of the GPP's data memory. The scheduling of load/store operations and memory access handling have been optimized to minimize the latency introduced by memory accesses. The system is able to dynamically switch the execution between the GPP and the RPU when executing the original binaries of the input application. Our proof-of-concept prototype achieved geometric mean speedups of 1.60 × and 1.18 × for, respectively, a set of 37 benchmarks and a subset considering the 9 most complex benchmarks. With respect to a previous version of our approach, we achieved geometric mean speedup improvements from 1.22 to 1.53 for the 10 benchmarks previously used. © 2014 ACM.},
  affiliation     = {INESC TEC, Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal},
  art_number      = {A1},
  author_keywords = {FPGA; Hardware acceleration; Hardware/software architectures; Instruction trace; Megablock; Memory access; Microblaze; Reconfigurable processor},
  document_type   = {Article},
  doi             = {10.1145/2629468},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84911387509&doi=10.1145%2f2629468&partnerID=40&md5=e9721894d65e206d921d000f01dd7ee1},
}

@Book{Wu2014,
  title         = {Software cruising: A new technology for building concurrent software monitor},
  year          = {2014},
  author        = {Wu, D. and Liu, P. and Zeng, Q. and Tian, D.},
  volume        = {9781461492788},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {We introduce a novel concurrent software monitoring technology, called software cruising. It leverages multicore architectures and utilizes lock-free data structures and algorithms to achieve efficient and scalable security monitoring. Applications include, but are not limited to, heap buffer integrity checking, kernel memory cruising, data structure and object invariant checking, rootkit detection, and information provenance and flow checking. In the software cruising framework, one or more dedicated threads, called cruising threads, are running concurrently with the monitored user or kernel code, to constantly check, or cruise, for security violations. We believe the software cruising technology would result in a game-changing capability in security monitoring for the cloud-based and traditional computing and network systems. We have developed two prototypical cruising systems: Cruiser, a lock-free concurrent heap buffer overflow monitor in user space, and Kruiser, a semi-synchronized non-blocking OS kernel cruiser. Our experimental results showed that software cruising can be deployed in practice with modest overhead. In user space, heap buffer overflow cruising incurs only 5 % performance overhead on average for the SPEC CPU2006 benchmark, and the Apache throughput slowdown is only 3 % maximum and negligible on average. In kernel space, it is negligible for SPEC, and 3.8 % for Apache. Both technologies can be deployed in large scale for cloud data centers and server farms in an automated manner. © 2014 Springer Science+Business Media New York. All rights are reserved.},
  affiliation   = {Pennsylvania State University, University Park, PA 16802, United States; Beijing Institute of Technology, Beijing, China},
  document_type = {Book Chapter},
  doi           = {10.1007/978-1-4614-9278-8_14},
  journal       = {Secure Cloud Computing},
  pages         = {303-324},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84929886310&doi=10.1007%2f978-1-4614-9278-8_14&partnerID=40&md5=db3c9087207ab0c3fb79cd1c9830ada6},
}

@Article{Brosig2014,
  author          = {Brosig, F. and Huber, N. and Kounev, S.},
  title           = {Architecture-level software performance abstractions for online performance prediction},
  journal         = {Science of Computer Programming},
  year            = {2014},
  volume          = {90},
  number          = {PART B},
  pages           = {71-92},
  note            = {cited By 20},
  __markedentry   = {[Nichl:6]},
  abstract        = {Modern service-oriented enterprise systems have increasingly complex and dynamic loosely-coupled architectures that often exhibit poor performance and resource efficiency and have high operating costs. This is due to the inability to predict at run-time the effect of workload changes on performance-relevant application-level dependencies and adapt the system configuration accordingly. Architecture-level performance models provide a powerful tool for performance prediction, however, current approaches to modeling the context of software components are not suitable for use at run-time. In this paper, we analyze typical online performance prediction scenarios and propose a performance meta-model for (i) expressing and resolving parameter and context dependencies, (ii) modeling service abstractions at different levels of granularity and (iii) modeling the deployment of software components in complex resource landscapes. The presented meta-model is a subset of the Descartes Meta-Model (DMM) for online performance prediction, specifically designed for use in online scenarios. We motivate and validate our approach in the context of realistic and representative online performance prediction scenarios based on the SPECjEnterprise2010 standard benchmark. © 2013 Elsevier B.V. All rights reserved.},
  affiliation     = {Karlsruhe Institute of Technology, Am Fasanengarten 5, Karlsruhe, Germany},
  author_keywords = {Architecture-level performance meta-model; Online performance prediction; Parameter and context dependencies},
  document_type   = {Article},
  doi             = {10.1016/j.scico.2013.06.004},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84901197910&doi=10.1016%2fj.scico.2013.06.004&partnerID=40&md5=78b2535755bdd6c2f1da7dd86b92d588},
}

@Article{Grant2014,
  author        = {Grant, B.},
  title         = {The new standard in diver communications},
  journal       = {Sea Technology},
  year          = {2014},
  volume        = {55},
  number        = {5},
  pages         = {17-18},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Nautronix completed an investigation into the current capability of available diver communication systems. It was realized that there had been a lack of signifcant innovation in this market in recent years. It was highlighted that industry had a wish for improved clarity of speech to saturation divers, a system that is easier to use, and one that enables quicker fault fnding to decrease dive downtime due to loss of communications. Research was conducted into the various hardware and software architectures and platforms that could be utilized within a new system. A signifcant number of areas were identifed that would enable a new generation of diver communications systems to be created that would exceed the expectations of industry. NASDive was designed from the ground up to offer a completely new approach to diver communication. With its background architecture, touch-screen user interfaces and increased functionality, Nautronix believed this system would be the new benchmark product in the market.},
  affiliation   = {Nautronix, United States},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85026855885&partnerID=40&md5=66d5e6a7e1d0e3de15f9e1e3caf30a52},
}

@Article{Riliskis2014,
  author          = {Riliskis, L. and Osipov, E.},
  title           = {Maestro: An orchestration framework for large-scale WSN simulations},
  journal         = {Sensors (Switzerland)},
  year            = {2014},
  volume          = {14},
  number          = {3},
  pages           = {5392-5414},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Contemporary wireless sensor networks (WSNs) have evolved into large and complex systems and are one of the main technologies used in cyber-physical systems and the Internet of Things. Extensive research on WSNs has led to the development of diverse solutions at all levels of software architecture, including protocol stacks for communications. This multitude of solutions is due to the limited computational power and restrictions on energy consumption that must be accounted for when designing typical WSN systems. It is therefore challenging to develop, test and validate even small WSN applications, and this process can easily consume significant resources. Simulations are inexpensive tools for testing, verifying and generally experimenting with new technologies in a repeatable fashion. Consequently, as the size of the systems to be tested increases, so does the need for large-scale simulations. This article describes a tool called Maestro for the automation of large-scale simulation and investigates the feasibility of using cloud computing facilities for such task. Using tools that are built into Maestro, we demonstrate a feasible approach for benchmarking cloud infrastructure in order to identify cloud Virtual Machine (VM)instances that provide an optimal balance of performance and cost for a given simulation. © 2014 by the authors; licensee MDPI, Basel, Switzerland.},
  affiliation     = {Department of Computer Science, Electrical and Space Engineering, Luleå University of Technology, Luleå 971-87, Sweden},
  author_keywords = {Amazon AWS; Cloud computing; Simulations; Wireless sensor networks},
  document_type   = {Article},
  doi             = {10.3390/s140305392},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84896441869&doi=10.3390%2fs140305392&partnerID=40&md5=07e8ce30558eeff71d07b59dcd412bfa},
}

@Article{Ding2014,
  author          = {Ding, J.-H. and Chang, Y.-T. and Guo, Z.-D. and Li, K.-C. and Chung, Y.-C.},
  title           = {An efficient and comprehensive scheduler on Asymmetric Multicore Architecture systems},
  journal         = {Journal of Systems Architecture},
  year            = {2014},
  volume          = {60},
  number          = {3},
  pages           = {305-314},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Several studies have shown that Asymmetric Multicore Processors (AMPs) systems, which are composed of processors with different hardware characteristics, present better performance and power when compared to homogeneous systems. With Moore's law behavior still lasting, core-count growth creates typical non-uniform memory accesses (NUMA). Existing schedulers assume that the underlying architecture is homogeneous, and as consequence, they may not be well suited for AMP and NUMA systems, since they, respectively, do not properly explore hardware elements asymmetry, while improving memory utilization by avoid multi-processes data starvation. In this paper we propose a new scheduler, namely NUMA-aware Scheduler, to accommodate the next generation of AMP architectures in terms of architecture asymmetry and processes starvation. Experimental results show that the average speedup is 1.36 times faster than default Linux scheduler through evaluation using PARSEC benchmarks, demonstrating that the proposed technique is promising when compared to other prior studies. © 2014 Elsevier B.V.},
  affiliation     = {Dept. of Computer Science, National Tsing Hua University, No. 101, Kuang-Fu Road, Hsinchu 30013, Taiwan; Dept. of Computer Science and Information Engineering, Providence University, Taiwan},
  author_keywords = {Asymmetric architecture; NUMA architecture; Scheduling; Single-ISA heterogeneous multicore processors},
  document_type   = {Article},
  doi             = {10.1016/j.sysarc.2013.05.006},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84897631996&doi=10.1016%2fj.sysarc.2013.05.006&partnerID=40&md5=25a5c2423622fa296c3ddc918d37db67},
}

@Article{Goldberg2014,
  author        = {Goldberg, H.S. and Paterno, M.D. and Rocha, B.H. and Schaeffer, M. and Wright, A. and Erickson, J.L. and Middleton, B.},
  title         = {A highly scalable, interoperable clinical decision support service},
  journal       = {Journal of the American Medical Informatics Association},
  year          = {2014},
  volume        = {21},
  number        = {E2},
  pages         = {e55-e62},
  note          = {cited By 16},
  __markedentry = {[Nichl:6]},
  abstract      = {Objective: To create a clinical decision support (CDS) system that is shareable across healthcare delivery systems and settings over large geographic regions. Materials and methods: The enterprise clinical rules service (ECRS) realizes nine design principles through a series of enterprise java beans and leverages off-theshelf rules management systems in order to provide consistent, maintainable, and scalable decision support in a variety of settings. Results: The ECRS is deployed at Partners HealthCare System (PHS) and is in use for a series of trials by members of the CDS consortium, including internally developed systems at PHS, the Regenstrief Institute, and vendor-based systems deployed at locations in Oregon and New Jersey. Performance measures indicate that the ECRS provides sub-second response time when measured apart from services required to retrieve data and assemble the continuity of care document used as input. Discussion: We consider related work, design decisions, comparisons with emerging national standards, and discuss uses and limitations of the ECRS. Conclusions: ECRS design, implementation, and use in CDS consortium trials indicate that it provides the flexibility and modularity needed for broad use and performs adequately. Future work will investigate additional CDS patterns, alternative methods of data passing, and further optimizations in ECRS performance.},
  affiliation   = {Division of General Internal Medicine and Primary Care, Brigham and Women's Hospital, Boston, MA, United States; Harvard Medical School, Boston, MA, United States; Information Systems, Partners HealthCare, Boston, MA, United States; Department of Biomedical Informatics and the Informatics Center, Vanderbilt University Medical Center, Nashville, TN, United States},
  document_type = {Article},
  doi           = {10.1136/amiajnl-2013-001990},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84893585104&doi=10.1136%2famiajnl-2013-001990&partnerID=40&md5=9a8fe050dd57798f617ab8db42064606},
}

@Conference{Heulot2014,
  author        = {Heulot, J. and Pelcat, M. and Nezan, J.-F. and Oliva, Y. and Aridhi, S. and Bhattacharyya, S.S.},
  title         = {Just-in-time scheduling techniques for multicore signal processing systems},
  year          = {2014},
  pages         = {25-29},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper introduces a novel multicore scheduling method that leverages a parameterized dataflow Model of Computation (MoC). This method, which we have named Just-In-Time Multicore Scheduling (JIT-MS), aims to efficiently schedule Parameterized and Interfaced Synchronous DataFlow (PiSDF) graphs on multicore architectures. This method exploits features of PiSDF to And locally static regions that exhibit predictable communications. This paper uses a multicore signal processing benchmark to demonstrate that the JIT-MS scheduler can exploit more parallelism than a conventional multicore task scheduler based on task creation and dispatch. Experimental results of the JIT-MS on an 8-core Texas Instruments Keystone Digital Signal Processor (DSP) are compared with those obtained from the OpenMP implementation provided by Texas Instruments. Results shows latency improvements of up to 26% for multicore signal processing systems. © 2014 IEEE.},
  affiliation   = {IETR, INSA Rennes, CNRS UMR 6164, Rennes, France; University of Maryland, College Park, United States; Texas Instruments France, 5 Chemin Des Presses, Cagnes-Sur-Mer, United States},
  art_number    = {7032071},
  document_type = {Conference Paper},
  doi           = {10.1109/GlobalSIP.2014.7032071},
  journal       = {2014 IEEE Global Conference on Signal and Information Processing, GlobalSIP 2014},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84974558043&doi=10.1109%2fGlobalSIP.2014.7032071&partnerID=40&md5=08e0aa2ad3d1a5c56a9b5d6668e65f46},
}

@Article{Savadi2014,
  author          = {Savadi, A. and Deldari, H.},
  title           = {Measurement of the latency parameters of the Multi-BSP model: A multicore benchmarking approach},
  journal         = {Journal of Supercomputing},
  year            = {2014},
  volume          = {67},
  number          = {2},
  pages           = {565-584},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Computer benchmarking is a common method for measuring the parameters of a computational model. It helps to measure the parameters of any computer. With the emergence of multicore computers, the evaluation of computers was brought under consideration. Since these types of computers can be viewed and considered as parallel computers, the evaluation methods for parallel computers may be appropriate for multicore computers. However, because multicore architectures seriously focus on cache hierarchy, there is a need for new and different benchmarks to evaluate them correctly. To this end, this paper presents a method for measuring the parameters of one of the most famous multicore computational models, namely Multi-Bulk Synchronous Parallel (Multi-BSP). This method measures the hardware latency parameters of multicore computers, namely communication latency (gi ) and synchronization latency (Li ) for all levels of the cache memory hierarchy in a bottom-up manner. By determining the parameters, the performance of algorithms on multicore architectures can be evaluated as a sequence. © Springer Science+Business Media New York 2013.},
  affiliation     = {Computer Engineering Dept., Ferdowsi University of Mashhad, FUM, Mashhad, Iran},
  author_keywords = {Benchmark; Cache memory hierarchy; Computational model; Multi-BSP; Multicore architectures},
  document_type   = {Article},
  doi             = {10.1007/s11227-013-1018-4},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84896715193&doi=10.1007%2fs11227-013-1018-4&partnerID=40&md5=d4d46429bec0ff253742d69bed1313e6},
}

@Article{Scholliers2014,
  author          = {Scholliers, C. and Tanter, É. and De Meuter, W.},
  title           = {Parallel actor monitors: Disentangling task-level parallelism from data partitioning in the actor model},
  journal         = {Science of Computer Programming},
  year            = {2014},
  volume          = {80},
  number          = {PART A},
  pages           = {52-64},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {While the actor model of concurrency is well appreciated for its ease of use, its scalability is often criticized. Indeed, the fact that execution within an actor is sequential prevents certain actor systems to take advantage of multicore architectures. In order to combine scalability and ease of use, we propose Parallel Actor Monitors (PAMs), as a means to relax the sequentiality of intra-actor activity in a structured and controlled way. A PAM is a modular, reusable scheduler that permits one to introduce intra-actor parallelism in a local and abstract manner. PAM allows the stepwise refinement of local parallelism within a system on a per-actor basis, without having to deal with low-level synchronization details and locks. We present the general model of PAM and its instantiation in the AmbientTalk language. Benchmarks confirm the expected performance gain. © 2013 Elsevier B.V. All rights reserved.},
  affiliation     = {Software Languages Lab, Vrije Universiteit Brussel, Pleinlaan 2, Elsene, Belgium; PLEIAD Laboratory, Computer Science Department (DCC), University of Chile, Avenida Blanco Encalada 2120, Santiago, Chile},
  author_keywords = {Actors; Concurrency; Efficiency; Monitors},
  document_type   = {Conference Paper},
  doi             = {10.1016/j.scico.2013.03.011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84889887583&doi=10.1016%2fj.scico.2013.03.011&partnerID=40&md5=f0d0987d8d321635ecdb7ff59cdd662f},
}

@Article{Sievi-Korte2014,
  author          = {Sievi-Korte, O. and Koskimies, K. and Mäkinen, E.},
  title           = {Techniques for Genetic Software Architecture Design},
  journal         = {Computer Journal},
  year            = {2014},
  volume          = {58},
  number          = {11},
  pages           = {3141-3170},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Software architecture design is a complex task, requiring handling and balancing multiple tradeoffs. In this paper, the potential of genetic algorithms (GAs) in automated software architecture design is explored, assuming that software architecture is constructed of patterns. We have implemented different techniques based on GAs with variations on algorithmic operations and evaluation functions. We perform an extensive case study using a real framework system as a benchmark. The solutions are analyzed and compared with the man-made design of the framework. Our purpose is to study what kind of pattern configurations the algorithm is able to produce, how close they are to the ones used by a human and whether modifying the algorithm gives better solutions. Results show that 60% of the patterns proposed by the algorithm can be seen as well-placed, but there are big differences between the techniques and certain patterns are significantly more difficult for the algorithm to handle than others. © The British Computer Society 2015. All rights reserved.},
  affiliation     = {Department of Pervasive Computing, Tampere University of Technology, Korkeakoulunkatu 1, P.O. Box 553, Tampere, 33101, Finland; School of Information Sciences, University of Tampere, Kanslerinrinne 1, Tampere, 33014, Finland},
  author_keywords = {design patterns; genetic algorithm; search-based software engineering; software architecture},
  document_type   = {Article},
  doi             = {10.1093/comjnl/bxv049},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84947705522&doi=10.1093%2fcomjnl%2fbxv049&partnerID=40&md5=b47e390cd9102212c5593bddffad2cd9},
}

@Article{Happe2014,
  author          = {Happe, L. and Buhnova, B. and Reussner, R.},
  title           = {Stateful component-based performance models},
  journal         = {Software and Systems Modeling},
  year            = {2014},
  volume          = {13},
  number          = {4},
  pages           = {1319-1343},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {The accuracy of performance-prediction models is crucial for widespread adoption of performance prediction in industry. One of the essential accuracy-influencing aspects of software systems is the dependence of system behaviour on a configuration, context or history related state of the system, typically reflected with a (persistent) system attribute. Even in the domain of component-based software engineering, the presence of state-reflecting attributes (the so-called internal states) is a natural ingredient of the systems, implying the existence of stateful services, stateful components and stateful systems as such. Currently, there is no consensus on the definition or method to include state-related information in component-based prediction models. Besides the task to identify and localise different types of stateful information across component-based software architecture, the issue is to balance the expressiveness and complexity of prediction models via an effective abstraction of state modelling. In this paper, we identify and classify stateful information in component-based software systems, study the performance impact of the individual state categories, and discuss the costs of their modelling in terms of the increased model size. The observations are formulated into a set of heuristics-guiding software engineers in state modelling. Finally, practical effect of state modelling on software performance is evaluated on a real-world case study, the SPECjms2007 Benchmark. The observed deviation of measurements and predictions was significantly decreased by more precise models of stateful dependencies. © 2013, Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Karlsruhe Institute of Technology, Karlsruhe, Germany; Masaryk University, Brno, Czech Republic},
  author_keywords = {Performance prediction; Prediction accuracy; Stateful components},
  document_type   = {Article},
  doi             = {10.1007/s10270-013-0336-6},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84911008093&doi=10.1007%2fs10270-013-0336-6&partnerID=40&md5=a8be1980dd6f2f36e44fd4c67b2efc10},
}

@Article{Detten2014,
  author          = {von Detten, M. and Platenius, M.C. and Becker, S.},
  title           = {Reengineering component-based software systems with Archimetrix},
  journal         = {Software and Systems Modeling},
  year            = {2014},
  volume          = {13},
  number          = {4},
  pages           = {1239-1268},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {Many software development, planning, or analysis tasks require an up-to-date software architecture documentation. However, this documentation is often outdated, unavailable, or at least not available as a formal model which analysis tools could use. Reverse engineering methods try to fill this gap. However, as they process the system’s source code, they are easily misled by design deficiencies (e.g., violations of component encapsulation) which leaked into the code during the system’s evolution. Despite the high impact of design deficiencies on the quality of the resulting software architecture models, none of the surveyed related works is able to cope with them during the reverse engineering process. Therefore, we have developed the Archimetrix approach which semiautomatically recovers the system’s concrete architecture in a formal model while simultaneously detecting and removing design deficiencies. We have validated Archimetrix on a case study system and two implementation variants of the CoCoME benchmark system. Results show that the removal of relevant design deficiencies leads to an architecture model which more closely matches the system’s conceptual architecture. © 2013, Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Software Engineering Group, Heinz Nixdorf Institute, University of Paderborn, Paderborn, Germany},
  author_keywords = {Architecture reconstruction; CoCoME; Code metrics; Component-based software systems; Deficiency detection; Design deficiencies; Reengineering; Reverse engineering; Software architecture},
  document_type   = {Article},
  doi             = {10.1007/s10270-013-0341-9},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84911007721&doi=10.1007%2fs10270-013-0341-9&partnerID=40&md5=cea63e20cf17b734b02a16566c330945},
}

@Conference{Sen2014,
  author          = {Sen, A. and Kara, G. and Deniz, E. and Niar, S.},
  title           = {Fast system level benchmarks for multicore architectures},
  year            = {2014},
  pages           = {635-638},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {We present a framework that automatically generates system level synthetic benchmarks from traditional benchmarks. Synthetic benchmarks have similar performance behavior as the original benchmarks that they are generated from and they can run faster. Synthetics can also be used as proxies where original applications are not available in source form. In experiments we observe that not only are our system level benchmarks much smaller than the real benchmarks that they are generated from but they are also much faster. For example, when we generate synthetic benchmarks from the well-known multicore benchmark suite, PARSEC, our benchmarks have an average speedup of 149x over PARSEC benchmarks. We also observe that the performance behavior of synthetics have more than 85% similarity to the real benchmarks. © 2014 IEEE.},
  affiliation     = {Department of Computer Engineering, Bogazici University, Istanbul, 34342, Turkey; ISTV2 UVHC, Campus Mont Houy, Valenciennes Cedex 9, 59313, France},
  art_number      = {6927301},
  author_keywords = {Multicore architecture; Parallel patterns; Performance evaluation; Synthetic benchmarks; SystemC},
  document_type   = {Conference Paper},
  doi             = {10.1109/DSD.2014.16},
  journal         = {Proceedings - 2014 17th Euromicro Conference on Digital System Design, DSD 2014},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84928780866&doi=10.1109%2fDSD.2014.16&partnerID=40&md5=8691e4260395d52b10cf1c0cdd9c6c80},
}

@Conference{Decker2014,
  author          = {Decker, N. and Kuhn, F. and Thoma, D.},
  title           = {Runtime verification of web services for interconnected medical devices},
  year            = {2014},
  pages           = {235-244},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents a framework to ensure the correctness of service-oriented architectures based on runtime verification techniques. Traditionally, the reliability of safety critical systems is ensured by testing the complete system including all subsystems. When those systems are designed as service-oriented architectures, and independently developed subsystems are composed to new systems at runtime, this approach is no longer viable. Instead, the presented framework uses runtime monitors synthesised from high-level specifications to ensure safety constraints. The framework has been designed for the interconnection of medical devices in the operating room. As a case study, the framework is applied to the interconnection of an ultrasound dissector and a microscope. Benchmarks show that the monitoring overhead is negligible in this setting. © 2014 IEEE.},
  affiliation     = {Institute for Software Engineering and Programming Lanugages, University of Lübeck, Germany; Graduate School for Computing in Medicine and Life Sciences, Institute of Telematics, University of Lübeck, Germany},
  art_number      = {6982630},
  author_keywords = {Automata; LTL; Medical Devices; Runtime Verification; SMT; Web Services},
  document_type   = {Conference Paper},
  doi             = {10.1109/ISSRE.2014.16},
  journal         = {Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84928665497&doi=10.1109%2fISSRE.2014.16&partnerID=40&md5=abc502bae58b4585dac9096168d9ce26},
}

@Article{Alaniz2014,
  author        = {Alaniz, M. and Nesmachnow, S. and Goglin, B. and Iturriaga, S. and Gil-Costa, V. and Printista, M.},
  title         = {MBSPDiscover: An automatic benchmark for MultiBSP performance analysis},
  journal       = {Communications in Computer and Information Science},
  year          = {2014},
  volume        = {485},
  pages         = {158-172},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {Multi-Bulk Synchronous Parallel (MultiBSP) is a recently proposed parallel programming model for multicore machines that extends the classic BSP model. MultiBSP is very useful to design algorithms and estimate their running time, which are hard to do in High Performance Computing applications. For a correct estimation of the running time, the main parameters of the MultiBSP model for different multicore architectures need to be determined. This article presents a benchmark proposal for measuring the parameters that characterize the communication and synchronization cost for the model. Our approach discovers automatically the hierarchical structure of the multicore architecture by using a specific tool (hwloc) that allows obtaining runtime information about the machine. We describe the design, implementation and the results of benchmarking two multicore machines. Furthermore, we report the validation of the proposed method by using a real Multi- BSP implementation of the vector inner product algorithm and comparing the predicted execution time against the real execution time. © Springer-Verlag Berlin Heidelberg 2014.},
  affiliation   = {Universidad Nacional de San Luis, Argentina; Universidad de la República, Uruguay; Inria Bordeaux–Sud-Ouest, University of Bordeaux, France},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84908627776&partnerID=40&md5=ec5fd206ad710619e6c9c9657786dcad},
}

@Conference{Saez2014,
  author          = {Sáez, N. and Basden, A. and Guzmán, D. and Dubost, N. and Berdja, A.},
  title           = {Using DARC in a multi-object AO bench and in a dome seeing instrument},
  year            = {2014},
  volume          = {9152},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Durham adaptive Optics Real Time Controller (DARC)1 is a real-time system for astronomical adaptive optics systems originally developed at Durham University and in use for the CANARY instrument. One of its main strengths is to be a generic and high performance real-time controller running on an off-the-shelf Linux computer. We are using DARC for two different implementations: BEAGLE,2 a Multi-Object AO (MOAO) bench system to experiment with novel tomographic reconstructors and LOTUCE2,3 an in-dome turbulence instrument. We present the software architecture for each application, current benchmarks and lessons learned for current and future DARC developers. © 2014 SPIE.},
  affiliation     = {Ponticfiia Universidad Católica de Chile, Centre for Astro-Engineering, Av.Vicuna Mackenna, 4860, Santiago, Chile; Dept. of Electrical Engineering, Pontificia Universidad Católica, Santiago, Chile; University of Durham, Department of Physics, Centre for Advanced Instrumentation, South Road, Durham DH1 3LE, United Kingdom},
  art_number      = {91521X},
  author_keywords = {Adaptive Optics; Charge Coupled Device; DARC; Multi-Object Adaptive Optics; real-time control},
  document_type   = {Conference Paper},
  doi             = {10.1117/12.2055590},
  journal         = {Proceedings of SPIE - The International Society for Optical Engineering},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84906901597&doi=10.1117%2f12.2055590&partnerID=40&md5=2923f7280c092dc06b9767ff96e3955b},
}

@Conference{Eliades2014,
  author          = {Eliades, D.G. and Kyriakou, M. and Polycarpou, M.M.},
  title           = {Sensor placement in water distribution systems using the S-PLACE Toolkit},
  year            = {2014},
  volume          = {70},
  pages           = {602-611},
  note            = {cited By 17},
  __markedentry   = {[Nichl:6]},
  abstract        = {This work presents a new software, the Sensor Placement (S-PLACE) Toolkit, for computing at which locations to install contaminant sensors in water distribution systems to reduce the impact risks. The S-PLACE Toolkit has been designed to be user-friendly, suitable for both the professional and the research community, programmed in Matlab utilizing the EPANET software library, with a modular software architecture to make it extensible. The use of the software is illustrated using benchmark networks which capture different types of real network topologies. © 2013 The Authors. Published by Elsevier Ltd.},
  affiliation     = {KIOS Research Center for Intelligent Systems and Networks, ECE Department, University of Cyprus, 75 Kallipoleos Ave., Nicosia CY-1678, Cyprus},
  author_keywords = {EPANET; Matlab; S-PLACE; Sensor placement; Toolkit; Water distribution},
  document_type   = {Conference Paper},
  doi             = {10.1016/j.proeng.2014.02.066},
  journal         = {Procedia Engineering},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899695638&doi=10.1016%2fj.proeng.2014.02.066&partnerID=40&md5=a92f23f0312c0f672b5af5f0e073749f},
}

@Conference{Chen2014,
  author          = {Chen, X. and Ho, C.P. and Osman, R. and Harrison, P.G. and Knottenbelt, W.J.},
  title           = {Understanding, modelling, and improving the performance of web applications in multicore virtualised environments},
  year            = {2014},
  pages           = {197-207},
  note            = {cited By 12},
  __markedentry   = {[Nichl:6]},
  abstract        = {As the computing industry enters the Cloud era, multicore architectures and virtualisation technologies are replacing traditional IT infrastructures. However, the complex relationship between applications and system resources in multi-core virtualised environments is not well understood. Workloads such as web services and on-line financial applications have the requirement of high performance but benchmark analysis suggests that these applications do not optimally benefit from a higher number of cores. In this paper, we try to understand the scalability behaviour of network/CPU intensive applications running on multicore architectures. We begin by benchmarking the Petstore web application, noting the systematic imbalance that arises with respect to per-core workload. Having identified the reason for this phenomenon, we propose a queueing model which, when appropriately parametrised, reflects the trend in our benchmark results for up to 8 cores. Key to our approach is providing a fine-grained model which incorporates the idiosyncrasies of the operating system and the multiple CPU cores. Analysis of the model suggests a straightforward way to mitigate the observed bottleneck, which can be practically realised by the deployment of multiple virtual NICs within our VM. Next we make blind predictions to forecast performance with multiple virtual NICs. The validation results show that the model is able to predict the expected performance with relative errors ranging between 8 and 26%. Copyright is held by the owner/author(s). Publication rights licensed to ACM.},
  affiliation     = {Department of Computing, Imperial College London, SW7 2AZ, United Kingdom},
  author_keywords = {Benchmarking; Multicore; Performance modelling; Virtualisation; Web applications},
  document_type   = {Conference Paper},
  doi             = {10.1145/2568088.2568102},
  journal         = {ICPE 2014 - Proceedings of the 5th ACM/SPEC International Conference on Performance Engineering},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899698849&doi=10.1145%2f2568088.2568102&partnerID=40&md5=1f697d0820293d6238ef58b2c52c151e},
}

@Article{Azarian2014,
  author          = {Azarian, A. and Cardoso, J.M.P.},
  title           = {Coarse/fine-grained approaches for pipelining computing stages in fpga-based multicore architectures},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2014},
  volume          = {8806},
  pages           = {266-278},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {In recent years, there has been increasing interest on using task-level pipelining to accelerate the overall execution of applications mainly consisting of producer/consumer tasks. This paper presents coarse/fine-grained data flow synchronization approaches to achieve pipelining execution of the producer/consumer tasks in FPGA-based multicore architectures. Our approaches are able to speedup the overall execution of successive, data-dependent tasks, by using multiple cores and specific customization features provided by FPGAs. An important component of our approach is the use of customized inter-stage buffer schemes to communicate data and to synchronize the cores associated to the producer/consumer tasks. The experimental results show the feasibility of the approach when dealing with producer/consumer tasks with out-of-order communication and reveal noticeable performance improvements for a number of benchmarks over a single core implementation and not using task-level pipelining. © Springer International Publishing Switzerland 2014.},
  affiliation     = {University of Porto and INESC-TEC, Porto, Portugal},
  author_keywords = {Data synchronization; FPGA; Multicore Architectures; Producer/ Consumer; Task-level Pipelining},
  document_type   = {Conference Paper},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84920080728&partnerID=40&md5=810074296c38812322c5f75e3bd39a4a},
}

@Conference{Buono2014,
  author          = {Buono, D. and De Matteis, T. and Mencagli, G. and Vanneschi, M.},
  title           = {Optimizing message-passing on multicore architectures using hardware multi-threading},
  year            = {2014},
  pages           = {262-270},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {Shared-memory and message-passing are two opposite models to develop parallel computations. The shared-memory model, adopted by existing frameworks such as OpenMP, represents a de-facto standard on multi-/many-core architectures. However, message-passing deserves to be studied for its inherent properties in terms of portability and flexibility as well as for its better ease of debugging. Achieving good performance from the use of messages in shared-memory architectures requires an efficient implementation of the run-time support. This paper investigates the definition of a delegation mechanism on multi-threaded architectures able to: (i) overlap communications with calculation phases, (ii) parallelize distribution and collective operations. Our ideas have been exemplified using two parallel benchmarks on the Intel Phi, showing that in these applications our message-passing support outperforms MPI and reaches similar performance compared to standard OpenMP implementations. © 2014 IEEE.},
  affiliation     = {Department of Computer Science, University of Pisa, Largo B. Pontecorvo, 3, I-56127 Pisa, Italy},
  art_number      = {6787285},
  author_keywords = {communications-calculation threading. overlapping; hardware multi-threading; message passing; multi-core; shared memory},
  document_type   = {Conference Paper},
  doi             = {10.1109/PDP.2014.63},
  journal         = {Proceedings - 2014 22nd Euromicro International Conference on Parallel, Distributed, and Network-Based Processing, PDP 2014},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899410018&doi=10.1109%2fPDP.2014.63&partnerID=40&md5=742e783bbf7b8ca7eb76ea7600e82697},
}

@Conference{Mohamedin2014,
  author          = {Mohamedin, M. and Palmieri, R. and Ravindran, B.},
  title           = {On making transactional applications resilient to data corruption faults},
  year            = {2014},
  pages           = {213-220},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures are becoming increasingly prone to transient faults and data corruption. Relying on a multicore architecture is the common solution for increasing performance and scalability of core applications including transactional applications. In this paper we present SoftX, a low-invasive protocol for supporting execution of transactional applications relying on speculative processing and dedicated committer threads. Upon starting a transaction, SoftX forks a number of threads running the same transaction independently. The commit phase is handled by dedicated threads for optimizing synchronization's overhead. We conduct an evaluation study showing the performance obtained with the implementation of SoftX on a 48 cores AMD machine, running List, Bank and TPC-C benchmarks. Results reveal better performance than classical replication-based fault-tolerant systems and limited overhead with respect to non fault-tolerant protocols. We ported SoftX to a message-passing architecture, Tilera TILE-Gx. Hardware message-passing is an important emerging trend in multicore architectures. Our experiments on Tilera show that SoftX is still more efficient than replication. © 2014 IEEE.},
  affiliation     = {Virginia Tech, United States},
  art_number      = {6924230},
  author_keywords = {Fault tolerance; Multicore processing; Transactional systems},
  document_type   = {Conference Paper},
  doi             = {10.1109/NCA.2014.39},
  journal         = {Proceedings - 2014 IEEE 13th International Symposium on Network Computing and Applications, NCA 2014},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84911025316&doi=10.1109%2fNCA.2014.39&partnerID=40&md5=c82faca6915abc381248ae4bf8660713},
}

@Conference{Poluri2014,
  author          = {Poluri, P. and Louri, A.},
  title           = {An improved router design for reliable on-chip networks},
  year            = {2014},
  pages           = {283-292},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {Aggressive technology scaling into the deep nanometer regime has made the Network-on-Chip (NoC) in multicore architectures increasingly vulnerable to faults. This has accelerated the need for designing reliable NoCs. To this end, we propose a reliable NoC router architecture capable of tolerating multiple permanent faults. The proposed router achieves a better reliability without incurring too much area and power overhead as compared to the baseline NoC router or other fault-tolerant routers. Reliability analysis using Mean Time to Failure (MTTF) reveals that our proposed router is six times more reliable than the baseline NoC router (without protection). We also compare our proposed router with other existing fault-tolerant routers such as Bullet Proof, Vicis and RoCo using Silicon Protection Factor (SPF) as a metric. SPF analysis shows that our proposed router is more reliable than the mentioned existing fault tolerant routers. Hardware synthesis performed by Cadence Encounter RTL Compiler using commercial 45nm technology library shows that the correction circuitry incurs an area overhead of 31% and power overhead of 30%. Latency analysis on a 64-core mesh based NoC simulated using GEM5 and running SPLASH-2 and PARSEC benchmark application traffic shows that in the presence of multiple faults, our proposed router increases the overall latency by only 10% and 13% respectively while providing better reliability. © 2014 IEEE.},
  affiliation     = {Department of Electrical and Computer Engineering, University of Arizona, Tucson, United States},
  art_number      = {6877263},
  author_keywords = {Area; Network-on-Chip; Power; Reliability},
  document_type   = {Conference Paper},
  doi             = {10.1109/IPDPS.2014.39},
  journal         = {Proceedings of the International Parallel and Distributed Processing Symposium, IPDPS},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84906658505&doi=10.1109%2fIPDPS.2014.39&partnerID=40&md5=8def4d9ebfe23bd82739e90f4f6262b6},
}

@Conference{Bueno2014,
  author        = {Bueno, M.A.F. and De Holanda, J.A.M. and Pereira, E. and Marques, E.},
  title         = {Operating system support to an online hardware-software co-design scheduler for heterogeneous multicore architectures},
  year          = {2014},
  note          = {cited By 2},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper aims at designing and implementing a scheduler model for heterogeneous multiprocessor architectures based on software and hardware. As a proof of concept, the scheduler model was applied to the Linux operating system running on the SPARC Leon3 processor. In this sense, performance monitors have been implemented within the processors, which identify demands of processes in real-time. For each process, its demand is projected for the other processors in the architecture and then, it is performed a balancing to maximize the total system performance by distributing processes among processors. The Hungarian maximization algorithm, used in balancing scheduler was developed in hardware, and provides greater parallelism and performance in the execution of the algorithm. The scheduler has been validated through the parallel execution of several benchmarks, resulting in decreased execution times compared to the scheduler without the heterogeneity support. © 2014 IEEE.},
  affiliation   = {USP - University of São Paulo, Institute of Mathematics and Computer Science, Sao Carlos, Brazil},
  art_number    = {6910514},
  document_type = {Conference Paper},
  doi           = {10.1109/RTCSA.2014.6910514},
  journal       = {RTCSA 2014 - 20th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84908637359&doi=10.1109%2fRTCSA.2014.6910514&partnerID=40&md5=908ce86242e623c813ebc2fe4a899719},
}

@Conference{Wang2014,
  author        = {Wang, J. and Beu, J. and Bheda, R. and Conte, T. and Dong, Z. and Kersey, C. and Rasquinha, M. and Riley, G. and Song, W. and Xiao, H. and Xu, P. and Yalamanchili, S.},
  title         = {Manifold: A parallel simulation framework for multicore systems},
  year          = {2014},
  pages         = {106-115},
  note          = {cited By 40},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper presents Manifold, an open-source parallel simulation framework for multicore architectures. It consists of a parallel simulation kernel, a set of microarchitecture components, and an integrated library of power, thermal, reliability, and energy models. Using the components as building blocks, users can assemble multicore architecture simulation models and perform serial or parallel simulations to study the architectural and/or the physical characteristics of the models. Users can also create new components for Manifold or port existing models. Importantly, Manifold's component-based design provides the user with the ability to easily replace a component with another for efficient explorations of the design space. It also allows components to evolve independently and making it easy for simulators to incorporate new components as they become available. The distinguishing features of Manifold include i) transparent parallel execution, ii) integration of power, thermal, reliability, and energy models, iii) full system simulation, e.g., operating system and system binaries, and iv) component-based design. In this paper we provide a description of the software architecture of Manifold, and its main elements - a parallel multicore emulator front-end and a parallel component-based back-end timing model. We describe a few simulators that are built with Manifold components to illustrate its flexibility, and present test results of the scalability obtained on full-system simulation of coherent shared-memory multicore models with 16, 32, and 64 cores executing PARSEC and SPLASH-2 benchmarks. © 2014 IEEE.},
  affiliation   = {School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, 30332-0250, United States},
  art_number    = {6844466},
  document_type = {Conference Paper},
  doi           = {10.1109/ISPASS.2014.6844466},
  journal       = {ISPASS 2014 - IEEE International Symposium on Performance Analysis of Systems and Software},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84904504588&doi=10.1109%2fISPASS.2014.6844466&partnerID=40&md5=bc4b7ee4ccf1a870f8a491fa996084cc},
}

@Article{Chattopadhyay2014,
  author          = {Chattopadhyay, S. and Chong, L.K. and Roychoudhury, A. and Kelter, T. and Marwedel, P. and Falk, H.},
  title           = {A unified WCET analysis framework for multicore platforms},
  journal         = {Transactions on Embedded Computing Systems},
  year            = {2014},
  volume          = {13},
  number          = {4 SPEC. ISSUE},
  note            = {cited By 22},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the advent of multicore architectures, worst-case execution time (WCET) analysis has become an increasingly difficult problem. In this article, we propose a unified WCET analysis framework for multicore processors featuring both shared cache and shared bus. Compared to other previous works, our work differs by modeling the interaction of shared cache and shared bus with other basic microarchitectural components (e.g., pipeline and branch predictor). In addition, our framework does not assume a timing anomaly free multicore architecture for computing the WCET. A detailed experiment methodology suggests that we can obtain reasonably tight WCET estimates in a wide range of benchmark programs. © 2014 ACM.},
  affiliation     = {Linköping University, SE-581 83 Linköping, Sweden; National University of Singapore, 21 Lower Kent Ridge Road, Singapore 119077, Singapore; Technical University of Dortmund, 44221 Dortmund, Germany; Ulm University, Helmholtzstrasse 18, 89081 Ulm, Germany},
  art_number      = {124},
  author_keywords = {Multicore; Shared bus; Shared cache; WCET},
  document_type   = {Article},
  doi             = {10.1145/2584654},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84905734416&doi=10.1145%2f2584654&partnerID=40&md5=3c0871f81a8ea4ed7227ce3cb6fbb261},
}

@Conference{Chennupati2014,
  author          = {Chennupati, G. and Fitzgerald, J. and Ryan, C.},
  title           = {On the efficiency of Multi-core Grammatical Evolution (MCGE) evolving multi-core parallel programs},
  year            = {2014},
  pages           = {238-243},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper we investigate a novel technique that optimizes the execution time of Grammatical Evolution through the usage of on-chip multiple processors. This technique, Multicore Grammatical Evolution (MCGE) evolves natively parallel programs with the help of OpenMP primitives through the grammars, such that not only can we exploit parallelism while evolving individuals, but the final individuals produced can also be executed on parallel architectures even outside the evolutionary system. We test MCGE on two difficult benchmark GP problems and show its efficiency in exploiting the power of the multicore architectures. We further discuss that, on these problems, the system evolves longer individuals while they are evaluated quicker than their serial implementation. © 2014 IEEE.},
  affiliation     = {Bio-Computing and Developmental Systems Group, University of Limerick, Ireland},
  art_number      = {6921885},
  author_keywords = {Grammatical Evolution; Multi-cores; OpenMP; Parallel Programming; Symbolic Regression},
  document_type   = {Conference Paper},
  doi             = {10.1109/NaBIC.2014.6921885},
  journal         = {2014 6th World Congress on Nature and Biologically Inspired Computing, NaBIC 2014},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84911885480&doi=10.1109%2fNaBIC.2014.6921885&partnerID=40&md5=80db0da8fab32c6c255d2c597a693a01},
}

@Conference{Margara2014,
  author          = {Margara, A. and Cugola, G. and Tamburrelli, G.},
  title           = {Learning from the past: Automated rule generation for complex event processing},
  year            = {2014},
  pages           = {47-58},
  note            = {cited By 41},
  __markedentry   = {[Nichl:6]},
  abstract        = {Complex Event Processing (CEP) systems aim at processing large flows of events to discover situations of interest. In CEP, the processing takes place according to user-defined rules, which specify the (causal) relations between the observed events and the phenomena to be detected. We claim that the complexity of writing such rules is a limiting factor for the diffusion of CEP. In this paper, we tackle this problem by introducing iCEP, a novel framework that learns, from historical traces, the hidden causality between the received events and the situations to detect, and uses them to automatically generate CEP rules. The paper introduces three main contributions. It provides a precise definition for the problem of automated CEP rules generation. It dicusses a general approach to this research challenge that builds on three fundamental pillars: decomposition into subproblems, modularity of solutions, and ad-hoc learning algorithms. It provides a concrete implementation of this approach, the iCEP framework, and evaluates its precision in a broad range of situations, using both synthetic benchmarks and real traces from a traffic monitoring scenario. © 2014 ACM.},
  affiliation     = {Dept. of Computer Science, Vrije Universiteit Amsterdam, Netherlands; Dip. di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Italy; Faculty of Informatics, Università Della Svizzera Italiana Lugano, Italy},
  author_keywords = {complex event processing; learning; rule generation},
  document_type   = {Conference Paper},
  doi             = {10.1145/2611286.2611289},
  journal         = {DEBS 2014 - Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84903132818&doi=10.1145%2f2611286.2611289&partnerID=40&md5=f6409a4bd8bbe135246cf0c7a52452b4},
}

@Article{Lyberis2014,
  author          = {Lyberis, S. and Kalokerinos, G. and Lygerakis, M. and Papaefstathiou, V. and Mavroidis, I. and Katevenis, M. and Pnevmatikatos, D. and Nikolopoulos, D.S.},
  title           = {FPGA prototyping of emerging manycore architectures for parallel programming research using Formic boards},
  journal         = {Journal of Systems Architecture},
  year            = {2014},
  volume          = {60},
  number          = {6},
  pages           = {481-493},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance evaluation of parallel software and architectural exploration of innovative hardware support face a common challenge with emerging manycore platforms: they are limited by the slow running time and the low accuracy of software simulators. Manycore FPGA prototypes are difficult to build, but they offer great rewards. Software running on such prototypes runs orders of magnitude faster than current simulators. Moreover, researchers gain significant architectural insight during the modeling process. We use the Formic FPGA prototyping board [1], which specifically targets scalable and cost-efficient multi-board prototyping, to build and test a 64-board model of a 512-core, MicroBlaze-based, non-coherent hardware prototype with a full network-on-chip in a 3D-mesh topology. We expand the hardware architecture to include the ARM Versatile Express platforms and build a 520-core heterogeneous prototype of 8 Cortex-A9 cores and 512 MicroBlaze cores. We then develop an MPI library for the prototype and evaluate it extensively using several bare-metal and MPI benchmarks. We find that our processor prototype is highly scalable, models faithfully single-chip multicore architectures, and is a very efficient platform for parallel programming research, being 50,000 times faster than software simulation. © 2014 Elsevier B.V. All rights reserved.},
  affiliation     = {Institute of Computer Science, Foundation for Research and Technology - Hellas (FORTH-ICS), Greece; Computer Science Department, University of Crete, Greece; Electronic and Computer Engineering Department, Technical University of Crete, Greece; School of Electronics, Electrical Engineering and Computer Science, Queen's University of Belfast, United Kingdom},
  author_keywords = {Formic; FPGA; Manycore; Multi-board prototyping; Non-coherent architecture},
  document_type   = {Article},
  doi             = {10.1016/j.sysarc.2014.03.002},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84901643574&doi=10.1016%2fj.sysarc.2014.03.002&partnerID=40&md5=3b7c014e572fc609ca0c1486274cf17f},
}

@Conference{Heulot2014a,
  author        = {Heulot, J. and Pelcat, M. and Desnos, K. and Nezan, J.-F. and Aridhi, S.},
  title         = {Spider: A synchronous parameterized and interfaced dataflow-based RTOS for multicore DSPS},
  year          = {2014},
  pages         = {167-171},
  note          = {cited By 11},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper introduces a novel Real-Time Operating System (RTOS) based on a parameterized dataflow Model of Computation (MoC). This RTOS, called Synchronous Parameterized and Interfaced Dataflow Embedded Runtime (SPiDER), aims at efficiently scheduling Parameterized and Interfaced Synchronous Dataflow (PiSDF) graphs on multicore architectures. It exploits features of PiSDF to locate locally static regions that exhibit predictable application behavior. This paper uses a multicore signal processing benchmark to demonstrate that the SPiDER runtime can exploit more parallelism than a conventional multicore task scheduler. By comparing experimental results of the SPiDER runtime on an 8-core Texas Instruments Keystone I Digital Signal Processor (DSP) with those obtained from the OpenMP framework, latency improvements of up to 26% are demonstrated. © 2014 IEEE.},
  affiliation   = {IETR, INSA Rennes, UMR CNRS 6164, 20 Avenue des Buttes de Coäsmes, Rennes, 35708, France; Texas Instrument France, 5 Chemin Des Presses, 4 Allée Technopolis, Cagnes-Sur-Mer, France},
  art_number    = {6924381},
  document_type = {Conference Paper},
  doi           = {10.1109/EDERC.2014.6924381},
  journal       = {EDERC 2014 - Proceedings of the 6th European Embedded Design in Education and Research Conference},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84912090736&doi=10.1109%2fEDERC.2014.6924381&partnerID=40&md5=1440488ca7a2e31aa6b85365d355acbc},
}

@Article{Canedo2014,
  author          = {Canedo, A. and Ludwig, H. and Al Faruque, M.A.},
  title           = {High communication throughput and low scan cycle time with multi/many-core programmable logic controllers},
  journal         = {IEEE Embedded Systems Letters},
  year            = {2014},
  volume          = {6},
  number          = {2},
  pages           = {21-24},
  note            = {cited By 12},
  __markedentry   = {[Nichl:6]},
  abstract        = {Programmable logic controllers (PLCs) are hard real-time embedded systems designed to interact with (through sensors and actuators) and control physical processes in pharmaceutical, manufacturing, energy, and automotive industries. PLCs are the fundamental building blocks in modern industrial automation systems; they are designed to operate in extreme, harsh environments for decades without interruption. This letter presents and evaluates a novel, scalable hardware/software architecture for multi/many-core PLCs capable of pipelining the 'sensing-executing-actuating' stages of a control system and thus reducing the scan cycle time (SCT) and maintaining a high-throughput communication (HTC). SCT and HTC are two of the most important key performance indicators (KPIs) in industrial control systems because they determine the accuracy of the control algorithms. © 2014 IEEE.},
  affiliation     = {Siemens Corporation, Corporate Technology, Princeton, NJ 08540, United States; Department of Electrical Engineering and Computer Science, University of California, Irvine, CA 92697, United States},
  art_number      = {6710138},
  author_keywords = {Compilation methods; cyber-physical systems; embedded control systems; programming languages for embedded real-time; real-time systems},
  document_type   = {Article},
  doi             = {10.1109/LES.2014.2299731},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84902190442&doi=10.1109%2fLES.2014.2299731&partnerID=40&md5=98398284ed5659d389deda6ba709fbb0},
}

@Conference{Stanik2014,
  author          = {Stanik, A. and Koerner, M. and Lymberopoulos, L.},
  title           = {Sla-driven federated cloud networking: Quality of service for cloud-based Software Defined Networks},
  year            = {2014},
  volume          = {34},
  pages           = {655-660},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {Since the cloud paradigm becomes more and more popular for the dynamic resources allocation, new techniques and performance improvements for scalability as well as new cloud services on all three layers of the cloud stack were developed. Furthermore, another well covered topic is cloud federation concerning processing power and strength. However, the flexibility of the cloud is limited in terms of network services and federated networking between autonomous cloud data-center. With the promising opportunities of Software Defined Networking (SDN), this gap can be closed and enables cloud environments to establish networking federations between autonomous data-centers and virtual network partitioning in a single cloud infrastructure. In this paper we are introducing and describing an architectural approach for a generic layered model and API based software architecture to orchestrate and federate heterogeneous networks. In particular, we present an architecture that enables Quality of Service (QoS) aware configurations of network resources in a cloud infrastructure of one data-center and federated networking between different SDN based cloud networks over and above the data-center network edge. Furthermore, this architecture uses a Service Level Agreement (SLA) protocol and language to expose Key Performance Indicators (KPI) and to negotiate appropriated QoS constrains which are applied to the virtually sliced underlying network substrate. In this way, capabilities of the orchestration and the current utilization of the network are building the foundation for dynamic negotiated SLAs and the within guaranteed QoS of network resources. The approach presented in this paper will change today's IT landscape and allows every organization to purchase required network characteristics on demand. © 2014 The Authors. Published by Elsevier B.V.},
  affiliation     = {Technische Universitöt Berlin, Einsteinufer 17, Berlin, 10587, Germany; EXUS S.A, 1 Estias Str. and 73-75 Messogion Av., Athens, 11526, Greece},
  author_keywords = {Cloud Computing; Cloud Federation; Federated Networking; OpenFlow; Service Level Agreements; Software Defined Networking; WS-Agreement},
  document_type   = {Conference Paper},
  doi             = {10.1016/j.procs.2014.07.093},
  journal         = {Procedia Computer Science},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84906813808&doi=10.1016%2fj.procs.2014.07.093&partnerID=40&md5=c5cae038de74ea035a17c3aeecd8ab88},
}

@Article{Sivaramakrishnan2014,
  author        = {Sivaramakrishnan, K.C. and Ziarek, L. and Jagannathan, S.},
  title         = {MultiMLton: A multicore-aware runtime for standard ML},
  journal       = {Journal of Functional Programming},
  year          = {2014},
  volume        = {24},
  number        = {6},
  pages         = {613-674},
  note          = {cited By 7},
  __markedentry = {[Nichl:6]},
  abstract      = {MULTIMLTON is an extension of the MLton compiler and runtime system that targets scalable, multicore architectures. It provides specific support for ACML, a derivative of Concurrent ML that allows for the construction of composable asynchronous events. To effectively manage asynchrony, we require the runtime to efficiently handle potentially large numbers of lightweight, short-lived threads, many of which are created specifically to deal with the implicit concurrency introduced by asynchronous events. Scalability demands also dictate that the runtime minimize global coordination. MULTIMLTON therefore implements a split-heap memory manager that allows mutators and collectors running on different cores to operate mostly independently. More significantly, MULTIMLTON exploits the premise that there is a surfeit of available concurrency in ACML programs to realize a new collector design that completely eliminates the need for read barriers, a source of significant overhead in other managed runtimes. These two symbiotic features - a thread design specifically tailored to support asynchronous communication, and a memory manager that exploits lightweight concurrency to greatly reduce barrier overheads - are MULTIMLTON's key novelties. In this article, we describe the rationale, design, and implementation of these features, and provide experimental results over a range of parallel benchmarks and different multicore architectures including an 864 core Azul Vega 3, and a 48 core non-coherent Intel SCC (Single-Cloud Computer), that justify our design decisions. © Cambridge University Press.},
  affiliation   = {Purdue University, West Lafayette, IN, United States; SUNY BuffaloNY, United States},
  document_type = {Review},
  doi           = {10.1017/S0956796814000161},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84922418704&doi=10.1017%2fS0956796814000161&partnerID=40&md5=916975ad8d91db59bd05aa8805d7c646},
}

@Article{Jeyapaul2014,
  author          = {Jeyapaul, R. and Hong, F. and Rhisheekesan, A. and Shrivastava, A. and Lee, K.},
  title           = {UnSync-CMP: Multicore CMP architecture for energy-efficient soft-error reliability},
  journal         = {IEEE Transactions on Parallel and Distributed Systems},
  year            = {2014},
  volume          = {25},
  number          = {1},
  pages           = {254-263},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {Reducing device dimensions, increasing transistor densities, and smaller timing windows, expose the vulnerability of processors to soft errors induced by charge carrying particles. Since these factors are only consequences of the inevitable advancement in processor technology, the industry has been forced to improve reliability on general purpose chip multiprocessors (CMPs). With the availability of increased hardware resources, redundancy-based techniques are the most promising methods to eradicate soft-error failures in CMP systems. In this work, we propose a novel customizable and redundant CMP architecture (UnSync) that utilizes hardware-based detection mechanisms (most of which are readily available in the processor), to reduce overheads during error-free executions. In the presence of errors (which are infrequent), the always forward execution enabled recovery mechanism provides for resilience in the system. The inherent nature of our architecture framework supports customization of the redundancy, and thereby provides means to achieve possible performance-reliability tradeoffs in many-core systems. We provide a redundancy-based soft-error resilient CMP architecture for both write-through and write-back cache configurations. We design a detailed RTL model of our UnSync architecture and perform hardware synthesis to compare the hardware (power/area) overheads incurred. We compare the same with those of the Reunion technique, a state-of-the-art redundant multicore architecture. We also perform cycle-accurate simulations over a wide range of SPEC2000, and MiBench benchmarks to evaluate the performance efficiency achieved over that of the Reunion architecture. Experimental results show that, our UnSync architecture reduces power consumption by 34.5 percent and improves performance by up to 20 percent with 13.3 percent less area overhead, when compared to the Reunion architecture for the same level of reliability achieved. © 2015 IEEE.},
  affiliation     = {Compiler Microarchitecture Lab, School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, 699 South Mill Avenue, Tempe, AZ 85281, United States; Dependable Computing Lab, Yonsei University, 50 Yonsei-ro, Seodaemun-gu, Seoul, 120-749, South Korea},
  art_number      = {6410312},
  author_keywords = {CMP; Multicore architecture; power efficiency; reliability; soft error},
  document_type   = {Article},
  doi             = {10.1109/TPDS.2013.14},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84930653090&doi=10.1109%2fTPDS.2013.14&partnerID=40&md5=e98f1b6d6f5dd3685da69173126ae686},
}

@Conference{Tudoran2014,
  author          = {Tudoran, R. and Nano, O. and Santos, I. and Costan, A. and Soncu, H. and Bougé, L. and Antoniu, G.},
  title           = {JetStream: Enabling high performance event streaming across cloud data-centers},
  year            = {2014},
  pages           = {23-34},
  note            = {cited By 18},
  __markedentry   = {[Nichl:6]},
  abstract        = {The easily-accessible computation power offered by cloud infrastructures coupled with the revolution of Big Data are expanding the scale and speed at which data analysis is performed. In their quest for finding the Value in the 3 Vs of Big Data, applications process larger data sets, within and across clouds. Enabling fast data transfers across geographically distributed sites becomes particularly important for applications which manage continuous streams of events in real time. Scientific applications (e.g. the Ocean Observatory Initiative or the ATLAS experiment) as well as commercial ones (e.g. Microsoft's Bing and Office 365 large-scale services) operate on tens of data-centers around the globe and follow similar patterns: they aggregate monitoring data, assess the QoS or run global data mining queries based on inter site event stream processing. In this paper, we propose a set of strategies for efficient transfers of events between cloud data-centers and we introduce JetStream: a prototype implementing these strategies as a high performance batch-based streaming middleware. JetStream is able to self-adapt to the streaming conditions by modeling and monitoring a set of context parameters. It further aggregates the available bandwidth by enabling multi-route streaming across cloud sites. The prototype was validated on tens of nodes from US and Europe data-centers of the Windows Azure cloud using synthetic benchmarks and with application code from the context of the Alice experiment at CERN. The results show an increase in transfer rate of 250 times over individual event streaming. Besides, introducing an adaptive transfer strategy brings an additional 25% gain. Finally, the transfer rate can further be tripled thanks to the use of multi-route streaming. © 2014 ACM.},
  affiliation     = {IRISA, ENS Rennes, France; Microsoft Research - ATL Europe, Munich, Germany; IRISA, INSA Rennes, France; ENS Rennes, France; Inria Rennes-Bretagne Atlantique, Rennes, France},
  author_keywords = {cloud computing; event streaming; high performance data management; multi data-centers},
  document_type   = {Conference Paper},
  doi             = {10.1145/2611286.2611298},
  journal         = {DEBS 2014 - Proceedings of the 8th ACM International Conference on Distributed Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84903161711&doi=10.1145%2f2611286.2611298&partnerID=40&md5=55fc44ba661d8dd6140e5235ad38ab97},
}

@Article{Qadri2014,
  author          = {Qadri, M.Y. and McDonald Maier, K.D. and Qadri, N.N.},
  title           = {Energy and throughput aware fuzzy logic based reconfiguration for MPSoCs},
  journal         = {Journal of Intelligent and Fuzzy Systems},
  year            = {2014},
  volume          = {26},
  number          = {1},
  pages           = {101-113},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures offer an amount of parallelism that is often underutilized, as a result these underutilized resources become a liability instead of advantage. Inefficient resource sharing on the chip can have a negative impact on the performance of an application and may result in greater energy consumption. A large body of research now focuses on reconfigurable multicore architectures in order to support algorithms to find optimal solutions for improved energy and throughput balance. An ideal system would be able to optimize such reconfigurable systems to a level that optimum resources are allocated to a particular workload and all the other underutilized resources remain inactive for greater energy savings. This paper presents a fuzzy logic based reconfiguration engine targeted to optimize a multicore architecture according to the workload requirements for optimum balance between power and performance of the system. The proposed fuzzy logic reconfiguration engine is designed around a 16-core SCMP architecture comprising of reconfigurable cache memories, power gated cores and adaptive on-chip network routers for minimizing leakage energy effects for inactive components. A coarse grained architecture was selected for being able to reconfigure faster, thus making it feasible to be used for runtime adaptation schemes. The presented architecture is analyzed over a set of OpenMP based parallel benchmarks and results show significant energy savings in all cases. © 2014 IOS Press and the authors.},
  affiliation     = {School of Computer Science and Electronic Engineering, University of Essex, CO4 3SQ, Colchester, United Kingdom; Department of Electrical Engineering, COMSATS Institute of Information Technology, Wah Cantt, Pakistan},
  author_keywords = {Energy efficiency; Fuzzy logic; Multicore processing; Reconfiguration},
  document_type   = {Article},
  doi             = {10.3233/IFS-120718},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84890583846&doi=10.3233%2fIFS-120718&partnerID=40&md5=bd35a6ccac1e68df6755cea3719a87ce},
}

@Article{Sudhir2014,
  author          = {Sudhir, N.S. and Manvi, S.S.},
  title           = {Design and development of miniature dual antenna GPS-GLONASS receiver for uninterrupted and accurate navigation},
  journal         = {Journal of Telecommunications and Information Technology},
  year            = {2014},
  volume          = {2014},
  number          = {4},
  pages           = {108-115},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Global Positioning System (GPS), Global Navigation Satellite System (GLONASS), and GPS-GLONASS receivers are commonly used for navigation. However, there are some applications where a single antenna interface to a GPS or GPS-GLONASS receiver will not suffice. For example, an airborne platform such as an Unmanned Aerial Vehicles (UAV) will need multiple antennae during maneuvering. Also, some applications will need redundancy of antenna connectivity to prevent loss of positioning if a link to satellite fails. The scope of this work is to design a dual antenna GPS-GLONASS navigation receiver and implement it in a very small form-factor to serve multiple needs such as: provide redundancy when a link fails, and provide uninterrupted navigation even under maneuvering, also provide improved performance by combining data from both signal paths. Both hardware and software architectures are analyzed before implementation. A set of objectives are identified for the receiver which will serve as the benchmarks against which the receiver will be validated. Both analysis and objectives are highlighted in this paper. The results from the tests conducted on such a dual antenna GPS-GLONASS receiver have given positive results on several counts that promise a wider target audience for such a solution.},
  affiliation     = {GNSS and Embedded Systems Lab., Accord Software Systems Pvt. Ltd., Bangalore, India; Wireless Research Lab., Reva Institute of Technology and Management, Rukmini Knowledge Park Yelhanka, Bangalore, India},
  author_keywords = {Dual-antenna receiver; Embedded system; GLO-NASS; GPS; Miniaturization},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84920446251&partnerID=40&md5=fc10754eaad32487f4fe9b142edc509b},
}

@Conference{Ratnalikar2014,
  author          = {Ratnalikar, P. and Chauhan, A.},
  title           = {Automatic parallelism through macro dataflow in high-level array languages},
  year            = {2014},
  pages           = {489-490},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Dataflow computation is a powerful paradigm for parallel computing that is especially attractive on modern machines with multiple avenues for parallelism. However, adopting this model has been challenging as neither hardware- nor language-based approaches have been successful, except, in specialized contexts. We argue that general-purpose array languages, such as MATLAB, are good candidates for automatic translation to macro dataflow-style execution, where each array operation naturally maps to a macro dataflow operation and the model can be efficiently executed on contemporary multicore architecture. We support our argument with a fully automatic compilation technique to translate MATLAB programs to dynamic dataflow graphs that are capable of handling unbounded structured control flow. These graphs can be executed on multicore machines in an event driven fashion with the help of a runtime system built on top of Intel's Threading Building Blocks (TBB). By letting each task itself be data parallel, we are able to leverage existing data-parallel libraries and utilize parallelism at multiple levels. Our experiments on a set of benchmarks show speedups of up to 18x using our approach, over the original data-parallel code on a machine with two 16-core processors. © 2014 Authors.},
  affiliation     = {School of Informatics and Computing, Indiana University, Bloomington, IN, United States; Google Inc., United States},
  author_keywords = {dataflow computation; matlab; task-parallelism},
  document_type   = {Conference Paper},
  doi             = {10.1145/2628071.2628131},
  journal         = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84907068955&doi=10.1145%2f2628071.2628131&partnerID=40&md5=cbb41f86f4ff04a833ef3c4fea7c49d2},
}

@Conference{Bendjoudi2014,
  author          = {Bendjoudi, A. and Chekini, M. and Gharbi, M. and Mehdi, M. and Benatchba, K. and Sitayeb-Benbouzid, F. and Melab, N.},
  title           = {Parallel B\&amp;B algorithm for hybrid multi-core/GPU architectures},
  year            = {2014},
  pages           = {914-921},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {B&amp;B algorithms are well known techniques for exact solving of combinatorial optimization problems (COP). They perform an implicit enumeration of the search space instead of exhaustive one. Based on a pruning technique, they reduce considerably the computation time required to explore the whole search space. Nevertheless, these algorithms remain inefficient when dealing with large combinatorial optimization instances. They are time-intensive and they require a huge computing power to be solved optimally. Nowadays, multi-core-based processors and GPU accelerators are often coupled together to achieve impressive performances. However, classical B&amp;B algorithms must be rethought to deal with their two divergent architectures. In this paper, we propose a new B&amp;B approach exploiting both the multi-core aspect of actual processors and GPU accelerators. The proposed approaches have been executed to solve FSP instances that are well-known combinatorial optimization benchmarks. Real experiments have been carried out on an Intel Xeon 64-bit quad-core processor E5520 coupled to an Nvidia Tesla C2075 GPU device. The results show that our hybrid B&amp;B approach speeds up the execution time up to x123 over the sequential mono-core B&amp;B algorithm. © 2013 IEEE.},
  affiliation     = {CERIST Research Center, 3 rue des frères Aissou, 16030 Ben-Aknoun, Algiers, Algeria; Ecole Suprieur DInformatique (ESI), Oued Smar, Algiers, Algeria; Universite Lille1, France LIFL/UMR CNRS 8022, 59655 - Villeneuve dAscq Cedex, France},
  art_number      = {6832012},
  author_keywords = {Flowshop problem; GPU computing; Multicore architectures; Parallel B&amp;B Algorithms},
  document_type   = {Conference Paper},
  doi             = {10.1109/HPCC.and.EUC.2013.130},
  journal         = {Proceedings - 2013 IEEE International Conference on High Performance Computing and Communications, HPCC 2013 and 2013 IEEE International Conference on Embedded and Ubiquitous Computing, EUC 2013},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84903978093&doi=10.1109%2fHPCC.and.EUC.2013.130&partnerID=40&md5=912b2d5b7e615d79419d106acfab7074},
}

@Article{Jung2014,
  author          = {Jung, C. and Pabst, D. and Ham, M. and Stehli, M. and Rothe, M.},
  title           = {An effective problem decomposition method for scheduling of diffusion processes based on mixed integer linear programming},
  journal         = {IEEE Transactions on Semiconductor Manufacturing},
  year            = {2014},
  volume          = {27},
  number          = {3},
  pages           = {357-363},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {Diffusion processes in semiconductor fabrication facilities (Fabs) refer to the series of processes from wafer cleaning processes to furnace processes. Most furnace tools are batch tools, with large batch sizes, and have relatively long process times, when compared to the other processes. Strict time window constraints link cleaning processes with furnace processes for quality control. Those operational requirements for diffusion processes make their scheduling very difficult. This paper proposes an advanced scheduling approach based on a rolling horizon scheduling concept. Due to the combinatorial nature of the scheduling problem, the complexity of the problem increases exponentially, when the number of jobs and tools increase. However, the computation time allowed for the scheduler is limited in practice, because the variability in most Fabs requires schedulers to update the schedule in short intervals. We suggest an mixed integer linear programming model for diffusion processes, and propose an effective decomposition method to deal with this complexity problem. The decomposition method repeats multiple scheduling iterations, as it gradually extends the number of runs on tools, enabling the scheduler to generate near-optimal schedules in limited time intervals. The scheduler could make large improvements on key performance indicators, such as time window violation rates, batch sizes, throughput, etc. The software architecture of the scheduler implementation is also addressed in this paper. © 1988-2012 IEEE.},
  affiliation     = {GLOBALFOUNDRIES Inc., Malta, NY 12020, United States; School of Engineering and Computational Sciences, Liberty University, Lynchburg, VA 24515, United States; GLOBALFOUNDRIES Inc., Dresden 10109, Germany},
  art_number      = {6858074},
  author_keywords = {batch tools; decomposition method; Diffusion processes; mixed integer linear programming; scheduling; time window constraints},
  document_type   = {Article},
  doi             = {10.1109/TSM.2014.2337310},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84905864172&doi=10.1109%2fTSM.2014.2337310&partnerID=40&md5=a691420cb0693fdb283dd103ca6336ff},
}

@Article{Dongarra2014,
  author          = {Dongarra, J. and Faverge, M. and Ltaief, H. and Luszczek, P.},
  title           = {Achieving numerical accuracy and high performance using recursive tile LU factorization with partial pivoting},
  journal         = {Concurrency Computation Practice and Experience},
  year            = {2014},
  volume          = {26},
  number          = {7},
  pages           = {1408-1431},
  note            = {cited By 16},
  __markedentry   = {[Nichl:6]},
  abstract        = {The LU factorization is an important numerical algorithm for solving systems of linear equations in science and engineering and is a characteristic of many dense linear algebra computations. For example, it has become the de facto numerical algorithm implemented within the LINPACK benchmark to rank the most powerful supercomputers in the world, collected by the TOP500 website. Multicore processors continue to present challenges to the development of fast and robust numerical software due to the increasing levels of hardware parallelism and widening gap between core and memory speeds. In this context, the difficulty in developing new algorithms for the scientific community resides in the combination of two goals: achieving high performance while maintaining the accuracy of the numerical algorithm. This paper proposes a new approach for computing the LU factorization in parallel on multicore architectures, which not only improves the overall performance but also sustains the numerical quality of the standard LU factorization algorithm with partial pivoting. While the update of the trailing submatrix is computationally intensive and highly parallel, the inherently problematic portion of the LU factorization is the panel factorization due to its memory-bound characteristic as well as the atomicity of selecting the appropriate pivots. Our approach uses a parallel fine-grained recursive formulation of the panel factorization step and implements the update of the trailing submatrix with the tile algorithm. Based on conflict-free partitioning of the data and lockless synchronization mechanisms, our implementation lets the overall computation flow naturally without contention. The dynamic runtime system called QUARK is then able to schedule tasks with heterogeneous granularities and to transparently introduce algorithmic lookahead. The performance results of our implementation are competitive compared to the currently available software packages and libraries. For example, it is up to 40% faster when compared to the equivalent Intel MKL routine and up to threefold faster than LAPACK with multithreaded Intel MKL BLAS. Copyright © 2013 John Wiley & Sons, Ltd. Copyright © 2013 John Wiley & Sons, Ltd.},
  affiliation     = {Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN, United States; KAUST Supercomputing Laboratory, Thuwal, Saudi Arabia},
  author_keywords = {LU factorization; parallel linear algebra; recursion; shared memory synchronization; threaded parallelism},
  document_type   = {Article},
  doi             = {10.1002/cpe.3110},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899446461&doi=10.1002%2fcpe.3110&partnerID=40&md5=b6cfa3b860e6ea1f15c74e3dc9267918},
}

@Conference{Flodin2014,
  author        = {Flodin, J. and Lampka, K. and Yi, W.},
  title         = {Dynamic budgeting for settling DRAM contention of co-running hard and soft real-time tasks},
  year          = {2014},
  pages         = {151-159},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {In modern non-customized multicore architectures, computing cores commonly share large parts of the memory hierarchy. This paper presents a scheme for controlling the sharing of main memory among cores, respectively the concurrently executing real-time tasks. This is important for the following: concurrent memory accesses are served sequentially by the memory controller. As task execution stalls until memory fetches are served, the latter significantly contributes to the execution time of the tasks. With multiple real-time tasks concurrently competing for the access to the memory, the main memory can easily become the Achilles heel for the timing correctness of the tasks. To provide hard timing guarantees, release of access requests issued to the main memory has therefore to be controlled. Run-time budgeting is a well accepted technique for controlling and coordinating the use of a shared resource, particularly when the underlying hardware cannot be altered. Whilst guaranteeing timing correctness of the hard real-time applications, worst-case based resource budgeting commonly leads to performance degradations of the co-running (so called soft real-time) applications. In this paper we propose to combine worst-case based resource budgeting with run-time monitoring for dynamically reconfiguring the budget schemes. Thereby we aim at increasing the responsiveness of the soft real-time applications, while satisfying the strict timing constraints of the co-running hard real-time tasks. We have implemented the proposed scheme in a microkernel and present its empirical evaluation for which an industrial benchmark suite has been employed. © 2014 IEEE.},
  affiliation   = {Department of Information Technology, Uppsala University, Sweden},
  art_number    = {6871199},
  document_type = {Conference Paper},
  doi           = {10.1109/SIES.2014.6871199},
  journal       = {Proceedings of the 9th IEEE International Symposium on Industrial Embedded Systems, SIES 2014},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84906705344&doi=10.1109%2fSIES.2014.6871199&partnerID=40&md5=6b03e822eb55db205fa21b5900868992},
}

@Article{Giles-Corti2014,
  author        = {Giles-Corti, B. and Macaulay, G. and Middleton, N. and Boruff, B. and Bull, F. and Butterworth, I. and Badland, H. and Mavoa, S. and Roberts, R. and Christian, H.},
  title         = {Developing a research and practice tool to measure walkability: A demonstration project},
  journal       = {Health Promotion Journal of Australia},
  year          = {2014},
  volume        = {25},
  number        = {3},
  pages         = {160-166},
  note          = {cited By 21},
  __markedentry = {[Nichl:6]},
  abstract      = {Issue addressed Growing evidence shows that higher-density, mixed-use, pedestrian-friendly neighbourhoods encourage active transport, including transport-related walking. Despite widespread recognition of the benefits of creating more walkable neighbourhoods, there remains a gap between the rhetoric of the need for walkability and the creation of walkable neighbourhoods. Moreover, there is little objective data to benchmark the walkability of neighbourhoods within and between Australian cities in order to monitor planning and design intervention progress and to assess built environment and urban policy interventions required to achieve increased walkability. This paper describes a demonstration project that aimed to develop, trial and validate a 'Walkability Index Tool' that could be used by policy makers and practitioners to assess the walkability of local areas; or by researchers to access geospatial data assessing walkability. The overall aim of the project was to develop an automated geospatial tool capable of creating walkability indices for neighbourhoods at user-specified scales. Methods The tool is based on open-source software architecture, within the Australian Urban Research Infrastructure Network (AURIN) framework, and incorporates key sub-component spatial measures of walkability (street connectivity, density and land use mix). Results Using state-based data, we demonstrated it was possible to create an automated walkability index. However, due to the lack of availability of consistent of national data measuring land use mix, at this stage it has not been possible to create a national walkability measure. The next stage of the project is to increase useability of the tool within the AURIN portal and to explore options for alternative spatial data sources that will enable the development of a valid national walkability index. Conclusion AURIN's open-source Walkability Index Tool is a first step in demonstrating the potential benefit of a tool that could measure walkability across Australia. It also demonstrates the value of making accurate spatial data available for research purposes. So what? There remains a gap between urban policy and practice, in terms of creating walkable neighbourhoods. When fully implemented, AURIN's walkability tool could be used to benchmark Australian cities against which planning and urban design decisions could be assessed to monitor progress towards achieving policy goals. Making cleaned data readily available for research purposes through a common portal could also save time and financial resources.},
  affiliation   = {McCaughey Vic Health Centre for Community Wellbeing, School of Population and Global Health, University of Melbourne, 207 Bouverie Street, Carlton, VIC 3010, Australia; Centre for the Built Environment and Health, School of Population Health, University of Western Australia, 35 Stirling Highway, Crawley, WA 6009, Australia; NJM Spatial, 11 Leon Road, Dalkeithe, WA 6009, Australia; North West Region Victorian Department of Health, 145 Smith Street, Fitzroy, VIC 3065, Australia; Telethon Kids Institute, University of Western Australia, PO Box 855, West Perth, WA 6872, Australia; NHMRC CRE in Healthy Liveable Communities, School of Population and Global Health, University of Melbourne, 207 Bouverie Street, Carlton, VIC 3010, Australia},
  document_type = {Article},
  doi           = {10.1071/HE14050},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84916887192&doi=10.1071%2fHE14050&partnerID=40&md5=c35f6edfcb5142c74f81c507bb5907d2},
}

@Conference{Kerschbaum2014,
  author          = {Kerschbaum, F.},
  title           = {Client-controlled cloud encryption},
  year            = {2014},
  pages           = {1542-1543},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Customers of cloud service demand control over their data. Next to threats to intellectual property, legal requirements and risks, such as data protection compliance or the possibility of a subpoena of the cloud service provider, also pose restrictions. A commonly proposed and implemented solution is to encrypt the data on the client and retain the key at the client. In this tutorial we will review •the available encryption methods, such deterministic [2], order-preserving [1, 3, 4, 13], homomorphic [7, 11], searchable (functional) encryption [14, 5] and secure multi-party computation [15], • possible attacks on currently deployed systems like dictionary and frequency attacks, • architectures integrating these solutions into SaaS and PaaS (DBaaS) applications. Each possible solution offers a trade-off between security, performance or cost and functionality. Solutions existing in the market provide rather low security, but high performance at little implementation cost. There exist a number of research projects that provide much better security, but also at a higher cost-often realized in the effort to implement and deploy the solution in existing landscapes. Consider fully homomorphic encryption: it is highly secure, but extremely inefficient. Furthermore it does not support data sharing. It is important to understand the trade-offs and implications provided by the different encryption schemes in order to be able to design systems that make the best use of them. When combining technologies cleverly interesting systems design can arise that can provide even break-through functionality or security. We will look at current options available on the market, but more importantly explore design choices currently foreshadowed by the security research community. The most commonly deployed encryption schemes are deterministic, but they are susceptible to simple attacks, such as a dictionary-a chosen plaintext-attack or a frequency-a ciphertext only-attack. Furthermore, when deploying these schemes operational issues arise in key management. Neither is it easily possible to re-key in case of a compromise nor has each user its individual key, but one key is shared. Extended schemes such as adjustable encryption as used in e.g. CryptDB or searchable encryption as developed in the IARPA SPAR program offer better security, but also require slight to significant modifications of the database. Hence, these solutions require different architectures when deploying them in existing landscapes and applications. When deploying these solutions the integration should be as transparent as possible. The less an application needs to change, the cheaper is integration. In secure multi-party computation as investigated by the European project PRACTICE a number of languages, compilers and frameworks exist. Nevertheless, secure computation requires the trust to be distributed among a set of cloud service providers. This tutorial will give an overview of the encryption schemes, their functions, security models and approximate performance characteristics. It will then compare the architectures that exist for deploying and integrating these encryption schemes and their implications on the existing applications. The material will be technical, but not at the cryptographic details. Rather we will look at system aspects of defining the right interfaces and look at possible attacks. We will briefly discuss possible applications, such as benchmarking [8, 9] or supply chain management [6, 10, 12]. The tutorial is aimed at researchers-and maybe practitioners-wanting to get an overview of approaches to clientcontrolled cloud encryption. It will compare the different technologies and highlight the open problems often overlooked in research projects. It hence provides the opportunity to researchers to discover new problems and to practitioners make better informed decisions. Prerequisites for the tutorial are fundamentals in computer security and software architecture. Specific knowledge about cryptography is helpful, but not necessary. Copyright is held by the author/owner(s).},
  affiliation     = {SAP, Karlsruhe, Germany},
  author_keywords = {Cloud; Encryption; Tutorial},
  document_type   = {Conference Paper},
  doi             = {10.1145/2660267.2660577},
  journal         = {Proceedings of the ACM Conference on Computer and Communications Security},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84910645887&doi=10.1145%2f2660267.2660577&partnerID=40&md5=bd44aecccbda792f6ad33ca263647c69},
}

@Conference{Patel2014,
  author          = {Patel, H. and Temple, M. and Baldwin, R. and Ramsey, B.},
  title           = {Application of ensemble decision tree classifiers to zig bee device network authentication using RF-DNA fingerprinting},
  year            = {2014},
  pages           = {176-184c},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {The popularity of ZigBee devices continues to grow in home automation, transportation, traffic management and Industrial Control System (ICS) applications given their low-cost and low-power. However, the decentralized architecture of ZigBee ad-hoc networks creates unique security challenges to ensure only authentic devices are granted network access. RF-Distinct Native Attribute (RF-DNA) fingerprinting provides enhanced device authentication reliability using a Fisherbased Multiple Discriminant Analysis/Maximum Likelihood (MDA/ML) classification process to distinguish between devices in low Signal-to-Noise Ratio (SNR) environments. However, MDA/ML performance inherently degrades when RF-DNA features do not satisfy Gaussian normality conditions which often occurs in real-world scenarios where RF multipath and interference from other devices is present. We introduce non-parametric Random Forest (RndF) and Multi-Class AdaBoost (MCA) ensemble classifiers into the RF-DNA fingerprinting arena and demonstrate improved ZigBee device authentication. Results are compared with parametric MDA/ML and Generalized Relevance Learning Vector Quantization-Improved (GRLVQI) classifier results using identical input feature sets. Dimensionally reduced subsets of most relevant fingerprint features are identified using three methods, including a pre-classification Kolmogorov-Smirnoff (KS) test and postclassification RndF and GRLVQI feature relevance rankings. Relative to MDA/ML processing, an SNR=18.0 dB improvement is realized at an arbitrary correct classification rate (%C) benchmark of %C=90% using the ensemble methods; for all SNR?[0,30] dB, considered %C improvement over MDA/ML ranged from 9% to 24%. Relative to GRLVQI processing, ensemble methods again provided improvement for all SNR, with best improvement of %C=10% achieved at SNR=0.0 dB. Additional results for rogue device assessment at the SNR=12.0 dB (%C=90%) show that the ensemble methods correctly reject 31 of 36 rogue access attempts based on Receiver Operating Curve (ROC) curve analysis and an arbitrary Rogue Accept Rate of RAR < 10%. This is better than MDA/ML and GRLVQI which only rejected 25/36 and 28/36 rogue access attempts respectively. The key benefit of ensemble method processing is improved rogue rejection in noisy environments- gains of 4.0 dB and 18.0 dB are realized over GRLVQI and MDA/ML, respectively. Collectively considering demonstrated %C and rogue rejection capability, the use of ensemble methods improves ZigBee network authentication and enhances antispoofing protection afforded by RF-DNA fingerprinting. © Copyright The Authors, 2014. All Rights Reserved.},
  affiliation     = {Air Force Institute of Technology, Wright-Patterson AFB, United States; Riverside Research, Beavercreek, United States},
  author_keywords = {Adaboost; Authentication; GRLVQI; MDA/ML; Random forest; RF-DNA fingerprinting; Verification; ZigBee},
  document_type   = {Conference Paper},
  journal         = {9th International Conference on Cyber Warfare and Security 2014, ICCWS 2014},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84931091579&partnerID=40&md5=c361e895a1c2188233e5381e54b5a210},
}

@Article{Yang2013,
  author          = {Yang, H.-Y. and Shi, X.-H. and Sun, Q.-Y. and Yan, W.-L. and Yan, X. and Jin, M.-Z.},
  title           = {OpenCL micro benchmarks: Testing the performance of GPGPU software and hardware architecture},
  journal         = {Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics},
  year            = {2013},
  volume          = {35},
  number          = {12},
  pages           = {2631-2642},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {The general-purpose graphic processing unit (GPGPU) has become one of the major hardware platforms for parallel computing nowadays. Open computing language (OpenCL) is an open and royalty-free standard for cross-platform parallel programming of modern processors, including graphic processing units (GPU), etc. A novel OpenCL micro benchmark suite is introduced, which is used to evaluate the software and hardware performance of GPGPU architecture. The micro benchmark suite tests the single precision floating-point capability of GPGPU, the reading and writing bandwidth and the access patterns of all memory types in the GPU memory hierarchy, etc. These OpenCL micro benchmarks have high reference values to both the OpenCL and GPGPU programmers and software architecture designers.},
  affiliation     = {School of Computer Science and Engineering, Beihang University, Beijing 100191, China},
  author_keywords = {Computer system architecture; General-purpose graphic processing unit (GPGPU); Micro benchmark; Open computing language (OpenCL)},
  document_type   = {Article},
  doi             = {10.3969/j.issn.1001-506X.2013.12.30},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84891947689&doi=10.3969%2fj.issn.1001-506X.2013.12.30&partnerID=40&md5=a453d1f17700cc763c9b990b627b37ac},
}

@Article{Elangovan2013,
  author        = {Elangovan, V.K. and Badia, R.M. and Parra, E.A.},
  title         = {OmpSs-OpenCL programming model for heterogeneous systems},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2013},
  volume        = {7760 LNCS},
  pages         = {96-111},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {The advent of heterogeneous computing has forced programmers to use platform specific programming paradigms in order to achieve maximum performance. This approach has a steep learning curve for programmers and also has detrimental influence on productivity and code re-usability. To help with this situation, OpenCL an open-source, parallel computing API for cross platform computations was conceived. OpenCL provides a homogeneous view of the computational resources (CPU and GPU) thereby enabling software portability across different platforms. Although OpenCL resolves software portability issues, the programming paradigm presents low programmability and additionally falls short in performance. In this paper we focus on integrating OpenCL framework with the OmpSs task based programming model using Nanos run time infrastructure to address these shortcomings. This would enable the programmer to skip cumbersome OpenCL constructs including OpenCL plaform creation, compilation, kernel building, kernel argument setting and memory transfers, instead write a sequential program with annotated pragmas. Our proposal mainly focuses on how to exploit the best of the underlying hardware platform with greater ease in programming and to gain significant performance using the data parallelism offered by the OpenCL run time for GPUs and multicore architectures. We have evaluated the platform with important benchmarks and have noticed substantial ease in programming with comparable performance. © © Springer-Verlag Berlin Heidelberg 2013.},
  affiliation   = {Barcelona Supercomputing Center, Spanish National Research Council (CSIC), Spain; Universitat Politècnica de Catalunya, Spanish National Research Council (CSIC), Spain; Artificial Intelligence Research Institute (IIIA), Spanish National Research Council (CSIC), Spain},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-37658-0_7},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84893061829&doi=10.1007%2f978-3-642-37658-0_7&partnerID=40&md5=3b62923f817c1e3298d0da559ee6ecc4},
}

@Article{Luo2013,
  author          = {Luo, Y. and Hsu, W.-C. and Zhai, A.},
  title           = {The design and implementation of heterogeneous multicore systems for energy-efficient speculative thread execution},
  journal         = {Transactions on Architecture and Code Optimization},
  year            = {2013},
  volume          = {10},
  number          = {4},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the emergence of multicore processors, various aggressive execution models have been proposed to exploit fine-grained thread-level parallelism, taking advantage of the fast on-chip interconnection communication. However, the aggressive nature of these execution models often leads to excessive energy consumption incommensurate to execution time reduction. In the context of Thread-Level Speculation, we demonstrated that on a same-ISA heterogeneous multicore system, by dynamically deciding how on-chip resources are utilized, speculative threads can achieve performance gain in an energy-efficient way. Through a systematic design space exploration, we built a multicore architecture that integrates heterogeneous components of processing cores and first-level caches. To cope with processor reconfiguration overheads, we introduced runtime mechanisms to mitigate their impacts. To match program execution with the most energy-efficient processor configuration, the system was equipped with a dynamic resource allocation scheme that characterizes program behaviors using novel processor counters. We evaluated the proposed heterogeneous system with a diverse set of benchmark programs from SPEC CPU2000 and CPU20006 suites. Compared to the most efficient homogeneous TLS implementation, we achieved similar performance but consumed 18% less energy. Compared to the most efficient homogeneous uniprocessor running sequential programs, we improved performance by 29% and reduced energy consumption by 3.6%, which is a 42% improvement in energy-delay-squared product. © 2013 ACM.},
  affiliation     = {Advanced Micro Devices, One AMD Place, Sunnyvale, CA 94088, United States; National Chiao Tung University, Taiwan; University of Minnesota, Twin Cities, United States},
  art_number      = {26},
  author_keywords = {Dynamic resource allocation; Energy efficiency; Heterogeneous multicore; Thread-Level Speculation},
  document_type   = {Article},
  doi             = {10.1145/2541228.2541233},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84891753321&doi=10.1145%2f2541228.2541233&partnerID=40&md5=daf520f3ba75c01c7e8c0d4be3ee615b},
}

@Conference{Bai2013,
  author        = {Bai, K. and Shrivastava, A.},
  title         = {Automatic and efficient heap data management for limited local memory multicore architectures},
  year          = {2013},
  pages         = {593-598},
  note          = {cited By 18},
  __markedentry = {[Nichl:6]},
  abstract      = {Limited Local Memory (LLM) multi-core architectures substitute cache with scratch pad memories (SPM), and therefore have much lower power consumption. As they lack of automatic memory management, programming on such architectures becomes challenging, in the sense that it requires the programmer/compiler to efficiently manage the limited local memory. Managing heap data of the tasks executing in the cores of an LLM multi-core is an important problem. This paper presents a fully automated and efficient scheme for heap data management. Specifically, we propose i) code transformation for automation of heap management, with seamless support for multi-level pointers, and ii) improved data structures to more efficiently manage unlimited heap data. Experimental results on several benchmarks from MiBench demonstrate an average 43% performance improvement over previous approach [1]. © 2013 EDAA.},
  affiliation   = {Compiler and Microarchitecture Laboratory, Arizona State University, Tempe, AZ 85281, United States},
  art_number    = {6513576},
  document_type = {Conference Paper},
  journal       = {Proceedings -Design, Automation and Test in Europe, DATE},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84885624220&partnerID=40&md5=f551e2ce66350c4b29dc768992077271},
}

@Conference{2013,
  title         = {Proceedings - 2013 22nd Australasian Conference on Software Engineering, ASWEC 2013},
  year          = {2013},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 23 papers. The topics discussed include: on the semantics of scenario-based specification based on timed computational tree logic; rule-based behaviour engineering: integrated, intuitive formal rule modelling; decomposing distributed software architectures for the determination and incorporation of security and other non-functional requirements; evaluating the application and understanding of elementary programming patterns; a change impact size estimation approach during the software development; integrating issue tracking systems with community-based question and answering websites; on the application of inequality indices in comparative software analysis; significant requirements engineering practices for software development outsourcing; development of robust traceability benchmarks; from toy to tool: extending tag clouds for software and information visualisation; and an empirical experiment on analogy-based software cost estimation with CUDA framework.},
  document_type = {Conference Review},
  journal       = {Proceedings of the Australian Software Engineering Conference, ASWEC},
  page_count    = {236},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84885228177&partnerID=40&md5=fb714262725c28361ce4c84c8f49d821},
}

@Article{Cerotti2013,
  author        = {Cerotti, D. and Gribaudo, M. and Piazzolla, P. and Serazzi, G.},
  title         = {End-to-end performance of multi-core systems in cloud environments},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2013},
  volume        = {8168 LNCS},
  pages         = {221-235},
  note          = {cited By 6},
  __markedentry = {[Nichl:6]},
  abstract      = {Multi-core systems are widespread in all types of computing systems, from embedded to high-end servers, and are achievable in almost all public cloud providers. The sophistication of the hardware and software architectures make the performance studies of such systems very complicated. Further complexity is introduced by the virtual environments which are the basis of all clouds paradigms. While there have been several studies concerning the performance of multi-core systems considered stand alone, few of them are focused on the end-to-end performance of these systems when accessed through virtualized platforms. In this paper we describe the results obtained with experiments on both Amazon EC2 and VirtualBox platforms. The experiments are performed with some of the DaCapo benchmarks and with IOzone. The objective is to explore at a high abstraction level how the interference between the characteristics of the applications and those of the architectures impact on the performance that users of multi-core systems experience. We also designed some expressions that, although the high-level of abstraction and the low complexity, have a good precision with regard to the performance prediction of the overall system. We think this is a first step toward understanding the end-to-end performance that a multi-core system is able to provide when accessed through a cloud platform. © 2013 Springer-Verlag Berlin Heidelberg.},
  affiliation   = {Dip. di Elettronica e Informazione, Politecnico di Milano, via Ponzio 34/5, 20133 Milano, Italy},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-40725-3-17},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84884824257&doi=10.1007%2f978-3-642-40725-3-17&partnerID=40&md5=3bf42bcdd4ceaf770f8c806273f022c0},
}

@Article{Newburn2013,
  author          = {Newburn, C.J. and Deodhar, R. and Dmitriev, S. and Murty, R. and Narayanaswamy, R. and Wiegert, J. and Chinchilla, F. and McGuire, R.},
  title           = {Offload compiler runtime for the Intel® Xeon Phi™ coprocessor},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2013},
  volume          = {7905 LNCS},
  pages           = {239-254},
  note            = {cited By 14},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Intel® Xeon Phi™ coprocessor platform enables offload of computation from a host processor to a coprocessor that is a fully-functional Intel® Architecture CPU. This paper presents the C/C++ and Fortran compiler offload runtime for that coprocessor. The paper addresses why offload to a coprocessor is useful, how it is specified, and what the conditions for the profitability of offload are. It also serves as a guide to potential third-party developers of offload runtimes, such as a gcc-based offload compiler, ports of existing commercial offloading compilers to Intel® Xeon Phi™ coprocessor such as CAPS®, and third-party offload library vendors that Intel is working with, such as NAG® and MAGMA®. It describes the software architecture and design of the offload compiler runtime. It enumerates the key performance features for this heterogeneous computing stack, related to initialization, data movement and invocation. Finally, it evaluates the performance impact of those features for a set of directed micro-benchmarks and larger workloads. © 2013 Springer-Verlag.},
  affiliation     = {Intel Corporation, United States},
  author_keywords = {acceleration; compiler; coprocessor; heterogeneous; multicore; offload; runtime},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-38750-0_18},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84884476155&doi=10.1007%2f978-3-642-38750-0_18&partnerID=40&md5=6fd4da66026fb52449f71cfa09d69938},
}

@Conference{Soule2013,
  author          = {Soulé, R. and Gordon, M.I. and Amarasinghe, S. and Grimm, R. and Hirzel, M.},
  title           = {Dynamic expressivity with static optimization for streaming languages},
  year            = {2013},
  pages           = {159-170},
  note            = {cited By 18},
  __markedentry   = {[Nichl:6]},
  abstract        = {Developers increasingly use streaming languages to write applications that process large volumes of data with high throughput. Unfortunately, when picking which streaming language to use, they face a difficult choice. On the one hand, dynamically scheduled languages allow developers to write a wider range of applications, but cannot take advantage of many crucial optimizations. On the other hand, statically scheduled languages are extremely performant, but have difficulty expressing many important streaming applications. This paper presents the design of a hybrid scheduler for stream processing languages. The compiler partitions the streaming application into coarse-grained subgraphs separated by dynamic rate boundaries. It then applies static optimizations to those subgraphs. We have implemented this scheduler as an extension to the Streamlt compiler. To evaluate its performance, we compare it to three scheduling techniques used by dynamic systems (OS thread, demand, and no-op) on a combination of micro-benchmarks and real-world inspired synthetic benchmarks. Our scheduler not only allows the previously static version of Streamlt to run dynamic rate applications, but it outperforms the three dynamic alternatives. This demonstrates that our scheduler strikes the right balance between expressivity and performance for stream processing languages. Copyright © 2013 ACM.},
  affiliation     = {Cornell University, United States; Massachusetts Institute of Technology, United States; New York University, United States; IBM Research, United States},
  author_keywords = {Stream processing; Streamlt},
  document_type   = {Conference Paper},
  doi             = {10.1145/2488222.2488255},
  journal         = {DEBS 2013 - Proceedings of the 7th ACM International Conference on Distributed Event-Based Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84881181133&doi=10.1145%2f2488222.2488255&partnerID=40&md5=9ca027629bfa0fc56df4e4e62d95a731},
}

@Article{Bai2013a,
  author          = {Bai, K. and Shrivastava, A.},
  title           = {A software-only scheme for managing heap data on limited local memory(LLM) multicore processors},
  journal         = {Transactions on Embedded Computing Systems},
  year            = {2013},
  volume          = {13},
  number          = {1},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article presents a scheme for managing heap data in the local memory present in each core of a limited local memory (LLM) multicore architecture. Although managing heap data semi-automatically with software cache is feasible, it may require modifications of other thread codes. Crossthread modifications are very difficult to code and debug, and will become more complex and challenging as we increase the number of cores. In this article, we propose an intuitive programming interface, which is an automatic and scalable scheme for heap data management. Besides, for embedded applications, where the maximum heap size can be profiled, we propose several optimizations on our heap management to significantly decrease the library overheads. Our experiments on several benchmarks from MiBench executing on the Sony Playstation 3 show that our scheme is natural to use, and if we know the maximum size of heap data, our optimizations can improve application performance by an average of 14%. © 2013 ACM.},
  affiliation     = {Compiler and Microarchitecture Laboratory, Arizona State University, Tempe, AZ 85281, United States},
  art_number      = {5},
  author_keywords = {Embedded systems; Heap data; IBM Cell BE; Local memory; MPI; Multicore processor; Scratch pad memory},
  document_type   = {Article},
  doi             = {10.1145/2501626.2501632},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84883866164&doi=10.1145%2f2501626.2501632&partnerID=40&md5=2725fb8541374a07215b20cbdc348e3e},
}

@Article{Bergstrom2013,
  author          = {Bergstrom, L. and Reppy, J. and Rosen, S. and Shaw, A. and Fluet, M. and Rainey, M.},
  title           = {Data-only flattening for nested data parallelism},
  journal         = {ACM SIGPLAN Notices},
  year            = {2013},
  volume          = {48},
  number          = {8},
  pages           = {81-91},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Data parallelism has proven to be an effective technique for highlevel programming of a certain class of parallel applications, but it is not well suited to irregular parallel computations. Blelloch and others proposed nested data parallelism (NDP) as a language mechanism for programming irregular parallel applications in a declarative data-parallel style. The key to this approach is a compiler transformation that flattens the NDP computation and data structures into a form that can be executed efficiently on a widevector SIMD architecture. Unfortunately, this technique is ill suited to execution on today's multicore machines. We present a new technique, called data-only flattening, for the compilation of NDP, which is suitable for multicore architectures. Data-only flattening transforms nested data structures in order to expose programs to various optimizations while leaving control structures intact. We present a formal semantics of data-only flattening in a core language with a rewriting system. We demonstrate the effectiveness of this technique in the Parallel ML implementation and we report encouraging experimental results across various benchmark applications. Copyright © 2013 ACM 978-1-4503-1922-5/13/02. . . $15.00.},
  affiliation     = {University of Chicago, United States; Rochester Institute of Technology, United States; Max Planck Institute for Software Systems, United States},
  author_keywords = {Compilers; Multicore; NESL; Nested data parallelism},
  document_type   = {Conference Paper},
  doi             = {10.1145/2517327.2442525},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84885236291&doi=10.1145%2f2517327.2442525&partnerID=40&md5=e21746b4424e847e42794f66b9f387ef},
}

@Conference{Lin2013,
  author          = {Lin, C. and Nagarajan, V. and Gupta, R.},
  title           = {Address-aware fences},
  year            = {2013},
  pages           = {313-323},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {Many modern multicore architectures support shared memory for ease of programming and relaxed memory models to deliver high performance. With relaxed memory models, memory accesses can be reordered dynamically and seen by other processors. Therefore, fence instructions are provided to enforce the memory orderings that are critical to the correctness of a program. However, fence instructions are costly as they cause the processor to stall. Prior works have observed that most of the executions of fence instructions are unnecessary. In this paper we propose address-aware fence, a hardware solution for reducing the overhead of fence instructions without resorting to speculation. Address-aware fence only enforces memory orderings that are necessary to maintain the effect that the traditional fence strives to enforce. This is achieved by dynamically checking a condition for when an execution of a fence must take effect and delay the memory accesses following the fence. When a fence instruction is encountered, first, necessary memory addresses are collected to form a watchlist, and then, only the memory accesses to addresses that are contained in the watchlist are delayed. The memory accesses whose addresses are not contained in the watchlist are allowed to complete without waiting for the completion of pending memory accesses from before the fence. Our experiments conducted on a group of concurrent lock-free algorithms and SPLASH-2 benchmarks show that address-aware fence eliminates nearly all the overhead due to fences and achieves an average improvement of 12.2% on programs with traditional fences. © 2013 ACM.},
  affiliation     = {CSE Department, University of California, Riverside, CA 92521, United States; School of Informatics, University of Edinburgh, Edinburgh, United Kingdom},
  author_keywords = {fence instructions; memory models; microarchitecture},
  document_type   = {Conference Paper},
  doi             = {10.1145/2464996.2465015},
  journal         = {Proceedings of the International Conference on Supercomputing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84879836112&doi=10.1145%2f2464996.2465015&partnerID=40&md5=0e0f59934a1a528a51bcd5a1d5a15c86},
}

@Article{Ni2013,
  author          = {Ni, F. and Long, X. and Wan, H. and Gao, X.},
  title           = {WSPS: A workload segmented parallel simulation methodology to accelerate detailed simulation},
  journal         = {Journal of Computers (Finland)},
  year            = {2013},
  volume          = {8},
  number          = {7},
  pages           = {1704-1714},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Software architecture simulators are indispensable tools in modern processor design. According to the granularity of simulation, they can be classified into the fast functional simulation and the slow detailed one. The detailed simulator takes far longer time than the functional simulator when simulating the same workload. Based on the duration difference of them, we propose a Workload Segmented Parallel Simulation (WSPS) methodology to accelerate the detailed simulation by simulating different segments of the workload concurrently. The results on SPEC2Kint benchmarks show that, when programs are divided into 64 segments, the speedup is about 11.5, with the relative error of CPI and L1 cache hit-rate remaining lower than 1.5% and 0.01%, respectively. Also, the analysis indicates that WSPSbased simulation can achieve even much higher speedup when using more complicated simulation models, and its duration can approach that of the functional simulation with the accuracy remaining acceptable if the workload size is large enough. © 2013 ACADEMY PUBLISHER.},
  affiliation     = {State Key Laboratory of Software Development Environment, School of Computer Science and Technology, Beihang University, Beijing, China},
  author_keywords = {Acceleration; Detailed simulation; Functional simulation; Parallel simulation; Segment},
  document_type   = {Article},
  doi             = {10.4304/jcp.8.7.1704-1714},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84879669021&doi=10.4304%2fjcp.8.7.1704-1714&partnerID=40&md5=725c2ec3ea784cb173e1c8cbb2d8363c},
}

@Article{Bertran2013,
  author          = {Bertran, R. and Gonzelez, M. and Martorell, X. and Navarro, N. and Ayguade, E.},
  title           = {A systematic methodology to generate decomposable and responsive power models for CMPs},
  journal         = {IEEE Transactions on Computers},
  year            = {2013},
  volume          = {62},
  number          = {7},
  pages           = {1289-1302},
  note            = {cited By 36},
  __markedentry   = {[Nichl:6]},
  abstract        = {Power modeling based on performance monitoring counters (PMCs) attracted the interest of researchers since it became a quick approach to understand the power behavior of real systems. Consequently, several power-aware policies use models to guide their decisions. Hence, the presence of power models that are informative, accurate, and capable of detecting power phases is critical to improve the success of power-saving techniques. Additionally, the design of current processors varied considerably with the appearance of CMPs (multiple cores sharing resources). Thus, PMC-based power models warrant further investigation on current energy-efficient multicore processors. In this paper, we present a systematic methodology to produce decomposable PMC-based power models on current multicore architectures. Apart from being able to estimate the power consumption accurately, the models provide per component power consumption, supplying extra insights about power behavior. Moreover, we study their responsiveness -the capacity to detect power phases-. Specifically, we produce power models for an Intel Core 2 Duo with one and two cores enabled for all the DVFS configurations. The models are empirically validated using the SPECcpu2006, NAS and LMBENCH benchmarks. Finally, we compare the models against existing approaches concluding that the proposed methodology produces more accurate, responsive, and informative models. © 1968-2012 IEEE.},
  affiliation     = {Department of Computer Architecture, Barcelona Supercomputing Center, Universitat Politecnica de Catalunya, C. Jordi Girona 1-3, Barcelona 08034, Spain},
  art_number      = {6189333},
  author_keywords = {dynamic voltage and frequency scaling; energy accounting; Modeling techniques; performance counters; power modeling; responsiveness},
  document_type   = {Article},
  doi             = {10.1109/TC.2012.97},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84878528002&doi=10.1109%2fTC.2012.97&partnerID=40&md5=0907f1a6a310859f0f97baa303542157},
}

@Article{Schreuder2013,
  author        = {Schreuder, M. and Höhne, J. and Blankertz, B. and Haufe, S. and Dickhaus, T. and Tangermann, M.},
  title         = {Optimizing event-related potential based brain-computer interfaces: A systematic evaluation of dynamic stopping methods},
  journal       = {Journal of Neural Engineering},
  year          = {2013},
  volume        = {10},
  number        = {3},
  note          = {cited By 56},
  __markedentry = {[Nichl:6]},
  abstract      = {Objective. In brain-computer interface (BCI) research, systems based on event-related potentials (ERP) are considered particularly successful and robust. This stems in part from the repeated stimulation which counteracts the low signal-to-noise ratio in electroencephalograms. Repeated stimulation leads to an optimization problem, as more repetitions also cost more time. The optimal number of repetitions thus represents a data-dependent trade-off between the stimulation time and the obtained accuracy. Several methods for dealing with this have been proposed as 'early stopping', 'dynamic stopping' or 'adaptive stimulation'. Despite their high potential for BCI systems at the patient's bedside, those methods are typically ignored in current BCI literature. The goal of the current study is to assess the benefit of these methods. Approach. This study assesses for the first time the existing methods on a common benchmark of both artificially generated data and real BCI data of 83 BCI sessions, allowing for a direct comparison between these methods in the context of text entry. Main results. The results clearly show the beneficial effect on the online performance of a BCI system, if the trade-off between the number of stimulus repetitions and accuracy is optimized. All assessed methods work very well for data of good subjects, and worse for data of low-performing subjects. Most methods, however, are robust in the sense that they do not reduce the performance below the baseline of a simple no stopping strategy. Significance. Since all methods can be realized as a module between the BCI and an application, minimal changes are needed to include these methods into existing BCI software architectures. Furthermore, the hyperparameters of most methods depend to a large extend on only a single variable - the discriminability of the training data. For the convenience of BCI practitioners, the present study proposes linear regression coefficients for directly estimating the hyperparameters from the data based on this discriminability. The data that were used in this publication are made publicly available to benchmark future methods. © 2013 IOP Publishing Ltd.},
  affiliation   = {Machine Learning Laboratory, Berlin Institute of Technology, Marchstraße 23, 10537, Berlin, Germany},
  art_number    = {036025},
  document_type = {Article},
  doi           = {10.1088/1741-2560/10/3/036025},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84878300244&doi=10.1088%2f1741-2560%2f10%2f3%2f036025&partnerID=40&md5=dff17d2b9b78aeb07b02b46447a18b2e},
}

@Conference{Gidra2013,
  author          = {Gidra, L. and Thomas, G. and Sopena, J. and Shapiro, M.},
  title           = {A study of the scalability of stop-the-world garbage collectors on multicores},
  year            = {2013},
  pages           = {229-239},
  note            = {cited By 18},
  __markedentry   = {[Nichl:6]},
  abstract        = {Large-scale multicore architectures create new challenges for garbage collectors (GCs). In particular, throughput-oriented stopthe- world algorithms demonstrate good performance with a small number of cores, but have been shown to degrade badly beyond approximately 8 cores on a 48-core with OpenJDK 7. This negative result raises the question whether the stop-the-world design has intrinsic limitations that would require a radically different approach. Our study suggests that the answer is no, and that there is no compelling scalability reason to discard the existing highly-optimised throughput-oriented GC code on contemporary hardware. This paper studies the default throughput-oriented garbage collector of OpenJDK 7, called Parallel Scavenge. We identify its bottlenecks, and show how to eliminate them using well-established parallel programming techniques. On the SPECjbb2005, SPECjvm2008 and DaCapo 9.12 benchmarks, the improved GC matches the performance of Parallel Scavenge at low core count, but scales well, up to 48 cores. Categories and Subject Descriptors D.4.2 [Software]: Garbage collection General Terms Experimentation, Performance. Copyright © 2013 ACM.},
  affiliation     = {LIP6-INRIA, Université Pierre et Marie Curie, 4 place Jussieu, Paris, France},
  author_keywords = {Garbage collection; Multicore; NUMA},
  document_type   = {Conference Paper},
  doi             = {10.1145/2451116.2451142},
  journal         = {International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84875677989&doi=10.1145%2f2451116.2451142&partnerID=40&md5=3acc94b74bbc70e3ff2b365bd80a571b},
}

@Article{Gidra2013a,
  author          = {Gidra, L. and Thomas, G. and Sopena, J. and Shapiro, M.},
  title           = {A study of the scalability of stop-the-world garbage collectors on multicores},
  journal         = {ACM SIGPLAN Notices},
  year            = {2013},
  volume          = {48},
  number          = {4},
  pages           = {229-239},
  note            = {cited By 12},
  __markedentry   = {[Nichl:6]},
  abstract        = {Large-scale multicore architectures create new challenges for garbage collectors (GCs). In particular, throughput-oriented stopthe- world algorithms demonstrate good performance with a small number of cores, but have been shown to degrade badly beyond approximately 8 cores on a 48-core with OpenJDK 7. This negative result raises the question whether the stop-the-world design has intrinsic limitations that would require a radically different approach. Our study suggests that the answer is no, and that there is no compelling scalability reason to discard the existing highly-optimised throughput-oriented GC code on contemporary hardware. This paper studies the default throughput-oriented garbage collector of OpenJDK 7, called Parallel Scavenge. We identify its bottlenecks, and show how to eliminate them using well-established parallel programming techniques. On the SPECjbb2005, SPECjvm2008 and DaCapo 9.12 benchmarks, the improved GC matches the performance of Parallel Scavenge at low core count, but scales well, up to 48 cores.},
  affiliation     = {LIP6-INRIA, Université Pierre et Marie Curie, 4 place Jussieu, Paris, France},
  author_keywords = {Garbage collection; Multicore; NUMA},
  document_type   = {Conference Paper},
  doi             = {10.1145/2499368.2451142},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84880111076&doi=10.1145%2f2499368.2451142&partnerID=40&md5=acee1518d510cef4374d32a48f514591},
}

@Article{Kehl2013,
  author          = {Kehl, N. and Rosenstiel, W.},
  title           = {Circuit level concurrent error detection in FSMs},
  journal         = {Journal of Electronic Testing: Theory and Applications (JETTA)},
  year            = {2013},
  volume          = {29},
  number          = {2},
  pages           = {185-192},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Finite state machines (FSMs) are contained in many building blocks of digital electronic circuits. Such electronic circuits are prone to transient errors, caused e.g. by cosmic radiation, and to permanent errors. In this article, the authors give an overview of known error detection methods for FSMs. One method (dependent state encoding for dynamic error detection) is described in detail, as well as the problems arising when the method is applied to a practical example. Additionally, the authors propose a modification of the method above. For several benchmark circuits, this modification shows better results, compared to the state-of-the-art implementation. © 2013 Springer Science+Business Media New York.},
  affiliation     = {Bosch Engineering GmbH, Abstatt, Germany; University of Tübingen, Tübingen, Germany},
  author_keywords = {Concurrent error detection; Finite state machines},
  document_type   = {Conference Paper},
  doi             = {10.1007/s10836-013-5375-y},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84877826171&doi=10.1007%2fs10836-013-5375-y&partnerID=40&md5=106f743cf467d3368fb4cb34c5502300},
}

@Conference{Bergstrom2013a,
  author          = {Bergstrom, L. and Reppy, J. and Rosen, S. and Shaw, A. and Fluet, M. and Rainey, M.},
  title           = {Data-only flattening for nested data parallelism},
  year            = {2013},
  pages           = {81-91},
  note            = {cited By 11},
  __markedentry   = {[Nichl:6]},
  abstract        = {Data parallelism has proven to be an effective technique for high-level programming of a certain class of parallel applications, but it is not well suited to irregular parallel computations. Blelloch and others proposed nested data parallelism (NDP) as a language mechanism for programming irregular parallel applications in a declarative data-parallel style. The key to this approach is a compiler transformation that flattens the NDP computation and data structures into a form that can be executed efficiently on a wide-vector SIMD architecture. Unfortunately, this technique is ill suited to execution on today's multicore machines. We present a new technique, called data-only flattening, for the compilation of NDP, which is suitable for multicore architectures. Data-only flattening transforms nested data structures in order to expose programs to various optimizations while leaving control structures intact. We present a formal semantics of data-only flattening in a core language with a rewriting system. We demonstrate the effectiveness of this technique in the Parallel ML implementation and we report encouraging experimental results across various benchmark applications. © 2013 ACM.},
  affiliation     = {University of Chicago, Chicago, IL, United States; Rochester Institute of Technology, Rochester, NY, United States; Max Planck Institute for Software Systems, Kaiserslautern, Germany},
  author_keywords = {compilers; flattening; multicore; nesl; nested data parallelism},
  document_type   = {Conference Paper},
  doi             = {10.1145/2442516.2442525},
  journal         = {Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, PPOPP},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84875141168&doi=10.1145%2f2442516.2442525&partnerID=40&md5=29c1e9fe29afdf926d5c4434c3a3789d},
}

@Article{Philippsen2013,
  author        = {Philippsen, M. and Tillmann, N. and Brinkers, D.},
  title         = {Double inspection for run-time loop parallelization},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2013},
  volume        = {7146 LNCS},
  pages         = {46-60},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {The Inspector/Executor is well-known for parallelizing loops with irregular access patterns that cannot be analyzed statically. The downsides of existing inspectors are that it is hard to amortize their high run-time overheads by actually executing the loop in parallel, that they can only be applied to loops with dependencies that do not change during their execution and that they are often specifically designed for array codes and are in general not applicable in object oriented just-in-time compilation. In this paper we present an inspector that inspects a loop twice to detect if it is fully parallelizable. It works for arbitrary memory access patterns, is conservative as it notices if changing data dependencies would cause errors in a potential parallel execution, and most importantly, as it is designed for current multicore architectures it is fast - despite of its double inspection effort: it pays off at its first use. On benchmarks we can amortize the inspection overhead and outperform the sequential version from 2 or 3 cores onward. © 2013 Springer-Verlag.},
  affiliation   = {Computer Science Dept., Programming Systems Group, University of Erlangen-Nuremberg, Erlangen, Germany; Microsoft Research, One Microsoft Way, Redmond, WA, United States},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-36036-7_4},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84872699818&doi=10.1007%2f978-3-642-36036-7_4&partnerID=40&md5=485621ad9eab0b2d7c5aee57d79c80b1},
}

@Article{Yildiz2013,
  author          = {Yildiz, B. and Fox, G.C.},
  title           = {Toward a modular and efficient distribution for Web service handlers},
  journal         = {Concurrency Computation Practice and Experience},
  year            = {2013},
  volume          = {25},
  number          = {3},
  pages           = {410-426},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Over the last few decades, distributed systems have architecturally evolved. One recent evolutionary step is SOA. The SOA model is perfectly engendered in Web services, which provide software platforms for building applications as services. Web services utilize supportive capabilities such as security, reliability, and monitoring. These capabilities are typically provisioned as handlers, which incrementally add new features. Even though handlers are very important, the method of utilization is crucial for obtaining potential benefits. Every attempt to support a service with an additional handler increases the chance of an overwhelmingly crowded handler chain. Moreover, a handler may become a bottleneck because of its comparably higher processing time. In this paper, we present the Distributed Handler Architecture to provide an efficient, scalable, and modular architecture. The performance and scalability benchmarks show that the distributed and parallel handler executions are very promising for suitable handler configurations. The paper is concluded with remarks on the fundamentals of a promising computing environment for Web service handlers. Copyright © 2012 John Wiley & Sons, Ltd. Copyright © 2012 John Wiley & Sons, Ltd.},
  affiliation     = {Department of Computer Engineering, TOBB University of Economics and Technology, Ankara, Turkey; School of Informatics and Computing, Indiana University, Bloomington IN, United States; Community Grids Lab, Indiana University, Bloomington IN, United States},
  author_keywords = {distributed computing; parallel computing; pipelining; service-oriented architecture; Web service; Web service handler},
  document_type   = {Article},
  doi             = {10.1002/cpe.2854},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84873727428&doi=10.1002%2fcpe.2854&partnerID=40&md5=6173b1a652bc4f3003006166d83155b5},
}

@Conference{Alves2013,
  author        = {Alves, M.A.Z. and Villavieja, C. and Diener, M. and Navaux, P.O.A.},
  title         = {Energy efficient last level caches via last read/write prediction},
  year          = {2013},
  pages         = {73-80},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {The size of the Last Level Caches (LLC) in multicore architectures is increasing, and so is their power consumption. However, most of this power is wasted on unused or invalid cache lines. For dirty cache lines, the LLC waits until the line is evicted to be written back to memory. Hence, dirty lines compete for the memory bandwidth with read requests (prefetch and demand), increasing pressure on the memory controller. This paper proposes a Dead Line and Early Write-Back Predictor (DEWP) to improve the energy efficiency of the LLC. DEWP early evicts dead cache lines with an average accuracy of 94%, and only 2% false positives. DEWP also allows scheduling of dirty lines for early eviction, allowing earlier write-backs. Using DEWP over a set of single and multi-threaded benchmarks, we obtain an average of 61% static energy savings, while maintaining the performance, for both inclusive and non-inclusive LLCs. © 2013 IEEE.},
  affiliation   = {Informatics Institute, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, United States},
  art_number    = {6702582},
  document_type = {Conference Paper},
  doi           = {10.1109/SBAC-PAD.2013.12},
  journal       = {Proceedings - Symposium on Computer Architecture and High Performance Computing},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84893582258&doi=10.1109%2fSBAC-PAD.2013.12&partnerID=40&md5=37d67bbe54e433978b0a4e51b11e2530},
}

@Conference{Wimmer2013,
  author          = {Wimmer, M. and Poter, M. and Traff, J.L.},
  title           = {The pheet task-scheduling framework on the intel® Xeon Phi coprocessor and other multicore architectures},
  year            = {2013},
  pages           = {1587-1596},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {Pheet, a task-scheduling framework that allows for easy customization of internal data-structures, is a research vehicle for experimenting with high-level application and low-level architectural support for task-parallel programming models. Pheet is highly configurable, and allows comparison between different implementations of data structures used in the scheduler, as well as comparisons between entirely different schedulers (typically using work-stealing). Pheet is being used to investigate high-level task-parallel support mechanisms that allow applications to influence scheduling decisions and behavior. One such mechanism, that we use in this work, is scheduling strategies. Previous Pheet benchmarking was done on conventional multicore architectures from AMD and Intel. In this paper we discuss the performance of Pheet on a prototype Intel Xeon Phi coprocessor with 61 cores. We compare these results to Pheet on three conventional multicore architectures. Using two benchmarks from the mostly non-numerical/combinatorial Pheet suite we find that the Xeon Phi coprocessor provides considerably better scalability than the other architectures, with more than a 70x speedup on the 61-core Knights Corner prototype system when using 4-way SMT, although not achieving the same absolute performance. For our research, the Xeon Phi coprocessor is an interesting architecture for implementing and evaluating fine-grained task-parallel parallel algorithm implementations. © 2013 IEEE.},
  affiliation     = {Faculty of Informatics, Institute of Information Systems, Vienna University of Technology, 1040 Vienna/Wien, Austria},
  art_number      = {6651055},
  author_keywords = {fine-grained multi threaded applications; priorities; strategies; Work-stealing; Xeon Phi},
  document_type   = {Conference Paper},
  doi             = {10.1109/IPDPSW.2013.22},
  journal         = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899718508&doi=10.1109%2fIPDPSW.2013.22&partnerID=40&md5=a746c1ba49316d24420f7a698e26d04a},
}

@Conference{Newburn2013a,
  author          = {Newburn, C.J. and Dmitriev, S. and Narayanaswamy, R. and Wiegert, J. and Murty, R. and Chinchilla, F. and Deodhar, R. and McGuire, R.},
  title           = {Offload compiler runtime for the intel® Xeon Phi coprocessor},
  year            = {2013},
  pages           = {1213-1224},
  note            = {cited By 27},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Intel® Xeon Phi coprocessor platform has a new software stack that enables new programming models. One such model is offload of computation from a host processor to a coprocessor that is a fully-capable Intel® Architecture CPU, namely, the Intel® Xeon Phi coprocessor. The purpose of that offload is to improve response time and/or throughput. This paper presents the compiler offload software runtime infrastructure for the Intel® Xeon Phi coprocessor, which includes a production C/C++ and Fortran compiler that enables offload to that coprocessor, and an underlying Intel® Many Integrated Core (Intel® MIC) platform software stack that enables offloading. The paper shares the insights that grow out of the experience of a multi-year, intensive development effort. It addresses end users' questions about offload with the compiler offload runtime, namely, why offload to a co-processor is useful, how it is specified, and what the conditions for the profitability of offload are. It also serves as a guide to potential third-party developers of offload runtimes, such as a gcc-based offload compiler, ports of existing commercial offloading compilers to Intel® Xeon Phi coprocessor such as CAPS®, and third-party offload library vendors that Intel is working with, such as NAG® and MAGMA®. It describes the software architecture and design of the offload compiler runtime. It enumerates the key performance features for this heterogeneous computing stack, related to initializa-tion, data movement and invocation. Finally, it evaluates the performance impact of those features for a set of directed micro-benchmarks and larger workloads. © 2013 IEEE.},
  art_number      = {6651008},
  author_keywords = {compiler; coprocessor; heterogeneous; MIC; offload; runtime; Xeon Phi},
  document_type   = {Conference Paper},
  doi             = {10.1109/IPDPSW.2013.251},
  journal         = {Proceedings - IEEE 27th International Parallel and Distributed Processing Symposium Workshops and PhD Forum, IPDPSW 2013},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899715193&doi=10.1109%2fIPDPSW.2013.251&partnerID=40&md5=ff065eed43fda20026735cb524f01423},
}

@Conference{Azarian2013,
  author        = {Azarian, A.},
  title         = {Pipelining computing stages in configurable multicore architectures},
  year          = {2013},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Recently, there has been increasing interest on using task-level pipelining to accelerate the overall execution of applications mainly consisting of Producer-Consumer (P/C) tasks. In this PhD work we propose an approach to achieve pipelining execution of P/C pairs of tasks in FPGA-based multicore architectures. The current approach is able to speedup the overall execution of successive, data-dependent tasks, by using multiple cores and specific customization features provided by FPGAs. An important component of our approach is the use of customized inter-stage buffer schemes to communicate data and to synchronize the cores associated to the P/C tasks. To improve the performance, we propose a technique to optimize out-of-order communication between P/C pairs when the consumer requests more than once each data element produced, a behavior present in many applications (e.g., image processing). The current FPGA-based experimental results show the feasibility of our approach in both in-order and out-of-order P/C tasks. Moreover, the results using our approach to task-level pipelining and a multicore architecture reveal noticeable performance improvements for a number of benchmarks over a single core implementation without using task-level pipelining. © 2013 IEEE.},
  affiliation   = {Faculdade de Engenharia, Universidade Do Porto, INESC TEC, Portugal},
  art_number    = {6645611},
  document_type = {Conference Paper},
  doi           = {10.1109/FPL.2013.6645611},
  journal       = {2013 23rd International Conference on Field Programmable Logic and Applications, FPL 2013 - Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84898604839&doi=10.1109%2fFPL.2013.6645611&partnerID=40&md5=2e781febcb20e55ab2d1bbf6d7e273f2},
}

@Conference{Baixo2013,
  author        = {Baixo, A. and Porto, J.P. and Araujo, G.},
  title         = {Cache-based cross-iteration coherence for speculative parallelization},
  year          = {2013},
  pages         = {216-225},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Maximal utilization of cores in multicore architectures is key to realize the potential performance available from higher density devices. In order to achieve scalable performance, parallelization techniques rely on carefully tunning speculative architecture support, run-time environment and software-based transformations. Hardware and software mechanisms have already been proposed to address this problem. They either require deep (and risky) changes on the existing hardware and cache coherence protocols, or exhibit poor performance scalability for a range of applications. The addition of cache tags as an enabler for data versioning, recently announced by the industry (i.e. IBM BlueGene/Q), could allow a better exploitation of parallelism at the microarchitecture level. In this paper, we present an execution model that supports both DOPIPE-based speculation and traditional speculative parallelization techniques. It is based on a simple cache tagging approach for data versioning, which integrates smoothly with typical cache coherence protocols, not requiring any changes to them. Experimental results, using SPEC and PARSEC benchmarks, reveal substantial speedups in a 24-core simulated CMP, while demonstrate improved scalability when compared to a software-only approach. © 2013 IEEE.},
  affiliation   = {University of Washinghton, Seattle, United States; Google Inc., Mountain View, United States; IC-UNICAMP, Campinas, Brazil},
  art_number    = {6799113},
  document_type = {Conference Paper},
  doi           = {10.1109/HiPC.2013.6799113},
  journal       = {20th Annual International Conference on High Performance Computing, HiPC 2013},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84900327352&doi=10.1109%2fHiPC.2013.6799113&partnerID=40&md5=8650e942b7811a69cf27393c3c4807ac},
}

@Conference{Wan2013,
  author        = {Wan, Z. and Xiao, H. and Joshi, Y. and Yalamanchili, S.},
  title         = {Co-design of multicore architectures and microfluidic cooling for 3D stacked ICs},
  year          = {2013},
  pages         = {237-242},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {In this paper, we investigate the co-design of multicore architectures and microfluidic cooling for 3D stacked ICs. The architecture is a 16 core, x86 multicore die stacked with a second die hosting an L2 SRAM cache. First, a multicore x86 compatible cycle-level microarchitecture simulator was constructed and integrated with physical power models. The simulator executes benchmark programs to create power traces that drive thermal analysis. Second, the thermal characteristics under liquid cooling were investigated using a compact thermal model. Four alternative packaging organizations were studied and compared. Greatest overall temperature reduction is achieved under a given pumping power, with two tiers and two microgaps with the high power dissipation tier on the top. Third, an optimization of the pin fin parameters including the diameter, height, and longitudinal and transversal spacing was performed. This optimization is shown to achieve up to 40% improvement in energy/instruction and significant reductions in leakage power. © 2013 IEEE.},
  affiliation   = {Georgia Institute of Technology, Atlanta GA, United States},
  art_number    = {6675182},
  document_type = {Conference Paper},
  doi           = {10.1109/THERMINIC.2013.6675182},
  journal       = {THERMINIC 2013 - 19th International Workshop on Thermal Investigations of ICs and Systems, Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899013322&doi=10.1109%2fTHERMINIC.2013.6675182&partnerID=40&md5=fa04f05444554bfffaf7e00431168fc3},
}

@Conference{Choi2013,
  author          = {Choi, H. and Burgstaller, B.},
  title           = {Non-blocking parallel subset construction on shared-memory multicore architectures},
  year            = {2013},
  volume          = {140},
  pages           = {13-20},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {We discuss ways to effectively parallelize the sub- set construction algorithm, which is used to con- vert non-deterministic finite automata (NFAs) to deterministic finite automata (DFAs). This con- version is at the heart of string pattern match- ing based on regular expressions and thus has many applications in text processing, compilers, scripting languages and web browsers, security and more recently also with DNA sequence analysis. We discuss sources of parallelism in the sequen- tial algorithm and their profitability on shared- memory multicore architectures. Our NFA and DFA data-structures are designed to improve scal- ability and keep communication and synchroniza- tion overhead to a minimum. We present three dif- ferent ways for synchronization; the performance of our non-blocking synchronization based on a compare-and-swap (CAS) primitive compares fa- vorably to a lock-based approach. We consider structural NFA properties and their relationship to scalability on highly-parallel multicore architec- tures. We demonstrate the efficiency of our paral- lel subset construction algorithm through several benchmarks run on a 4-CPU (40 cores) node of the Intel Manycore Testing Lab. Achieved speedups are up to a factor of 32x with 40 cores. © 2013 Australian Computer Society, Inc.},
  affiliation     = {Department of Computer Science, Yonsei University, Seoul, South Korea},
  author_keywords = {Concurrent data-structures; Non-blocking synchroniza-tion; Shared-memory multicore architectures; Subset construction},
  document_type   = {Conference Paper},
  journal         = {Conferences in Research and Practice in Information Technology Series},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84906993782&partnerID=40&md5=02dbf7c257489c9120907c3dd769205e},
}

@Article{Zanini2013,
  author          = {Zanini, F. and Atienza, D. and De Micheli, G.},
  title           = {A combined sensor placement and convex optimization approach for thermal management in 3D-MPSoC with liquid cooling},
  journal         = {Integration, the VLSI Journal},
  year            = {2013},
  volume          = {46},
  number          = {1},
  pages           = {33-43},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {Modern high-performance processors employ thermal management systems, which rely on accurate readings of on-die thermal sensors. Systematic tools for analysis and determination of best allocation and placement of thermal sensors is therefore a highly relevant problem. Moreover liquid cooling has emerged as a promising solution for addressing the elevated temperatures in 3D Multi-Processor Systems-on-Chips (MPSoCs). In this work, we present a combined sensor placement and convex optimization approach for thermal management in 3D-MPSoC with liquid cooling. This approach first finds the best locations inside the 3D-MPSoC where thermal sensors can be placed using a greedy approach. Then, the temperature sensing information is subsequently used by our convex-based thermal management policy to optimize the performance of the MPSoC while guaranteeing a reliable working condition. We perform experiments on a 3D multicore architecture case-study using benchmarks ranging from web-accessing to playing multimedia. Our results show a reduction up to 10× in the number of required sensors. Moreover our policy satisfies performance requirements, while reducing cooling energy by up to 72% compared with traditional state of the art liquid cooling techniques. The proposed policy also keeps the thermal profile up to 18°C lower compared with state of the art 3D thermal management techniques using variable-flow liquid cooling. © 2011 Elsevier B.V.},
  affiliation     = {Integrated Systems Lab (LSI), Switzerland; Embedded Systems Lab (ESL), Ecole Polytechnique Fédérale de Lausanne (EPFL), Switzerland},
  author_keywords = {3D; Cooling; Liquid; Management; MPSoC; Placement; Thermal},
  document_type   = {Article},
  doi             = {10.1016/j.vlsi.2011.12.003},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84867672796&doi=10.1016%2fj.vlsi.2011.12.003&partnerID=40&md5=7c3fa242086a824df1b99e5a50a143fe},
}

@Conference{Rico2012,
  author          = {Rico, T.M. and Pilla, M.L. and Du Bois, A.R.},
  title           = {Energy consumption on software transactional memories},
  year            = {2012},
  pages           = {194-201},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the spreading of multicore architectures, new challenges have been added to software development. Among those, efficiently avoiding race conditions through synchronization is one of the greatest difficulties in concurrent programming. Transactional memories have been proposed to reduce the issues and limitations found on previous synchronization techniques based in locks, such as mutexes, semaphores, and monitors. The use of transactions allows for a higher abstraction on writing code, leaving for the compiler or library the determination of which variables can be accessed concurrently. The runtime system is responsible for detecting conflicts during execution, and solving them in a way that the desired semantics is preserved. In this context, this paper analyzes energy consumption and performance of three Software Transactional Memory implementations, TL2, TinySTM, and Swiss TM, using the STAMP benchmarks. Different from previous works, the workloads are not simulated but executed in a computer. The results show that TinySTM and Swiss TM have very similar performance, even more when the number of threads is increased. On the other hand, TL2 presents worse performance for all benchmarks but Genome. Energy consumption closely follows the same trend, as no specific power management is employed, hence execution time is the main variable determining power. © 2012 IEEE.},
  affiliation     = {PPGC - CDTec - UFPEL, R. Gomes Carneiro, 1, 96010-610 - Pelotas, RS, Brazil},
  art_number      = {6391782},
  author_keywords = {green computing; parallel processing; software transactional memories},
  document_type   = {Conference Paper},
  doi             = {10.1109/WSCAD-SSC.2012.26},
  journal         = {Proceedings - 13th Symposium on Computing Systems, WSCAD-SSC 2012},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84872510927&doi=10.1109%2fWSCAD-SSC.2012.26&partnerID=40&md5=f1352cec69a5b8d54b0efab09dc52b41},
}

@Article{Baldassin2012,
  author          = {Baldassin, A. and Goldstein, F. and Azevedo, R.},
  title           = {A transactional runtime system for the Cell/BE architecture},
  journal         = {Journal of Parallel and Distributed Computing},
  year            = {2012},
  volume          = {72},
  number          = {12},
  pages           = {1535-1546},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Single-core architectures have hit the end of the road and industry and academia are currently exploiting new multicore design alternatives. In special, heterogeneous multicore architectures have attracted a lot of attention but developing applications for such architectures is not an easy task due to the lack of appropriate tools and programming models. We present the design of a runtime system for the Cell/BE architecture that works with memory transactions. Transactional programs are automatically instrumented by the compiler, shortening development time and avoiding synchronization mistakes usually present in lock-based approaches (such as deadlock). Experimental results conducted with a prototype implementation and the STAMP benchmark show good scalability for applications with moderate to low contention levels, and whose transactions are not too small. For those cases in which a small performance loss is admissible, we believe that the ease of programming provided by transactions greatly pays off. © 2012 Elsevier Inc. All rights reserved.},
  affiliation     = {UNESP-Univ Estadual Paulista, DEMAC, Av. 24A1515 - Bairro Bela Vista, 1515 - Bairro Bela Vista, Rio Claro, Brazil; University of Campinas (UNICAMP), IC, Av. Albert Einstein, 1251 - Cidade Universitaria, Campinas, Brazil},
  author_keywords = {Multiprocessors; Parallel programming; Transactional memory},
  document_type   = {Article},
  doi             = {10.1016/j.jpdc.2012.08.001},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84867790488&doi=10.1016%2fj.jpdc.2012.08.001&partnerID=40&md5=6f5e4a84710a29f82d1fe3104d6b1631},
}

@Conference{Lloyd2012,
  author          = {Lloyd, W. and David, O. and Lyon, J. and Rojas, K.W. and Ascough II, J.C. and Green, T.R. and Carlson, J.R.},
  title           = {The cloud services Innovation platform - Eenabling service-based environmental modelling using infrastructure-as-a-service cloud computing},
  year            = {2012},
  pages           = {1208-1215},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {Service oriented architectures allow modelling engines to be hosted over the Internet abstracting physical hardware configuration and software deployments from model users. Many existing environmental models are deployed as desktop applications running on user's personal computers (PCs). Migration to service-based modelling centralizes the modelling functions to service hosts on the Internet. Users no longer require high-end PCs to run models and model updates encapsulating science advances can be disseminated more rapidly by hosting the modelling functions centrally via an Internet host instead of requiring software updates to user's PCs. In this paper we present the Cloud Services Innovation Platform (CSIP), an Infrastructure-as-a-Service cloud application architecture, used to prototype development of distributed and scalable environmental modelling services. CSIP aims to provide modelling as a service to support both interactive (synchronous) and batch (asynchronous) modelling. CSIP enables cloud-based computing resources to be harnessed for both new and existing environmental models supporting the disaggregation of work into subtasks which execute in parallel using a scalable number of virtual machines. This paper presents CSIP's implementation using the RUSLE2 model as a prototype model. RUSLE2 model service benchmarks are presented to demonstrate performance gains from using cloud resources. We also provide benchmarks for virtualization overhead observed using popular virtual machine hypervisors and demonstrate how application profile characteristics significantly impact performance when virtualized.},
  affiliation     = {Dept.of Computer Science, Colorado State University, Fort Collins, CO 80523, United States; Dept. of Civil and Environmental Engineering, Colorado State University, Fort Collins, CO 80523, United States; USDA-ARS, ASRU, United States; USDA-NRCS, 2150 Centre Ave., Fort Collins, CO 80526, United States},
  author_keywords = {Environmental modelling; Infrastructure-as-a-service; Virtualization},
  document_type   = {Conference Paper},
  journal         = {iEMSs 2012 - Managing Resources of a Limited Planet: Proceedings of the 6th Biennial Meeting of the International Environmental Modelling and Software Society},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84894127379&partnerID=40&md5=8770f1611cadbbb04cdfc1b61bdc4a2d},
}

@Article{Chu2012,
  author          = {Chu, S.-L. and Chen, S.-R.},
  title           = {Speedup wall: The performance limitation of multicore architectures in electrical perspective},
  journal         = {Energy Education Science and Technology Part A: Energy Science and Research},
  year            = {2012},
  volume          = {30},
  number          = {SUPPL.2},
  pages           = {525-540},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The continuous improving of semiconductor technology makes the ubiquity of multicore system. In order to understand the potential performance limitation of various multicore systems, a new performance metric, speedup wall, is proposed to evaluate the potential possibility performance enhancement of a multicore system in this study. Instead of theoretical metrics provided by vendors, this study evaluates five variant multicore systems by using five benchmarks with different computing characteristics. These benchmarks are parallelize by Pthread and OpenMP paradigms, then compiled by native compilers of the target machine with highest optimizing level. The speedup wall of these architectures are provided and discussed later. The speedup functions of five multicore platforms on five benchmarks are concluded to illustrate the trends of each platform for different workloads. The proposed results also illustrate that branch density and memory contention will largely degrade the performance. Wish this work will be the preliminary step of who want to explore the optimizing space of the software on multicore systems. The scientific programmers can find the suitable optimizing techniques accordingly. © Sila Science. All Rights Reserved.},
  affiliation     = {Chung Yuan Christian University, Department of Information and Computer Engineering, 200, Chung Pei Rd., Chung Li, 32023, Taiwan},
  author_keywords = {Multicore architecture; OpenMP; Pthread; Speedup wall},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84882638267&partnerID=40&md5=1e2f6ae0f262c2b51b66e193da8c55dc},
}

@Conference{Morris2012,
  author        = {Morris, R. and Kodi, A.K. and Louri, A.},
  title         = {3D-NoC: Reconfigurable 3D photonic on-chip interconnect for multicores},
  year          = {2012},
  pages         = {413-418},
  note          = {cited By 11},
  __markedentry = {[Nichl:6]},
  abstract      = {The power dissipation of metallic interconnects in future multicore architectures is projected to be a major bottleneck as we scale to sub-nanometer regime. This has motivated researchers to develop alternate power-efficient technology solutions to the performance limitations of future multicores. Nanophotonic interconnects (NIs) is a disruptive technology solution that is capable of delivering the communication bandwidth at low power dissipation when the number of cores is scaled to large numbers. Similarly, 3D stacking is another interconnect technology solution that can lead to low energy/bit for communication. In this paper, we propose to combine NIs with with 3D stacking to develop a scalable, reconfigurable, power-efficient and high-performance interconnect for future many-core systems called 3D-NoC. We propose to develop a multi-layer NIs that can dynamically reconfigure without system intervention and allocate channel bandwidth from less utilized links to more utilized communication links. Our simulation results indicate that the performance can be further improved by 10%-25% for Splash-2, PARSEC and SPEC CPU2006 benchmarks. © 2012 IEEE.},
  affiliation   = {Electrical Engineering and Computer Science, Ohio University, Athens, OH 45701, United States; Electrical and Computer Engineering, University of Arizona, Tucson, AZ 85721, United States},
  art_number    = {6378672},
  document_type = {Conference Paper},
  doi           = {10.1109/ICCD.2012.6378672},
  journal       = {Proceedings - IEEE International Conference on Computer Design: VLSI in Computers and Processors},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84872092518&doi=10.1109%2fICCD.2012.6378672&partnerID=40&md5=9adc7326e262ced2f109764d98ba16df},
}

@Conference{Wang2012,
  author        = {Wang, Y. and Wang, X. and Chen, Y.},
  title         = {Energy-efficient virtual machine scheduling in performance-asymmetric multi-core architectures},
  year          = {2012},
  pages         = {288-294},
  note          = {cited By 10},
  __markedentry = {[Nichl:6]},
  abstract      = {Multi-core architectures with asymmetric core performance have recently shown great promise, because applications with different needs can benefit from either the high performance of a fast core or the high parallelism and power efficiency of a group of slow cores. This performance heterogeneity can be particularly beneficial to applications running in virtual machines (VMs) on virtualized servers, which often have different needs and exhibit different performance and power characteristics. Therefore, scheduling VMs on performance-asymmetric multicore architectures can have a great impact on a system's overall energy efficiency. Unfortunately, existing VM managers, such as Xen, have not taken the heterogeneity into account and thus often result in low energy efficiencies. In this paper, we propose a novel VM scheduling algorithm that exploits core performance heterogeneity to optimize the overall system energy efficiency. We first introduce a metric termed energy-efficiency factor to characterize the power and performance behaviors of the applications hosted by VMs on different cores.We then present a method to dynamically estimate the VM's energy-efficiency factors and then map the VMs to heterogeneous cores, such that the energy efficiency of the entire system is maximized. We implement the proposed algorithm in Xen and evaluate it with standard benchmarks on a real testbed. The experimental results show that our solution improves the system energy efficiency (i.e., performance per watt) by 13.5% on average and up to 55% for some benchmarks, compared to the default Xen scheduler. © 2012 IFIP.},
  affiliation   = {University of Tennessee, Knoxville, TN, United States; Ohio State University, United States; HP Laboratories, United States},
  art_number    = {6380032},
  document_type = {Conference Paper},
  journal       = {Proceedings of the 2012 8th International Conference on Network and Service Management, CNSM 2012},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84872058916&partnerID=40&md5=3ff966d431694f375869938dbf30d228},
}

@Conference{Amerio2012,
  author        = {Amerio, S. and Ammendola, R. and Biagioni, A. and Bastieri, D. and Benjamin, D. and Frezza, O. and Gelain, S. and Ketchum, W. and Kim, Y.K. and Lo Cicero, F. and Lonardo, A. and Liu, T. and Lucchesi, D. and Paolucci, P.S. and Poprocki, S. and Rossetti, D. and Simula, F. and Tosoratto, L. and Urso, G. and Vicini, P. and Wittich, P.},
  title         = {Applications of GPUs to online track reconstruction in HEP experiments},
  year          = {2012},
  pages         = {1806-1811},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {One of the most important issues that particle physics experiments at hadron colliders have to solve is realtime selection of the most interesting events. Typical collision frequencies do not allow all events to be written to tape for offline analysis, and in most cases, only a small fraction can be saved. The most commonly used strategy is based on two or three selection levels, with the low level ones usually exploiting dedicated hardware to decide within a few to ten microseconds if the event should be kept or not. This strict time requirement has made the usage of commercial devices inadequate, but recent improvements to Graphics Processing Units (GPUs) have substantially changed the conditions. Thanks to their highly parallel, multi-threaded, multicore architecture with remarkable computational power and high memory bandwidth, these commercial devices may be used in scientific applications, among which the event selection system (trigger) in particular may benefit, even at low levels. This paper describes the results of an R&D project to study the performance of GPU technology for low latency applications, such as HEP fast tracking trigger algorithms. On two different setups, we measure the latency to transfer data to/from the GPU, exploring the timing of different I/O technologies on different GPU models. We then describe the implementation and the performance of a track fitting algorithm which mimics the CDF Silicon Vertex Tracker. These studies provide performance benchmarks to investigate the potential and limitations of GPUs for future real-time applications in HEP experiments. © 2012 IEEE.},
  affiliation   = {INFN Padova, Italy; INFN Tor Vergata, Italy; INFN Roma, Italy; University of Padova, Italy; Duke University, United States; University of Chicago, United States; Fermi National Accelerator Laboratory, United States; Cornell University, United States; ORMA Software, United States},
  art_number    = {6551422},
  document_type = {Conference Paper},
  doi           = {10.1109/NSSMIC.2012.6551422},
  journal       = {IEEE Nuclear Science Symposium Conference Record},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84881588692&doi=10.1109%2fNSSMIC.2012.6551422&partnerID=40&md5=fe6bf8d7aab77a2b4fde4aaa2d2584fe},
}

@Article{Lue2012,
  author          = {Lü, Z. and Gao, Z. and Lü, Y.},
  title           = {A flight simulator that grouping aircrafts simultaneously take off and land in open grid computing environment},
  journal         = {Applied Mechanics and Materials},
  year            = {2012},
  volume          = {182-183},
  pages           = {1292-1297},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {The performance of airplane in commercial airline environment is determined by, and therefore an indicator of performance measure of, the thermodynamic properties of airplane. The aim of this study was to establish the use of simulators to determine aircraft accident for a flight of airplanes and evaluate the potential of new airspace structure and airport's runway. This indicates that there is a possibility of obtaining airplane performance from analysis and verification simulating airplane. As compared with AIRBUS Full Flight Simulator, a multiple aircrafts flight simulator that grouping aircrafts simultaneously take off and land was presented, which is basis on a parallel distributed computing in Open Grid Computing Environment (OGCE), and service oriented architecture (SOA) of software in multiple aircraft simulator, the performance of collaborative flight of multiple aircrafts is evaluated. © (2012) Trans Tech Publications, Switzerland.},
  affiliation     = {Sch. of IT and Engineering, Tianjin University of Technology and Education, Tianjin 300222, China; Centre for Digitizing Modeling, Huadi Computer Group Co. Ltd, Beijing 100195, China; Petro-Cyberworks Information Technology Co. Ltd, Beijing 100007, China; Shenyang Institute of Automation, Chinese Academy of Sciences, Shenyang 110015, China},
  author_keywords = {Distributed Artificial Intelligence (DAI); Grouping aircrafts flight simulator; Mobile Simulation Grid (MSG); Open Grid Computing Environment (OGCE); Parallel robots; Software architectures and engineering; System modeling and simulation},
  document_type   = {Conference Paper},
  doi             = {10.4028/www.scientific.net/AMM.182-183.1292},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84869783957&doi=10.4028%2fwww.scientific.net%2fAMM.182-183.1292&partnerID=40&md5=b8882ce48574dac8e37d25d1610d5927},
}

@Conference{Giannopoulou2012,
  author          = {Giannopoulou, G. and Lampka, K. and Stoimenov, N. and Thiele, L.},
  title           = {Timed model checking with abstractions: Towards worst-case response time analysis in resource-sharing manycore systems},
  year            = {2012},
  pages           = {63-72},
  note            = {cited By 24},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures are increasingly used nowadays in embedded real-time systems. Parallel execution of tasks feigns the possibility of a massive increase in performance. However, this is usually not achieved because of contention on shared resources. Concurrently executing tasks mutually block their accesses to the shared resource, causing non-deterministic delays. Timing analysis of tasks in such systems is then far from trivial. Recently, several analytic methods have been proposed for this purpose, however, they cannot model complex arbitration schemes such as FlexRay which is a common bus arbitration protocol in the automotive industry. This paper considers real-time tasks composed of superblocks, i. e., sequences of computation and resource accessing phases. Resource accesses such as accesses to memories and caches are synchronous, i. e., they cause execution on the processing core to stall until the access is served. For such systems, the paper presents a state-based modeling and analysis approach based on Timed Automata which can model accurately arbitration schemes of any complexity. Based on it, we compute safe bounds on the worst-case response times of tasks. The scalability of the approach is increased significantly by abstracting several cores and their tasks with one arrival curve, which represents their resource accesses and computation times. This curve is then incorporated into the Timed Automata model of the system. The accuracy and scalability of the approach are evaluated with a real-world application from the automotive industry and benchmark applications. Copyright 2012 ACM.},
  affiliation     = {Computer Engineering and Networks Laboratory, ETH Zurich, 8092 Zurich, Switzerland; Information Technology Department, Uppsala University, Sweden},
  author_keywords = {Multi-core systems; Resource contention; Worst-case response time analysis},
  document_type   = {Conference Paper},
  doi             = {10.1145/2380356.2380372},
  journal         = {EMSOFT'12 - Proceedings of the 10th ACM International Conference on Embedded Software 2012, Co-located with ESWEEK},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84869071944&doi=10.1145%2f2380356.2380372&partnerID=40&md5=ae75ac1b68855d756c28e604abb7399c},
}

@Conference{Vu2012,
  author          = {Vu, D. and Kuang, J. and Bhuyan, L.},
  title           = {An adaptive dynamic scheduling scheme for H.264/AVC decoding on multicore architecture},
  year            = {2012},
  pages           = {491-496},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Parallelizing H.264/AVC decoding on multicore architectures is challenged by its inherent structural and functional dependencies at both frame and macro-block levels, as macro-blocks and certain frame types must be decoded in a sequential order. So far, dynamic scheduling scheme with recursive tail submit [1], as one of the best existing algorithms, provides a good throughput performance by exploiting macro-block level parallelism and mitigating global queue contention. Nevertheless, it fails to achieve an optimal performance due to 1) the use of global queue, which incurs substantial synchronization overhead when the number of cores increases and 2) the unawareness of cache locality with respect to the underlying hierarchical core/cache topology that results in unnecessary latency, communication cost and load imbalance. In this paper, we propose an adaptive dynamic scheduling scheme that employs multiple local queues to reduce lock contention, and assigns tasks in a cache locality aware and load-balancing fashion so that neighboring macro-blocks are preferably dispatched to nearby cores. We design, implement and evaluate our scheme on a 32-core cc-NUMA SGI server. Compared to existing alternatives by running real benchmark applications, we observe that our scheme produces higher throughput and lower latency with more balanced workload and less communication cost. © 2012 IEEE.},
  affiliation     = {Computer Science and Engineering Department, University of California, Riverside, 900 University Ave, Riverside, CA 92521, United States},
  art_number      = {6298449},
  author_keywords = {core/cache topology; H.264/AVC-decoding; macro-block level parallelism; multicore architecture},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICME.2012.9},
  journal         = {Proceedings - IEEE International Conference on Multimedia and Expo},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84868134729&doi=10.1109%2fICME.2012.9&partnerID=40&md5=9890f2b681b17084a8c5eb1dbfd07ec8},
}

@Conference{Lee2012,
  author          = {Lee, J. and Liu, Z. and Tian, X. and Woo, D.H. and Shi, W. and Boumber, D.},
  title           = {Acceleration of bulk memory operations in a heterogeneous multicore architecture},
  year            = {2012},
  pages           = {423-424},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, we present a novel approach of using the integrated GPU to accelerate conventional operations that are normally performed by the CPUs, the bulk memory operations, such as memcpy or memset. Offloading the bulk memory operations to the GPU has many advantages, i) the throughput driven GPU outperforms the CPU on the bulk memory operations; ii) for on-die GPU with unified cache between the GPU and the CPU, the GPU private caches can be leveraged by the CPU for storing moved data and reducing the CPU cache bottleneck; iii) with additional lightweight hardware, asynchronous offload can be supported as well; and iv) different from the prior arts using dedicated hardware copy engines (e.g., DMA), our approach leverages the exiting GPU hardware resources as much as possible. The performance results based on our solution showed that offloaded bulk memory operations outperform CPU up to 4.3 times in micro benchmarks while still using less resources. Using eight real world applications and a cycle based full system simulation environment, the results showed 30% speedup for five, more than 20% speedup for two of the eight applications. Copyright © 2012 by the Association for Computing Machinery, Inc. (ACM).},
  affiliation     = {University of Houston, Houston, TX 77004, United States; Intel Labs, Santa Clara, CA 95054, United States},
  author_keywords = {Bulk memory operation; GPU; Heterogeneous multicore architecture; SIMD},
  document_type   = {Conference Paper},
  doi             = {10.1145/2370816.2370877},
  journal         = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84867514580&doi=10.1145%2f2370816.2370877&partnerID=40&md5=3246efdea277a2c6a2af81980796c7e6},
}

@Conference{Strazdins2012,
  author          = {Strazdins, P.E.},
  title           = {Experiences in teaching a specialty multicore computing course},
  year            = {2012},
  pages           = {1283-1288},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {We detail the design and experiences in delivering a specialty multicore computing course whose materials are openly available. The course ambitiously covers three multicore programming paradigms: shared memory (OpenMP), device (CUDA) and message passing (RCCE), and involves significant practical work on their respective platforms: an UltraSPARC T2, Fermi GPU and the Intel Single-Chip Cloud Computer. Specialized multicore architecture topics include chip multiprocessing, virtualization support, on-chip accelerators and networks, transactional memory and speculative execution. The mode of delivery emphasizes the relationship between programming performance and the underlying computer architecture, necessitating the need to provide suitable infrastructure in the form of instrumented test programs and the use of performance evaluation tools. Further infrastructure had to be created to facilitate the safe, convenient and efficient use by students on the GPU and Single-Chip Cloud Computer. The programming assignments, based on the theme of the LINPACK benchmark, also required significant infrastructure for reliably determining correctness and assisting debugging. While the course assumed as background knowledge an introductory computer systems and concurrency course, we found that students could learn device programming in a short time, by building on their knowledge of shared memory programming. However, we found that more time is needed for learning message passing. We also found that, provided students had a suitably strong computer systems background, they could successfully meet the course's learning objectives, although the skill of correctly interpreting performance data remains difficult to learn when suitable performance analysis tools are not available. © 2012 IEEE.},
  affiliation     = {Research School of Computer Science, Australian National University, Australia},
  art_number      = {6270788},
  author_keywords = {computing education; multicore computing; parallel computing},
  document_type   = {Conference Paper},
  doi             = {10.1109/IPDPSW.2012.168},
  journal         = {Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2012},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84867434756&doi=10.1109%2fIPDPSW.2012.168&partnerID=40&md5=a94e0c21260f790cd8e96e36ebb742c1},
}

@Conference{Vasar2012,
  author          = {Vasar, M. and Srirama, S.N. and Dumas, M.},
  title           = {Framework for monitoring and testing web application scalability on the cloud},
  year            = {2012},
  pages           = {53-60},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {By allowing resources to be acquired on-demand and in variable amounts, cloud computing provides an appealing environment for deploying pilot projects and for performance testing of Web applications and services. However, setting up cloud environments for performance testing still requires a significant amount of manual effort. To aid performance engineers in this task, we developed a framework that integrates several common benchmarking and monitoring tools. The framework helps performance engineers to test applications under various configurations and loads. Furthermore, the framework supports dynamic server allocation based on incoming load using a response-time-aware heuristics. We validated the framework by deploying and stress-testing the MediaWiki application. An experimental evaluation was conducted aimed at comparing the response-time-aware heuristics against Amazon Auto-Scale. Copyright 2012 ACM.},
  affiliation     = {Institute of Computer Science, University of Tartu J., Liivi 2, Tartu, Estonia},
  author_keywords = {Auto-scaling; Cloud computing; Performance testing; Web application},
  document_type   = {Conference Paper},
  doi             = {10.1145/2361999.2362008},
  journal         = {ACM International Conference Proceeding Series},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84866879899&doi=10.1145%2f2361999.2362008&partnerID=40&md5=1378b27bd84561827c5c4452240f214f},
}

@Conference{Lifflander2012,
  author          = {Lifflander, J. and Miller, P. and Venkataraman, R. and Arya, A. and Kale, L. and Jones, T.},
  title           = {Mapping dense LU factorization on multicore supercomputer nodes},
  year            = {2012},
  pages           = {596-606},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {Dense LU factorization is a prominent benchmark used to rank the performance of supercomputers. Many implementations use block-cyclic distributions of matrix blocks onto a two-dimensional process grid. The process grid dimensions drive a trade-off between communication and computation and are architecture- and implementation-sensitive. The critical panel factorization steps can be made less communication-bound by overlapping asynchronous collectives for pivoting with the computation of rank-k updates. By shifting the computation-communication trade-off, a modified block-cyclic distribution can beneficially exploit more available parallelism on the critical path, and reduce panel factorization's memory hierarchy contention on now-ubiquitous multicore architectures. During active panel factorization, rank-1 updates stream through memory with minimal reuse. In a column-major process grid, the performance of this access pattern degrades as too many streaming processors contend for access to memory. A block-cyclic mapping in the row-major order does not encounter this problem, but consequently sacrifices node and network locality in the critical pivoting steps. We introduce 'striding' to vary between the two extremes of row- and column-major process grids. The maximum available parallelism in the critical path work (active panel factorization, triangular solves, and subsequent broadcasts) is bounded by the length or width of the process grid. Increasing one dimension of the process grid decreases the number of distinct processes and nodes in the other dimension. To increase the harnessed parallelism in both dimensions, we start with a tall process grid. We then apply periodic 'rotation' to this grid to restore exploited parallelism along the row to previous levels. As a test-bed for further mapping experiments, we describe a dense LU implementation that allows a block distribution to be defined as a general function of block to processor. Other mappings can be tested with only small, local changes to the code. © 2012 IEEE.},
  affiliation     = {University of Illinois Urbana-Champaign, United States; Oak Ridge National Laboratory, United States},
  art_number      = {6267862},
  author_keywords = {amd istanbul opteron; bluegene; cache miss; charm++; cray xt; dense lu factorization; hpl; intel nehalem xeon; linpack; mapping; memory hierarchy contention; multicore; parallelism; process grid; scalapack},
  document_type   = {Conference Paper},
  doi             = {10.1109/IPDPS.2012.61},
  journal         = {Proceedings of the 2012 IEEE 26th International Parallel and Distributed Processing Symposium, IPDPS 2012},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84866883811&doi=10.1109%2fIPDPS.2012.61&partnerID=40&md5=fc94a577f0ec126436b0c35245fa2725},
}

@Article{Chen2012,
  author          = {Chen, Y. and Zhu, H. and Jin, H. and Sun, X.-H.},
  title           = {Algorithm-level Feedback-controlled Adaptive data prefetcher: Accelerating data access for high-performance processors},
  journal         = {Parallel Computing},
  year            = {2012},
  volume          = {38},
  number          = {10-11},
  pages           = {533-551},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {The rapid advance of processor architectures such as the emerged multicore architectures and the substantially increased computing capability on chip have put more pressure on the sluggish memory systems than ever. In the meantime, many applications become more and more data intensive. Data-access delay, not the processor speed, becomes the leading performance bottleneck of high-performance computing. Data prefetching is an effective solution to accelerating applications' data access and bridging the growing gap between computing speed and data-access speed. Existing works of prefetching, however, are very conservative in general, due to the computing power consumption concern of the past. They suffer low effectiveness especially when applications' access pattern changes. In this study, we propose an Algorithm-level Feedback-controlled Adaptive (AFA) data prefetcher to address these issues. The AFA prefetcher is based on the Data-Access History Cache, a hardware structure that is specifically designed for data access acceleration. It provides an algorithm-level adaptation and is capable of dynamically adapting to appropriate prefetching algorithms at runtime. We have conducted extensive simulation testing with the SimpleScalar simulator to validate the design and to analyze the performance gain. The simulation results show that the AFA prefetcher is effective and achieves considerable IPC (Instructions Per Cycle) improvement for 21 representative SPEC-CPU benchmarks. © 2012 Elsevier B.V. All rights reserved.},
  affiliation     = {Department of Computer Science, Texas Tech University, United States; Department of Electrical and Computer Engineering, University of Illinois, Urbana-Champaign, United States; Department of Computer Science, Illinois Institute of Technology, United States},
  author_keywords = {Adaptive prefetching; Application accelerator; Data access acceleration; Data prefetching; High-performance processors; Memory hierarchy; Memory wall},
  document_type   = {Article},
  doi             = {10.1016/j.parco.2012.06.002},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84864795006&doi=10.1016%2fj.parco.2012.06.002&partnerID=40&md5=b26a590c04b8654da0d519f983912b12},
}

@Article{Tekli2012,
  author          = {Tekli, J.M. and Damiani, E. and Chbeir, R. and Gianini, G.},
  title           = {SOAP processing performance and enhancement},
  journal         = {IEEE Transactions on Services Computing},
  year            = {2012},
  volume          = {5},
  number          = {3},
  pages           = {387-403},
  note            = {cited By 26},
  __markedentry   = {[Nichl:6]},
  abstract        = {The web services (WS) technology provides a comprehensive solution for representing, discovering, and invoking services in a wide variety of environments, including Service Oriented Architectures (SOA ) and grid computing systems. At the core of WS technology lie a number of XML-based standards, such as the Simple Object Access Protocol (SOAP), that have successfully ensured WS extensibility, transparency, and interoperability. Nonetheless, there is an increasing demand to enhance WS performance, which is severely impaired by XML's verbosity. SOAP communications produce considerable network traffic, making them unfit for distributed, loosely coupled, and heterogeneous computing environments such as the open Internet. Also, they introduce higher latency and processing delays than other technologies, like Java RMI and CORBA. WS research has recently focused on SOAP performance enhancement. Many approaches build on the observation that SOAP message exchange usually involves highly similar messages (those created by the same implementation usually have the same structure, and those sent from a server to multiple clients tend to show similarities in structure and content). Similarity evaluation and differential encoding have thus emerged as SOAP performance enhancement techniques. The main idea is to identify the common parts of SOAP messages, to be processed only once, avoiding a large amount of overhead. Other approaches investigate nontraditional processor architectures, including micro- and macrolevel parallel processing solutions, so as to further increase the processing rates of SOAP/XML software toolkits. This survey paper provides a concise, yet comprehensive review of the research efforts aimed at SOAP performance enhancement. A unified view of the problem is provided, covering almost every phase of SOAP processing, ranging over message parsing, serialization, deserialization, compression, multicasting, security evaluation, and data/instruction-level processing. © 2012 IEEE.},
  affiliation     = {Department of Computer Science and Statistics (ICMC), University of Sao Paulo, Av. Trabalhador Saocarlense, 400, 13566-590 São Carlos, SP, Brazil; Department of Information and Technology, University of Milan, via Bramante 65, 26013 Crema, Italy; LE2I Laboratory, UMR-CNRS, University of Bourgogne, 9 Alain Savary, 21078 Dijon, France},
  art_number      = {5719596},
  author_keywords = {and protection; integrity; performance evaluation; performance measures; security; Web-based Services; XML/XSL/RDF},
  document_type   = {Article},
  doi             = {10.1109/TSC.2011.11},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865688779&doi=10.1109%2fTSC.2011.11&partnerID=40&md5=e29ad517c484bb2d9e5da08af7e37aa0},
}

@Article{Lee2012a,
  author          = {Lee, C.K. and Mohd Yusof, N. and Selan, N.E. and Lukose, D.},
  title           = {Service oriented architecture for semantic data access layer},
  journal         = {Communications in Computer and Information Science},
  year            = {2012},
  volume          = {295 CCIS},
  pages           = {93-102},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents the Service Oriented Architecture (SOA)-based Semantic Data Access Layer (DAL) component approach which simplifies access to triple stores. Built with the benefits of web service and SOA, this component provides interoperability, loose coupling and programming-language independent access to different triple stores on various computer systems. Limitations of SPARQL query, SPARQL endpoint and the triples indexing mechanism as well as our solution are discussed. In this paper, particular attention is given to the performance comparison between our component and cache management, indexing facilities and native triple store vendors using the Lehigh University Benchmark (LUBM). The main contribution of our component is to provide an easy-one-stop common and reusable Application Programming Interface (API), to build high performance Semantic-based applications, without the need to develop yet another back-end system. © 2012 Springer-Verlag.},
  affiliation     = {MIMOS Berhad, Technology Park Malaysia, 57000 Kuala Lumpur, Malaysia},
  author_keywords = {Data Access Layer; Extensible Markup Language; Semantic; Service Oriented Architecture; Web Services},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-32826-8_10},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865594303&doi=10.1007%2f978-3-642-32826-8_10&partnerID=40&md5=982d99612b703934a60a71ae0161eded},
}

@Article{Vaccarella2012,
  author          = {Vaccarella, A. and Enquobahrie, A. and Ferrigno, G. and Momi, E.D.},
  title           = {Modular multiple sensors information management for computer-integrated surgery},
  journal         = {International Journal of Medical Robotics and Computer Assisted Surgery},
  year            = {2012},
  volume          = {8},
  number          = {3},
  pages           = {253-260},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {Background: In the past 20years, technological advancements have modified the concept of modern operating rooms (ORs) with the introduction of computer-integrated surgery (CIS) systems, which promise to enhance the outcomes, safety and standardization of surgical procedures. With CIS, different types of sensor (mainly position-sensing devices, force sensors and intra-operative imaging devices) are widely used. Recently, the need for a combined use of different sensors raised issues related to synchronization and spatial consistency of data from different sources of information. Methods: In this study, we propose a centralized, multi-sensor management software architecture for a distributed CIS system, which addresses sensor information consistency in both space and time. The software was developed as a data server module in a client-server architecture, using two open-source software libraries: Image-Guided Surgery Toolkit (IGSTK) and OpenCV. The ROBOCAST project (FP7 ICT 215190), which aims at integrating robotic and navigation devices and technologies in order to improve the outcome of the surgical intervention, was used as the benchmark. An experimental protocol was designed in order to prove the feasibility of a centralized module for data acquisition and to test the application latency when dealing with optical and electromagnetic tracking systems and ultrasound (US) imaging devices. Results: Our results show that a centralized approach is suitable for minimizing synchronization errors; latency in the client-server communication was estimated to be 2ms (median value) for tracking systems and 40ms (median value) for US images. Conclusion: The proposed centralized approach proved to be adequate for neurosurgery requirements. Latency introduced by the proposed architecture does not affect tracking system performance in terms of frame rate and limits US images frame rate at 25 fps, which is acceptable for providing visual feedback to the surgeon in the OR. © 2012 John Wiley & Sons, Ltd.},
  affiliation     = {Dipartimento di Bioingegneria, Politecnico di Milano, Milano, Italy; Kitware Inc., Clifton Park, NY, United States; ITIA, National Research Council, Milano, Italy},
  author_keywords = {IGSTK; Robotic neurosurgery; Sensors management; Surgical navigation},
  document_type   = {Article},
  doi             = {10.1002/rcs.1412},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865401190&doi=10.1002%2frcs.1412&partnerID=40&md5=685f862efdcc1ac365140dc59507bb24},
}

@Conference{Baresi2012,
  author        = {Baresi, L. and Müller, H.A.},
  title         = {ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems: Foreword},
  year          = {2012},
  pages         = {iii-iv},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {SEAMS 2012, the 7th ACM/IEEE International Symposium on Software Engineering for Adaptive and Self-Managing Systems, was held on June 4-5, 2012 in Zurich, Switzerland. As in previous years SEAMS is co-located with the ACM/IEEE International Conference on Software Engineering (ICSE). The increasing complexity, distribution, and dynamism of many software-intensive systems impose self-managing capabilities as key requirements. These systems must be able to adapt themselves at runtime to cope with changes in operating environments, variability of resources, new user needs, intrusions, or faults. The goal is to preserve effective, secure, and safe operation and react to changes with no (or limited) human intervention. Solutions to complement software systems with self-managing and self-adaptive capabilities have been proposed by researchers in many different areas, including software architecture, fault-tolerant computing, robotics, control systems, programming languages, runtime program analysis and verification, and biologically-inspired computing. The SEAMS Symposium focuses on the software engineering aspects, including the methods, techniques, and tools that can be used to support self-adaptive, self-managing, self-healing, selfoptimizing, and self-configuring software systems. The objective of the SEAMS Symposium is to bring together researchers and practitioners from many diverse areas to investigate, discuss, and examine thoroughly the fundamental principles, state of the art, and critical challenges of self-adaptive and self-managing systems. This second year of SEAMS as a symposium continues to attract researchers and practitioners from the different disciplines interested in self-adaptive systems. We received 50 submissions, and after a thorough review process - each paper was reviewed by at least three program committee members - accepted 14 full papers and 5 short papers. The SEAMS 2012 program comprises two keynotes, presentations of the 19 technical papers, a panel on runtime validation and verification, and short reports on on-going activities in the self-adaptive systems community. The invited presentations feature two key research topics of self-adaptive systems. The technical papers cover many different aspects of self-adaptive systems. Authors analyze and summarize existing systems and literature in the form of taxonomies and reference models and address requirements elicitation as well as design and validation of selfadaptive, service-based, and distributed systems. Researchers exploit models, stochastic verification and control theory to develop innovative solutions. The SEAMS 2012 Symposium also emphasizes the importance of identifying significant exemplars for the community to use as benchmarks to validate and compare approaches. In an exemplar session, anchored by an interesting traffic control system paper, paper presenters have the opportunity to outline their favorite example system. © 2012 IEEE.},
  art_number    = {6224406},
  document_type = {Conference Paper},
  doi           = {10.1109/SEAMS.2012.6224406},
  journal       = {ICSE Workshop on Software Engineering for Adaptive and Self-Managing Systems},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865111598&doi=10.1109%2fSEAMS.2012.6224406&partnerID=40&md5=3b7014863c1aaf49d0d397733ebe6194},
}

@Conference{Hirzel2012,
  author          = {Hirzel, M.},
  title           = {Partition and compose: Parallel complex event processing},
  year            = {2012},
  pages           = {191-200},
  note            = {cited By 55},
  __markedentry   = {[Nichl:6]},
  abstract        = {Complex event processing uses patterns to detect composite events in streams of simple events. Typically, the events are logically partitioned by some key. For instance, the key can be the stock symbol in stock quotes, the author in tweets, the vehicle in transportation, or the patient in health-care. Composite event patterns often become meaningful only after partitioning. For instance, a pattern over stock quotes is typically meaningful over quotes for the same stock symbol. This paper proposes a pattern syntax and translation scheme organized around the notion of partitions. Besides making patterns meaningful, partitioning also benefits performance, since different keys can be processed in parallel. We have implemented partitioned parallel complex event processing as an extension to IBM's System S highperformance streaming platform. Our experiments with several benchmarks from finance and social media demonstrate processing speeds of up to 830,000 events per second, and substantial speedups for expensive patterns parallelized on multi-core machines as well as multi-machine clusters. Partitioning the event stream before detecting composite events makes event processing both more intuitive and parallel. Copyright © 2012 ACM.},
  affiliation     = {IBM T.J. Watson Research Center, United States},
  author_keywords = {Automata; CEP; Composite events; Parallelism; Pattern matching; Regular expressions; SPL; Stream processing},
  document_type   = {Conference Paper},
  doi             = {10.1145/2335484.2335506},
  journal         = {Proceedings of the 6th ACM International Conference on Distributed Event-Based Systems, DEBS'12},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865029392&doi=10.1145%2f2335484.2335506&partnerID=40&md5=ec6c2526f0bf34188b9972ddfae2f915},
}

@Conference{Yoo2012,
  author          = {Yoo, W. and Larson, K. and Baugh, L. and Kim, S. and Campbell, R.H.},
  title           = {ADP: Automated diagnosis of performance pathologies using hardware events},
  year            = {2012},
  volume          = {40},
  number          = {1 SPEC. ISS.},
  pages           = {283-294},
  note            = {cited By 14},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance characterization of applications' hardware behavior is essential for making the best use of available hardware resources. Modern architectures offer access to many hardware events that are capable of providing information to reveal architectural performance bottlenecks throughout the core and memory hierarchy. These events can provide programmers with unique and powerful insights into the causes of the resource bottlenecks in their applications. However, interpreting these events has been a significant challenge. We present an automated system that uses machine learning to identify an application's performance problems. Our system provides programmers with insights about the performance of their applications while shielding them from the onerous task of digesting hardware events. It uses a decision tree algorithm, random forests on our micro-benchmarks to fingerprint the performance problems. Our system divides a profiled application into functions and automatically classifies each function by the dominant hardware resource bottlenecks. Using the classifications from the hotspot functions, we were able to achieve an average speedup of 1.73 from three applications in the PARSEC benchmark suite. Our system provides programmers with a guideline of where, what, and how to fix the detected performance problems in applications, which would have otherwise required considerable architectural knowledge. © 2012 ACM.},
  affiliation     = {Department of Computer Science, University of Illinois at Urbana-Champaign, United States; Intel Corp., United States},
  author_keywords = {fingerprint; hardware event; machine learning; micro-benchmark; performance analysis; resource bottleneck},
  document_type   = {Conference Paper},
  doi             = {10.1145/2254756.2254791},
  journal         = {Performance Evaluation Review},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84864665235&doi=10.1145%2f2254756.2254791&partnerID=40&md5=d0d45028313a31935cb687e78764928d},
}

@Conference{Farhad2012,
  author          = {Farhad, S.M. and Ko, Y. and Burgstaller, B. and Scholz, B.},
  title           = {Profile-Guided deployment of stream programs on multicores},
  year            = {2012},
  pages           = {79-88},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {Because multicore architectures have become the industry standard, programming abstractions for concurrent programming are of key importance. Stream programming languages facilitate application domains characterized by regular sequences of data, such as multimedia, graphics, signal processing and networking. With stream programs, computations are expressed through independent actors that interact through FIFO data channels. A major challenge with stream programs is to load-balance actors among available processing cores. The workload of a stream program is determined by actor execution times and the communication overhead induced by data channels. Estimating communication costs on cache-coherent shared-memory multiprocessors is difficult, because data movements are abstracted away by the cache coherence protocol. Standard execution time profiling techniques cannot separate actor execution times from communication costs, because communication costs manifest in terms of execution time overhead. In this work we present a unified Integer Linear Programming (ILP) formulation that balances the workload of stream programs on cache-coherent multicore architectures. For estimating the communication costs of data channels, we devise a novel profiling scheme that minimizes the number of profiling steps. We conduct experiments across a range of StreamIt benchmarks and show that our method achieves a speedup of up to 4.02x on 6 processors. The number of profiling steps is on average only 17% of an exhaustive profiling run over all data channels of a stream program. Copyright © 2012 ACM.},
  affiliation     = {NICTA, University of Sydney, Australia; Yonsei University, South Korea; University of Sydney, Australia},
  author_keywords = {Multicore; Stream programming; StreamIt},
  document_type   = {Conference Paper},
  doi             = {10.1145/2248418.2248430},
  journal         = {Proceedings of the ACM SIGPLAN Conference on Languages, Compilers, and Tools for Embedded Systems (LCTES)},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84864134748&doi=10.1145%2f2248418.2248430&partnerID=40&md5=1d9da892955d711a501ca6e249b67572},
}

@Conference{Tao2012,
  author        = {Tao, J. and Fürlinger, K. and Wang, L. and Marten, H.},
  title         = {A performance study of virtual machines on multicore architectures},
  year          = {2012},
  pages         = {89-96},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {Cloud computing has promoted the widespread use of virtualized machines. A question arises: How does virtualization influence the performance of running applications? The answer must be a common interest of application developers and users. This paper describes the results of our performance evaluation on a virtualized multicore machine. We tested a set of benchmark applications and detected some general features that should be considered when running applications on a virtualized multicore machine. We also studied the application execution behavior using profiling tools. We found the reason for unexpectedly poor performance of an OpenMP application in a virtualized setting and optimized the program. The optimization resulted in a significant performance gain. © 2012 IEEE.},
  affiliation   = {Steinbuch Center for Computing, Karlsruhe Institute of Technology, Germany; Department of Computer Science, Ludwig-Maximilians-Universität, München, Germany; Center for Earth Observation and Digital Earth, Chinese Academy of Sciences, China},
  art_number    = {6169534},
  document_type = {Conference Paper},
  doi           = {10.1109/PDP.2012.77},
  journal       = {Proceedings - 20th Euromicro International Conference on Parallel, Distributed and Network-Based Processing, PDP 2012},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84862128482&doi=10.1109%2fPDP.2012.77&partnerID=40&md5=d57ba99c2bf7c85ed5988b316663066b},
}

@Article{Farhad2012a,
  author          = {Farhad, S.M. and Ko, Y. and Burgstaller, B. and Scholz, B.},
  title           = {Orchestration by approximation mapping stream programs onto multicore architectures},
  journal         = {ACM SIGPLAN Notices},
  year            = {2012},
  volume          = {47},
  number          = {4},
  pages           = {357-367},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {We present a novel 2-approximation algorithm for deploying stream graphs on multicore computers and a stream graph transformation that eliminates bottlenecks. The key technical insight is a data rate transfer model that enables the computation of a "closed form", i.e., the data rate transfer function of an actor depending on the arrival rate of the stream program. A combinatorial optimization problem uses the closed form to maximize the throughput of the stream program. Although the problem is inherently NP-hard, we present an efficient and effective 2-approximation algorithm that provides a lower bound on the quality of the solution. We introduce a transformation that uses the closed form to identify and eliminate bottlenecks. We show experimentally that state-of-the art integer linear programming approaches for orchestrating stream graphs are (1) intractable or at least impractical for larger stream graphs and larger number of processors and (2) our 2-approximation algorithm is highly efficient and its results are close to the optimal solution for a standard set of StreamIt benchmark programs. Copyright © 2011 ACM.},
  affiliation     = {University of Sydney, Sydney, Australia; Yonsei University, Seoul, South Korea; NICTA, Locked Bag 9013, Alexandria NSW 1435, Australia},
  author_keywords = {Multicore; Stream programming; StreamIt},
  document_type   = {Conference Paper},
  doi             = {10.1145/2248487.1950406},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84863833341&doi=10.1145%2f2248487.1950406&partnerID=40&md5=403c450dc99ff53fd35bcfb0e468a499},
}

@Conference{Khan2012,
  author        = {Khan, A. and Vijayaraghavan, M. and Boyd-Wickizer, S. and Arvind},
  title         = {Fast and cycle-accurate modeling of a multicore processor},
  year          = {2012},
  pages         = {178-187},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {An ideal simulator allows an architect to swiftly explore design alternatives and accurately determine their impact on performance. Design exploration requires simulators to be easily modifiable, and accurate performance estimates require detailed models. Unfortunately, detailed modeling not only impacts the ease with which a simulator can be modified, but also the speed at which it can be executed, resulting in fidelity being traded for simulation speed. Although FPGA-based simulators have dramatically higher speed than software simulators, sacrificing fidelity is still common. In this paper we present Arete, an FPGA-based processor simulator, which offers high performance along with accuracy and modifiability. We begin with a cycle-level specification of a multicore architecture which includes realistic in-order cores and detailed models of shared, coherent memory and on-chip network. We then describe how this specification is implemented faithfully and efficiently on FPGAs. Arete delivers a performance of up to 11 MIPS per core. We run a subset of the PARSEC benchmark suite on top of off-the-shelf SMP Linux, and achieve an average performance of 55 MIPS for an 8-core model.We also describe two significant architectural explorations: one involving three different branch predictors and the other requiring major modifications to the cache-coherence protocol. © 2012 IEEE.},
  affiliation   = {Computer Science and Artificial Intelligence Laboratory, Massachusetts Institute of Technology, Cambridge, MA, United States},
  art_number    = {6189224},
  document_type = {Conference Paper},
  doi           = {10.1109/ISPASS.2012.6189224},
  journal       = {ISPASS 2012 - IEEE International Symposium on Performance Analysis of Systems and Software},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84862109141&doi=10.1109%2fISPASS.2012.6189224&partnerID=40&md5=44b560ce2a14c9d15757bc061882a799},
}

@Article{Farhad2012b,
  author          = {Farhad, S.M. and Ko, Y. and Burgstaller, B. and Scholz, B.},
  title           = {Profile-guided deployment of stream programs on multicores},
  journal         = {ACM SIGPLAN Notices},
  year            = {2012},
  volume          = {47},
  number          = {5},
  pages           = {79-88},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Because multicore architectures have become the industry standard, programming abstractions for concurrent programming are of key importance. Stream programming languages facilitate application domains characterized by regular sequences of data, such as multimedia, graphics, signal processing and networking. With stream programs, computations are expressed through independent actors that interact through FIFO data channels. A major challenge with stream programs is to load-balance actors among available processing cores. The workload of a stream program is determined by actor execution times and the communication overhead induced by data channels. Estimating communication costs on cache-coherent shared-memory multiprocessors is difficult, because data movements are abstracted away by the cache coherence protocol. Standard execution time profiling techniques cannot separate actor execution times from communication costs, because communication costs manifest in terms of execution time overhead. In this work we present a unified Integer Linear Programming (ILP) formulation that balances the workload of stream programs on cache-coherent multicore architectures. For estimating the communication costs of data channels, we devise a novel profiling scheme that minimizes the number of profiling steps. We conduct experiments across a range of StreamIt benchmarks and show that our method achieves a speedup of up to 4.02x on 6 processors. The number of profiling steps is on average only 17% of an exhaustive profiling run over all data channels of a stream program. Copyright © 2012 ACM.},
  affiliation     = {University of Sydney, NICTA, Australia; Yonsei University, South Korea; University of Sydney, Australia},
  author_keywords = {Multicore; Stream programming; StreamIt},
  document_type   = {Conference Paper},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84866339152&partnerID=40&md5=1a21b4ae20ba0ed558bd0ada05aa39c4},
}

@Article{Avakian2012,
  author        = {Avakian, A. and Agrawal, N. and Vemuri, R.},
  title         = {Reconfigurable multicore architecture for dynamic processor reallocation},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2012},
  volume        = {7199 LNCS},
  pages         = {329-334},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {One of the challenges of multicore design is providing data quickly to all the processor cores running on a system. Recent proposals of hybrid and reconfigurable interconnect architectures try to take advantage of data locality to a certain extent by grouping processors that work on the same data. In this paper, we propose migrating processors instead of data to take advantage of data locality. This is realized by implementing a reconfigurable interconnect that allows reassignment of processor cores to different routers at runtime. We present the proposed architecture in detail, show a segmented hardware implementation of the proposed architecture, and discuss experimental results using PARSEC benchmark showing the performance gains of the proposed architecture. Our results show a gain in average L2 access time of up to 24% when implementing the proposed architecture compared to a hybrid architecture without reconfiguration. Finally we present area and performance data based on a detailed Verilog model and synthesis of the proposed architecture. © 2012 Springer-Verlag.},
  affiliation   = {School of Electronics and Computing Systems, University of Cincinnati, United States},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-28365-9_28},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84859475147&doi=10.1007%2f978-3-642-28365-9_28&partnerID=40&md5=22af6c21171442db2fa94871af278096},
}

@Article{Breuer2012,
  author          = {Breuer, T. and Giorgana MacEdo, G.R. and Hartanto, R. and Hochgeschwender, N. and Holz, D. and Hegger, F. and Jin, Z. and Müller, C. and Paulus, J. and Reckhaus, M. and Álvarez Ruiz, J.A. and Plöger, P.G. and Kraetzschmar, G.K.},
  title           = {Johnny: An autonomous service robot for domestic environments},
  journal         = {Journal of Intelligent and Robotic Systems: Theory and Applications},
  year            = {2012},
  volume          = {66},
  number          = {1-2},
  pages           = {245-272},
  note            = {cited By 29},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this article we describe the architecture, algorithms and real-world benchmarks performed by Johnny Jackanapes, an autonomous service robot for domestic environments. Johnny serves as a research and development platform to explore, develop and integrate capabilities required for real-world domestic service applications. We present a control architecture which allows to cope with various and changing domestic service robot tasks. A software architecture supporting the rapid integration of functionality into a complete system is as well presented. Further, we describe novel and robust algorithms centered around multi-modal human robot interaction, semantic scene understanding and SLAM. Evaluation of the complete system has been performed during the last years in the RoboCup@Home competition where Johnnys outstanding performance led to successful participation. The results and lessons learned of these benchmarks are explained in more detail. © 2011 Springer Science+Business Media B.V.},
  affiliation     = {Bonn-Rhine-Sieg University of Applied Sciences, Grantham-Allee 20, Sankt Augustin 53757, Germany; DFKI-Robotics Innovation Center, Bremen 28359, Germany; Rheinische Friedrich-Wilhelms-Universität, Bonn 53113, Germany},
  author_keywords = {Domestic service robots; Human robot interaction; Semantic scene understanding},
  document_type   = {Article},
  doi             = {10.1007/s10846-011-9608-y},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84857783657&doi=10.1007%2fs10846-011-9608-y&partnerID=40&md5=39640ea6ee4585bb1cbb3149ed3a9a06},
}

@Article{Lee2012c,
  author          = {Lee, S. and Lee, I.},
  title           = {Selective restart of threads for efficient thread-level speculation on multicore architecture},
  journal         = {IEICE Electronics Express},
  year            = {2012},
  volume          = {9},
  number          = {4},
  pages           = {290-295},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {An efficient recovery method for thread-level speculation (TLS) is proposed. The method tracks the inter-thread data dependence as a method for identifying those threads that are obviously unaffected by a data dependence violation. The method is simple to implement. Still, the simulation results using benchmark applications show that the method can significantly reduce the number of unnecessary thread restarts and consequently improve the performance of TLS. Specifically, when compared with the baseline TLS, TLS with the proposed method is 2.3 times faster for IS, 1.7 times faster for equake, and 3.5 times faster for mcf with the use of 64 cores. With the method, the performance of TLS increases steadily up to 64 cores for IS, equake, and mcf, while the speedup of the baseline TLS starts to saturate at 8 or 16 cores. © IEICE 2012.},
  affiliation     = {School of Electrical and Computer Engineering, Hanyang University, Hangdang-Dong 17, Sungdong-Gu, Seoul 133-791, South Korea},
  author_keywords = {Multicore architecture; Selective restart; Thread-level speculation},
  document_type   = {Article},
  doi             = {10.1587/elex.9.290},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84863298350&doi=10.1587%2felex.9.290&partnerID=40&md5=fe68e7771f97e9b55d5eb955b078effe},
}

@Article{Gonzalez-Dominguez2012,
  author        = {González-Domínguez, J. and Taboada, G.L. and Fraguela, B.B. and Martín, M.J. and Touriño, J.},
  title         = {Automatic mapping of parallel applications on multicore architectures using the Servet benchmark suite},
  journal       = {Computers and Electrical Engineering},
  year          = {2012},
  volume        = {38},
  number        = {2},
  pages         = {258-269},
  note          = {cited By 7},
  __markedentry = {[Nichl:6]},
  abstract      = {Servet is a suite of benchmarks focused on detecting a set of parameters with high influence on the overall performance of multicore systems. These parameters can be used for autotuning codes to increase their performance on multicore clusters. Although Servet has been proved to detect accurately cache hierarchies, bandwidths and bottlenecks in memory accesses, as well as the communication overhead among cores, up to now the impact of the use of this information on application performance optimization has not been assessed. This paper presents a novel algorithm that automatically uses Servet for mapping parallel applications on multicore systems and analyzes its impact on three testbeds using three different parallel programming models: message-passing, shared memory and partitioned global address space (PGAS). Our results show that a suitable mapping policy based on the data provided by this tool can significantly improve the performance of parallel applications without source code modification. © 2011 Elsevier Ltd. All rights reserved.},
  affiliation   = {Computer Architecture Group, Department of Electronics and Systems, University of A Coruña, Campus de Elviña s/n, 15071 A Coruña, Spain},
  document_type = {Article},
  doi           = {10.1016/j.compeleceng.2011.12.007},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84858069790&doi=10.1016%2fj.compeleceng.2011.12.007&partnerID=40&md5=0bdf9fba7a57f1a7f79eb6f49b08e102},
}

@Article{Candiello2012,
  author          = {Candiello, A. and Albarelli, A. and Cortesi, A.},
  title           = {Quality and impact monitoring for local eGovernment services},
  journal         = {Transforming Government: People, Process and Policy},
  year            = {2012},
  volume          = {6},
  number          = {1},
  pages           = {112-125},
  note            = {cited By 14},
  __markedentry   = {[Nichl:6]},
  abstract        = {Purpose: The purpose of this paper is to introduce a layered, comprehensive model of quality of service (QoS) for local eGovernment, and discuss its feasibility on a regional eGovernment case study. The eGovernment online services are becoming a key infrastructure for advanced countries. They allow significant efficiency gains in different sectors of society, offering benefits for individual citizens and for the community as a whole. The deployment of online services alone is not sufficient in order to qualify an eGovernment strategy. The intrinsic and perceived quality of services offered, as well as the actual impact of new functionalities, should be properly measured and taken into account. Design/methodology/approach: This paper presents an applied research study for a quality-focused evolution of a service-oriented architecture for local eGovernment portals. This investigation was based on three main layers: the perceived quality and effective impact of services (G2C layer), the effectiveness of the deployed processes (WFM layer) and finally, the system-level efficiency (G2G layer). Findings: The measurement of quality with respect to eGovernment services is a complex task which requires appropriate tools to tackle the different aspects of the problem. Specifically, active and passive tools (respectively surveys and usage analysis) should be used to evaluate the quality perceived by the users as well as the utility of the service itself. The efficiency of the back office workflow must be estimated measuring statistical and dynamical indicators. Finally, technical measures should be used to monitor the responsiveness and scalability of software implementations and deployment systems. Social implications: A better knowledge regarding (e-)Government service delivery processes, their QoS and their impact on the society can empower both citizens and local administrators, and can help them to better improve the effectiveness of local government. Originality/value: The multi-layered quality measurement architecture proposed in this paper offers local governments the capability to systematically monitor and analyse the quality of their online services. The business process management technologies allow citizens to get a better knowledge of the service delivery processes; the QoS measurements allow to improve control on them; and the eGovernment Intelligence model allows to better quantify their actual social impact. © Emerald Group Publishing Limited.},
  affiliation     = {Dipartimento di Informatica, Università Ca' Foscari Venezia, Venezia, Italy},
  author_keywords = {Business process management; E-government; Local government; Metrics; Performance measures; Quality of service; webbots},
  document_type   = {Article},
  doi             = {10.1108/17506161211214859},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84858428251&doi=10.1108%2f17506161211214859&partnerID=40&md5=c0f4a36e4d8bbf76067c2a968c6aed93},
}

@Article{Cornelis2012,
  author        = {Cornelis, H. and Rodriguez, A.L. and Coop, A.D. and Bower, J.M.},
  title         = {Python as a federation tool for GENESIS 3.0},
  journal       = {PLoS ONE},
  year          = {2012},
  volume        = {7},
  number        = {1},
  note          = {cited By 15},
  __markedentry = {[Nichl:6]},
  abstract      = {The GENESIS simulation platform was one of the first broad-scale modeling systems in computational biology to encourage modelers to develop and share model features and components. Supported by a large developer community, it participated in innovative simulator technologies such as benchmarking, parallelization, and declarative model specification and was the first neural simulator to define bindings for the Python scripting language. An important feature of the latest version of GENESIS is that it decomposes into self-contained software components complying with the Computational Biology Initiative federated software architecture. This architecture allows separate scripting bindings to be defined for different necessary components of the simulator, e.g., the mathematical solvers and graphical user interface. Python is a scripting language that provides rich sets of freely available open source libraries. With clean dynamic object-oriented designs, they produce highly readable code and are widely employed in specialized areas of software component integration. We employ a simplified wrapper and interface generator to examine an application programming interface and make it available to a given scripting language. This allows independent software components to be 'glued' together and connected to external libraries and applications from user-defined Python or Perl scripts. We illustrate our approach with three examples of Python scripting. (1) Generate and run a simple single-compartment model neuron connected to a stand-alone mathematical solver. (2) Interface a mathematical solver with GENESIS 3.0 to explore a neuron morphology from either an interactive command-line or graphical user interface. (3) Apply scripting bindings to connect the GENESIS 3.0 simulator to external graphical libraries and an open source three dimensional content creation suite that supports visualization of models based on electron microscopy and their conversion to computational models. Employed in this way, the stand-alone software components of the GENESIS 3.0 simulator provide a framework for progressive federated software development in computational neuroscience. © 2012 Cornelis et al.},
  affiliation   = {Research Imaging Institute, University of Texas Health Science Center at San Antonio, San Antonio, TX, United States; Department of Epidemiology and Biostatistics, University of Texas Health Science Center at San Antonio, San Antonio, TX, United States},
  art_number    = {e29018},
  document_type = {Article},
  doi           = {10.1371/journal.pone.0029018},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84855374368&doi=10.1371%2fjournal.pone.0029018&partnerID=40&md5=608b45ba48c2acaaa6c48c50280777bd},
}

@Article{Arigliano2012,
  author          = {Arigliano, F. and Bianchini, D. and Cappiello, C. and Corallo, A. and Ceravolo, P. and Damiani, E. and De Antonellis, V. and Pernici, B. and Plebani, P. and Storelli, D. and Vicari, C.},
  title           = {Monitoring business processes in the networked enterprise},
  journal         = {Lecture Notes in Business Information Processing},
  year            = {2012},
  volume          = {116 LNBIP},
  pages           = {21-38},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Object Management Group (OMG) is promoting the Model Driven Architecture (MDA) approach to support interaction among enterprises based on business process models. Based on this approach, we discuss in this paper how to specify performance indicators among the levels with different degree of abstraction suggested in MDA. These indicators will drive the monitoring activities to check the execution of business processes involving networked enterprises. Connecting the different levels we also decrease the cost of implementing metrics as the measurement of the entities at one level can be based on the lower level. © 2012 IFIP International Federation for Information Processing.},
  affiliation     = {Dipartimento di Tecnologie dell'Informazione, Università degli Studi di Milano, Italy; Centro Cultura Innovativa d'Impresa, Università del Salento, Lecce, Italy; Dipartimento di Elettronica Ed Informazione, Politecnico di Milano, Italy; Research and Development Laboratory - Engineering, Ingegneria Informatica S.p.A., Italy; Dipartimento di Elettronica Per l'Automazione, Università degli Studi di Brescia, Italy},
  author_keywords = {model driven architecture; performance indicators; rules; trends; violations},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-34044-4_2},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84868313316&doi=10.1007%2f978-3-642-34044-4_2&partnerID=40&md5=8cd5193d1f82624ed83c25b632dbaf92},
}

@Article{Pricopi2012,
  author          = {Pricopi, M. and Mitra, T.},
  title           = {Bahurupi: A polymorphic heterogeneous multi-core architecture},
  journal         = {Transactions on Architecture and Code Optimization},
  year            = {2012},
  volume          = {8},
  number          = {4},
  note            = {cited By 29},
  __markedentry   = {[Nichl:6]},
  abstract        = {Computing systems have made an irreversible transition towards parallel architectures with the emergence of multi-cores. Moreover, power and thermal limits in embedded systems mandate the deployment of many simpler cores rather than a few complex cores on chip. Consumer electronic devices, on the other hand, need to support an ever-changing set of diverse applications with varying performance demands. While some applications can benefit from thread-level parallelism offered by multi-core solutions, there still exist a large number of applications with substantial amount of sequential code. The sequential programs suffer from limited exploitation of instruction-level parallelism in simple cores. We propose a reconfigurable multicore architecture, called Bahurupi, that can successfully reconcile the conflicting demands of instruction-level and thread-level parallelism. Bahurupi can accelerate the performance of serial code by dynamically forming coalition of two or more simple cores to offer increased instruction-level parallelism. In particular, Bahurupi can efficiently merge 2-4 simple 2-way out-of-order cores to reach or even surpass the performance of more complex and power-hungry 4-way or 8-way out-of-order core. Compared to baseline 2-way core, quad-core Bahurupi achieves up to 5.61 speedup (average 4.08 speedup) for embedded workloads. On an average, quad-core Bahurupi achieves 17% performance improvement and 43% improvement in energy consumption compared to 8-way out-of-order baseline core on a diverse set of embedded benchmark applications. © 2012 ACM.},
  affiliation     = {Computer Science Department, School of Computing, National University of Singapore, Singapore},
  art_number      = {22},
  author_keywords = {Instruction-level parallelism; Multi-core; Thread-level parallelism},
  document_type   = {Article},
  doi             = {10.1145/2086696.2086701},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84857835805&doi=10.1145%2f2086696.2086701&partnerID=40&md5=59da34c2f0a4396e422e47250b18a37e},
}

@Conference{Panda2011,
  author        = {Panda, A. and Avakian, A. and Vemuri, R.},
  title         = {Configurable workload generators for multicore architectures},
  year          = {2011},
  pages         = {179-184},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Proposed multicore architectures are usually evaluated using two types of benchmarks: application and synthetic. Application benchmarks use well understood computations to generate well defined workloads. In contrast, synthetic benchmarks are tunable to generate a range of custom workloads. Both classes are currently limited. Existing application benchmarks are inflexible. And the options offered by synthetic benchmarks are too limited to generate a large variety of workload patterns. In this paper we propose novel workload generation methodologies that allow system developers to generate custom benchmarks for desired workload conditions for a variety of existing and multicore architectures. Specifically we describe two configurable workload generators, which we name ConWork and CompWork. ConWork is a configurable synthetic workload generator using which artificial traffic among the processors and memories can be generated. CompWork is a configurable computational workload generator, which can be used to specify vector and matrix applications so as to elicit the desired computational workloads among the processors. Together the two generators provide a number of options to generate workloads to evaluate a variety of performance metrics of existing and emerging multicore architectures including bus based SoCs, packet switching NoCs and hybrids. © 2011 IEEE.},
  affiliation   = {School of Electronics and Computing Systems, University of Cincinnati, United States},
  art_number    = {6085077},
  document_type = {Conference Paper},
  doi           = {10.1109/SOCC.2011.6085077},
  journal       = {International System on Chip Conference},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84255188992&doi=10.1109%2fSOCC.2011.6085077&partnerID=40&md5=82545d27e7c4bb392a67ec2e72cc8e21},
}

@Conference{Khan2011,
  author          = {Khan, A. and Kang, K. and Kyung, C.-M.},
  title           = {Exploiting maximum throughput in 3D multicore architectures with stacked NUCA cache},
  year            = {2011},
  pages           = {130-135},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Demands for high performance are growing rapidly and multiple processor cores and huge caches are required to meet these requirements. 3D integration provides us a very bright option to encounter this by integrating numerous cores and cache layers in a single chip. Temperature however becomes a problem in 3D integration due to increased power density. A methodology to exploit maximum performance while keeping the temperature under a given limit has been proposed in this paper. We have solved for the optimum clock frequencies, cache capacity and the placement of cache banks for each core to get the maximum throughput. Experiments are done on multiple benchmark programs and a peak 53% and an average 49% improvement in performance as compared to the base case which assigns same frequency and number of banks to each core is found. © 2011 IEEE.},
  affiliation     = {Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea},
  art_number      = {6081634},
  author_keywords = {3D Integrated Circuits; Instructions per second; Non uniform Cache Architecture; Temperature management},
  document_type   = {Conference Paper},
  doi             = {10.1109/VLSISoC.2011.6081634},
  journal         = {2011 IEEE/IFIP 19th International Conference on VLSI and System-on-Chip, VLSI-SoC 2011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-83755219439&doi=10.1109%2fVLSISoC.2011.6081634&partnerID=40&md5=2b714a64a75ac12f95533539f13f981c},
}

@Book{Yan2011,
  title         = {Web service enabled online laboratory},
  year          = {2011},
  author        = {Yan, Y. and Liang, Y. and Roy, A. and Du, X.},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Online experimentation allows students from anywhere to operate remote instruments at any time. The current techniques constrain users to bind to products from one company and install client side software. We use Web services and Service Oriented Architecture to improve the interoperability and usability of the remote instruments. Under a service oriented architecture for online experiment system, a generic methodology to wrap commercial instruments using IVI and VISA standard as Web services is developed. We enhance the instrument Web services into stateful services so that they can manage user booking and persist experiment results. We also benchmark the performance of this system when SOAP is used as the wire format for communication and propose solutions to optimize performance. In order to avoid any installation at the client side, the authors develop Web 2.0 based techniques to display the virtual instrument panel and real time signals with just a standard Web browser. The technique developed in this article can be widely used for different real laboratories, such as microelectronics, chemical engineering, polymer crystallization, structural engineering, and signal processing. © 2012, IGI Global.},
  affiliation   = {Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; National Research Council, Canada; University of New Brunswick, Canada},
  document_type = {Book Chapter},
  doi           = {10.4018/978-1-61350-104-7.ch016},
  journal       = {Innovations, Standards and Practices of Web Services: Emerging Research Topics},
  pages         = {363-381},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899262178&doi=10.4018%2f978-1-61350-104-7.ch016&partnerID=40&md5=3b41ee31569813cd91ec49861336c96d},
}

@Conference{Gupta2011,
  author          = {Gupta, G. and Sohi, G.S.},
  title           = {Dataflow execution of sequential imperative programs on multicore architectures},
  year            = {2011},
  pages           = {59-70},
  note            = {cited By 36},
  __markedentry   = {[Nichl:6]},
  abstract        = {As multicore processors become the default, researchers are aggressively looking for program execution models that make it easier to use the available resources. Multithreaded programming models that rely on statically-parallel programs have gained prevalence. Most of the existing research is directed at adapting and enhancing such models, alleviating their drawbacks, and simplifying their usage. This paper takes a different approach and proposes a novel execution model to achieve parallel execution of statically-sequential programs. It dynamically parallelizes the execution of suitably-written sequential programs, in a dataflow fashion, on multiple processing cores. Significantly, the execution is race-free and determinate. Thus the model eases program development and yet exploits available parallelism. This paper describes the implementation of a software runtime library that implements the proposed execution model on existing commercial multicore machines. We present results from experiments running benchmark programs, using both the proposed technique as well as traditional parallel programming, on three different systems. We find that in addition to easing the development of the benchmarks, the approach is resource-efficient and achieves performance similar to the traditional approach, using stock compilers, operating systems and hardware, despite the overheads of an all-software implementation of the model. © 2011 ACM.},
  affiliation     = {Computer Sciences Department, University of Wisconsin-Madison, Madison, WI, United States},
  author_keywords = {dataflow; determinacy; multicore; programming},
  document_type   = {Conference Paper},
  doi             = {10.1145/2155620.2155628},
  journal         = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84858770555&doi=10.1145%2f2155620.2155628&partnerID=40&md5=1956d743403a29f10e431c4cccb539dc},
}

@Conference{Lee2011,
  author        = {Lee, J. and Shin, M. and Kim, H. and Kim, J. and Huh, J.},
  title         = {Exploiting mutual awareness between prefetchers and on-chip networks in multi-cores},
  year          = {2011},
  pages         = {177-178},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {The unique characteristics of prefetch traffic have not been considered in on-chip network design for multicore architectures. Most prefetchers are often oblivious to the network congestion when generating prefetech requests. In this work, we investigate the interaction between prefetchers and on-chip networks and exploit the synergy of these two components in multi-core architectures. We explore prefetchaware on-chip networks that differentiates between prefetch and demand traffic by prioritizing demand traffic. In addition, we propose prefetch control mechanism based on network congestion. Our evaluations show that the combination of the proposed prefetch-aware router architecture and congestionsensitive prefetch control improves the performance of benchmarks by 11-13% on average, up to 30% on some of the workloads. © 2011 IEEE.},
  affiliation   = {Department of Computer Science, KAIST, South Korea},
  art_number    = {6113803},
  document_type = {Conference Paper},
  doi           = {10.1109/PACT.2011.27},
  journal       = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84863072247&doi=10.1109%2fPACT.2011.27&partnerID=40&md5=d580061190855b3df2d1c98d1e484bd6},
}

@Conference{Zhang2011,
  author          = {Zhang, Y. and Ding, W. and Liu, J. and Kandemir, M.},
  title           = {Optimizing data layouts for parallel computation on multicores},
  year            = {2011},
  pages           = {143-154},
  note            = {cited By 14},
  __markedentry   = {[Nichl:6]},
  abstract        = {The emergence of multicore platforms offers several opportunities for boosting application performance. These opportunities, which include parallelism and data locality benefits, require strong support from compilers as well as operating systems. Current compiler research targeting multicores mostly focuses on code restructuring and mapping. In this work, we explore automatic data layout transformation targeting multithreaded applications running on multicores. Our transformation considers both data access patterns exhibited by different threads of a multithreaded application and the onchip cache topology of the target multicore architecture. It automatically determines a customized memory layout for each target array to minimize potential cache conflicts across threads. Our experiments show that, our optimization brings significant benefits over state-of-the-art data locality optimization strategies when tested using 30 benchmark programs on an Intel multicore machine. The results also indicate that this strategy is able to scale to larger core counts and it performs better with increased data set sizes. © 2011 IEEE.},
  affiliation     = {Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA 16802, United States},
  art_number      = {6113796},
  author_keywords = {Cache hierarchy-aware; Data layout transformation; Multicore},
  document_type   = {Conference Paper},
  doi             = {10.1109/PACT.2011.20},
  journal         = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84863057721&doi=10.1109%2fPACT.2011.20&partnerID=40&md5=b832c9a5c3e43048cea4e705dc092f8e},
}

@Book{Sithole2011,
  title         = {Quality of service monitoring strategies in service oriented architecture environments using processor hardware performance metrics},
  year          = {2011},
  author        = {Sithole, E. and McClean, S. and Scotney, B. and Parr, G. and Moore, A. and Bustard, D. and Dawson, S.},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The sharp growth in data-intensive applications such as social, professional networking and online commerce services, multimedia applications, as well as the convergence of mobile, wireless, and internet technologies, is greatly influencing the shape and makeup of on-demand enterprise computing environments. In response to the global needs for on-demand computing services, a number of trends have emerged, one of which is the growth of computing infrastructures in terms of the number of computing node entities and the widening in geophysical distributions of deployed node elements. Another development has been the increased complexity in the technical composition of the business computing space due to the diversity of technologies that are employed in IT implementations. Given the huge scales in infrastructure sizes and data handling requirements, as well as the dispersion of compute nodes and technology disparities that are associated with emerging computing infrastructures, the task of quantifying performance for capacity planning, Service Level Agreement (SLA) enforcements, and Quality of Service (QoS) guarantees becomes very challenging to fulfil. In order to come up with a viable strategy for evaluating operational performance on computing nodes, we propose the use of on-chip registers called Performance Monitoring Counters (PMCs), which form part of the processor hardware. The use of PMC measurements is largely non-intrusive and highlights performance issues associated with runtime execution on the CPU hardware architecture. Our proposed strategy of employing PMC data thus overcomes major shortcomings of existing benchmarking approaches such as overheads in the software functionality and the inability to offer detailed insight into the various stages of CPU and memory hardware operation. © 2012, IGI Global.},
  affiliation   = {University of Ulster at Coleraine, United Kingdom; SAP Research Belfast, United Kingdom},
  document_type = {Book Chapter},
  doi           = {10.4018/978-1-61350-432-1.ch005},
  journal       = {Handbook of Research on Service-Oriented Systems and Non-Functional Properties: Future Directions},
  pages         = {86-114},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899336790&doi=10.4018%2f978-1-61350-432-1.ch005&partnerID=40&md5=6b09851abc0028c8b6c0be3e89ce78bf},
}

@Conference{Beebe2011,
  author          = {Beebe, W. and Crutchfield, R. and Schroeder, L.},
  title           = {Lessons learned from the pilot development of a SOA-based LVC interoperability framework},
  year            = {2011},
  pages           = {185-195},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Achieving true interoperability between the Live, Virtual and Constructive (LVC) domains is challenging. Existing systems were developed in a stovepipe fashion, over decades, and to changing standards - leading to systems that were not designed to interoperate with one another. Many approaches to achieve interoperability have been tried, but a commercial Services Oriented Architecture (SOA) based interoperability layer has never yet been implemented. The High Level Task Live Virtual Constructive Architecture Roadmap - Implementation (LVCAR-I), sponsored by the Office of Undersecretary for Defense, Personnel and Readiness (Training), tasked The MITRE Corporation to develop a pilot effort to determine if commercial SOA architectures, software and principles are an appropriate solution space for achieving LVC interoperability. MITRE selected a small use case to portray key technical difficulties inherent in interoperating LVC systems; entity versus aggregate representations, real-time versus time-managed, and two distinctly different data models. The use case selected bridges the Joint Live Virtual Constructive (JLVC) / Entity Resolution Federation (ERF) constructive federation and the Joint Land Component Constructive Training Capability (JLCCTC) Multi-Resolution Federation (MRF). The pilot is designed around an open source application server, JBoss, and uses as many open standards as possible. A common data abstraction layer in the application server provides an abstraction of the storage mechanism through the Java Persistence API (JPA) standard and allows for non-system-specific storage of shared data. Integration with existing legacy systems uses a two-part adaptor / plug-in architecture where the adaptor connects directly to the existing infrastructure and communicates with its plug-in counterpart inside the application server infrastructure. The pilot also includes a sample of other services that would be required for a complete interoperability framework. These services are designed around open standards to illustrate SOA principles like composition and re-use and provide situational awareness visualization, enumerations management and translation, and technical monitoring capabilities. The SOA pilot successfully provided a limited interoperability framework based on the constraints of the use case selected and the level of effort involved. A number of lessons learned were captured regarding the use of SOA architectures in the LVC interoperability domain. While SOA is not a 'silver bullet' for interoperability architectures, it is a promising paradigm and the pilot discovered no barriers to its use. More experimentation, performance benchmarking, and investigation into maintenance and life-cycle development issues should be executed to fully determine how well SOA architectures can solve the LVC interoperability problem.},
  affiliation     = {MITRE Corporation, 3504 Lake Lynda Drive, Orlando, FL 32817, United States},
  author_keywords = {Interoperability; LVC; SOA},
  document_type   = {Conference Paper},
  journal         = {Spring Simulation Interoperability Workshop 2011, 2011 Spring SIW},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84865502192&partnerID=40&md5=9069b3666f9db8cb5085be7102f23783},
}

@Conference{Krishna2011,
  author        = {Krishna, T. and Peh, L.-S. and Beckmann, B.M. and Reinhardt, S.K.},
  title         = {Towards the ideal on-chip fabric for 1-to-many and many-to-1 communication},
  year          = {2011},
  pages         = {71-82},
  note          = {cited By 57},
  __markedentry = {[Nichl:6]},
  abstract      = {The prevalence of multicore architectures has accentuated the need for scalable cache coherence solutions. Many of the proposed designs use a mix of 1-to-1, 1-to-many (1-to-M), and many-to-1 (M-to-1) communication to maintain data coherence and consistency. The on-chip network is the communication backbone that needs to handle all these flows efficiently to allow these protocols to scale. However, most research in on-chip networks has focused on optimizing only 1-to-1 traffic. There has been some recent work addressing 1-to-M traffic by proposing the forking of multicast packets within the network at routers, but these techniques incur high packet delays and power penalties. There has been little research in addressing M-to-1 traffic. We propose two in-network techniques, Flow Across Network Over Uncongested Trees (FANOUT) and Flow AggregatioN In-Network (FANIN), which perform efficient 1-to-M forking and M-to-1 aggregation, respectively, such that packets incur only single-cycle delays at most routers along their path, thus approaching an ideal network (one that incurs only wire delay/energy). Full-system simulations on a 64-core CMP with SPLASH-2 and PARSEC benchmarks show that FANOUT and FANIN together reduce runtime by 14.9% and network energy by 40.2%, on average, compared to state-of-the-art networks, operating at just 1% and 9.6% above the runtime and energy of an ideal network. © 2011 ACM.},
  affiliation   = {Department of EECS, MIT, Cambridge, MA, United States; AMD Research, Bellevue, WA, United States},
  document_type = {Conference Paper},
  doi           = {10.1145/2155620.2155630},
  journal       = {Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84858790896&doi=10.1145%2f2155620.2155630&partnerID=40&md5=5ee39ccd29cd1fda902b866e9ae8defc},
}

@Conference{Karlsson2011,
  author          = {Karlsson, C. and Davies, T. and Ding, C. and Liu, H. and Chen, Z.},
  title           = {Optimizing process-to-core mappings for two dimensional broadcast/reduce on multicore architectures},
  year            = {2011},
  pages           = {404-413},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {In today's high performance computing, many MPI programs (e.g., ScaLAPACK applications, High Performance Linpack Benchmark HPL, and many PDE solvers based on domain decomposition methods) organize their computational processes as multidimensional process grids. Communications are often necessary in each dimension. Multidimensional broadcast, where a broadcast has to be performed in each dimension, is one of the many operations in applications that use multidimensional process grids. In this paper, we study the impact of the MPI process-to-core mapping on the performance of multidimensional broadcast operations. We show that the default process-to-core mappings in today's state-of-the-art MPI implementations are often sub-optimal for multidimensional broadcast. We propose an application-level multicore-aware process-to-core re-mapping scheme that is capable of achieving optimal performance for multidimensional broadcast operations. The proposed multicore-aware process-to-core re-mapping scheme improves the performance of multidimensional broadcast operations by up to 64% over the default mapping scheme on the world's current eighth fastest supercomputer, Kraken, at the Oak Ridge National Laboratory. © 2011 IEEE.},
  affiliation     = {Department of Mathematical and Computer Sciences, Colorado School of Mines, Golden, CO, United States},
  art_number      = {6047208},
  author_keywords = {Cartesian topology; Collective communication; Message passing interface (MPI); Multicore; Process-to-core mapping},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICPP.2011.26},
  journal         = {Proceedings of the International Conference on Parallel Processing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80155187635&doi=10.1109%2fICPP.2011.26&partnerID=40&md5=d321eaa0e70f39f7a693e76a4b89b2d6},
}

@Conference{Tudor2011,
  author        = {Tudor, B.M. and Teo, Y.M. and See, S.},
  title         = {Understanding off-chip memory contention of parallel programs in multicore systems},
  year          = {2011},
  pages         = {602-611},
  note          = {cited By 21},
  __markedentry = {[Nichl:6]},
  abstract      = {Memory contention is an important performance issue in current multicore architectures. In this paper, we focus on understanding how off-chip memory contention affects the performance of parallel applications. Using measurements conducted on state-of-the-art multicore systems, we observed that off-chip memory traffic is not always bursty, as it was previously reported in literature. Burstiness depends on the problem size. Small problem sizes lead to bursty memory traffic, and generate small off-chip contention. In contrast, when large program sizes cause memory contention, the memory traffic is non-bursty. Based on these observations, we propose an analytical model that relates the growth of memory contention to the number of active cores and to the problem size, for both uniform (UMA) and non-uniform memory access (NUMA) systems. Our model differs from measurements on average by less than 14%. Contention for off-chip memory grows exponentially with the number of active cores, but adding additional memory controllers reduces the memory contention. For programs such as the pentadiagonal solver SP from NPB benchmark, with a large matrix of 1623 elements (input size C), our analysis shows that memory contention increases the total number of processor cycles to execute the program by more than ten times on a machine with 24 cores. © 2011 IEEE.},
  affiliation   = {Department of Computer Science, National University of Singapore, 13 Computing Drive, Singapore 117417, Singapore; NVIDIA, Nordic European Center, 3 International Business Park, #01-20A, Singapore 609927, Singapore},
  art_number    = {6047228},
  document_type = {Conference Paper},
  doi           = {10.1109/ICPP.2011.59},
  journal       = {Proceedings of the International Conference on Parallel Processing},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80155154142&doi=10.1109%2fICPP.2011.59&partnerID=40&md5=f6690b6204c87fb72a9ba21d919695e2},
}

@Conference{Jeyapaul2011,
  author        = {Jeyapaul, R. and Hong, F. and Rhisheekesan, A. and Shrivastava, A. and Lee, K.},
  title         = {UnSync: A soft error resilient redundant multicore architecture},
  year          = {2011},
  pages         = {632-641},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {Reducing device dimensions, increasing transistor densities, and smaller timing windows, expose the vulnerability of processors to soft errors induced by charge carrying particles. Since these factors are only consequences of the inevitable advancement in processor technology, the industry has been forced to improve reliability on general purpose Chip Multiprocessors (CMPs). With the availability of increased hardware resources, redundancy based techniques are the most promising methods to eradicate soft error failures in CMP systems. In this work, we propose a novel redundant CMP architecture (UnSync) that utilizes hardware based detection mechanisms (most of which are readily available in the processor), to reduce overheads during error free executions. In the presence of errors (which are infrequent), the always forward execution enabled recovery mechanism provides for resilience in the system. We design a detailed RTL model of our UnSync architecture and perform hardware synthesis to compare the hardware (power/area) overheads incurred. We compare the same with those of the Reunion technique, a state-of-the-art redundant multi-core architecture. We also perform cycle-accurate simulations over a wide range of SPEC2000, and MiBench benchmarks to evaluate the performance efficiency achieved over that of the Reunion architecture. Experimental results show that, our UnSync architecture reduces power consumption by 34.5% and improves performance by up to 20% with 13.3% less area overhead, when compared to Reunion architecture for the same level of reliability achieved. © 2011 IEEE.},
  affiliation   = {Compiler Microarchiteture Lab, Arizona State University, Tempe, AZ, United States; Dependable Computing Lab, Yonsei University, Seoul, South Korea},
  art_number    = {6047231},
  document_type = {Conference Paper},
  doi           = {10.1109/ICPP.2011.76},
  journal       = {Proceedings of the International Conference on Parallel Processing},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80155154135&doi=10.1109%2fICPP.2011.76&partnerID=40&md5=31e796fdc5c1bf3966fdd1485b44b048},
}

@Conference{Graffi2011,
  author        = {Graffi, K.},
  title         = {PeerfactSim.KOM: A P2P system simulator experiences and lessons learned},
  year          = {2011},
  pages         = {154-155},
  note          = {cited By 35},
  __markedentry = {[Nichl:6]},
  abstract      = {Research on peer-to-peer (p2p) and distributed systems needs evaluation tools to predict and observe the behavior of protocols and mechanisms in large scale networks. PeerfactSim.KOM1 [1] is a simulator for large scale distributed/p2p systems aiming at the evaluation of interdependencies in multi-layered p2p systems. The simulator is written in Java, is event-based and mainly used in p2p research projects4. The main development of PeerfactSim.KOM started in 2005 and is driven since 2006 by the project "QuaP2P"2, which aims at the systematic improvement and benchmarking of p2p systems. Further users of the simulator are working in the project "On-the-fly Computing"3 aiming at researching p2p-based service oriented architectures. Both projects5 state severe requirements on the evaluation of multi-layered and large-scale distributed systems. We describe the architecture of PeerfactSim.KOM supporting these requirements in Section II, present the workflow, selected experiences and lessons learned in Section III and conclude the overview in Section IV. © 2011 IEEE.},
  affiliation   = {University of Paderborn, Theory of Distributed Systems, Frstenalle 11, 33102 Paderborn, Germany},
  art_number    = {6038673},
  document_type = {Conference Paper},
  doi           = {10.1109/P2P.2011.6038673},
  journal       = {2011 IEEE International Conference on Peer-to-Peer Computing, P2P 2011 - Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80054973178&doi=10.1109%2fP2P.2011.6038673&partnerID=40&md5=1473fc1753ff8976787b773ecfd47a7f},
}

@Conference{Khan2011a,
  author          = {Khan, A. and Kang, K. and Kyung, C.-M.},
  title           = {Squeezing maximizing performance out of 3D cache-stacked multicore architectures},
  year            = {2011},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {3D integration is one of the most promising options to fulfill the demands of high performance and large cache by integrating multiple processor cores and 3D stacked cache. There are however temperature problems in 3D integration. This paper presents a method for performance maximization of a 3D cache-stacked multicore system keeping the temperature under a given limit while by assigning the clock frequencies and number of cache banks to each core according to the requirement. We have done experiments on multiple benchmark programs and have found a peak 32% and an average 29.8% improvement in performance as compared to the base case which assigns the same frequency and the same number of banks to each core. © 2011 IEEE.},
  affiliation     = {Department of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea},
  art_number      = {6026350},
  author_keywords = {3D Integrated Circuits; Instructions per second; Non uniform Cache; Temperature management},
  document_type   = {Conference Paper},
  doi             = {10.1109/MWSCAS.2011.6026350},
  journal         = {Midwest Symposium on Circuits and Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80053634213&doi=10.1109%2fMWSCAS.2011.6026350&partnerID=40&md5=723753296d4c02a98aa35a54bf2480e7},
}

@Article{Roy2011,
  author          = {Roy, S. and Ranganathan, N. and Katkoori, S.},
  title           = {State-retentive power gating of register files in multicore processors featuring multithreaded in-order cores},
  journal         = {IEEE Transactions on Computers},
  year            = {2011},
  volume          = {60},
  number          = {11},
  pages           = {1547-1560},
  note            = {cited By 20},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this work, we investigate state-retentive power gating of register files for leakage reduction in multicore processors supporting multithreading. In an in-order core, when a thread gets blocked due to a memory stall, the corresponding register file can be placed in a low leakage state through power gating for leakage reduction. When the memory stall gets resolved, the register file is activated for being accessed again. Since the contents of the register file are not lost and restored on wakeup, this is referred to as state-retentive power gating of register files. While state-retentive power gating in single cores has been studied in the literature, it is being investigated for multicore architectures for the first time in this work. We propose specific techniques to implement state-retentive power gating for three different multicore processor configurations based on the multithreading model: 1) coarse-grained multithreading, 2) fine-grained multithreading, and 3) simultaneous multithreading. The proposed techniques can be implemented as design extensions within the control units of the in-order cores. Each technique uses two different modes of leakage states: low-leakage savings and low wake-up and high-leakage savings and high wake-up latency. The overhead due to wake-up latency is completely avoided in two techniques while it is hidden for most part in the third approach, either by overlapping the wake-up process with the thread context switching latency or by executing instructions from other threads ready for execution. The proposed techniques were evaluated through simulations with multiprogrammed workloads comprised of SPEC 2000 integer benchmarks. Experimental results show that in an 8-core processor executing 64 threads, the average leakage savings were 42 percent in coarse-grained multithreading, while they were between seven percent and eight percent for finegrained and simultaneous multithreading. © 2006 IEEE.},
  affiliation     = {Advanced Micro Devices, 7171 Southwest Pkwy., Austin, TX 78735, United States; Department of Computer Science and Engineering, University of South Florida, Tampa, FL 33620, United States; University of South Florida University of South Florida, Tampa, United States},
  art_number      = {5669257},
  author_keywords = {CGMT; FGMT; in-order; M5; Niagara; SMT},
  document_type   = {Article},
  doi             = {10.1109/TC.2010.249},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80053481605&doi=10.1109%2fTC.2010.249&partnerID=40&md5=aeb5dfb8f9c97b1deac0646a0f37a040},
}

@Conference{Hirzalla2011,
  author          = {Hirzalla, M.A. and Zisman, A. and Cleland-Huang, J.},
  title           = {Using traceability to support SOA impact analysis},
  year            = {2011},
  pages           = {145-152},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {Service Oriented Architecture (SOA) has been recognized as an important paradigm for software engineering. Several organizations are in the process of adopting and evolving SOA deployments. In this paper we present IntelliTrace, an intelligent traceability framework to support impact analysis across different modeling layers of a SOA-based system. The framework uses traceability links among different SOA artifacts to analyze the impact that changes in SOA-based systems can have in key performance indicators. The change impact analysis is triggered by different situations such as changes at the service level, business process level, goal level, key performance indicators, and SOA infrastructure. A prototype tool has been implemented in order to illustrate and evaluate the framework. An extensive case study built around an online airline reservation system is used to evaluate the framework. © 2011 IEEE.},
  affiliation     = {School of Computing, DePaul University, 243 S. Wabash Ave., Chicago, IL 60604, United States; IBM, United States; School of Informatics, City University London, Northampton Square, London, EC1V 0HB, United Kingdom},
  art_number      = {6012706},
  author_keywords = {Impact analysis; Intelligent traceability; Key performance indicator},
  document_type   = {Conference Paper},
  doi             = {10.1109/SERVICES.2011.103},
  journal         = {Proceedings - 2011 IEEE World Congress on Services, SERVICES 2011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80053424839&doi=10.1109%2fSERVICES.2011.103&partnerID=40&md5=5129815253e8a159f661276530b79833},
}

@Conference{Jammes2011,
  author        = {Jammes, F.},
  title         = {Real time device level Service-Oriented Architectures},
  year          = {2011},
  pages         = {1722-1726},
  note          = {cited By 6},
  __markedentry = {[Nichl:6]},
  abstract      = {DPWS (Device Profile for Web services) provides a standard solution for implementing SOA (Service Oriented Architectures) at device level. However, the achieved performance is sometimes not sufficient, especially for process control applications, where a performance improvement of an order of magnitude of 10 is targeted. Among all possible solutions, implementing EXI (Efficient XMl, the most promising Binary-XML solution) coupled with DPWS, is investigated through the FP7 AESOP collaborative project. The software architecture of the corresponding stack has been defined, and the first full implementations in industrial devices will be available in 2011 for benchmarks and final technology choices. © 2011 IEEE.},
  affiliation   = {Schneider Electric - Strategy and Innovation, Grenoble, France},
  art_number    = {5984321},
  document_type = {Conference Paper},
  doi           = {10.1109/ISIE.2011.5984321},
  journal       = {Proceedings - ISIE 2011: 2011 IEEE International Symposium on Industrial Electronics},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80052832969&doi=10.1109%2fISIE.2011.5984321&partnerID=40&md5=6568543fd6aab3995efe4fcfce6ea83f},
}

@Conference{Collet2011,
  author        = {Collet, A. and Crebier, J.-C. and Chureau, A.},
  title         = {Multi-cell battery emulator for advanced battery management system benchmarking},
  year          = {2011},
  pages         = {1093-1099},
  note          = {cited By 16},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper presents a hardware-in-the-loop battery simulation platform capable of emulating individual cells of a series connected string. The platform enables an efficient and reproducible way to benchmark battery management systems. The focus is put on battery modeling, embedded system architecture and software development. A practical implementation of a lithium-ion cell emulator based on a linear amplifier is described. © 2011 IEEE.},
  affiliation   = {G2Elab, Grenoble Institute of Technology, Grenoble, France},
  art_number    = {5984312},
  document_type = {Conference Paper},
  doi           = {10.1109/ISIE.2011.5984312},
  journal       = {Proceedings - ISIE 2011: 2011 IEEE International Symposium on Industrial Electronics},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80052832967&doi=10.1109%2fISIE.2011.5984312&partnerID=40&md5=7515757d272b0bed6b8d4b8f110dc2ee},
}

@Article{Crosetto2011,
  author          = {Crosetto, P. and Deparis, S. and Fourestey, G. and Quarteroni, A.},
  title           = {Parallel algorithms for fluid-structure interaction problems in haemodynamics},
  journal         = {SIAM Journal on Scientific Computing},
  year            = {2011},
  volume          = {33},
  number          = {4},
  pages           = {1598-1622},
  note            = {cited By 66},
  __markedentry   = {[Nichl:6]},
  abstract        = {The increasing computational load required by most applications and the limits in hardware performances affecting scientific computing contributed in the last decades to the development of parallel software and architectures. In fluid-structure interaction (FSI) for haemodynamic applications, parallelization and scalability are key issues (see [L. Formaggia, A. Quarteroni, and A. Veneziani, eds., Cardiovascular Mathematics: Modeling and Simulation of the Circulatory System, Modeling, Simulation and Applications 1, Springer, Milan, 2009]). In this work we introduce a class of parallel preconditioners for the FSI problem obtained by exploiting the block-structure of the linear system. We stress the possibility of extending the approach to a general linear system with a block-structure, then we provide a bound in the condition number of the preconditioned system in terms of the conditioning of the preconditioned diagonal blocks, and finally we show that the construction and evaluation of the devised preconditioner is modular. The preconditioners are tested on a benchmark three-dimensional (3D) geometry discretized in both a coarse and a fine mesh, as well as on two physiological aorta geometries. The simulations that we have performed show an advantage in using the block preconditioners introduced and confirm our theoretical results. © 2011 Societ y for Industrial and Applied Mathematics.},
  affiliation     = {IACS, Modelling and Scientific Computing (CMCS), EPFL, CH-1015 Lausanne, Switzerland; MOX-Dipartimento di Matematica F. Brioschi, Politecnico di Milano, I-20133 Milan, Italy},
  author_keywords = {Blood-flow models; Finite elements; Fluid-structure interaction; Parallel algorithms; Preconditioners},
  document_type   = {Article},
  doi             = {10.1137/090772836},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80052729982&doi=10.1137%2f090772836&partnerID=40&md5=98f49e0692bd8c498730f7380422012f},
}

@Conference{Wang2011,
  author          = {Wang, W. and Mishra, P. and Ranka, S.},
  title           = {Dynamic cache reconfiguration and partitioning for energy optimization in real-time multi-core systems},
  year            = {2011},
  pages           = {948-953},
  note            = {cited By 52},
  __markedentry   = {[Nichl:6]},
  abstract        = {Multicore architectures, especially chip multi-processors, have been widely acknowledged as a successful design paradigm. Existing approaches primarily target application-driven partitioning of the shared cache to alleviate inter-core cache interference so that both performance and energy efficiency are improved. Dynamic cache reconfiguration is a promising technique in reducing energy consumption of the cache subsystem for uniprocessor systems. In this paper, we present a novel energy optimization technique which employs both dynamic reconfiguration of private caches and partitioning of the shared cache for multicore systems with real-time tasks. Our static profiling based algorithm is designed to judiciously find beneficial cache configurations (of private caches) for each task as well as partition factors (of the shared cache) for each core so that the energy consumption is minimized while task deadline is satisfied. Experimental results using real benchmarks demonstrate that our approach can achieve 29.29% energy saving on average compared to systems employing only cache partitioning. © 2011 ACM.},
  affiliation     = {Department of Computer and Information Science and Engineering, University of Florida, Gainesville, FL, United States},
  art_number      = {5981887},
  author_keywords = {cache; dynamic reconfiguration; energy optimization; Multicore systems; real-time systems},
  document_type   = {Conference Paper},
  journal         = {Proceedings - Design Automation Conference},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80052672484&partnerID=40&md5=f33a63ba92c5e2bd5400f4b2cf5f5aab},
}

@Conference{Li2011,
  author          = {Li, D. and Cameron, K.W. and Nikolopoulos, D.S. and De Supinski, B.R. and Schulz, M.},
  title           = {Scalable memory registration for high performance networks using helper threads},
  year            = {2011},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Remote DMA (RDMA) enables high performance networks to reduce data copying between an application and the operating system (OS). However RDMA operations in some high performance networks require communication memory explicitly registered with the network adapter and pinned by the OS. Memory registration and pinning limits the flexibility of the memory system and reduces the amount of memory that user processes can allocate. These issues become more significant on multicore platforms, since registered memory demand grows linearly with the number of processor cores. In this paper we propose a new memory registration/deregistration strategy to reduce registered memory on multicore architectures for HPC applications. We hide the cost of dynamic memory management by offloading all dynamic memory registration and deregistration requests to a dedicated memory management helper thread. We investigate design policies and performance implications of the helper thread approach. We evaluate our framework with the NAS parallel benchmarks, for which our registration scheme significantly reduces the registered memory (23.62% on average and up to 49.39%) and avoids memory registration/deregistration costs for reused communication memory. We show that our system enables the execution of problem sizes that could not complete under existing memory registration strategies. © 2011 ACM.},
  affiliation     = {Department of Computer Science, Virginia Tech., United States; FORTH-ICS, University of Crete, Greece; Lawrence Livermore National Lab., United States},
  author_keywords = {C.3.2 [Computer-Communication Networks]: Network Operations - Network Management; D.4.1 [Operating Systems]: Process Management - Threads; D.4.4 [Operating Systems]: Communication Management - Network Communication; Design; Management; Performance},
  document_type   = {Conference Paper},
  doi             = {10.1145/2016604.2016652},
  journal         = {Proceedings of the 8th ACM International Conference on Computing Frontiers, CF'11},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80052519432&doi=10.1145%2f2016604.2016652&partnerID=40&md5=f415495f2edbfe5e6b4c1528e6e3b97c},
}

@Conference{Lin2011,
  author          = {Lin, C.-C. and Liu, P. and Wu, J.-J.},
  title           = {Novel approach for finding optimization opportunities in multicore architectures},
  year            = {2011},
  pages           = {238-243},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Compiler techniques for program optimizations have been well studied for single-thread programs. With the advance of multi-core architectures, compiler optimizations for multi-threaded parallel programs have started to draw research attention in recent years. Optimizations for multi-threaded parallel programs on multi-core architectures are much more difficult because of the complicated interaction and resource competition between threads. Therefore, identifying the appropriate code segments for performing optimization becomes one of the most challenging issues. In this work, we propose a novel technique to identify the code segments that exhibit unstable performance behavior and show that by applying appropriate optimizations to such code segments, the performance of the parallel program can be improved. Our technique is based on a simple and efficient sampling method that analyzes variations in the performance variance of basic blocks to classify basic blocks into "stable" and "unstable" ones. "Stable" basic blocks have low average coefficient of variation(CoV) while "unstable" ones have CoV higher than a threshold value. Such analysis results can be used to determine the "unstable" code segments that may benefit from runtime optimizations. Our experiment results on the SPEC OMP2001 benchmark suite demonstrate that the proposed method is effective in finding "unstable" code segments. © 2011 IEEE.},
  affiliation     = {Institute of Information Science, Academia Sinica, Taipei, Taiwan; Dept. of CSIE, Graduate Institute of Networking and Multimedia, National Taiwan University, Taipei, Taiwan},
  art_number      = {5951912},
  author_keywords = {Multi-core architecture; Optimization opportunities for parallel programs; Parallel program behavior analysis; Sampling based runtime technique},
  document_type   = {Conference Paper},
  doi             = {10.1109/ISPA.2011.30},
  journal         = {Proceedings - 9th IEEE International Symposium on Parallel and Distributed Processing with Applications, ISPA 2011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80051644437&doi=10.1109%2fISPA.2011.30&partnerID=40&md5=785caacf174812d4d23e0655695adcd7},
}

@Conference{Ghosh2011,
  author          = {Ghosh, A. and Hui, Y.-K. and Chiang, M.},
  title           = {Model-based architecture analysis for wireless healthcare},
  year            = {2011},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {The primary challenges in deploying a wireless healthcare solution stem from real-time, distributed resource constraints, as well as stringent clinical requirements of reliability, safety, device interoperability, QoS guarantee, and privacy/security. Although optimized solutions exist for each individual element of the system, the complex, distributed, and concurrent interactions among multiple subsystems make system integration a costly bottleneck. We show that end-to-end modeling and analysis using the formalisms of architecture description languages like AADL can alleviate the hurdles of system integration and provide an effective way of addressing these challenges, thereby, making deployments possible. © 2011 ACM.},
  affiliation     = {Dept. of Electrical Engineering, Princeton University, NJ, United States; Dept. of Internal Medicine, Pennsylvania Hospital, PA, United States},
  author_keywords = {D.2.1 [Software Engineering]: Requirements/Specifications - software architectures, interoperability; D.2.8 [Software Engineering]: Metrics - performance measures; Design; Performance; Reliability},
  document_type   = {Conference Paper},
  doi             = {10.1145/2007036.2007051},
  journal         = {Proceedings of the International Symposium on Mobile Ad Hoc Networking and Computing (MobiHoc)},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80051569813&doi=10.1145%2f2007036.2007051&partnerID=40&md5=bfd1a82d79a1d00a682e719f9bdebd72},
}

@Article{Adamczewski-Musch2011,
  author          = {Adamczewski-Musch, J. and Essel, H.G. and Linev, S.},
  title           = {The DABC framework interface to readout hardware},
  journal         = {IEEE Transactions on Nuclear Science},
  year            = {2011},
  volume          = {58},
  number          = {4 PART 1},
  pages           = {1728-1732},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Data Acquisition Backbone Core (DABC) is a new GSI software framework to run a data acquisition with distributed event building on high performance Linux clusters. Experiment specific front-end electronics is to be integrated to the software by means of hardware interface plug-ins like Device and Transport classes. DABC offers elaborate mechanisms for multiprocessing, buffer management, and dataflow throttling. These are transparently available for all implemented plug-ins. Device plug-ins can link a DABC node to remote readout hardware via network connections like Ethernet. Other Device plug-ins can communicate on the Linux device driver level with custom boards directly inserted at the node. Besides delivering the data input, a DABC Device can also provide control access to the connected hardware. This functionality can be used for setting up, or monitoring the front-ends from the application via DABC parameters and commands. An implementation example is a multipurpose PCI Express Optical Receiver (PEXOR) board developed at GSI. This board features an FPGA and 4 optical links and may be used for various front-ends, depending on the FPGA programming. A kernel driver and the DABC Device plug-in for this board have been developed and tested. They are described here with some performance benchmark results. As another example, DABC is applied for data taking during test beam times of the Compressed Baryonic Matter (CBM) experiment from 2008 to 2010. Here the front-end readout controller boards (ROC) were integrated to the DABC hardware interface, both for a UDP based Ethernet protocol, and for optical connections to a custom PCIe board. © 2006 IEEE.},
  affiliation     = {Experiment Electronics Department, GSI Helmholtzzentrum für Schwerionenforschung, 64291 Darmstadt, Germany},
  art_number      = {5892917},
  author_keywords = {Data acquisition; device driver; object oriented programming; software architecture},
  document_type   = {Conference Paper},
  doi             = {10.1109/TNS.2011.2158112},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84860423565&doi=10.1109%2fTNS.2011.2158112&partnerID=40&md5=9a9d6307b1bde9babff1a2a798e0dca2},
}

@Conference{Barragan2011,
  author          = {Barragán, E.H. and Steves, J.J.},
  title           = {Performance analisys on multicore system using PAPI},
  year            = {2011},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {The rapid growth of multicore systems, and approaches that they have taken, allow complex processes that before were only possible to run on supercomputers, can now be run on low-cost solutions also called "commodity hardware". Such solutions can be implemented using processors consumer market (Intel and AMD). When these solutions scale to scientific computing requirements, is essential to have methods to measure the performance that they offer and how these behave under different workloads. Due to the large number of load types on the market, and even within the scientific computing, it is necessary to introduce "typical workload" that can serve as support in the acquisition and evaluation processes solutions, having a high degree of certainty on this operation. In this research proposes a practical approach to the evaluation and presents the results of tests performed on equipment AMD and Intel multicore architectures. © 2011 IEEE.},
  affiliation     = {Universidad Oberta de Cataluña, Cataluña, Spain; Universidad Autónoma de Bucaramanga, Buramanga, Colombia},
  art_number      = {5936277},
  author_keywords = {Benchmark; Multicore; Multicore Programming; OpenMP; PAPI; Performance; SMT},
  document_type   = {Conference Paper},
  doi             = {10.1109/COLOMCC.2011.5936277},
  journal         = {2011 6th Colombian Computing Congress, CCC 2011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79960683667&doi=10.1109%2fCOLOMCC.2011.5936277&partnerID=40&md5=68297735ad7f7c9367f47b4ef89e03b9},
}

@Conference{Tang2011,
  author          = {Tang, L. and Mars, J. and Soffa, M.L.},
  title           = {Contentiousness vs. sensitivity: Improving contention aware runtime systems on multicore architectures},
  year            = {2011},
  pages           = {12-21},
  note            = {cited By 29},
  __markedentry   = {[Nichl:6]},
  abstract        = {Runtime systems to mitigate memory resource contention problems on multicore processors have recently attracted much research attention. One critical component of these runtimes is the indicators to rank and classify applications based on their contention characteristics. However, although there has been significant research effort, application contention characteristics remain not well understood and indicators have not been thoroughly evaluated. In this paper we performed a thorough study of applications' contention characteristics to develop better indicators to improve contention-aware runtime systems. The contention characteristics are composed of an application's contentiousness, and its sensitivity to contention. We show that contentiousness and sensitivity are not strongly correlated, and contrary to prior work, a single indicator is not adequate to predict both. Also, while prior work argues that last level cache miss rate is one of the best indicators to predict an application's contention characteristics, we show that depending on the workloads, it can often be misleading. We then present prediction models that consider contention in various memory resources. Our regression analysis establishes an accurate model to predict application contentiousness. The analysis also demonstrates that performance counters alone may not be sufficient to accurately predict application sensitivity to contention. Our evaluation using SPEC CPU2006 benchmarks shows that when predicting an application's contentiousness, the linear correlation coefficient R2 of our predictor and the real measured contentiousness is 0.834, as opposed to 0.224 when using last level cache miss rate. © 2011 ACM.},
  affiliation     = {University of Virginia, United States},
  author_keywords = {contention aware runtimes; contentiousness vs sensitivity; memory subsystems; multicore processors; scheduling},
  document_type   = {Conference Paper},
  doi             = {10.1145/2000417.2000419},
  journal         = {ACM International Conference Proceeding Series},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79960216651&doi=10.1145%2f2000417.2000419&partnerID=40&md5=84b6834d66a904512b7e6c40f5bdd74b},
}

@Article{Li2011a,
  author          = {Li, J. and Wu, C. and Hsu, W.-C.},
  title           = {Efficient and effective misaligned data access handling in a dynamic binary translation system},
  journal         = {Transactions on Architecture and Code Optimization},
  year            = {2011},
  volume          = {8},
  number          = {2},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Binary Translation (BT) has been commonly used to migrate application software across Instruction Set Architectures (ISAs). Some architectures, such as X86, allow Misaligned Data Accesses (MDAs), while most modern architectures require natural data alignments. In a binary translation system, where the source ISA allows MDA and the target ISA does not, memory operations must be carefully translated. Naive translation may cause frequent misaligned data access traps to occur at runtime on the target machine and severely slow down the migrated application. This article evaluates different approaches in handling MDA in a binary translation system including how to identify MDA candidates and how to translate such memory instructions. This article also proposes some new mechanisms to more effectively deal with MDAs. Extensive measurements based on SPEC CPU2000 and CPU2006 benchmarks show that the proposed approaches are more effective than existing methods and getting close to the performance upper bound of MDA handling. © 2011 ACM.},
  affiliation     = {Institute of Computing Technology, Graduate University, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan},
  art_number      = {7},
  author_keywords = {Binary translation; Misaligned memory access; Optimization},
  document_type   = {Article},
  doi             = {10.1145/1970386.1970388},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79960783448&doi=10.1145%2f1970386.1970388&partnerID=40&md5=ded31839e5be74f966224563776f2474},
}

@Article{BurinDesRoziers2011,
  author          = {Burin Des Roziers, C. and Chelius, G. and Ducrocq, T. and Fleury, E. and Fraboulet, A. and Gallais, A. and Mitton, N. and Noél, T. and Vandaele, J.},
  title           = {Using senslAB* as a first class scientific tool for large scale wireless sensor network experiments},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2011},
  volume          = {6640 LNCS},
  number          = {PART 1},
  pages           = {147-159},
  note            = {cited By 29},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents a description of SensLAB(Very Large Scale Open Wireless Sensor Network Testbed) that has been developed and deployed in order to allow the evaluation through experimentations of scalable wireless sensor network protocols and applications. SensLAB's main and most important goal is to offer an accurate open access multi-users scientific tool to support the design, the development tuning, and the experimentation of real large-scale sensor network applications. The SensLAB testbed is composed of 1024 nodes over 4 sites. Each site hosts 256 sensor nodes with specific characteristics in order to offer a wide spectrum of possibilities and heterogeneity. Within a given site, each one of the 256 nodes is able both to communicate via its radio interface to its neighbors and to be configured as a sink node to exchange data with any other "sink node". The hardware and software architectures that allow to reserve, configure, deploy firmwares and gather experimental data and monitoring information are described. We also present demonstration examples to illustrate the use of the SensLAB testbed and encourage researchers to test and benchmark their applications/protocols on a large scale WSN testbed. © 2011 IFIP International Federation for Information Processing.},
  affiliation     = {ENS de Lyon, 15 parvis Reńe Descartes, 69342 Lyon Cedex 07, France; INRIA, France; INSA de Lyon, France; Université de Strasbourg, France; Université de Lyon, France},
  author_keywords = {Experiments; Monitoring; Radio; Testbed; Wireless Sensor Network},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-20757-0_12},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79956025455&doi=10.1007%2f978-3-642-20757-0_12&partnerID=40&md5=96715b0d08a1ddac6e25581d3d83bc21},
}

@Article{Bingham2011,
  author          = {Bingham, B.S. and Walls, J.M. and Eustice, R.M.},
  title           = {Development of a flexible command and control software architecture for marine robotic applications},
  journal         = {Marine Technology Society Journal},
  year            = {2011},
  volume          = {45},
  number          = {3},
  pages           = {25-36},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper reports the implementation of a supervisory control framework andmodular software architecture built around the lightweight communication and marshalling (LCM) publish/subscribe message passing system. In particular, we examine two diverse marine robotics applications using this modular system: (i) the development of an unmanned port security vehicle, a robotic surface platform to support first responders reacting to transportation security incidents in harbor environments, and (ii) the adaptation of a commercial off-the-shelf autonomous underwater vehicle (the Ocean-Server Iver2) for visual feature-based navigation. In both cases, the modular vehicle software infrastructures are based around the open-source LCM software library for low-latency, real-time message passing. To elucidate the real-world application of LCM in marine robotic systems, we present the software architecture of these two successful marine robotic applications and illustrate the capabilities and flexibilities of this approach to real-time marine robotics. We present benchmarking test results comparing the throughput of LCM with the Mission-Oriented Operating Suite, another robot software system popular in marine robotics. Experimental results demonstrate the capacity of the LCM framework to make large amounts of actionable information available to the operator and to allow for distributed supervisory control. We also provide a discussion of the qualitative tradeoffs involved in selecting software infrastructure for supervisory control.},
  affiliation     = {Department of Mechanical Engineering, University of Hawaii, Manoa, United States; Department of MechanicalEngineering, University of Michigan, United States; Department of Naval Architectureand Marine Engineering, University of Michigan, United States},
  author_keywords = {Autonomous vehicles; Interprocess control; Maritime domain awareness; Supervisory control},
  document_type   = {Article},
  doi             = {10.4031/MTSJ.45.3.4},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84855806604&doi=10.4031%2fMTSJ.45.3.4&partnerID=40&md5=c3f914e712bb73697c2fc5a5a89235ef},
}

@Conference{Castro2011,
  author          = {Castro, M. and Georgiev, K. and Marangozova-Martin, V. and Méhaut, J.-F. and Fernandes, L.G. and Santana, M.},
  title           = {Analysis and tracing of applications based on software transactional memory on multicore architectures},
  year            = {2011},
  pages           = {199-206},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Transactional Memory (TM) is a new programming paradigm that offers an alternative to traditional lock-based concurrency mechanisms. It offers a higher-level programming interface and promises to greatly simplify the development of correct concurrent applications on multicore architectures. However, simplicity often comes with an important performance deterioration and given the variety of TM implementations it is still a challenge to know what kind of applications can really take advantage of TM. In order to gain some insight on these issues, helping developers to understand and improve the performance of TM applications, we propose a generic approach for collecting and tracing relevant information about transactions. Our solution can be applied to different Software Transactional Memory (STM) libraries and applications as it does not modify neither the target application nor the STM library source codes. We show that the collected information can be helpful in order to comprehend the performance of TM applications. © 2011 IEEE.},
  affiliation     = {MESCAL Research Group, INRIA, Grenoble University, France; GMAP Research Group, PPGCC, PUCRS, Brazil; STMicroelectronics, Crolles, France},
  art_number      = {5738988},
  author_keywords = {benchmark; software transactional memory; tracing mechanism},
  document_type   = {Conference Paper},
  doi             = {10.1109/PDP.2011.27},
  journal         = {Proceedings - 19th International Euromicro Conference on Parallel, Distributed, and Network-Based Processing, PDP 2011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79954993454&doi=10.1109%2fPDP.2011.27&partnerID=40&md5=6c122569bc6cdd9e72788527cf9e01cf},
}

@Conference{2011,
  title         = {ICPE'11 - Proceedings of the 2nd Joint WOSP/SIPEW International Conference on Performance Engineering},
  year          = {2011},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 59 papers. The topics discussed include: proprietary code to non-proprietary benchmarks: synthesis techniques for scalable benchmarks; performance analysis of domain specific visual models; performance modeling in MapReduce environments: challenges and opportunities; computing first passage time distributions in stochastic well-formed nets; detection and solution of software performance antipatterns in Palladio architectural models; an approach for scalability-bottleneck solution: identification and elimination of scalability bottlenecks in a DBMS; experience building non-functional requirement models of a complex industrial architecture; relative roles of instruction count and cycles per instruction in WCET estimation; an automatic trace based performance evaluation model building for parallel distributed systems; and real-world performance modeling of enterprise service oriented architectures: delivering business value with complexity and constraints.},
  document_type = {Conference Review},
  journal       = {ICPE'11 - Proceedings of the 2nd Joint WOSP/SIPEW International Conference on Performance Engineering},
  page_count    = {520},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79953856863&partnerID=40&md5=9d3f2b4af9737b5bfbfcdf184b794517},
}

@Article{Mendes2011,
  author          = {Mendes, J. and Araújo, R. and Sousa, P. and Apóstolo, F. and Alves, L.},
  title           = {An architecture for adaptive fuzzy control in industrial environments},
  journal         = {Computers in Industry},
  year            = {2011},
  volume          = {62},
  number          = {3},
  pages           = {364-373},
  note            = {cited By 17},
  __markedentry   = {[Nichl:6]},
  abstract        = {The paper presents an architecture for adaptive fuzzy control of industrial systems. Both conventional and adaptive fuzzy control can be designed. The control methodology can integrate a priori knowledge about the control and/or about the plant, with on-line control adaptation mechanisms to cope with time-varying and/or uncertain plant parameters. The paper presents the fuzzy control software architecture that can be integrated in industrial processing and communication structures. It includes four distinct modules: a mathematical fuzzy library, a graphical user interface (GUI), fuzzy controller, and industrial communication. Three types of adaptive fuzzy control methods have been studied, and compared: (1) direct adaptive, (2) indirect adaptive, and (3) combined direct/indirect adaptive. An experimental benchmark composed of two mechanically coupled electrical DC motors has been employed to study the performance of the presented control architectures. The first motor acts as an actuator, while the second motor is used to generate nonlinearities and/or time-varying load. Results indicate that all tested controllers have good performance in overcoming changes of DC motor load. © 2010 Elsevier B.V. All rights reserved.},
  affiliation     = {DEEC-Department of Electrical and Computer Engineering, University of Coimbra, Pólo II, PT-3030-290 Coimbra, Portugal; ISR-Institute of Systems and Robotics, DEEC-Department of Electrical and Computer Engineering, University of Coimbra, Pólo II, PT-3030-290 Coimbra, Portugal; AControl - Automation and Industrial Control, Ltd, Parque Empresarial de Eiras, Lote 5, PT-3020-265 Coimbra, Portugal},
  author_keywords = {Adaptive control; DC motor; Fuzzy control; Industrial communication; Lyapunov stability; Mathematical fuzzy library; Software architecture},
  document_type   = {Article},
  doi             = {10.1016/j.compind.2010.11.001},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79952451524&doi=10.1016%2fj.compind.2010.11.001&partnerID=40&md5=0f6e152f8162a73c7c3daedfe0ff49aa},
}

@Article{GarrettJr.2011,
  author          = {Garrett Jr., R.K. and Anderson, S. and Baron, N.T. and Moreland Jr., J.D.},
  title           = {Managing the interstitials, a System of Systems framework suited for the Ballistic Missile Defense System},
  journal         = {Systems Engineering},
  year            = {2011},
  volume          = {14},
  number          = {1},
  pages           = {87-109},
  note            = {cited By 45},
  __markedentry   = {[Nichl:6]},
  abstract        = {Recent engineering experiences with the Missile Defense Agency (MDA) Ballistic Missile Defense System (BMDS) highlight the need to analyze the BMDS System of Systems (SoS) including the numerous potential interactions between independently developed elements of the system. The term " interstitials" is used to define the domain of interfaces, interoperability, and integration between constituent systems in an SoS. The authors feel that this domain, at an SoS level, has received insufficient attention within systems engineering literature. The BMDS represents a challenging SoS case study as many of its initial elements were assembled from existing programs of record. The elements tend to perform as designed but their performance measures may not be consistent with the higher level SoS requirements. One of the BMDS challenges is interoperability, to focus the independent elements to interact in a number of ways, either subtle or overt, for a predictable and sustainable national capability. New capabilities desired by national leadership may involve modifications to kill chains, Command and Control (C2) constructs, improved coordination, and performance. These capabilities must be realized through modifications to programs of record and integration across elements of the system that have their own independent programmatic momentum. A challenge of SoS Engineering is to objectively evaluate competing solutions and assess the technical viability of tradeoff options. This paper will present a multifaceted technical approach for integrating a complex, adaptive SoS to achieve a functional capability. Architectural frameworks will be explored, a mathematical technique utilizing graph theory will be introduced, adjuncts to more traditional modeling and simulation techniques such as agent based modeling will be explored, and, finally, newly developed technical and managerial metrics to describe design maturity will be introduced. A theater BMDS construct will be used as a representative set of elements together with the interstitials representing the integration domain. Increased attention to the interstitial space of the overarching BMDS SoS construct and applying appropriate technical rigor and engineering due diligence with these added tools should greatly assist the BMDS in realizing its potential. © 2010 Wiley Periodicals, Inc.},
  affiliation     = {Naval Surface Warfare Center, Dahlgren Division (NSWCDD), Building 221, 6138 Norc Avenue, Dahlgren, VA 22448-5157, United States; Naval Surface Warfare Center, Dahlgren Division (NSWCDD), Building 1500, 17214 Avenue B, Dahlgren, VA 22448-5157, United States; Naval Surface Warfare Center, Dahlgren Division (NSWCDD), Building 1490, 19008 Wayside Drive, Dahlgren, VA 22448-5157, United States; 6149 Welsh Road, Dahlgren, VA 22448-5130, United States},
  author_keywords = {architecture; Ballistic Missile Defense; complexity; modeling and simulation; System of Systems},
  document_type   = {Article},
  doi             = {10.1002/sys.20173},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79251504256&doi=10.1002%2fsys.20173&partnerID=40&md5=f589b2d1b1a9baeab1125e7050f24fa7},
}

@Article{Distefano2011,
  author          = {Distefano, S. and Scarpa, M. and Puliafito, A.},
  title           = {From UML to Petri nets: The PCM-based methodology},
  journal         = {IEEE Transactions on Software Engineering},
  year            = {2011},
  volume          = {37},
  number          = {1},
  pages           = {65-79},
  note            = {cited By 44},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, we present an evaluation methodology to validate the performance of a UML model, representing a software architecture. The proposed approach is based on open and well-known standards: UML for software modeling and the OMG Profile for Schedulability, Performance, and Time Specification for the performance annotations into UML models. Such specifications are collected in an intermediate model, called the Performance Context Model (PCM). The intermediate model is translated into a performance model which is subsequently evaluated. The paper is focused on the mapping from the PCM to the performance domain. More specifically, we adopt Petri nets as the performance domain, specifying a mapping process based on a compositional approach we have entirely implemented in the ArgoPerformance tool. All of the rules to derive a Petri net from a PCM and the performance measures assessable from the former are carefully detailed. To validate the proposed technique, we provide an in-depth analysis of a web application for music streaming. © 2011 IEEE.},
  affiliation     = {Department of Mathematics, University of Messina, Contrada di Dio, S. Agata, Messina 98166, Sicily, Italy},
  art_number      = {5396344},
  author_keywords = {performances evaluation; Petri nets; Software engineering; software performance engineering; UML},
  document_type   = {Article},
  doi             = {10.1109/TSE.2010.10},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79551515140&doi=10.1109%2fTSE.2010.10&partnerID=40&md5=17418c9e4786e89ea8b67356240314d4},
}

@Article{Klug2011,
  author          = {Klug, T. and Ott, M. and Weidendorfer, J. and Trinitis, C.},
  title           = {Aautopin - Automated optimization of thread-to-core pinning on multicore systems},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2011},
  volume          = {6590},
  pages           = {219-235},
  note            = {cited By 20},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper we present a framework for automatic detection and application of the best binding between threads of a running parallel application and processor cores in a shared memory system, by making use of hardware performance counters. This is especially important within the scope of multicore architectures with shared cache levels. We demonstrate that many applications from the SPEC OMP benchmark show quite sensitive runtime behavior depending on the thread/core binding used. In our tests, the proposed framework is able to find the best binding in nearly all cases. The proposed framework is intended to supplement job scheduling systems for better automatic exploitation of systems with multicore processors, as well as making programmers aware of this issue by providing measurement logs. © 2011 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {Lehrstuhl für Rechnertechnik und Rechnerorganisation/ Parallelrechnerarchitektur (LRR/TUM), Technische Universität München, Boltzmannstrasse 3, Garching, München 85748, Germany},
  author_keywords = {automatic performance optimization; CMP; CPU binding; hardware performance counters; Multicore; thread placement},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-19448-1_12},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79955079921&doi=10.1007%2f978-3-642-19448-1_12&partnerID=40&md5=03e707352578fb7ef37c0a13c723f2e0},
}

@Article{Liu2011,
  author          = {Liu, Z.-Q. and Song, J.-Q. and Lu, F.-S. and Zhao, J.},
  title           = {A study of thread-based MPI communication accelerator},
  journal         = {Jisuanji Xuebao/Chinese Journal of Computers},
  year            = {2011},
  volume          = {34},
  number          = {1},
  pages           = {154-164},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Towards building a more effective MPI infrastructure for multicore systems, a thread-based MPI program accelerator, called MPIActor, is proposed in this paper. MPIActor is a transparent middleware to assist general MPI libraries. For any single-thread MPI program, the MPIActor is optional in compiling phase. With the join of MPIActor, the MPI processes in each node will be mapped as several threads of one process, and the intra-node communication will be enhanced by taking advantage of the light-weight thread-based mechanism. The authors have designed and implemented the point-to-point communication module. This paper details the mechanism, the communication architecture and key techniques, and evaluates it with OSU LATENCY benchmark on a real platform. The experimental results show that the introduction of MPIActor can achieve a 2X performance for transferring 8 KB and 4 MB messages on MVPICH2 and OpenMPI parallel environments.},
  affiliation     = {College of Computer, National University of Defense Technology, Changsha 410073, China},
  author_keywords = {MPI accelerator; MPI software architecture; MPIActor; Threaded MPI},
  document_type   = {Article},
  doi             = {10.3724/SP.J.1016.2011.00154},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79953070891&doi=10.3724%2fSP.J.1016.2011.00154&partnerID=40&md5=188db631ff329ece14c5ff0c762338bb},
}

@Article{Wang2011a,
  author          = {Wang, S. and Chen, H. and Shi, W.},
  title           = {SPAN: A software power analyzer for multicore computer systems},
  journal         = {Sustainable Computing: Informatics and Systems},
  year            = {2011},
  volume          = {1},
  number          = {1},
  pages           = {23-34},
  note            = {cited By 41},
  __markedentry   = {[Nichl:6]},
  abstract        = {Understanding the power dissipation behavior of an application/workload is the key to writing power-efficient software and designing energy-efficient computer systems. Power modeling based on performance monitoring counters (PMCs) is an effective approach to analyze and quantify power dissipation behaviors on a real computer system. One of the potential benefits is that software developers are able to optimize the power behavior of an application by adjusting its source code implementations. However, it is challenging to relate power dissipation to the execution of specific segments of source code directly. In addition, existing power models need to be further investigated by reconsidering multicore architecture processors with on-chip shared resources. Therefore, we need to adjust PMC-based power models from the developers perspective, and reevaluate them on multicore computer systems. In this paper, followed by a detailed classification of previous efforts on power profiling, we propose a two-level power model that estimates per-core power dissipation on chip multiprocessor (CMP) on-thefly by using only one PMC and frequency information from CPUs. The model attempts to satisfy the basic requirements from developer point of view: simplicity and applicability. Based on this model, we design and implement SPAN, a software power analyzer, to identify power behavior associated with source code. Given an application, SPAN is able to determine its power dissipation rate at the function-block level. We evaluate both the power model and SPAN on two general purpose multicore computer systems. The experimental results based on SPEC2008Cjvm benchmark suite show the average error rate of 5.40% across one core to six core validation. We also verify SPAN using the FT benchmark from NAS parallel benchmark suite and a synthetic workload. The overall estimated error of SPAN is under 3.00%. © 2010 Elsevier Inc. All rights reserved.},
  affiliation     = {Department of Computer Science, Wayne State University, Detroit, MI, United States},
  author_keywords = {Multicore; Power model; Software power profiling},
  document_type   = {Article},
  doi             = {10.1016/j.suscom.2010.10.002},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79953882618&doi=10.1016%2fj.suscom.2010.10.002&partnerID=40&md5=1c0434a36d1353b3aa843d40643a4abc},
}

@Article{Schildt2011,
  author          = {Schildt, S. and Morgenroth, J. and Pöttner, W.-B. and Wolf, L.},
  title           = {IBR-DTN: A lightweight, modular and highly portable Bundle Protocol implementation},
  journal         = {Electronic Communications of the EASST},
  year            = {2011},
  volume          = {37},
  note            = {cited By 70},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper we present IBR-DTN, a lightweight, modular and portable Bundle Protocol implementation and DTN daemon. IBR-DTN is especially suited for embedded platforms, which allows to leverage the benefits of a fully compliant Bundle Protocol daemon in cost sensitive distributed sensing applications. We line out IBR-DTN's extensible software architecture and introduce the modules included in the standard IBR-DTN distribution. To give an impression of the performance that can be expected when using IBR-DTN, we perform a series of benchmarks and compare the outcome with DTN2 performance, the reference implementation of the Bundle Protocol. © Kommunikation in Verteilten Systemen 2011.},
  affiliation     = {Institute of Operating Systems and Computer Networks Technische, Universität Braunschweig, Braunschweig, Germany},
  author_keywords = {Bundle Protocol; DTN; Embedded Linux},
  document_type   = {Article},
  doi             = {10.14279/tuj.eceasst.37.512.544},
  page_count      = {10},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80054035758&doi=10.14279%2ftuj.eceasst.37.512.544&partnerID=40&md5=8d9e3a1213d2bfddd808e9ca3593da47},
}

@Article{Lize2011,
  author          = {Lizé, B. and Sylvand, G.},
  title           = {Design, analysis, implementation and deployment of a high-performance, out-of-core, parallel, dense direct linear solver},
  journal         = {Civil-Comp Proceedings},
  year            = {2011},
  volume          = {95},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {We present the design, implementation and deployment of SPIDO2, a massively parallel out-Of-Core direct dense linear solver used by Boundary Element Method (BEM) codes developed at EADS InnovationWorks, and currently in production in all EADS business units. It is designed as a high performance drop-in replacement for a legacy solver, uniformly achieves very good scalability up to several hundreds cores (80% of peak performance), is portable and takes into account the new context of HPC, leveraging OpenMP and MPI for intra/inter-node parallelism. We provide here a detailed analysis of the algorithms and data structures used to achieve a very efficient out-of-core parallel decomposition (LU and LDLt), including run-time complexity validating the design choices. We then describe the implementation, benchmarks and validation methodology that led to the deployment of this solver in the BEM codes, and we finally provide insights on how we expect this design to be well-suited to the current and future advances in HPC technology. © Civil-Comp Press, 2011.},
  affiliation     = {EADS Innovation Works, Applied Mathematics, Suresnes, France; EADS Innovation Works, Applied Mathematics, Toulouse, France},
  author_keywords = {Boundary elements; Direct solver; High performance computing; Integral equations; Out-of-core; Parallel solver; Scalability; Scientific software architecture},
  document_type   = {Conference Paper},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84894148856&partnerID=40&md5=507655ac79d1d45fc57f0bf58627bfdd},
}

@Conference{Tino2010,
  author          = {Tino, A. and Khan, G.N.},
  title           = {Power and performance Tabu search based multicore network-on-chip design},
  year            = {2010},
  pages           = {74-81},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents a Tabu search based approach for the topology synthesis of application-specific multicore architectures using an automated design technique. The Tabu search method incorporates multiple objectives in order to generate an optimal NoC topology which accounts for both power and performance factors. The method generates a system-level floorplan in each major stage of the topology synthesis. By incorporating the floorplan information, it is possible to attain accurate values for power consumption of the routers and physical links, as well as manage the interconnections within the system. The technique also includes a contention analyzer that assesses performance and omits any potential bottlenecks. The contention analyzer uses a Layered Queuing Network approach to model the rendezvous interactions amongst system components. Several experiments are conducted using various SoC benchmark applications to compare the power and performance outcomes of the proposed technique. © 2010 IEEE.},
  affiliation     = {Electrical and Computer Engineering, Ryerson University, 350 Victoria St., Toronto, ON M5B 2K3, Canada},
  art_number      = {5599216},
  author_keywords = {Design automation; Multicore topology generation; Network-on-Chip; Tabu search optimization},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICPPW.2010.22},
  journal         = {Proceedings of the International Conference on Parallel Processing Workshops},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78649857165&doi=10.1109%2fICPPW.2010.22&partnerID=40&md5=5b763908cdf516ee22de60bdcb6e01fc},
}

@Article{Caputo2010,
  author          = {Caputo, E. and Corallo, A. and Damiani, E. and Passiante, G.},
  title           = {KPI modeling in MDA perspective},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6428 LNCS},
  pages           = {384-393},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {Enhancing competitiveness, shortening the response time to environmental changes, increasing profits and so forth are all goals that refer to the same concept: 'improvement'. Yet, these elements are bounded to the same necessity: 'measurement'. On such bases, this work intends to provide an operative framework which, using many heterogeneous typologies of tools and technologies, would enable enterprises to define, formalize and model key performance indicators (KPIs) according to Model Driven Architecture (MDA) vision. The tools required for achieving this goal belong to different categories, according to the particular step of the framework: the theories for identification of KPIs are the balanced scorecard (BSc) and the goal question metric (GQM); process modeling is realized trough BPMN (Business Process Modeling Notation); KPIs were modeled using semantics of business vocabulary and business rules (SBVR), so as to enable automatic parsing, according to MDA vision. Finally, the mathematical formulas were represented in machine readable format through MathML. © 2010 Springer-Verlag Berlin Heidelberg.},
  affiliation     = {CCII, ISUFI, University of Salento, Lecce, Italy; Dept. of Computer Technology, University of Milan, Italy},
  author_keywords = {KPI; MDA; Measure; Metric; Probe},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-16961-8_59},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78649514331&doi=10.1007%2f978-3-642-16961-8_59&partnerID=40&md5=a3d6b8b5e6a44a8d7c422b935b934144},
}

@Article{Klems2010,
  author          = {Klems, M. and Menzel, M. and Fischer, R.},
  title           = {Consistency benchmarking: Evaluating the consistency behavior of middleware services in the cloud},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6470 LNCS},
  pages           = {627-634},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {Cloud service providers such as Amazon Web Services offer a set of next-generation storage and messaging middleware services that can be utilized on-demand over the Internet. Outsourcing software into the cloud, however, confronts application developers with the challenge of understanding the behavior of distributed systems, which are out of their control. This work proposes an approach to benchmark the consistency behavior of services by example of Amazon Simple Queue Service (SQS), a hosted, Web-scale, distributed message queue that is exposed as a Web service. The data of our consistency benchmarking tests are evaluated with the metric harvest as described by Fox and Brewer (1999). Our tests with SQS indicate that the client-service interaction intensity has an influence on harvest. © 2010 Springer-Verlag.},
  affiliation     = {Karlsruhe Institute of Technology (KIT), Karlsruhe 76131, Germany; FZI Forschungszentrum Informatik, Haid-und-Neu-Straße 10-14, 76131 Karlsruhe, Germany},
  author_keywords = {cloud computing; distributed systems; service-oriented computing},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-17358-5_48},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78650775270&doi=10.1007%2f978-3-642-17358-5_48&partnerID=40&md5=173cddbb8de2d35ecb2aefa24939dcbb},
}

@Conference{Pannequin2010,
  author          = {Pannequin, R. and Thomas, A.},
  title           = {Emulica: An emulation-based benchmarking framework for production control experiments},
  year            = {2010},
  volume          = {10},
  number          = {PART 1},
  pages           = {320-325},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Evaluating a proposed control architecture nearly always requires experiments and applications. This paper propose an emulation-based benchmarking framework to design and run such experiments. The proposition is based on an architecture integrating the control system with the virtual controlled system (emulation model), and on generic constructs to build this emulation model. The main feature of this architecture is presented in this paper: the constructs used to build emulation model and their generic interface with the control system. Then, Emulica, an open-source implementation of the benchmarking environment is presented. Copyright © 2010 IFAC.},
  affiliation     = {Research Centre for Automatic Control (CRAN), CNRS (UMR 7029), Nancy University, 27, Rue Du Merle Blanc, F-88000 Épinal, France},
  author_keywords = {Emulation of complex manufacturing systems; Simulation-based performance evaluation; Software architecture},
  document_type   = {Conference Paper},
  journal         = {IFAC Proceedings Volumes (IFAC-PapersOnline)},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80051972315&partnerID=40&md5=4022958334f3b3de9d94b011a03b54b0},
}

@Book{Fu2010,
  title         = {Architecture-centered integrated verification},
  year          = {2010},
  author        = {Fu, Y. and Dong, Z. and He, X.},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {This chapter presents an architecture-centered verification approach to large scale complex software systems by integrating model checking with runtime verification. A software architecture design provides a high-level abstraction of system topology, functionality, and/or behavior, which provides a basis for system understanding and analysis as well as a foundation for subsequent detailed design and implementation. Therefore, software architecture plays a critical role in the software development process. Reasoning and analysis of software architecture model can detect errors in an early stage, further reduce the errors in the final product and highly improve the software quality. First identified are the two main streams of software architecture research groups-the groups that work on the architectural abstraction and semantic foundation, and the group works on the framework using object oriented concepts. Problematically, both architecture designs cannot generate correct products due to two reasons. On one hand, not all properties can be verified at design level because of the state space explosion problem, verification costs, and characteristics of open-system. On the other hand, a correct and valid software architecture design does not ensure a correct implementation due to the error-prone characteristics of the software development process. The approach aims at solving the above problems by including the analysis and verification of two different levels of software development process-design level and implementation level-and bridging the gap between software architecture analysis and verification and the software product. In the architecture design level, to make sure the design correctness and attack the large scale of complex systems, the compositional verification is used by dividing and verifying each component individually and synthesizing them based on the driving theory. Then for those properties that cannot be verified on the design level, the design model is translated to implementation and runtime verification technique is adapted to the program. This approach can highly reduce the work on the design verification and avoid the state-explosion problem using model checking. Moreover, this approach can ensure both design and implementation correctness, and can further provide a high confident final software product. This approach is based on Software Architecture Model (SAM) that was proposed by Florida International University in 1999. SAM is a formal specification and built on the pair of component-connector with two formalisms - Petri nets and temporal logic. The ACV approach places strong demands on an organization to articulate those quality attributes of primary importance. It also requires a selection of benchmark combination points with which to verify integrated properties. The purpose of the ACV is not to commend particular architectures, but to provide a method for verification and analysis of large scale software systems in architecture level. The future research works fall in two directions. In the compositional verification of SAM model, it is possible that there is circular waiting of certain data among different component and connectors. This problem was not discussed in the current work. The translation of SAM to implementation is based on the restricted Petri nets due to the undecidable issue of high level Petri nets. In the runtime analysis of implementation, extraction of the execution trace of the program is still needed to get a white box view, and further analysis of execution can provide more information of the product correctness. © 2011, IGI Global.},
  affiliation   = {Department of Computer Science, Alabama A and M University, United States; Middle Tennessee State University, United States; Florida International University, United States},
  document_type = {Book Chapter},
  doi           = {10.4018/978-1-60960-215-4.ch005},
  journal       = {Modern Software Engineering Concepts and Practices: Advanced Approaches},
  pages         = {104-124},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84898585350&doi=10.4018%2f978-1-60960-215-4.ch005&partnerID=40&md5=b34834fa47806f7c124e5a1caaa3bbc0},
}

@Conference{Saxena2010,
  author        = {Saxena, V. and Sabharwal, Y. and Bhatotia, P.},
  title         = {Performance evaluation and optimization of random memory access on multicores with high productivity},
  year          = {2010},
  note          = {cited By 8},
  __markedentry = {[Nichl:6]},
  abstract      = {The slow progress in memory access latencies in comparison to CPU speeds has resulted in memory accesses dominating code performance. While architectural enhancements have benefited applications with data locality and sequential access, random memory access still remains a cause for concern. Several benchmarks have been proposed to evaluate the random memory access performance on multicore architectures. However, the performance evaluation models used by the existing benchmarks do not fully capture the varying types of random access behaviour arising in practical applications. In this paper, we propose a new model for evaluating the performance of random memory access that better captures the random access behaviour demonstrated by applications in practice. We use our model to evaluate the performance of two popular multicore architectures, the Cell and the GPU. We also suggest novel optimizations on these architectures that significantly boost the performance for random accesses in comparison to conventional architectures. Performance improvements on these architectures typically come at the cost of reduced productivity considering the extra programming effort involved. To address this problem, we propose libraries that incorporate these optimizations and provide innovatively designed programming interfaces that can be used by the applications to achieve good performance without loss of productivity. ©2010 IEEE.},
  affiliation   = {IBM Research - India, New Delhi, India; Max Planck Institute for Software Systems, Germany},
  art_number    = {5713168},
  document_type = {Conference Paper},
  doi           = {10.1109/HIPC.2010.5713168},
  journal       = {17th International Conference on High Performance Computing, HiPC 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79952800435&doi=10.1109%2fHIPC.2010.5713168&partnerID=40&md5=63d2b3f943021341b8c90fcc25b0348b},
}

@Conference{Rao2010,
  author        = {Rao, D.S. and Schwan, K.},
  title         = {vNUMA-mgr: Managing VM memory on NUMA platforms},
  year          = {2010},
  note          = {cited By 14},
  __markedentry = {[Nichl:6]},
  abstract      = {Continuing improvements in the scale of many-core platforms are accompanied by increased asymmetry in their memory architectures. Such NUMA architectures, however, require systems software that understands this asymmetry to attain high levels of performance, leading to significant work in optimizing operating systems like Linux and Windows to increase locality of access to memory nodes and to consider differences in access latencies when accessing remote nodes. When running on today's virtualized hardware platforms, however, virtual machines (VMs) remain unaware of underlying memory architecture, leading to non-compliance with their operating system's (OS) policies for managing NUMA memory, and thereby, incurring undesirable performance overheads. This paper describes the vNUMA-mgr approach of dealing with the NUMA memory characteristics of future virtualized multicore architectures, which (1) properly manages VM memory at the hypervisor (VMM) level through a set of VMM-level strategies, coupled with (2) 'enlightening' guest VM OSes, if possible, to aid their memory management, and also (3) provides mechanisms to maintain the distribution of VM memory (across physical nodes), even when memory is over-provisioned. The vNUMA-mgr implementation in the Xen VMM is evaluated with respect to each of its memory allocation strategies. The evaluation, which uses a set of memory-intensive benchmarks representative of many High-Performance Computing (HPC) applications, shows 30-50% performance improvement on our platform. ©2010 IEEE.},
  affiliation   = {Georgia Institute of Technology, United States},
  art_number    = {5713191},
  document_type = {Conference Paper},
  doi           = {10.1109/HIPC.2010.5713191},
  journal       = {17th International Conference on High Performance Computing, HiPC 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79952803247&doi=10.1109%2fHIPC.2010.5713191&partnerID=40&md5=4cdb39321e1658c58efeced24b533610},
}

@Conference{Kaster2010,
  author        = {Kaster, F.O. and Kassemeyer, S. and Merkel, B. and Nix, O. and Hamprecht, F.A.},
  title         = {An object-oriented library for systematic training and comparison of classifiers for computer-assisted tumor diagnosis from MRSI measurements},
  year          = {2010},
  volume        = {574},
  pages         = {97-101},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {We present an object-oriented library for the systematic training, testing and benchmarking of classification algorithms for computer-assisted diagnosis tasks, with a focus on tumor probability estimation from magnetic resonance spectroscopy imaging (MRSI) measurements. In connection with a graphical user interface for data annotation, it allows clinical end users to flexibly adapt these classifiers towards changed classification tasks and to perform quality control of the results. This poses an advantage over previous classification software solutions, which had to be manually adapted by pattern recognition experts for every change in the data acquisition protocols. In this article, we concentrate on software architecture and design principles of this library.},
  affiliation   = {Ruprecht-Karls-Universität, Heidelberg, Germany; Deutsches Krebsforschungszentrum, Heidelberg, Germany; Fraunhofer MEVIS, Institut für Bildgestützte Medizin, Bremen, Germany},
  document_type = {Conference Paper},
  journal       = {CEUR Workshop Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84893784896&partnerID=40&md5=374e5021089068569da720ebcdccb0e3},
}

@Conference{Khuram2010,
  author          = {Khuram, N.S. and Symeonidis, A.L. and Majeed, A.},
  title           = {Wage - A Web services and agent-based generic auctioning environment},
  year            = {2010},
  pages           = {129-133},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {During the last few years online auctioning ranks among the top electronic commerce mechanisms in terms of revenue and is enjoying impressive resonance in a variety of goods and services globally. To this end, we focus our efforts in designing a generic mechanism for online auctioning, rather than the ad-hoc approaches that are widely in use to serve one (or a few) specific problems. The proposed generic infrastructure is envisioned to serve as an API or a tool for building different types of auctions, with different parameters and for different types of entities. Within the context of this work we study the existing auctioning mechanisms and, consequently, decide on a software architecture that would allow for the development of different auctioning scenarios upon developer demand. In order to ensure the versatility and expandability of the system, Web services are employed as the core architectural primitive. Based on their autonomous and communicative nature, Software Agents are employed to support online auctions, used as actors impersonating/ representing users in the system. This way, researchers only have to focus on developing efficient auctioning strategies, without troubling themselves on the environment their agents will operate in. After adopting the appropriate (already developed) interface, they can benchmark their agents' performance in any environment and under any constraints they want. © 2010 IADIS.},
  affiliation     = {Military College of Signals (NUST), Pakistan; Aristotle University of Thessaloniki, Thessaloniki, Greece},
  author_keywords = {Agent-based auctioning; Generic auctioning; Internet auctions; Interoperability; Web service architecture},
  document_type   = {Conference Paper},
  journal         = {Proc. of the IADIS Int. Conf. Intelligent Systems and Agents 2010, Proc. of the IADIS European Conference on Data Mining 2010, Part of the MCCSIS 2010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79955430143&partnerID=40&md5=b26224e11051cd7334a65f5f650166c8},
}

@Conference{HugoRodriguez2010,
  author          = {Hugo Rodriguez, M.J. and Esmeralda Lopez, A. and Magdalena Morales, G. and Ana Soriano, P.},
  title           = {Information system for the performance assessment of cooling equipment in power plants},
  year            = {2010},
  volume          = {2},
  pages           = {211-216},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, the main features, design, software architecture, and utilization of an offline performance assessment program for power plant cooling systems are illustrated. The program allows to calculate quickly and accurately the equipment performance indicators and diagnose the causes of inefficiencies presented in cooling tower, condenser, and cooling pumps. The program also presents to the users alternatives to solve the detected problems and to improve the system performance. The algorithms for calculating the key performance indicators are based on the documented standard methodology of the Heat Exchange Institute, Cooling Technology Institute, and the America Society of Mechanical Engineers. The assessment of the cooling system is carried out through the comparison between the current indicators and the expected ones, while the diagnosis is made by means of an expert system that uses the methodology of case-based reasoning.},
  affiliation     = {Thermal Processes Department, Electrical Research Institute, Reforma 113 Col. Palmira, Cuernavaca, Morelos, 62490, Mexico},
  author_keywords = {Case-Based Reasoning; Cooling System; Expert System; Performance Assessment},
  document_type   = {Conference Paper},
  journal         = {WMSCI 2010 - The 14th World Multi-Conference on Systemics, Cybernetics and Informatics, Proceedings},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84870184547&partnerID=40&md5=d00cc0e41cd876a9bb6206c281101b28},
}

@Conference{Saad2010,
  author        = {Saad, R.T. and Dal Zilio, S. and Berthomieu, B.},
  title         = {A general lock-free algorithm for parallel state space construction},
  year          = {2010},
  pages         = {8-16},
  note          = {cited By 7},
  __markedentry = {[Nichl:6]},
  abstract      = {Verification via model-checking is a very demanding activity in terms of computational resources. While there are still gains to be expected from algorithmic improvements, it is necessary to take advantage of the advances in computer hardware to tackle bigger models. Recent improvements in this area take the form of multiprocessor and multicore architectures with access to large memory space. We address the problem of generating the state space of finitestate transition systems; often a preliminary step for modelchecking. We propose a novel algorithm for enumerative state space construction targeted at shared memory systems. Our approach relies on the use of two data structures: a shared Bloom filter to coordinate the state space exploration distributed among several processors and local dictionaries to store the states. The goal is to limit synchronization overheads and to increase the locality of memory access without having to make constant use of locks to ensure data integrity. Bloom filters have already been applied for the probabilistic verification of systems; they are compact data structures used to encode sets, but in a way that false positives are possible, while false negatives are not. We circumvent this limitation and propose an original multiphase algorithm to perform exhaustive, deterministic, state space generations. We assess the performance of our algorithm on different benchmarks and compare our results with the solution proposed by Inggs and Barringer. © 2010 IEEE.},
  art_number    = {5698464},
  document_type = {Conference Paper},
  doi           = {10.1109/PDMC-HiBi.2010.10},
  journal       = {Proceedings of the 9th Int. Workshop on Parallel and Distributed Methods in Verification, PDMC 2010 - Joint with the 2nd Int. Workshop on High Performance Computational Systems Biology, HiBi 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79951777135&doi=10.1109%2fPDMC-HiBi.2010.10&partnerID=40&md5=43f245c355490409b0c1e1817757100e},
}

@Conference{Das2010,
  author        = {Das, A. and Rodrigues, R. and Koren, I. and Kundu, S.},
  title         = {A study on performance benefits of core morphing in an asymmetric multicore processor},
  year          = {2010},
  pages         = {17-22},
  note          = {cited By 5},
  __markedentry = {[Nichl:6]},
  abstract      = {Multicore architectures are designed so as to provide an acceptable level of performance per unit power for the majority of applications. Consequently, we must occasionally expect applications that could have benefited from a more powerful core in terms of either lower execution time and/or lower energy consumed. Fusing some of the resources of two (or more) cores to configure a more powerful core for such instances is a natural approach to deal with those few applications that have very high performance demands. However, a recent study has shown that fusing homogeneous cores is unlikely to benefit applications. In this paper we study the potential performance benefits of core morphing in a heterogeneous multicore processor that can be reconfigured at runtime. We consider as an example a dual core processor with one of the two cores being designed to target integer intensive applications while the other is better suited to floating-point intensive applications. These two cores can be fused into a single powerful core when an application that can benefit from such fusion is executing. We first discuss the design principles of the two individual cores so that the majority of the benchmarks that we consider execute in a satisfactory way. We then show that a small subset of the considered applications can greatly benefit from core morphing even in the case where two applications that could have been executed in parallel on the two cores are run, for some percentage of time, on the single morphed core. Our results indicate that a performance gain of up to 100% is achievable at a small hardware overhead of less than 1%. © 2010 IEEE.},
  affiliation   = {Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, United States},
  art_number    = {5647566},
  document_type = {Conference Paper},
  doi           = {10.1109/ICCD.2010.5647566},
  journal       = {Proceedings - IEEE International Conference on Computer Design: VLSI in Computers and Processors},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78650727106&doi=10.1109%2fICCD.2010.5647566&partnerID=40&md5=744f0550a3f9589b7ac61046d3b4b891},
}

@Article{Hacker2010,
  author        = {Hacker, H. and Trinitis, C. and Weidendorfer, J. and Brehm, M.},
  title         = {Considering GPGPU for HPC centers: Is it worth the effort?},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2010},
  volume        = {6310 LNCS},
  pages         = {118-130},
  note          = {cited By 9},
  __markedentry = {[Nichl:6]},
  abstract      = {In contrast to just a few years ago, the answer to the question "What system should we buy next to best assist our users" has become a lot more complicated for the operators of an HPC center today. In addition to multicore architectures, powerful accelerator systems have emerged, and the future looks heterogeneous. In this paper, we will concentrate on and apply the abovementioned question to a specific accelerator with its programming environment that has become increasingly popular: systems using graphics processors from NVidia, programmed with CUDA. Using three benchmarks encompassing main computational needs of scientific codes, we compare performance results with those obtained by systems with modern x86 multicore processors. Taking the experience from optimizing and running the codes into account, we discuss whether the presented performance numbers really apply to computing center users running codes in their everyday tasks. © 2010 Springer-Verlag.},
  affiliation   = {Department of Informatics, Technische Universität München, Germany; Leibniz Rechenzentrum, Garching bei München, Germany},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-16233-6_13},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78449290236&doi=10.1007%2f978-3-642-16233-6_13&partnerID=40&md5=59f4162a0ac1a60c7bdb709f7bf92ff2},
}

@Conference{Li2010,
  author          = {Li, Y. and Chen, H. and Zhu, M. and Chung, J.-Y.},
  title           = {Evaluating a service-oriented travel portal},
  year            = {2010},
  pages           = {229-235},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Previously, a service-oriented travel portal was proposed to providing self-service packages for tourists while providing a SAAS platform for trans-industry service vendors. The portal's business model and technical architecture have been addressed, and several travel services have been provided through dynamic composite service packages among flights, taxis, hotels, guides, parks and buses. To exend the portal to provide a better user-experience and a business success, testing management is becoming critial area to focus. Many of the existing testing methods, techniques, and tools cannot directly work with SOA. The unavailability of service code may affect the unit testing techniques. This article is to provide a testing framework based on an ongoing project for a travel portal. There are 6 components in the initially proposed framework, i.e., testing concerns, evaluation indicators, testing cases, testing tools, testing processes, and evaluation and reporting algorithms and tools. The framework is featured with service-related evaluation concerns and testing methods raised by the portal's nature of service-orientation. This paper will present basic technical and performance indicators and discuss the testing processes oriented to service development and operation. The proposed evaluation and testing methods for service-oriented systems could be a better practice and valuable for the other service-oriented applications to reference before they are put into market. © 2010 IEEE.},
  affiliation     = {Software School, Fudan University, Shanghai, China; IBM T. J. Watson Research Center, Yorktown Heights, NY, United States},
  art_number      = {5569900},
  author_keywords = {Service-oriented architecture; Software as a service (SAAS); Testing framework; Travel alliance; Travel portal},
  document_type   = {Conference Paper},
  doi             = {10.1109/SOSE.2010.45},
  journal         = {Proceedings - 5th IEEE International Symposium on Service-Oriented System Engineering, SOSE 2010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78449249729&doi=10.1109%2fSOSE.2010.45&partnerID=40&md5=4b725b301fdad264d25de196d45b5ab7},
}

@Article{Qasem2010,
  author          = {Qasem, A. and Guo, J. and Rahman, F. and Yi, Q.},
  title           = {Exposing tunable parameters in multi-threaded numerical code},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2010},
  volume          = {6289 LNCS},
  pages           = {46-60},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Achieving high performance on today's architectures requires careful orchestration of many optimization parameters. In particular, the presence of shared-caches on multicore architectures makes it necessary to consider, in concert, issues related to both parallelism and data locality. This paper presents a systematic and extensive exploration of thecombined search space of transformation parameters that affect both parallelism and data locality in multi-threaded numerical applications.We characterize the nature of the complex interaction between blocking, problem decomposition and selection of loops for parallelism. We identify key parameters for tuning and provide an automatic mechanism for exposing these parameters to a search tool. A series of experiments on two scientific benchmarks illustrates the non-orthogonality of the transformation search space and reiterates the need for integrated transformation heuristics for achieving high-performance on current multicore architectures. © 2010 Springer-Verlag.},
  affiliation     = {Texas State University, United States; University of Texas, San Antonio, TX, United States},
  author_keywords = {Autotuning; memory hierarchy; optimization; parallelism},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-15672-4_6},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78149243409&doi=10.1007%2f978-3-642-15672-4_6&partnerID=40&md5=9243334a18ee5dd31f15ca1f6765804d},
}

@Conference{Gustafsson2010,
  author        = {Gustafsson, O. and Amiri, K. and Andersson, D. and Blad, A. and Bonnet, C. and Cavallaro, J.R. and Declerck, J. and Dejonghe, A. and Eliardsson, P. and Glasse, M. and Hayar, A. and Hollevoet, L. and Hunter, C. and Joshi, M. and Kaltenberger, F. and Knopp, R. and Le, K. and Miljanic, Z. and Murphy, P. and Naessens, F. and Nikaein, N. and Nussbaum, D. and Pacalet, R. and Raghavan, P. and Sabharwal, A. and Sarode, O. and Spasojevic, P. and Sun, Y. and Tullberg, H.M. and Vander Aa, T. and Van Der Perre, L. and Wetterwald, M. and Wu, M.},
  title         = {Architectures for cognitive radio testbeds and demonstrators - An overview},
  year          = {2010},
  note          = {cited By 15},
  __markedentry = {[Nichl:6]},
  abstract      = {Wireless communication standards are developed at an ever-increasing rate of pace, and significant amounts of effort is put into research for new communication methods and concepts. On the physical layer, such topics include MIMO, cooperative communication, and error control coding, whereas research on the medium access layer includes link control, network topology, and cognitive radio. At the same time, implementations are moving from traditional fixed hardware architectures towards software, allowing more efficient development. Today, field-programmable gate arrays (FPGAs) and regular desktop computers are fast enough to handle complete baseband processing chains, and there are several platforms, both open-source and commercial, providing such solutions. The aims of this paper is to give an overview of five of the available platforms and their characteristics, and compare the features and performance measures of the different systems.},
  affiliation   = {Department of Electrical Engineering, Linköping University, Sweden; Center for Multimedia Communication, Rice University, United States; Swedish Defence Research Agency (FOI), Linköping, Sweden; Mobile Communications Department, Eurecom, Sophia Antipolis, France; IMEC, Belgium; WINLAB (Wireless Information Network Laboratory), Rutgers University, United States; System on Chip Laboratory, Telecom ParisTech, Sophia Antipolis, France},
  art_number    = {5577684},
  document_type = {Conference Paper},
  doi           = {10.4108/ICST.CROWNCOM2010.9290},
  journal       = {2010 Proceedings of the 5th International Conference on Cognitive Radio Oriented Wireless Networks and Communications, CROWNCom 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77958189749&doi=10.4108%2fICST.CROWNCOM2010.9290&partnerID=40&md5=9887d0ecc787f76f094ca02ca6ef0a11},
}

@Conference{Bourgade2010,
  author        = {Bourgade, R. and Rochange, C. and De Michiel, M. and Sainrat, P.},
  title         = {MBBA: A multi-bandwidth bus arbiter for hard real-time},
  year          = {2010},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {Multi-core architectures are being increasingly used in embedded systems as they offer several advantages: improved hardware integration, low thermal dissipation and reduced energy consumption, while they make it possible to improve the computing power. In order to run real-time software on a multicore architecture, computing the Worst-Case Execution Time of every thread should be achievable. This notably involves bounding memory latencies by employing a predictable bus arbiter.However, state-of-the-art techniques prove to be irrelevant to schedule unbalanced workloads in which some threads require more bus bandwidth than the other ones. This paper proposes a new bus arbitration scheme that ensures that the shared bus latencies can be upper bounded. Compared to other schemes that make the bus latencies predictable, like the Round-Robin protocol, our approach defines several levels of bandwidth to meet requirements that may vary from one thread to another. Experimental results (WCET estimates) show that the worstcase bus latency is noticeably shortened, compared to Round- Robin, for the cores with highest priority that get the largest bandwidth. The relevance of the scheme is shown through an example workload composed of various benchmarks. © 2010 IEEE.},
  affiliation   = {Institut de Recherche en Informatique de Toulouse, CNRS - University of Toulouse, HiPEAC European Network of Excellence, France},
  art_number    = {5575754},
  document_type = {Conference Paper},
  doi           = {10.1109/EMC.2010.5575754},
  journal       = {2010 5th International Conference on Embedded and Multimedia Computing, EMC-10 - Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77958111360&doi=10.1109%2fEMC.2010.5575754&partnerID=40&md5=5522e26de64dbe6d32d2411c8e37e357},
}

@Conference{Schoeberl2010,
  author        = {Schoeberl, M. and Preußer, T.B. and Uhrig, S.},
  title         = {The embedded Java benchmark suite jembench},
  year          = {2010},
  pages         = {120-127},
  note          = {cited By 26},
  __markedentry = {[Nichl:6]},
  abstract      = {Requirements to embedded systems increase steadily. In parallel, also the performance of the processors used in these systems is improved leading to multithreaded and/or multicore architectures. Depending on the type of the embedded system, using Java is a more and more popular way for software development. In this paper, we present a Java benchmark suite that enables the comparison of different embedded Java platforms while solely assuming the availability of a CLDC API, the minimal configuration defined for the J2ME. The core of the benchmark suite consists of adapted realworld applications. Furthermore, the suite contains benchmarks to explore multi-core/multi-threaded systems. Hence, it is possible to determine the gain of a parallel execution platform compared to sequential execution. Additionally, the penalty of a sequential program running on a parallel platform can be measured. Our benchmarks are structured in micro, kernel, application, parallel, and streaming benchmarks. Copyright 2010 ACM.},
  affiliation   = {Department of Informatics and Mathematical Modeling, Technical University of Denmark, Denmark; Department of Computer Engineering, Technische Universität, Dresden, Germany; Department of Computer Science, University of Augsburg, Germany},
  document_type = {Conference Paper},
  doi           = {10.1145/1850771.1850789},
  journal       = {ACM International Conference Proceeding Series},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77957966560&doi=10.1145%2f1850771.1850789&partnerID=40&md5=fee3a06a8b36d0f911e68ff2c0630ebd},
}

@Conference{Bossu2010,
  author        = {Bossu, J. and Gruyer, D. and Smal, J.C. and Blosseville, J.M.},
  title         = {Validation and benchmarking for pedestrian video detection based on a sensors simulation platform},
  year          = {2010},
  pages         = {115-122},
  note          = {cited By 6},
  __markedentry = {[Nichl:6]},
  abstract      = {The evaluation stage is an important part in the validation of ADAS robustness. Moreover, the control and the repetitiveness of the experimentations were very difficult to conduct on real road due to safety reasons. Moreover, the lack of data/sensors or the complexity of the experiment are often very penalizing for a correct and exhaustive evaluation. It is for these reasons that LIVIC launched the development of a software simulation architecture (SiVIC), to support its research activities on ADAS. The use of such a simulation platform can provide both simulated sensors data and a ground truth reference for the validation stages. This paper proposes a general framework and a protocol in order to evaluate the result of pedestrian detection by camera processing. ©2010 IEEE.},
  affiliation   = {LEMCO (INRETS), Building 824, 14 route de la minière, 78000 Versailles - Satory, France; LIVIC (INRETS/CCPC), Building 824, 14 route de la minière, 78000 Versailles - Satory, France},
  art_number    = {5548031},
  document_type = {Conference Paper},
  doi           = {10.1109/IVS.2010.5548031},
  journal       = {IEEE Intelligent Vehicles Symposium, Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77956538470&doi=10.1109%2fIVS.2010.5548031&partnerID=40&md5=c5a82a6a11f63f711147715e823ad7a8},
}

@Conference{Zanini2010,
  author        = {Zanini, F. and Jones, C.N. and Atienza, D. and De Micheli, G.},
  title         = {Multicore thermal management using approximate explicit model predictive control},
  year          = {2010},
  pages         = {3321-3324},
  note          = {cited By 18},
  __markedentry = {[Nichl:6]},
  abstract      = {Meeting temperature constraints and reducing the hot-spots are critical for achieving reliable and efficient operation of complex multi-core systems. In this paper we aim at achieving an online smooth thermal control action that minimizes the performance loss as well as the computational and hardware overhead of embedding a thermal management system inside the MPSoC. The optimization problem considers the thermal profile of the system, its evolution over time and current time-varying workload requirements. We formulate this problem as a discrete-time control problem using model predictive control. The solution is computed off-line and partially on-line using an explicit approximate algorithm. This proposed method, compared with the optimum approach provides a significant reduction in hardware requirements and computational cost at the expense of a small loss in accuracy. We perform experiments on a model of the 8-core Niagara-1 multicore architecture using benchmarks ranging from web-accessing to playing multimedia. Results show that the proposed method provides comparable performance(loss up to 2.7%) versus the optimum solution with a reduction up to 72.5x in the the computational complexity. ©2010 IEEE.},
  affiliation   = {Laboratory of Integrated Systems (LSI), EPFL, Switzerland; Embedded Systems Laboratory (ESL), EPFL, Switzerland; Automatic Control Laboratory, ETH Zurich, Switzerland},
  art_number    = {5537891},
  document_type = {Conference Paper},
  doi           = {10.1109/ISCAS.2010.5537891},
  journal       = {ISCAS 2010 - 2010 IEEE International Symposium on Circuits and Systems: Nano-Bio Circuit Fabrics and Systems},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955995837&doi=10.1109%2fISCAS.2010.5537891&partnerID=40&md5=682b7106bf11af2d58014e4cd8cc90ec},
}

@Conference{Schulz2010,
  author          = {Schulz, F.},
  title           = {Towards measuring the degree of fulfillment of service level agreements},
  year            = {2010},
  volume          = {3},
  pages           = {273-276},
  note            = {cited By 12},
  __markedentry   = {[Nichl:6]},
  abstract        = {In service oriented architectures (SOA), the non-functional properties of services have been recognized to be highly important in addition to the functionality of services as a means to differentiate services according to quality considerations. Service level agreements (SLAs) are formalized contracts between service providers and service consumers that are used to define quality of service (QoS) properties. The violation of an SLA by the service provider typically results in a penalty to compensate the service consumer. In order to avoid such situations, the service provider needs to recognize critical service instances and to take appropriate countermeasures before a violation happens. Therefore a measure for quantifying the danger of SLA violation is needed as part of a service level management system. This paper proposes a concept for the definition and evaluation of such a metric that takes into account the underlying structure of the SLA as well as the available options for monitoring service quality parameters. Hence it becomes possible to obtain detailed information of the status of service fulfillment at runtime and to identify critical service instances. The methodology is exemplified with the availability property. © 2010 IEEE.},
  affiliation     = {SAP Research Karlsruhe, SAP AG, Karlsruhe, Germany},
  art_number      = {5513976},
  author_keywords = {Key performance indicator; Monitoring; Quality of service; Service level agreement; Service oriented architecture},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICIC.2010.254},
  journal         = {ICIC 2010 - 3rd International Conference on Information and Computing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955814246&doi=10.1109%2fICIC.2010.254&partnerID=40&md5=9835bf066da645d606d7b33ba8dfe4a5},
}

@Conference{Appel2010,
  author        = {Appel, S. and Sachs, K. and Buchmann, A.},
  title         = {Towards benchmarking of AMQP},
  year          = {2010},
  pages         = {99-100},
  note          = {cited By 11},
  __markedentry = {[Nichl:6]},
  abstract      = {With the increasing importance of event-based systems the performance of underlying event transporting systems, such as message oriented middleware (MOM), becomes business critical. Therefore, we see a strong need for benchmarks for such environments. Several messaging standards and protocols for middleware exist; most popular is the Java Message Service (JMS) which is defining an API rather than a wire protocol. An emerging standard is the new wire level protocol Advanced Message Queuing Protocol (AMQP). It originated in the financial sector and is developed by a consortium of over 20 member organizations. © 2010 ACM.},
  affiliation   = {TU Darmstadt, Germany},
  document_type = {Conference Paper},
  doi           = {10.1145/1827418.1827438},
  journal       = {Proceedings of the 4th ACM International Conference on Distributed Event-Based Systems, DEBS 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955837691&doi=10.1145%2f1827418.1827438&partnerID=40&md5=142d7695097650ba10e817fcf6866c29},
}

@Conference{Vidackovic2010,
  author          = {Vidačković, K. and Kellner, I. and Donald, J.},
  title           = {Business-oriented development methodology for complex event processing: Demonstration of an integrated approach for process monitoring},
  year            = {2010},
  pages           = {111-112},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this demonstration, we showcase an integrated approach for event-driven process monitoring in the Internet of Services. Our business-oriented development methodology for Complex Event Processing (CEP) utilizes the idea of the Zachman framework with modeling on different perspectives. Starting with the Business Motivation Model (BMM) on a strategic perspective, an Event Hierarchy with key performance indicators (KPIs), objectives, noticeable situations and reactions is successively built in a top-down approach. The demonstration includes an Eclipse-based prototype for modeling and a runtime component using the open source CEP engine Esper with a seamless integration. © 2010 ACM.},
  affiliation     = {Fraunhofer IAO, Nobelstr. 12, 70569 Stuttgart, Germany; Siemens AG, Otto-Hahn-Ring 6, 81739 Munich, Germany},
  author_keywords = {complex event processing; development methodology; event processing network; process monitoring; service engineering},
  document_type   = {Conference Paper},
  doi             = {10.1145/1827418.1827444},
  journal         = {Proceedings of the 4th ACM International Conference on Distributed Event-Based Systems, DEBS 2010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955837690&doi=10.1145%2f1827418.1827444&partnerID=40&md5=c356f4b479c91f8d8d6edc90e45747d2},
}

@Conference{Klein2010,
  author          = {Klein, A. and Jerzak, Z.},
  title           = {GINSENG for sustainable energy awareness: Flexible energy monitoring using wireless sensor nodes},
  year            = {2010},
  pages           = {109-110},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {To ensure sustainable manufacturing the energy consumption of production processes must be minimized. SAP Research is active in two European Community's Seventh Framework Program (FP7) projects: GINSENG and KAP. The combined aim of GINSENG and KAP projects is to allow for event-based monitoring of the Production Performance Indicators (PPI) that can be used to, e.g., shut down idle machines or to optimize the material flow. In this demo we illustrate how the energy consumption for lightening and heating can be controlled with the use of wireless sensor nodes that measure temperature, light and humidity. Events describing the indoor climate, originating from TelosB motes, are processed using the middleware developed within the GINSENG project and subsequently visualized using a dashboard. Visualization includes also warnings regarding excessive energy consumption, which can be simulated with light beams or hand warmth. © 2010 ACM.},
  affiliation     = {SAP Research Center Dresden, SAP AG, Germany},
  author_keywords = {energy awareness; event processing; middleware; real-time monitoring},
  document_type   = {Conference Paper},
  doi             = {10.1145/1827418.1827443},
  journal         = {Proceedings of the 4th ACM International Conference on Distributed Event-Based Systems, DEBS 2010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955792334&doi=10.1145%2f1827418.1827443&partnerID=40&md5=606de90280dac9edd1422030b17c40db},
}

@Article{Happe2010,
  author          = {Happe, J. and Becker, S. and Rathfelder, C. and Friedrich, H. and Reussner, R.H.},
  title           = {Parametric performance completions for model-driven performance prediction},
  journal         = {Performance Evaluation},
  year            = {2010},
  volume          = {67},
  number          = {8},
  pages           = {694-716},
  note            = {cited By 31},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance prediction methods can help software architects to identify potential performance problems, such as bottlenecks, in their software systems during the design phase. In such early stages of the software life-cycle, only a little information is available about the system's implementation and execution environment. However, these details are crucial for accurate performance predictions. Performance completions close the gap between available high-level models and required low-level details. Using model-driven technologies, transformations can include details of the implementation and execution environment into abstract performance models. However, existing approaches do not consider the relation of actual implementations and performance models used for prediction. Furthermore, they neglect the broad variety of possible implementations and middleware platforms, possible configurations, and possible usage scenarios. In this paper, we (i) establish a formal relation between generated performance models and generated code, (ii) introduce a design and application process for parametric performance completions, and (iii) develop a parametric performance completion for Message-oriented Middleware according to our method. Parametric performance completions are independent of a specific platform, reflect performance-relevant software configurations, and capture the influence of different usage scenarios. To evaluate the prediction accuracy of the completion for Message-oriented Middleware, we conducted a real-world case study with the SPECjms2007 Benchmark [http://www.spec.org/jms2007/]. The observed deviation of measurements and predictions was below 10% to 15%. © 2010 Elsevier B.V. All rights reserved.},
  affiliation     = {Forschungszentrum Informatik-FZI, 76131 Karlsruhe, Germany; Andrena Objects, 76131 Karlsruhe, Germany; Universität Karlsruhe (TH), 76131 Karlsruhe, Germany},
  author_keywords = {Message-oriented middleware; Model-driven performance engineering; Performance completion; Software architecture; Software performance engineering},
  document_type   = {Article},
  doi             = {10.1016/j.peva.2009.07.006},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955306842&doi=10.1016%2fj.peva.2009.07.006&partnerID=40&md5=19dedf6eb5aeb90276d7efb35e2b4019},
}

@Article{Li2010a,
  author          = {Li, H. and Sun, J. and Wu, J.},
  title           = {Predicting business failure using classification and regression tree: An empirical comparison with popular classical statistical methods and top classification mining methods},
  journal         = {Expert Systems with Applications},
  year            = {2010},
  volume          = {37},
  number          = {8},
  pages           = {5895-5904},
  note            = {cited By 58},
  __markedentry   = {[Nichl:6]},
  abstract        = {Predicting business failure is a very critical task for government officials, stock holders, managers, employees, investors and researchers, especially in nowadays competitive economic environment. Several top 10 data mining methods have become very popular alternatives in business failure prediction (BFP), e.g., support vector machine and k nearest neighbor. In comparison with the other classification mining methods, advantages of classification and regression tree (CART) methods include: simplicity of results, easy implementation, nonlinear estimation, being non-parametric, accuracy and stable. However, there are seldom researches in the area of BFP that witness the applicability of CART, another method among the top 10 algorithms in data mining. The aim of this research is to explore the performance of BFP using the commonly discussed data mining technique of CART. To demonstrate the effectiveness of BFP using CART, business failure predicting tasks were performed on the data set collected from companies listed in the Shanghai Stock Exchange and Shenzhen Stock Exchange. Thirty times' hold-out method was employed as the assessment, and the two commonly used methods in the top 10 data mining algorithms, i.e., support vector machine and k nearest neighbor, and the two baseline benchmark methods from statistic area, i.e., multiple discriminant analysis (MDA) and logistics regression, were employed as comparative methods. For comparative methods, stepwise method of MDA was employed to select optimal feature subset. Empirical results indicated that the optimal algorithm of CART outperforms all the comparative methods in terms of predictive performance and significance test in short-term BFP of Chinese listed companies. © 2010 Elsevier Ltd. All rights reserved.},
  affiliation     = {School of Business Administration, Zhejiang Normal University, 91 Subbox in P.O. Box 62, YingBinDaDao 688, Jinhua City, 321004 Zhejiang Province, China},
  author_keywords = {Business failure prediction (BFP); Classification and regression tree (CART); Data mining},
  document_type   = {Article},
  doi             = {10.1016/j.eswa.2010.02.016},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77951206489&doi=10.1016%2fj.eswa.2010.02.016&partnerID=40&md5=1f39ebdb291cad0709370606bcbcf5a3},
}

@Conference{2010,
  title         = {2010 ICSE Workshop on Software Engineering in Health Care, SEHC 2010, in Conjunction with the 32nd ACM/IEEE International Conference on Software Engineering, ICSE 2010},
  year          = {2010},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 14 papers. The topics discussed include: prototyping closed loop physiologic control with the medical device coordination framework; a low cost positioning and visualization system using smartphones for emergency ambulance service; patient preference elicitation empowerment; adaptation issues in software architectures of remote health care systems; a typology to support HIS design for collaborative healthcare delivery; social networking applications in health care: threats to the privacy and security of health information; a benchmark for evaluating software engineering techniques for improving medical processes; formal scenario-based requirements specification and test case generation in healthcare applications; towards improved security criteria for certification of electronic health record systems; and safety testing of computerized provider order entry systems.},
  document_type = {Conference Review},
  journal       = {Proceedings - International Conference on Software Engineering},
  page_count    = {124},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77954910707&partnerID=40&md5=23f5659c0e140be71058e1d856d5dafb},
}

@Conference{Bertran2010,
  author          = {Bertran, R. and Gonzalez, M. and Martorell, X. and Navarro, N. and Ayguade, E.},
  title           = {Decomposable and responsive power models for multicore processors using performance counters},
  year            = {2010},
  pages           = {147-158},
  note            = {cited By 98},
  __markedentry   = {[Nichl:6]},
  abstract        = {Power modeling based on performance monitoring counters (PMCs) attracted the interest of researchers since it became a quick approach to understand and analyse power behavior on real systems. As a result, several power-aware policies use power models to guide their decisions and to trigger low-level mechanisms such as voltage and frequency scaling. Hence, the presence of power models that are informative, accurate and capable of detecting power phases is critical to increase the power-aware research chances and to improve the success of power-saving techniques based on them. In addition, the design of current processors has varied considerably with the inclusion of multiple cores with some resources shared on a single die. As a result, PMC-based power models warrant further investigation on current energy-efficient multi-core processors. In this paper, we present a methodology to produce decomposable PMC-based power models on current multicore architectures. Apart from being able to estimate the power consumption accurately, the models provide per component power consumption, supplying extra insights about power behavior. Moreover, we validate their responsiveness -the capacity to detect power phases-. Specifically, we produce a set of power models for an Intel® Core™ 2 Duo. We model one and two cores for a wide set of DVFS configurations. The models are empirically validated by using the SPEC-cpu2006 benchmark suite and we compare them to other models built using existing approaches. Overall, we demonstrate that the proposed methodology produces more accurate and responsive power models. Concretely, our models show a [1.89-6]% error range and almost 100% accuracy in detecting phase variations above 0.5 watts. © 2010 ACM.},
  affiliation     = {Departament d'Arquitectura de Computadors, Universitat Politècnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain},
  author_keywords = {performance counters; power estimation},
  document_type   = {Conference Paper},
  doi             = {10.1145/1810085.1810108},
  journal         = {Proceedings of the International Conference on Supercomputing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77954716360&doi=10.1145%2f1810085.1810108&partnerID=40&md5=229c08bb84b59b0a936dffab23cc1357},
}

@Conference{Zhang2010,
  author          = {Zhang, X. and Jiang, S.},
  title           = {InterferenceRemoval: Removing interference of disk access for MPI programs through data replication},
  year            = {2010},
  pages           = {223-232},
  note            = {cited By 26},
  __markedentry   = {[Nichl:6]},
  abstract        = {As the number of I/O-intensive MPI programs becomes increasingly large, many efforts have been made to improve I/O performance, on both software and architecture sides. On the software side, researchers can optimize processes' access patterns, either individually (e.g., by using large and sequential requests in each process), or collectively (e.g., by using collective I/O). On the architecture side, files are striped over multiple I/O nodes for a high aggregate I/O throughput. However, a key weakness, the access interference on each I/O node, remains unaddressed in these efforts. When requests from multiple processes are served simultaneously by multiple I/O nodes, one I/O node has to concurrently serve requests from different processes. Usually the I/O node stores its data on the hard disks, and different process accesses different regions of a data set. When there are a burst of requests from multiple processes, requests from different processes to a disk compete with each other for its single disk head to access data. The disk efficiency can be significantly reduced due to frequent disk head seeks. In this paper, we propose a scheme, InterferenceRemoval, to eliminate I/O interference by taking advantage of optimized access patterns and potentially high throughput provided by multiple I/O nodes. It identifies segments of files that could be involved in the interfering accesses and replicates them to their respectively designated I/O nodes. When the interference is detected at an I/O node, some I/O requests can be re-directed to the replicas on other I/O nodes, so that each I/O node only serves requests from one or a limited number of processes. InterferenceRemoval has been implemented in the MPI library for high portability on top of the Lustre parallel file system. Our experiments with representative benchmarks, such as NPB BTIO and mpi-tile-io, show that it can significantly improve I/O performance of MPI programs. For example, the I/O throughput of mpi-tile-io can be increased by 105% as compared to that without using collective I/O, and by 23% as compared to that using collective I/O. © 2010 ACM.},
  affiliation     = {ECE Department, Wayne State University, Detroit, MI 48202, United States},
  author_keywords = {I/O interference; MPI program; MPI-IO},
  document_type   = {Conference Paper},
  doi             = {10.1145/1810085.1810116},
  journal         = {Proceedings of the International Conference on Supercomputing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77954708874&doi=10.1145%2f1810085.1810116&partnerID=40&md5=65606d04dcb894e1a77706085f60a301},
}

@Conference{Tiwari2010,
  author        = {Tiwari, D. and Lee, S. and Tuck, J. and Solihin, Y.},
  title         = {MMT: Exploiting fine-grained parallelism in dynamic memory management},
  year          = {2010},
  note          = {cited By 13},
  __markedentry = {[Nichl:6]},
  abstract      = {Dynamic memory management is one of the most expensive but ubiquitous operations in many C/C++ applications. Additional features such as security checks, while desirable, further worsen memory management overheads. With advent of multicore architecture, it is important to investigate how dynamic memory management overheads for sequential applications can be reduced. In this paper, we propose a new approach for accelerating dynamic memory management on multicore architecture, by offloading dynamic management functions to a separate thread that we refer to as memory management thread (MMT). We show that an efficient MMT design can give significant performance improvement by extracting parallelism while being agnostic to the underlying memory management library algorithms and data structures. We also show how parallelism provided by MMT can be beneicial for high overhead memory management tasks, for example, security checks related to memory management. We evaluate MMT on heap allocation-intensive benchmarks running on an Intel core 2 quad platform for two widely- used memory allocators: Doug Lea's and PHKmalloc allocators. On average, MMT achieves a speedup ratio of 1.19× for both allocators, while both the application and memory management libraries are unmodiied and are oblivious to the parallelization scheme. For PHKmalloc with security checks turned on, MMT reduces the security check overheads from 21% to only 1% on average.},
  affiliation   = {Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, United States},
  art_number    = {5470428},
  document_type = {Conference Paper},
  doi           = {10.1109/IPDPS.2010.5470428},
  journal       = {Proceedings of the 2010 IEEE International Symposium on Parallel and Distributed Processing, IPDPS 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77953986423&doi=10.1109%2fIPDPS.2010.5470428&partnerID=40&md5=408af18600032753ff89d7abb147d1f5},
}

@Conference{Yamashita2010,
  author        = {Yamashita, K.},
  title         = {Possibility of ESL- A software centric system design for multicore SoC in the upstream phase},
  year          = {2010},
  pages         = {805-808},
  note          = {cited By 5},
  __markedentry = {[Nichl:6]},
  abstract      = {The embedded systems for which both hardware and software are rapidly advancing and expanding, there is a growing need to be able to comprehensively and quantitatively estimate system performance at an early stage in the design process, especially multi-core based SoC. But it can be difficult to estimate system performance of actual target by employing only simple estimation methods. By using ESL technology, without implemented hardware, that enables high-precision assessment of system performance with software that runs on OS. By applying proposed assessment environment during upstream design of target system, it enables to research the characteristics of performance, bottle-neck and avoid the risk of the redesign. It will be the key-issue of ESL methodology. It might be tightly related with software architecture and it is different point of view from typical upstream design of hardware. We developed ESL based simulation environment for modeling logic SoC used in mobile phones at the upstream design phase (during planning phase, before logic design). We achieved high-precision assessment of system performance by using application level software benchmarks that runs on the Symbian OS, and at operating speeds that allow for iterated executions over a short time.},
  affiliation   = {Fujitsu Laboratories Ltd., 1-1, Kamikodanaka 4, Nakahara-ku, Kawasaki, Japan},
  art_number    = {5419779},
  document_type = {Conference Paper},
  doi           = {10.1109/ASPDAC.2010.5419779},
  journal       = {Proceedings of the Asia and South Pacific Design Automation Conference, ASP-DAC},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77951236512&doi=10.1109%2fASPDAC.2010.5419779&partnerID=40&md5=4abcaa41ca15df213f8420a9cb650c52},
}

@Article{Kumar2010,
  author          = {Kumar, S. and Faraj, A. and Mamidala, A.R. and Smith, B. and Dozsa, G. and Cernohous, B. and Gunnels, J. and Miller, D. and Ratterman, J. and Heidelberger, P.},
  title           = {Architecture of the component collective messaging interface},
  journal         = {International Journal of High Performance Computing Applications},
  year            = {2010},
  volume          = {24},
  number          = {1},
  pages           = {16-33},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {Different programming paradigms utilize a variety of collective communication operations, often with different semantics. We present the component collective messaging interface (CCMI) that can support asynchronous nonblocking collectives and is extensible to different programming paradigms and architectures. CCMI is designed with components written in the C++ programming language, allowing it to be reusable and extendible. Collective algorithmsare embodied in topological schedules and executors that execute them. Portability across architectures is enabled by the multisend data movement component. CCMI includes a programming language adaptor used to implement different APIs with different semantics for different paradigms. We study the effectiveness of CCMI on 16K nodes of Blue Gene/P machine and evaluate its performance for the barrier, broadcast, and allreduce collective operations and several application benchmarks. We also present the performance of the barrier collective on the Abe Infiniband cluster. The Author(s), 2010.},
  affiliation     = {IBM T. J. Watson Research Center, Yorktown Heights, NY 10598, United States; IBM Systems and Technology Group, Rochester, MN 55901, United States},
  author_keywords = {Active messages; Allreduce; Barrier; Blue gene; Broadcast; Collective communication; Component architecture; Message passing; MPI; Nonblocking collectives; Reduce; Software architecture},
  document_type   = {Article},
  doi             = {10.1177/1094342009359011},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77951032668&doi=10.1177%2f1094342009359011&partnerID=40&md5=513369cd6c37f02d41cb9e7d4ef7a507},
}

@Article{Woo2010,
  author          = {Woo, D.H. and Fryman, J.B. and Knies, A.D. and Lee, H.-H.S.},
  title           = {Chameleon: Virtualizing idle acceleration cores of a heterogeneous multicore processor for caching and prefetching},
  journal         = {Transactions on Architecture and Code Optimization},
  year            = {2010},
  volume          = {7},
  number          = {1},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {Heterogeneous multicore processors have emerged as an energy- and area-efficient architectural solution to improving performance for domain-specific applications such as those with a plethora of data-level parallelism. These processors typically contain a large number of small, compute-centric cores for acceleration while keeping one or two high-performance ILP cores on the die to guarantee single-thread performance. Although a major portion of the transistors are occupied by the acceleration cores, these resources will sit idle when running unparallelized legacy codes or the sequential part of an application. To address this underutilization issue, in this article, we introduce Chameleon, a flexible heterogeneous multicore architecture to virtualize these resources for enhancing memory performance when running sequential programs. The Chameleon architecture can dynamically virtualize the idle acceleration cores into a last-level cache, a data prefetcher, or a hybrid between these two techniques. In addition, Chameleon can operate in an adaptive mode that dynamically configures the acceleration cores between the hybrid mode and the prefetch-only mode by monitoring the effectiveness of the Chameleon cache mode. In our evaluation with SPEC2006 benchmark suite, different levels of performance improvements were achieved in different modes for different applications. In the case of the adaptive mode, Chameleon improves the performance of SPECint06 and SPECfp06 by 31% and 15%, on average. When considering only memory-intensive applications, Chameleon improves the system performance by 50% and 26% for SPECint06 and SPECfp06, respectively. © 2010 ACM.},
  affiliation     = {Georgia Institute of Technology, United States; Intel Corporation, United States},
  art_number      = {3},
  author_keywords = {Cache; Heterogeneous multicore; Idle core; Prefetching},
  document_type   = {Article},
  doi             = {10.1145/1736065.1736068},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77952078262&doi=10.1145%2f1736065.1736068&partnerID=40&md5=c2a258ad85a25d853e67513344d7a7a7},
}

@Article{Ujaldon2010,
  author          = {Ujaldon, M.},
  title           = {Using GPUs for accelerating electromagnetic simulations},
  journal         = {Applied Computational Electromagnetics Society Journal},
  year            = {2010},
  volume          = {25},
  number          = {4},
  pages           = {294-302},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {The computational power and memory bandwidth of graphics processing units (GPUs) have turned them into attractive platforms for general-purpose applications at significant speed gains versus their CPU counterparts [I]. In addition, an increasing number of today's state-ofthe-art supercomputers include commodity GPUs to bring us unprecedented levels of performance in terms of raw GFLOPS and GFLOPS/cost. Inspired by the latest trends and developments in GPUs, we propose a new paradigm for implementing on GPUs some of the major aspects of electromagnetic simulations, a domain traditionally used as a benchmark to run codes in some of the most expensive and powerful supercomputers worldwide. After reviewing related achievements and ongoing projects, we provide a guideline to exploit SIMD parallelism and high memory bandwidth using the CUDA programming model and hardware architecture offered by Nvidia graphics cards at an affordable cost. As a result, performance gains of several orders of magnitude can be attained versus threadlevel methods like pthreads used to run those simulations on emerging multicore architectures © 2010 ACES.},
  affiliation     = {Department of Computer Architecture, University of Malaga, Malaga 29071, Spain},
  author_keywords = {CUDA; Electromagnetic simulations; GPGPU; Graphics processors},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77958156369&partnerID=40&md5=34c28290335e6a34736a33fc81de283a},
}

@Conference{Lee2010,
  author          = {Lee, J. and Seo, S. and Lee, J.},
  title           = {A software-SVM-based transactional memory for multicore accelerator architectures with local memory},
  year            = {2010},
  volume          = {2010},
  pages           = {567-568},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {We propose a software transactional memory (STM) for heterogeneous multicores with small local memory. The heterogeneous multicore architecture consists of a general-purpose processor element (GPE) and multiple accelerator processor elements (APEs). The GPE is typically backed by a deep, on-chip cache hierarchy and hardware cache coherence. On the other hand, the APEs have small, explicitly addressed local memory that is not coherent with the main memory. Programmers of such multicore architectures suffer from explicit memory management and coherence problems. The STM for such multicores can alleviate the burden of the programmer and transparently handle data transfers at run time. Moreover, it makes the programmer free from controlling locks. Our TM is based on an existing software SVM for the accelerator architecture. The software SVM exploits software-managed caches and coherence protocols between the GPE and APEs. We also propose an optimization technique, called abort prediction, for the TM. It blocks a transaction from running until the chance of potential conflicts is eliminated. We implement the TM system and the optimization technique for a single Cell BE processor and evaluate their effectiveness with six compute-intensive benchmark applications.},
  affiliation     = {School of Computer Science and Engineering, Seoul National University, Seoul, South Korea},
  author_keywords = {heterogeneous multicores; software shared virtual memory; transactional memory},
  document_type   = {Conference Paper},
  doi             = {10.1145/1854273.1854355},
  journal         = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78149246741&doi=10.1145%2f1854273.1854355&partnerID=40&md5=bdc215d83a8f71f353bc8e65a7ea9963},
}

@Conference{Koutsoukos2010,
  author        = {Koutsoukos, X. and Biswas, G. and Mylaraswamy, D.A. and Hadden, G.D. and Mack, D. and Hamilton, D.},
  title         = {Benchmarking the vehicle integrated prognostic reasoner},
  year          = {2010},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper outlines a benchmarking approach for evaluating the diagnostic and prognostic capabilities of the Vehicle Integrated Prognostic Reasoner (VIPR), a vehicle-level reasoner and an architecture which aims to detect, diagnose, and predict adverse events during the flight of an aircraft. A number of diagnostic and prognostic metrics exist, but these standards are defined for well-circumscribed algorithms that apply to small subsystems. For layered reasoners, such as VIPR, the overall performance cannot be evaluated by metrics solely directed toward timely detection and accuracy of estimation of the faults in individual components. Among other factors, the overall vehicle reasoner performance is governed by the effectiveness of the communication schemes between the different monitors and reasoners in the architecture, and the ability to propagate and fuse relevant information to make accurate, consistent, and timely predictions at different levels of the reasoner hierarchy. To address these issues, we outline an extended set of diagnostic and prognostics metrics that can be used to evaluate the performance of layered architecture, and we discuss a software architecture as well as an evaluation plan for benchmarking VIPR.},
  affiliation   = {Institute for Software Integrated Systems, Vanderbilt University, Nashville, TN 37235, United States; Honeywell, 1985 Douglas Drive North, Golden Valley, MN 55422, United States; Lockheed Martin Space Company, Denver, CO 80201, United States},
  document_type = {Conference Paper},
  journal       = {Annual Conference of the Prognostics and Health Management Society, PHM 2010},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84920532476&partnerID=40&md5=86ba01f46cc6faea4de81fe6f30360a9},
}

@Conference{Sekkat2010,
  author          = {Sekkat, S. and Paris, J.L. and Kouiss, K. and Saadi, J. and Deshayes, L.},
  title           = {Developing integrated Performance Measurement System - Using MDA approach},
  year            = {2010},
  pages           = {107-113},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In an industrial context defined by more acute competition, performance measurement becomes a control tool. The Manufacturing Execution System (MES) software achieves control and execution of production functions, as Performances analysis. However, to choose the adequate performance indicators and to implement them is a difficult problem. Indeed, the enterprises need methods to specify and to install their Performance Measurement System (PMS). In this paper, we propose a methodology of performance indicators implementation. We use the Component Oriented Programming with a Model- Driven Architecture (MDA), to generate applications' final code. This method facilitates the design of Performances Measurement System and its implementation.},
  affiliation     = {LIMOS, IFMA Campus de Clermont-Ferrand/Les Cézeaux, BP 265, 63175 Aubière Cedex, France; ERCSSP, ENSAM Marjane II Beni M'hammed, BP 4024, Meknes Alismailia, Meknes, Morocco; LAP, ENSEM Casablanca, OASIS, Route d' EL JADIDA, Casablanca, Morocco; Manbet Technology, 5 Avenue Abou Taeib Al Moutanabi, Morocco},
  author_keywords = {Component Oriented Programming COP; Manufacturing Execution Systems MES; Model-driven; Performances Measurement Systems PMS},
  document_type   = {Conference Paper},
  journal         = {4th Int. Conference on Integrated Modeling and Analysis in Applied Control and Automation, IMAACA 2010, Held at the International Mediterranean and Latin American Modeling Multiconference, I3M 2010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84899423126&partnerID=40&md5=7725a2f61636e3fe5c3a6d2c80cb7d3d},
}

@Article{Wu2010,
  author          = {Wu, J. and Pan, X. and Yang, X.},
  title           = {Software prepromotion for non-uniform cache architecture},
  journal         = {Journal of Software},
  year            = {2010},
  volume          = {5},
  number          = {1},
  pages           = {11-19},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {As a solution to growing global wire delay, non-uniform cache architecture (NUCA) has already been a trend in large cache designs. The access time of NUCA is determined by the distance between the cache bank containing the required data and the processor. Thus, one of the important NUCA researches focuses on how to place data to be used into cache banks close to the processor. This paper proposes software prepromotion technique, which prepromote data using prepromotion instructions as similar as software prefetching does. Besides the basic software prepromotion, this paper also proposes smart multihop software prepromotion (SMSP), very long software prepromotion (VLSP) and their combination technique. SMSP intelligently chooses cache banks which the prepromoted data most ideally suit to being moved into. And VLSP prepromote multiple data using one instruction. Finally, we evaluate our approaches by testing 7 kernel benchmarks on a full-system simulator. The basic software prepromotion gets an average improvement of 2.6893% in IPC. The SMSP improves IPC by 7.0928% averagely. And the VLSP gets an IPC improvement of 7.2194% averagely. Lastly, after combining the SMSP and VLSP, the average improvement in IPC achieves 11.8650%. © 2010 ACADEMY PUBLISHER.},
  affiliation     = {National laboratory for parallel and distributed processing, Changsha, China},
  author_keywords = {NUCA; Prefetching; Smart multihop software prepromotion; Software prepromotion; Very long software prepromotion},
  document_type   = {Article},
  doi             = {10.4304/jsw.5.1.11-19},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78651580690&doi=10.4304%2fjsw.5.1.11-19&partnerID=40&md5=70014a08d852e0cc643b382b2d9afc83},
}

@Article{Shan2010,
  author          = {Shan, H. and Blagojević, F. and Min, S.-J. and Hargrove, P. and Jin, H. and Fuerlinger, K. and Koniges, A. and Wright, N.J.},
  title           = {A programming model performance study using the NAS parallel benchmarks},
  journal         = {Scientific Programming},
  year            = {2010},
  volume          = {18},
  number          = {3-4},
  pages           = {153-167},
  note            = {cited By 16},
  __markedentry   = {[Nichl:6]},
  abstract        = {Harnessing the power of multicore platforms is challenging due to the additional levels of parallelism present. In this paper we use the NAS Parallel Benchmarks to study three programming models, MPI, OpenMP and PGAS to understand their performance and memory usage characteristics on current multicore architectures. To understand these characteristics we use the Integrated Performance Monitoring tool and other ways to measure communication versus computation time, as well as the fraction of the run time spent in OpenMP. The benchmarks are run on two different Cray XT5 systems and an Infiniband cluster. Our results show that in general the three programming models exhibit very similar performance characteristics. In a few cases, OpenMP is significantly faster because it explicitly avoids communication. For these particular cases, we were able to re-write the UPC versions and achieve equal performance to OpenMP. Using OpenMP was also the most advantageous in terms of memory usage. Also we compare performance differences between the two Cray systems, which have quad-core and hex-core processors. We show that at scale the performance is almost always slower on the hex-core system because of increased contention for network resources. © 2010 - IOS Press and the authors. All rights reserved.},
  affiliation     = {Future Technology Group, Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA 94720, United States; NAS Division, NASA Ames Research Center, Moffett Field, CA, United States; University of California at Berkeley, EECS Department, Computer Science Division, Berkeley, CA, United States; NERSC, Lawrence Berkeley National Laboratory, Berkeley, CA, United States},
  author_keywords = {memory usage; MPI; OpenMP; performance study; Programming model; UPC},
  document_type   = {Article},
  doi             = {10.3233/SPR-2010-0306},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-79551704213&doi=10.3233%2fSPR-2010-0306&partnerID=40&md5=a968d0655be8d30450724cc9c1dad3e8},
}

@Conference{Kurian2010,
  author          = {Kurian, G. and Miller, J.E. and Psota, J. and Eastep, J. and Liu, J. and Michel, J. and Kimerling, L.C. and Agarwal, A.},
  title           = {ATAC: A 1000-core cache-coherent processor with on-chip optical network},
  year            = {2010},
  volume          = {2010},
  pages           = {477-488},
  note            = {cited By 141},
  __markedentry   = {[Nichl:6]},
  abstract        = {Based on current trends, multicore processors will have 1000 cores or more within the next decade. However, their promise of increased performance will only be realized if their inherent scaling and programming challenges are overcome. Fortunately, recent advances in nanophotonic device manufacturing are making CMOS-integrated optics a reality-interconnect technology which can provide significantly more bandwidth at lower power than conventional electrical signaling. Optical interconnect has the potential to enable massive scaling and preserve familiar programming models in future multicore chips. This paper presents ATAC, a new multicore architecture with integrated optics, and ACKwise, a novel cache coherence protocol designed to leverage ATAC's strengths. ATAC uses nanophotonic technology to implement a fast, efficient global broadcast network which helps address a number of the challenges that future multicores will face. ACKwise is a new directory-based cache coherence protocol that uses this broadcast mechanism to provide high performance and scalability. Based on 64-core and 1024-core simulations with Splash2, Parsec, and synthetic benchmarks, we show that ATAC with ACKwise out-performs a chip with conventional interconnect and cache coherence protocols. On 1024-core evaluations, ACKwise protocol on ATAC outperforms the best conventional cache coherence protocol on an electrical mesh network by 2.5x with Splash2 benchmarks and by 61% with synthetic benchmarks. © 2010 ACM.},
  affiliation     = {Massachusetts Institute of Technology, Cambridge, MA 02139, United States},
  author_keywords = {cache coherence; network-on-chip; photonic interconnect},
  document_type   = {Conference Paper},
  doi             = {10.1145/1854273.1854332},
  journal         = {Parallel Architectures and Compilation Techniques - Conference Proceedings, PACT},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78149271070&doi=10.1145%2f1854273.1854332&partnerID=40&md5=0220a9336e4883cc0b3a450d14063666},
}

@Article{Zier2010,
  author          = {Zier, D.A. and Lee, B.},
  title           = {Performance evaluation of dynamic speculative multithreading with the cascadia architecture},
  journal         = {IEEE Transactions on Parallel and Distributed Systems},
  year            = {2010},
  volume          = {21},
  number          = {1},
  pages           = {47-59},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {Thread-level parallelism (TLP) has been extensively studied in order to overcome the limitations of exploiting instruction-level parallelism (ILP) on high-performance superscalar processors. One promising method of exploiting TLP is Dynamic Speculative Multithreading (D-SpMT), which extracts multiple threads from a sequential program without compiler support or instruction set extensions. This paper introduces Cascadia, a D-SpMT multicore architecture that provides multigrain thread-level support and is used to evaluate the performance of several benchmarks. Cascadia applies a unique sustainable IPC (sIPC) metric on a comprehensive loop tree to select the best performing nested loop level to multithread. This paper also discusses the relationships that loops have on one another, in particular, how loop nesting levels can be extended through procedures. In addition, a detailed study is provided on the effects that thread granularity and interthread dependencies have on the entire system. © 2010 IEEE.},
  affiliation     = {NVIDIA Corporation, 20400 NW Amberwood Dr. #100, Beaverton, OR 97006, United States; School of Electrical Engineering and Computer Science, Oregon State University, 3117 Kelley Engineering Center, Corvallis, OR 97331, United States},
  art_number      = {4803832},
  author_keywords = {Multicore processors; Multithreading processors; Simulation; Speculative multithreading},
  document_type   = {Article},
  doi             = {10.1109/TPDS.2009.47},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-72649097104&doi=10.1109%2fTPDS.2009.47&partnerID=40&md5=08641c4d1a1ca2061eae7782aed62276},
}

@Book{Datta2010,
  title         = {Auto-tuning stencil computations on multicore and accelerators},
  year          = {2010},
  author        = {Datta, K. and Williams, S. and Volkov, V. and Carter, J. and Oliker, L. and Shalf, J. and Yelick, K.},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {The recent transformation from an environment where gains in computational performance came from increasing clock frequency and other hardware engineering innovations, to an environment where gains are realized through the deployment of ever increasing numbers of modest performance cores has profoundly changed the landscape of scientific application programming. This exponential increase in core count represents both an opportunity and a challenge: access to petascale simulation capabilities and beyond will require that this concurrency be efficiently exploited. The problem for application programmers is further compounded by the diversity of multicore architectures that are now emerging [4]. From relatively complex out-of-order CPUs with complex cache structures, to relatively simple cores that support hardware multithreading, to chips that require explicit use of software controlled memory, designing optimal code for these different platforms represents a serious impediment. An emerging solution to this problem is auto-tuning: the automatic generation of many versions of a code kernel that incorporate various tuning strategies, and the benchmarking of these to select the highest performing version. Typical tuning strategies might include: maximizing incore performance with loop unrolling and restructuring; maximizing memory bandwidth by exploiting non-uniform memory access (NUMA), engaging prefetch by directives; and minimizing memory traffic by cache blocking or array padding. Often a key parameter is associated with each tuning strategy (e.g., the amount of loop unrolling or the cache blocking factor), and these parameters must be explored in addition to the layering of the basic strategies themselves. © 2011 by Taylor and Francis Group, LLC.},
  affiliation   = {University of California, Berkeley, Berkeley, CA, United States; Lawrence Berkeley National Laboratory, Berkeley, CA, United States; University of California, Davis, Davis, CA, United States},
  document_type = {Book Chapter},
  doi           = {10.1201/b10376},
  journal       = {Scientific Computing with Multicore and Accelerators},
  pages         = {219-254},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-80053283808&doi=10.1201%2fb10376&partnerID=40&md5=5d5cdac517d17ffa2ab6e53230dfcff9},
}

@Conference{Kerr2009,
  author        = {Kerr, A. and Diamos, G. and Yalamanchili, S.},
  title         = {A characterization and analysis of PTX kernels},
  year          = {2009},
  pages         = {3-12},
  note          = {cited By 74},
  __markedentry = {[Nichl:6]},
  abstract      = {General purpose application development for GPUs (GPGPU) has recently gained momentum as a cost-effective approach for accelerating data- and compute-intensive applications. It has been driven by the introduction of C-based programming environments such as NVIDIA's CUDA [1], OpenCL [2], and Intel's Ct [3]. While significant effort has been focused on developing and evaluating applications and software tools, comparatively little has been devoted to the analysis and characterization of applications to assist future work in compiler optimizations, application re-structuring, and micro-architecture design. This paper proposes a set of metrics for GPU workloads and uses these metrics to analyze the behavior of GPU programs. We report on an analysis of over 50 kernels and applications including the full NVIDIA CUDA SDK and UIUC's Parboil Benchmark Suite covering control flow, data flow, parallelism, and memory behavior. The analysis was performed using a full function emulator we developed that implements the NVIDIA virtual machine referred to as PTX (Parallel Thread eXecution architecture) - a machine model and low level virtual ISA that is representative of ISAs for data parallel execution. The emulator can execute compiled kernels from the CUDA compiler, currently supports the full PTX 1.4 specification [4], and has been validated against the full CUDA SDK. The results quantify the importance of optimizations such as those for branch reconvergence, the prevalance of sharing between threads, and highlights opportunities for additional parallelism. © 2009 IEEE.},
  affiliation   = {School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta GA 30332-0250, United States},
  art_number    = {5306801},
  document_type = {Conference Paper},
  doi           = {10.1109/IISWC.2009.5306801},
  journal       = {Proceedings of the 2009 IEEE International Symposium on Workload Characterization, IISWC 2009},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70649104826&doi=10.1109%2fIISWC.2009.5306801&partnerID=40&md5=5741eb462cf563b8db6a43d68351d358},
}

@Conference{Teuteberg2009,
  author          = {Teuteberg, F. and Kluth, M. and Smolnik, S. and Ahlemann, F.},
  title           = {Semantic benchmarking of process models - An ontology-based approach},
  year            = {2009},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {This article suggests an approach which allows the costly analysis of processes, e.g., in service oriented architectures for benchmarking to be partially automated, so that the performance indicators, as well as qualitative differences between processes become apparent. The approach is based on using appropriate ontologies, which make the process models both syntactically and semantically comparable. In this article, we present a conceptual model for this new approach to process benchmarking, a framework, as well as a software prototype for analyzing and comparing individual process models. We provide an overview of our multi-method evaluation methodology and delineate the technical, conceptual, and economic evaluation perspectives with their respective outcomes. This analysis allowed us to determine whether our approach is generally suitable for generating novel and useful information on different process models that describe the same problem domain.},
  affiliation     = {University of Osnabrück, Institute of Information Management and Corporate Governance, Germany; European Business School (EBS), Institute of Research on Information Systems (IRIS), Germany},
  author_keywords = {Benchmarking; Business process management; Model analysis; Ontologies; Process modeling},
  document_type   = {Conference Paper},
  journal         = {ICIS 2009 Proceedings - Thirtieth International Conference on Information Systems},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84870967248&partnerID=40&md5=d35989be704ab8fe733035a7bb2f1f66},
}

@Conference{Jamal2009,
  author          = {Jamal, M.H. and Mustafa, G. and Waheed, A. and Mahmood, W.},
  title           = {An extensible infrastructure for benchmarking multi-core processors based systems},
  year            = {2009},
  pages           = {13-20},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {With wide adoption of multi-core processor based systems, there is a need for benchmarking such systems at both application and operating system levels. Developing benchmarks for multi-core systems is a cumbersome task due to underlying parallel architecture and complexity of parallel programming paradigms. In this paper, we introduce Multi-core Processor Architecture and Communication (MPAC) benchmarking library, which provides a common infrastructure for developing specification-driven micro-benchmarks, application benchmarks, and network traffic load generators. We describe the software architecture of MPAC and demonstrate its efficacy by implementing the specifications of well-known Stream and Netperf micro-benchmarks. We use these benchmarks to validate MPAC based performance measurements for single thread on Intel, AMD, and Cavium multi-core processors based platforms. We also develop a CPU micro-benchmark using our own specifications. In addition, we extend these micro-benchmarks through MPAC library to measure the scaling characteristics of our target multi-core processors based platforms.},
  affiliation     = {Al-Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan},
  author_keywords = {Benchmarking; Experimental design, analysis, and control; Load generation; Multi-core Processors; Performance measurement},
  document_type   = {Conference Paper},
  journal         = {International Symposium on Performance Evaluation of Computer and Telecommunication Systems 2009, SPECTS 2009, Part of the 2009 Summer Simulation Multiconference, SummerSim 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84871527766&partnerID=40&md5=20aeedd0c6c1316aea6b6d60b5739170},
}

@Conference{Stuermer2009,
  author        = {Stürmer, M. and Wellein, G. and Hager, G. and Köstler, H. and Rüde, U.},
  title         = {Challenges and potentials of emerging multicore architectures},
  year          = {2009},
  pages         = {551-566},
  note          = {cited By 9},
  __markedentry = {[Nichl:6]},
  abstract      = {We present performance results on two current multicore architectures, a STI (Sony, Toshiba, and IBM) Cell processor included in the new Playstation™ 3 and a Sun UltraSPARC T2 ("Niagara 2") machine. On the Niagara 2 we analyze typical performance patterns that emerge from the peculiar way the memory controllers are activated on this chip using the standard STREAM benchmark and a shared-memory parallel lattice Boltzmann code. On the Cell processor we measure the memory bandwidth and run performance tests for LBM simulations. Additionally, we show results for an application in image processing on the Cell processor, where it is required to solve nonlinear anisotropic PDEs.},
  affiliation   = {Lehrstuhl F. Systemsimulation, Universität Erlangen-Nürnberg, Cauerstraße 6, 91058 Erlangen, Germany; Regionales Rechenzentrum Erlangen, Martensstraße 1, 91058 Erlangen, Germany},
  document_type = {Conference Paper},
  journal       = {High Performance Computing in Science and Engineering, Garching/Munich 2007 - Transactions of the 3rd Joint HLRB and KONWIHR Status and Result Workshop},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84897583703&partnerID=40&md5=93b1f81b029573a46ed36355e989b254},
}

@Conference{Roubtsov2009,
  author          = {Roubtsov, S. and Serebrenik, A. and Van Brand, M.D.},
  title           = {Dn-based design quality comparison of industrial java applications},
  year            = {2009},
  pages           = {95-101},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {The normalized distance from the main sequence, denoted Dn, is a popular object-oriented metric introduced by Martin in 1994. While the metric has been designed for assessment of individual packages it has also been applied in practice to quality assessment of entire software architectures. This gap between the industrial practice and theoretical understanding has been recently addressed for Java open-source systems. Based on study of a benchmarks collection the authors proposed a statistical model characterizing (a) the average value of Dn, and (b) distribution of Dn. Contribution of the current work is twofold. First, we show feasibility of application of the Dn-based assessment above to commercial Java applications. Second, we validate the approach by showing that the results obtained are consistent with those obtained by means of a series of independent studies, such as layering, presence of cyclic dependencies and Chidamber's and Kemerer's metrics. ©2009 IEEE.},
  affiliation     = {Technische Universiteit Eindhoven, P.O. Box 513, 5600 MB Eindhoven, Netherlands},
  art_number      = {5501182},
  author_keywords = {Java; Maintainability; Software architecture; Software metrics},
  document_type   = {Conference Paper},
  doi             = {10.1109/CEE-SECR.2009.5501182},
  journal         = {2009 5th Central and Eastern European Software Engineering Conference in Russia, CEE-SECR 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77955260186&doi=10.1109%2fCEE-SECR.2009.5501182&partnerID=40&md5=f4a618ad21e1395e32e351cb18efc698},
}

@Conference{Duran2009,
  author          = {Duran, A. and Teruel, X. and Ferrer, R. and Martorell, X. and Ayguadé, E.},
  title           = {Barcelona openMP tasks suite: A set of benchmarks targeting the exploitation of task parallelism in openMP},
  year            = {2009},
  pages           = {124-131},
  note            = {cited By 167},
  __markedentry   = {[Nichl:6]},
  abstract        = {Traditional parallel applications have exploited regular parallelism, based on parallel loops. Only a few applications exploit sections parallelism. With the release of the new OpenMP specification (3.0), this programming model supports tasking. Parallel tasks allow the exploitation of irregular parallelism, but there is a lack of benchmarks exploiting tasks in OpenMP. With the current (and projected) multicore architectures that offer many more alternatives to execute parallel applications than traditional SMP machines, this kind of parallelism is increasingly important. And so, the need to have some set of benchmarks to evaluate it. In this paper, we motivate the need of having such a benchmarks suite, for irregular and/or recursive task parallelism. We present our proposal, the Barcelona OpenMP Tasks Suite (BOTS), with a set of applications exploiting regular and irregular parallelism, based on tasks. We present an overall evaluation of the BOTS benchmarks in an Altix system and we discuss some of the different experiments that can be done with the different compilation and runtime alternatives of the benchmarks. © 2009 IEEE.},
  affiliation     = {Computer Sciences Department, Barcelona Supercomputing Center, Jordi Girona, 31, Barcelona, Spain; Departament d'Arquitectura de Computadors, Universitat Politécnica de Catalunya, Jordi Girona, 1-3, Barcelona, Spain},
  art_number      = {5361951},
  author_keywords = {Benchmark suite; OpenMP; Task parallelism},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICPP.2009.64},
  journal         = {Proceedings of the International Conference on Parallel Processing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77951455429&doi=10.1109%2fICPP.2009.64&partnerID=40&md5=d5c112832ec9f08f168d09a543754601},
}

@Conference{Ferrer2009,
  author        = {Ferrer, R. and Beltran, V. and González, M. and Martorell, X. and Ayguadé, E.},
  title         = {Achieving high memory performance from heterogeneous architectures with the SARC programming model},
  year          = {2009},
  pages         = {15-21},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {Current heterogeneous multicore architectures, including the Cell/B.E., GPUs, and future developments, like Larrabee, require enormous programming efforts to efficiently run current parallel applications, achieving high performance. In this paper, we want to present the results we obtain from the coding with the SARC Programming Model, of two benchmarks, matrix multiply and conjugate gradient (NAS CG), with respect memory bandwidth. We show some sample loops annotated and the experience that we got trying to have our system executing them efficienly. Results indicate that the programming model is able to achieve up to 85% of the peak memory bandwidth on the Cell/B.E. processor. © 2009 ACM.},
  affiliation   = {Barcelona Supercomputing Center, Universitat Politècnica de Catalunya, Barcelona, Spain},
  document_type = {Conference Paper},
  doi           = {10.1145/1621960.1621963},
  journal       = {ACM International Conference Proceeding Series},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-74549167930&doi=10.1145%2f1621960.1621963&partnerID=40&md5=4058c6997f417d9fca5ea621ec815a8a},
}

@Conference{Dimoulas2009,
  author          = {Dimoulas, C. and Pucella, R. and Felleisen, M.},
  title           = {Future contracts},
  year            = {2009},
  pages           = {195-206},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {Many recent research projects focus on language support for behavioral software contracts, that is, assertions that govern the boundaries between software building blocks such as procedures, classes, or modules. Contracts primarily help locate bugs in programs, but they also tend to affect the performance of the program, especially as they become complex. In this paper, we introduce future contracts and parallel contract checking: software contracts annotated with future are checked in parallel with the main program, exploiting the now-common multiple-core architecture. We present both a model and a prototype implementation of our language design. Our model comprises a higher-order imperative language and we use it to prove the correctness of our design. Our implementation is robust enough to measure the performance of reasonably large benchmarks, demonstrating that the use of future contracts can lead to significant performance improvements. Copyright © 2009 ACM.},
  affiliation     = {Northeastern University, Boston, MA, United States},
  author_keywords = {Behavioral specifications; Contracts; Higher-order functions; Software reliability},
  document_type   = {Conference Paper},
  doi             = {10.1145/1599410.1599435},
  journal         = {PPDP'09 - Proceedings of the 11th International ACM SIGPLAN Symposium on Principles and Practice of Declarative Programming},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70450284428&doi=10.1145%2f1599410.1599435&partnerID=40&md5=5f1a7bc210337633c337b71861dba855},
}

@Conference{Biswas2009,
  author          = {Biswas, S. and Franklin, D. and Savage, A. and Dixon, R. and Sherwood, T. and Chong, F.T.},
  title           = {Multi-execution: Multicore caching for data-similar executions},
  year            = {2009},
  pages           = {164-173},
  note            = {cited By 27},
  __markedentry   = {[Nichl:6]},
  abstract        = {While microprocessor designers turn to multicore architectures to sustain performance expectations, the dramatic increase in parallelism of such architectures will put substantial demands on off-chip bandwidth and make the memory wall more significant than ever. This paper demonstrates that one profitable application of multicore processors is the execution of many similar instantiations of the same program. We identify that this model of execution is used in several practical scenarios and term it as "multi-execution." Often, each such instance utilizes very similar data. In conventional cache hierarchies, each instance would cache its own data independently. We propose the Mergeable cache architecture that detects data similarities and merges cache blocks, resulting in substantial savings in cache storage requirements. This leads to reductions in off-chip memory accesses and overall power usage, and increases in application performance. We present cycle-accurate simulation results of 8 benchmarks (6 from SPEC2000) to demonstrate that our technique provides a scalable solution and leads to significant speedups due to reductions in main memory accesses. For 8 cores running 8 similar executions of the same application and sharing an exclusive 4-MB, 8-way L2 cache, the Mergeable cache shows a speedup in execution by 2.5x on average (ranging from 0.93x to 6.92x), while posing an overhead of only 4.28% on cache area and 5.21% on power when it is used. Copyright 2009 ACM.},
  affiliation     = {Department of Computer Science, University of California, Santa Barbara, United States},
  author_keywords = {Cache design; CMP; Data similar execution},
  document_type   = {Conference Paper},
  doi             = {10.1145/1555754.1555777},
  journal         = {Proceedings - International Symposium on Computer Architecture},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70450231943&doi=10.1145%2f1555754.1555777&partnerID=40&md5=330d534279d8d6b64b9f8e07db82db66},
}

@Conference{Zhou2009,
  author        = {Zhou, Y. and Tan, Y.},
  title         = {GPU-based parallel particle swarm optimization},
  year          = {2009},
  pages         = {1493-1500},
  note          = {cited By 131},
  __markedentry = {[Nichl:6]},
  abstract      = {A novel parallel approach to run standard particle swarm optimization (SPSO) on Graphic Processing Unit (GPU) is presented in this paper. By using the general-purpose computing ability of GPU and based on the software platform of Compute Unified Device Architecture (CUDA) from NVIDIA, SPSO can be executed in parallel on GPU. Experiments are conducted by running SPSO both on GPU and CPU, respectively, to optimize four benchmark test functions. The running time of the SPSO based on GPU (GPU-SPSO) is greatly shortened compared to that of the SPSO on CPU (CPU-SPSO). Running speed of GPU-SPSO can be more than 11 times as fast as that of CPU-SPSO, with the same performance. compared to CPUSPSO, GPU-SPSO shows special speed advantages on large swarm population applications and hign dimensional problems, which can be widely used in real optimizing problems. © 2009 IEEE.},
  affiliation   = {Electronics Engineering and Computer Science, Department of Machine Intelligence, Peking University, Beijing 100871, China},
  art_number    = {4983119},
  document_type = {Conference Paper},
  doi           = {10.1109/CEC.2009.4983119},
  journal       = {2009 IEEE Congress on Evolutionary Computation, CEC 2009},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70450131391&doi=10.1109%2fCEC.2009.4983119&partnerID=40&md5=6733afa2572b1a1d649ee46e194d2255},
}

@Conference{Das2009,
  author          = {Das, S.R. and Hossain, A. and Li, J.F. and Petriu, E.M. and Biswas, S.N. and Jone, W.B. and Assaf, M.H.},
  title           = {Further studies on improved test efficiency in cores-based system-on-chips using ModelSim verification tool},
  year            = {2009},
  pages           = {1138-1143},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The complexity of modern digital circuits has increased enormously because of paradigm shift from system-onboard to designs embracing embedded cores-based system-onchips (SOCs). The ensuing intricacy has resulted in a huge challenge in setting up their appropriate fault analysis and testing environment. Though enormous efforts were directed to rapidly test very large-scale integration (VLSI) circuit chips under reasonable cost constraints, with technological advances, new barriers emerged. The subject paper, augmenting earlier works of authors, pertains to developing method that aims to test verify circuit architecture in a hardware-software co-design environment, specifically targeting embedded SOCs. The concept of design-for-testability (DFT) is utilized in this paper, using ModelSim simulation and verification tool, to test simulate the overall design. In earlier works, simulation experience on ISCAS 85 combinational benchmark circuits was provided. In this study, some partial simulation results on ISCAS 89 full scan sequential benchmark circuits are furnished because of space contraint, along with discussion of proposed algorithm and programming basisin a context of ModelSim. © 2009 IEEE.},
  affiliation     = {School of Information Technology and Engineering, Faculty of Engineering University of Ottawa, Ottawa, ON K1N 6N5, Canada; Department of Computer and Information Science, College of Arts and Sciences, Troy University, Montgomery, AL 36103, United States; Department of Electrical Engineering Technology, Georgia Southern University, Statesboro, GA 30460, United States; Department of Electrical and Computer Engineering, University of Cincinnati, Cincinnati, OH 45221, United States; University of Trinidad and Tobago, O'Meara Campus, Arima, Trinidad and Tobago, West Indies, Trinidad and Tobago},
  art_number      = {5168624},
  author_keywords = {Built-in self-test (BIST); Fault injection; Fault simulation; Module under test (MUT); System-on-chips (SOCs); Test pattern generator (TPG); Verilog HDL},
  document_type   = {Conference Paper},
  doi             = {10.1109/IMTC.2009.5168624},
  journal         = {2009 IEEE Intrumentation and Measurement Technology Conference, I2MTC 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70449825110&doi=10.1109%2fIMTC.2009.5168624&partnerID=40&md5=3de4188ef01235184c79d2e5e84a2fb1},
}

@Conference{Xekalakis2009,
  author          = {Xekalakis, P. and Ioannou, N. and Cintra, M.},
  title           = {Combining thread level speculation, helper threads, and runahead execution},
  year            = {2009},
  pages           = {410-420},
  note            = {cited By 18},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the current trend toward multicore architectures, improved execution performance can no longer be obtained via traditional single-thread instruction level parallelism (ILP), but, instead, via multithreaded execution. Generating thread-parallel programs is hard and thread-level speculation (TLS) has been suggested as an execution model that can speculatively exploit thread-level parallelism (TLP) even when thread independence cannot be guaranteed by the programmer/compiler. Alternatively, the helper threads (HT) execution model has been proposed where subordinate threads are executed in parallel with a main thread in order to improve the execution efficiency (i.e., ILP) of the latter. Yet another execution model, runahead execution (RA), has also been proposed where subordinate versions of the main thread are dynamically created especially to cope with long-latency operations, again with the aim of improving the execution efficiency of the main thread. Each one of these multithreaded execution models works best for different applications and application phases. In this paper we combine these three models into a single execution model and single hardware infrastructure such that the system can dynamically adapt to find the most appropriate multithreaded execution model. More specifically, TLS is favored whenever successful parallel execution of instructions in multiple threads (i.e., TLP) is possible and the system can seamlessly transition at run-time to the other models otherwise. In order to understand the tradeoffs involved, we also develop a performance model that allows one to quantitatively attribute overall performance gains to either TLP or ILP in such combined multithreaded execution model. Experimental results show that our unified execution model achieves speedups of up to 41.2%, with an average of 10.2%, over an existing state-of-the-art TLS system and speedups of up to 35.2%, with an average of 18.3%, over a flavor of runahead execution for a subset of the SPEC2000 Int benchmark suite. Copyright 2009 ACM.},
  affiliation     = {School of Informatics, University of Edinburgh, United Kingdom},
  art_number      = {1542333},
  author_keywords = {Helper threads; Multi-cores; Runahead execution; Thread-level speculation},
  document_type   = {Conference Paper},
  doi             = {10.1145/1542275.1542333},
  journal         = {Proceedings of the International Conference on Supercomputing},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70449709548&doi=10.1145%2f1542275.1542333&partnerID=40&md5=b5a4e83f0f7b8cbf074af5cb3743378f},
}

@Conference{Farooq2009,
  author        = {Farooq, U. and Parvez, H. and Marrakchi, Z. and Mehrez, H.},
  title         = {A new tree-based coarse-grained FPGA architecture},
  year          = {2009},
  pages         = {48-51},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {In this paper, we present a new multilevel hierarchical (Tree-based) coarse-grained FPGA architecture. This architecture comprises two unidirectional interconnects, a downward interconnect and an upward interconnect. The proposed architecture can support various kinds of coarse-grained blocks. These coarse-grained blocks are defined using an architecture description file. A new software flow has been developed to evaluate the proposed architecture. New tools are developed to support this software flow and they have resulted in the successful placement and routing of different DSP benchmarks on the proposed architecture. A comparison of coarse-grained Treebased and fine-grained Tree-based architectures is performed. This comparison reveals an average area gain of 41% for coarsegrained Tree-based architecture over fine-grained Tree-based architecture. Similarly a comparison of Tree-based and Meshbased coarse-grained architectures shows an average area saving of 60% for Tree-based coarse-grained architectures over Meshbased coarse-grained architectures. ©2009 IEEE.},
  affiliation   = {LIP6, Université Pierre et Marie Curie, 4, Place Jussieu, 75005 Paris, France},
  art_number    = {5201347},
  document_type = {Conference Paper},
  doi           = {10.1109/RME.2009.5201347},
  journal       = {2009 Ph.D. Research in Microelectronics and Electronics, PRIME 2009},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70449504075&doi=10.1109%2fRME.2009.5201347&partnerID=40&md5=6374f53d3470028783381df230212cef},
}

@Conference{Jamal2009a,
  author          = {Jamal, M.H. and Mustafa, G. and Waheed, A. and Mahmood, W.},
  title           = {An extensible infrastructure for benchmarking multi- core processors based systems},
  year            = {2009},
  pages           = {13-20},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {With wide adoption of multi-core processor based systems, there is a need for benchmarking such systems at both application and operating system levels. Developing benchmarks for multi-core systems is a cumbersome task due to underlying parallel architecture and complexity of parallel programming paradigms. In this paper, we introduce Multi-core Processor Architecture and Communication (MPAC) benchmarking library, which provides a common infrastructure for developing specification-driven micro-benchmarks, application benchmarks, and network traffic load generators. We describe the software architecture of MPAC and demonstrate its efficacy by implementing the specifications of well-known Stream and Netperf micro-benchmarks. We use these benchmarks to validate MPAC based performance measurements for single thread on Intel, AMD, and Cavium multi-core processors based platforms. We also develop a CPU micro-benchmark using our own specifications. In addition, we extend these micro-benchmarks through MPAC library to measure the scaling characteristics of our target multi-core processors based platforms.},
  affiliation     = {Al-Khawarizmi Institute of Computer Science, University of Engineering and Technology, Lahore, Pakistan},
  art_number      = {5224151},
  author_keywords = {And experimental design, analysis, and control; Benchmarking; Load generation; Multi-core processors; Performance measurement},
  document_type   = {Conference Paper},
  journal         = {Proceedings of the 2009 International Symposium on Performance Evaluation of Computer and Telecommunication Systems, SPECTS 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70449084900&partnerID=40&md5=762f23d30b7214850b35b71369e4602e},
}

@Conference{Maud2009,
  author          = {Maud, A.R.M. and Masud, S. and Ahmed, R.},
  title           = {Architecture level design space exploration of superscalar processor for multimedia applications},
  year            = {2009},
  pages           = {21-26},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, a variant of simulated annealing optimization has been used to derive a power efficient general purpose superscalar processor based on ARM Instruction Set Architecture. SimpleScalar architecture toolset in tandem with power estimation extension Wattch has been used for design space exploration. The use of common open source tools and models makes it easy to adapt the technique for other applications and architectures. MPEG2 decoder of the MPEG Software Simulation Group along with MP3 and JPEG decoders of MiBench Benchmark suite have been used to guide the architecture exploration. The optimization achieves an improvement in power of up to 50% for MPEG and JPEG decoders. The low transistor count and the ability of the optimum configuration to support complex real time multimedia standards makes it suitable for emerging handheld devices.},
  affiliation     = {Department of Computer Science and Engineering, Lahore University of Management Sciences, Sector-U, D.H.A., Lahore 54792, Pakistan; Department of Electrical and Computer Engineering, University of Wisconsin - Madison, MD, United States},
  art_number      = {5224148},
  author_keywords = {Architecture-level design; Design-space exploration; Multimedia applications; Power optimization; Superscalar processor},
  document_type   = {Conference Paper},
  journal         = {Proceedings of the 2009 International Symposium on Performance Evaluation of Computer and Telecommunication Systems, SPECTS 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70449094860&partnerID=40&md5=e7f5bbe1358b9a8cd01247445a832ccb},
}

@Conference{Pei2009,
  author        = {Pei, S. and Wu, B. and Du, M. and Chen, G. and Marzulo, L.A.J. and Franca, F.M.G.},
  title         = {SpMT WaveCache: Exploiting thread-level parallelism in WaveScalar},
  year          = {2009},
  volume        = {3},
  pages         = {530-535},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {Speculative Multithreading (SpMT) increases the performance by means of executing multiple threads speculatively to exploit thread-level parallelism. By combining software and hardware approaches, we have improved the capabilities of previous WaveScalar ISA on the basis of Transactional Memory system for the WaveCache Architecture. Threads are extracted at the course of static compiling, and speculatively executed as a thread-level transaction that is supported by extra hardware components, such as Thread-Context-Table (TCT) and Thread-Memory-History (TMH). We have evaluated the SpMT WaveCache with 6 real benchmarks from SPEC, Mediabench and Mibench. On the whole, the SpMT WaveCache outperforms superscalar architecture ranging from 2X to 3X, and great performance gains are achieved over original WaveCache and Transactional WaveCache as well. © 2008 IEEE.},
  affiliation   = {School of Computer Science, Fudan University, 200433 Shanghai, China; Systems Engineering and Computer Science Program, COPPE, Rio de Janeiro, Brazil},
  art_number    = {5170898},
  document_type = {Conference Paper},
  doi           = {10.1109/CSIE.2009.35},
  journal       = {2009 WRI World Congress on Computer Science and Information Engineering, CSIE 2009},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-69249230385&doi=10.1109%2fCSIE.2009.35&partnerID=40&md5=c94b2b9eb4a20302186810f7e61fadb8},
}

@Conference{Moazeni2009,
  author          = {Moazeni, M. and Bui, A. and Sarrafzadeh, M.},
  title           = {A memory optimization technique for software managed scratchpad memory in GPUs},
  year            = {2009},
  pages           = {43-49},
  note            = {cited By 18},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the appearance of massively parallel and inexpensive platforms such as the G80 generation of NVIDIA GPUs, more real-life applications will be designed or ported to these platforms. This requires structured transformation methods that remove existing application bottlenecks in these platforms. Balancing the usage of on-chip resources, used for improving the application performance, in these platforms is often non-intuitive and some applications will run into resource limits. In this paper, we present a memory optimization technique for the software-managed scratchpad memory in the G80 architecture to alleviate the constraints of using the scratchpad memory. We propose a memory optimization scheme that minimizes the usage of memory space by discovering the chances of memory reuse with the goal of maximizing the application performance. Our solution is based on graph coloring. We evaluated our memory optimization scheme by a set of experiments on an image processing benchmark suite in medical imaging domain using NVIDIA Quadro FX 5600 and CUDA. Implementations based on our proposed memory optimization scheme showed up to 37% decrease in execution time comparing to their naïve GPU implementations. ©2009 IEEE.},
  affiliation     = {Computer Science Department, University of California, Los Angeles, United States; Department of Radiological Sciences, University of California, Los Angeles, United States},
  art_number      = {5226334},
  author_keywords = {CUDA; GPU computing; Memory optimization},
  document_type   = {Conference Paper},
  doi             = {10.1109/SASP.2009.5226334},
  journal         = {2009 IEEE 7th Symposium on Application Specific Processors, SASP 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350786536&doi=10.1109%2fSASP.2009.5226334&partnerID=40&md5=51f9bbe0add7f1b69a7d3a3052e59e78},
}

@Conference{Plishker2009,
  author          = {Plishker, W. and Sane, N. and Bhattacharyya, S.S.},
  title           = {Mode grouping for more effective generalized scheduling of dynamic dataflow applications},
  year            = {2009},
  pages           = {923-926},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {For a number of years, dataflow concepts have provided designers of digital signal processing systems with environments capable of expressing high-level software architectures as well as low-level, performance-oriented kernels. To apply these proven techniques to new complex, dynamic applications, we identify repetitive sequences of atomic, repeatable actions ("modes") inside dynamic actors to expose more of the static nature of the application. In this work, we propose a mode grouping strategy that aids in the decomposition of a dynamic dataflow graph into a set of static dataflow graphs that interact dynamically. Mode grouping enables the discovery of larger static subgraphs improving scheduling results. We show that grouping modes results in improved schedules with lower memory requirements for implementations by up to 37% including a common imaging benchmark with dynamic behavior: 3D B-spline interpolation. Copyright 2009 ACM.},
  affiliation     = {Electrical and Computer Engineering Department, Institute for Advanced Computer Studies, University of Maryland, College Park, MD, United States},
  art_number      = {5227149},
  author_keywords = {Dataflow; Mode grouping; Scheduling},
  document_type   = {Conference Paper},
  journal         = {Proceedings - Design Automation Conference},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350710295&partnerID=40&md5=5d295a158510cdd86309cd69d22c7fee},
}

@Article{Mallon2009,
  author          = {Mallón, D.A. and Taboada, G.L. and Teijeiro, C. and Touriño, J. and Fraguela, B.B. and Gómez, A. and Doallo, R. and Mouriño, J.C.},
  title           = {Performance evaluation of MPI, UPC and OpenMP on multicore architectures},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2009},
  volume          = {5759 LNCS},
  pages           = {174-184},
  note            = {cited By 38},
  __markedentry   = {[Nichl:6]},
  abstract        = {The current trend to multicore architectures underscores the need of parallelism. While new languages and alternatives for supporting more efficiently these systems are proposed, MPI faces this new challenge. Therefore, up-to-date performance evaluations of current options for programming multicore systems are needed. This paper evaluates MPI performance against Unified Parallel C (UPC) and OpenMP on multicore architectures. From the analysis of the results, it can be concluded that MPI is generally the best choice on multicore systems with both shared and hybrid shared/distributed memory, as it takes the highest advantage of data locality, the key factor for performance in these systems. Regarding UPC, although it exploits efficiently the data layout in memory, it suffers from remote shared memory accesses, whereas OpenMP usually lacks efficient data locality support and is restricted to shared memory systems, which limits its scalability. © 2009 Springer Berlin Heidelberg.},
  affiliation     = {Galicia Supercomputing Center (CESGA), Santiago de Compostela, Spain; Computer Architecture Group, University of A Coruña, A Coruña, Spain},
  author_keywords = {MPI; Multicore Architectures; NAS Parallel Benchmarks (NPB); OpenMP; Performance Evaluation; UPC},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-642-03770-2-24},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350443842&doi=10.1007%2f978-3-642-03770-2-24&partnerID=40&md5=a042014c638133a5387d21267c2029fd},
}

@Article{Wang2009,
  author          = {Wang, Y. and Lafortune, S. and Kelly, T. and Kudlur, M. and Mahlke, S.},
  title           = {The theory of deadlock avoidance via discrete control},
  journal         = {ACM SIGPLAN Notices},
  year            = {2009},
  volume          = {44},
  number          = {1},
  pages           = {252-263},
  note            = {cited By 21},
  __markedentry   = {[Nichl:6]},
  abstract        = {Deadlock in multithreaded programs is an increasingly important problem as ubiquitous multicore architectures force parallelization upon an ever wider range of software. This paper presents a theoretical foundation for dynamic deadlock avoidance in concurrent programs that employ conventional mutual exclusion and synchronization primitives (e.g., multithreaded C/Pthreads programs). Beginning with control flow graphs extracted from program source code, we construct a formal model of the program and then apply Discrete Control Theory to automatically synthesize deadlockavoidance control logic that is implemented by program instrumentation. At run time, the control logic avoids deadlocks by postponing lock acquisitions. Discrete Control Theory guarantees that the program instrumented with our synthesized control logic cannot deadlock. Our method furthermore guarantees that the control logic is maximally permissive: it postpones lock acquisitions only when necessary to prevent deadlocks, and therefore permits maximal runtime concurrency. Our prototype for C/Pthreads scales to real software including Apache, OpenLDAP, and two kinds of benchmarks, automatically avoiding both injected and naturally occurring deadlocks while imposing modest runtime overheads.},
  affiliation     = {University of Michigan, United States; Hewlett-Packard Labs, United States},
  author_keywords = {Concurrent programming; Discrete control theory; Dynamic deadlock avoidance; Multicore processors; Multithreaded programming; Parallel programming},
  document_type   = {Conference Paper},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350304297&partnerID=40&md5=3453dd8bc8d12e7a4d03336d684e53d0},
}

@Conference{Zou2009,
  author          = {Zou, W.B. and Wu, Y. and Zhao, Y.},
  title           = {Automatic testing system for UHF passive RFID tag performance},
  year            = {2009},
  volume          = {2},
  pages           = {79-82},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {With the advancement of Radio Frequency Identification technology, various RFID products are emerging in our life. More and more government agencies and retailers have realized the potential of RFID technology and have mandated or recommended their suppliers to attach RFID tags to their products. The tag performance highly determines implement of these mandates and recommendations. In this paper, we discuss an Automatic testing system for UHF passive RFID tag performance. Both hardware and software architectures will be presented. © 2009 IEEE.},
  affiliation     = {Key Lab. of Integrated Microsystems, Peking University, Shenzhen Graduate School, Shenzhen, China},
  art_number      = {5116689},
  author_keywords = {Automatic testing system; Benchmarking; RFID; Tag performance},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICNDS.2009.100},
  journal         = {Proceedings - 2009 International Conference on Networking and Digital Society, ICNDS 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350075515&doi=10.1109%2fICNDS.2009.100&partnerID=40&md5=36ec80fda70703baeea70af4a04412e1},
}

@Conference{Serebrenik2009,
  author        = {Serebrenik, A. and Roubtsov, S. and Van Den Brand, M.},
  title         = {Dn-based architecture assessment of Java Open Source Software systems},
  year          = {2009},
  pages         = {198-207},
  note          = {cited By 9},
  __markedentry = {[Nichl:6]},
  abstract      = {Since their introduction in 1994 the Martin's metrics became popular in assessing object-oriented software architectures. While one of the Martin metrics, normalised distance from the main sequence Dn, has been originally designed with assessing individual packages, it has also been applied to assess quality of entire software architectures. The approach itself, however, has never been studied. In this paper we take the first step to formalising the Dn-based architecture assessment of Java Open Source software. We present two aggregate measures: average normalised distance from the main sequence D̄n, and parameter of the fitted statistical model λ. Applying these measures to a carefully selected collection of benchmarks we obtain a set of reference values that can be used to assess quality of a system architecture. Furthermore, we show that applying the same measures to different versions of the same system provides valuable insights in system architecture evolution. © 2009 IEEE.},
  affiliation   = {Eindhoven University of Technology, P.O. Box 513, 5600 MB Eindhoven, Netherlands},
  art_number    = {5090043},
  document_type = {Article},
  doi           = {10.1109/ICPC.2009.5090043},
  journal       = {IEEE International Conference on Program Comprehension},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70349996072&doi=10.1109%2fICPC.2009.5090043&partnerID=40&md5=b956131fa316ad9b1c176e89844bf74b},
}

@Conference{Momm2009,
  author          = {Momm, C. and Gebhart, M. and Abeck, S.},
  title           = {A model-driven approach for monitoring business performance in web service compositions},
  year            = {2009},
  pages           = {343-350},
  note            = {cited By 27},
  __markedentry   = {[Nichl:6]},
  abstract        = {Supporting business services through Web service compositions (WSC) as part of service-oriented architectures (SOA) involves business performance monitoring requirements. Their implementation results in additional development activities. To support these activities, we already contributed a model-driven approach to the development of monitored WSC as part of our preliminary work. In this paper, we present an extension to this approach, which focuses on supporting the specification and transformation of indicators to an executable implementation. To reduce development effort for this particular task, we provide a template-based mechanism for defining performance indicators. In combination with our preliminary work, now fully monitored WSC can be generated automatically from platformindependent design models. We demonstrate the applicability of the overall approach by instantiating an integrated development process for a target platform based on IBM SOA products and showing its application for a sample business process along with monitoring requirements. © 2009 IEEE.},
  affiliation     = {Software Engineering, FZI Research Center for Information Technology, Karlsruhe, Germany; Research Group Cooperation and Management, Universität Karlsruhe (TH), Karlsruhe, Germany},
  art_number      = {5072542},
  author_keywords = {Business performance monitoring; Business process management; Model-driven software development; Service-oriented architectures; Web service compositions},
  document_type   = {Conference Paper},
  doi             = {10.1109/ICIW.2009.57},
  journal         = {Proceedings of the 2009 4th International Conference on Internet and Web Applications and Services, ICIW 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70349743690&doi=10.1109%2fICIW.2009.57&partnerID=40&md5=7e9bc941a68feb9a2c024380df242b82},
}

@Article{Yuhong2009,
  author          = {Yuhong, Y. and Yong, L. and Roy, A. and Xinge, D.},
  title           = {Web service enabled online laboratory},
  journal         = {International Journal of Web Services Research},
  year            = {2009},
  volume          = {6},
  number          = {4},
  pages           = {75-93},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Online experimentation allows students from anywhere to operate remote instruments at any time. The current techniques constrain users to bind to products from one company and install client side software. We use Web services and Service Oriented Architecture to improve the interoperability and usability of the remote instruments. Under a service oriented architecture for online experiment system, a generic methodology to wrap commercial instruments using IVI and VISA standard as Web services is developed. We enhance the instrument Web services into stateful services so that they can manage user booking and persist experiment results. We also benchmark the performance of this system when SOAP is used as the wire format for communication and propose solutions to optimize performance. In order to avoid any installation at the client side, the authors develop Web 2.0 based techniques to display the virtual instrument panel and real time signals with just a standard Web browser. The technique developed in this article can be widely used for different real laboratories, such as microelectronics, chemical engineering, polymer crystallization, structural engineering, and signal processing. Copyright © 2009, IGI Global.},
  affiliation     = {Concordia University, Montreal, QC, Canada; National Research Council, Canada; University of New Brunswick, Canada},
  author_keywords = {E-learning; E-science; Online experiment; Online laboratory; Remote control; Web services},
  document_type   = {Article},
  doi             = {10.4018/jwsr.2009071304},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350070445&doi=10.4018%2fjwsr.2009071304&partnerID=40&md5=5ea6a46ae401b860b424e3705d464d13},
}

@Conference{Packirisamy2009,
  author        = {Packirisamy, V. and Zhai, A. and Hsu, W.-C. and Yew, P.-C. and Ngai, T.-F.},
  title         = {Exploring speculative parallelism in SPEC2006},
  year          = {2009},
  pages         = {77-88},
  note          = {cited By 28},
  __markedentry = {[Nichl:6]},
  abstract      = {The computer industry has adopted multi-threaded and multicore architectures as the clock rate increase stalled in early 2000's. It was hoped that the continuous improvement of single-program performance could be achieved through these architectures. However, traditional parallelizing compilers often fail to effectively parallelize general-purpose applications which typically have complex control flow and excessive pointer usage. Recently hardware techniques such as Transactional Memory (TM) and Thread- Level Speculation (TLS) have been proposed to simplify the task of parallelization by using speculative threads. Potential of speculative parallelism in general-purpose applications like SPEC CPU 2000 have been well studied and shown to be moderately successful. Preliminary work examining the potential parallelism in SPEC2006 deployed parallel threads with a restrictive TLS execution model and limited compiler support, and thus only showed limited performance potential. In this paper, we first analyze the cross-iteration dependence behavior of SPEC 2006 benchmarks and show that more parallelism potential is available in SPEC 2006 benchmarks, comparing to SPEC2000. We further use a state-of-the-art profile-driven TLS compiler to identify loops that can be speculatively parallelized. Overall, we found that with optimal loop selection we can potentially achieve an average speedup of 60% on four cores over what could be achieved by a traditional parallelizing compiler such as Intel's ICC compiler.We also found that an additional 11% improvement can be potentially obtained on selected benchmarks using 8 cores when we extend TLS on multiple loop levels as opposed to restricting to a single loop level. © 2009 IEEE.},
  affiliation   = {University of Minnesota, Minneapolis, United States; Intel Corporation},
  art_number    = {4919640},
  document_type = {Conference Paper},
  doi           = {10.1109/ISPASS.2009.4919640},
  journal       = {ISPASS 2009 - International Symposium on Performance Analysis of Systems and Software},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70349176695&doi=10.1109%2fISPASS.2009.4919640&partnerID=40&md5=e2394b7a2020b2715354c1f6748485a0},
}

@Conference{Badia2009,
  author          = {Badia, R.M. and Perez, J.M. and Ayguadé, E. and Labarta, J.},
  title           = {Impact of the memory hierarchy on shared memory architectures in multicore programming models},
  year            = {2009},
  pages           = {437-445},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {Many and multicore architectures put a big pressure in parallel programming but gives a unique opportunity to propose new programming models that automatically exploit the parallelism of these architectures. OpenMP is a very well known standard that exploits parallelism in shared memory architectures. SMPSs has recently been proposed as a task based programming model that exploits the parallelism at the task level and takes into account data dependencies between tasks. However, besides parallelism in the programming, the memory hierarchy impact in many/multi core architectures is a feature of large importance. This paper presents an evaluation of these two programming models with regard to the impact of different levels of the memory hierarchy in the duration of the application. The evaluation is based on tracefiles with hardware counters on the execution of a memory intensive benchmark in both programming models. © 2009 IEEE.},
  affiliation     = {Barcelona Supercomputing Center, Universitat Politècnica de Catalunya, Barcelona, Spain},
  art_number      = {4912964},
  author_keywords = {Locality exploitation; Programming models for multicore; SMP Superscalar; Task scheduling},
  document_type   = {Conference Paper},
  doi             = {10.1109/PDP.2009.56},
  journal         = {Proceedings of the 17th Euromicro International Conference on Parallel, Distributed and Network-Based Processing, PDP 2009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70349162466&doi=10.1109%2fPDP.2009.56&partnerID=40&md5=f9cdf526d207b077b05aba7e95a2f397},
}

@Conference{Tuszynski2009,
  author        = {Tuszynski, M.W.},
  title         = {Space interferometry mission flight software management challenges and lessons},
  year          = {2009},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The Space Interferometry Mission (SIM) under development at the Jet Propulsion Laboratory has been an ambitious project which when completed will determine the positions and distances of stars several hundred times more accurately than any previous program. This accuracy will allow SIM to determine the distances to stars throughout the galaxy and to probe nearby stars for Earth-sized planets. [1] However, it has been a roller coaster in terms of funding. After several false starts, the flight software development team finally ramped up in early 2004. The flight software development team built prototype programs for timing benchmarks, designed a new architecture, implemented a core infrastructure usable by a variety of future missions, and successfully developed and infused new technology along the way, before being disbanded due to funding cuts at the beginning of 2007. 12 Involving a level of computational performance with which JPL had no previous experience on flight projects, this was a challenging and new endeavor. Precision control algorithms required loop rates of up to 500 times per second to remove disturbances and provide the stability needed by the science cameras. Even with the BAE 750 PowerPC chosen as the baseline flight processor, the computing load had to be distributed amongst three active computers joined together and synchronized using a completely new IEEE1393 ring bus implementation. At the same time the institution embarked on a software process improvement path that recruited SIM FSW development as one of the pathfinders, reaching CMMI level 2 in late 2005 and level 3 just after SIM was descoped. The goal of this paper is to walk through the challenges encountered in the management of this development effort and to share the programmatic and technical lessons learned. Some of those lessons include: 1) How and when to successfully infuse new technology into your product. 2) Soul searching decisions in selecting programming languages, bus interfaces, and CPU platforms. 3) How to deal with an institution that is trying to revamp its SW processes. Hint: resistance is futile. 4) What new SW processes really paid off and made a dramatic improvement in the manager's ability to monitor and control developers work. ©2009 IEEE.},
  affiliation   = {Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91101, United States},
  art_number    = {4839719},
  document_type = {Conference Paper},
  doi           = {10.1109/AERO.2009.4839719},
  journal       = {IEEE Aerospace Conference Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70349101315&doi=10.1109%2fAERO.2009.4839719&partnerID=40&md5=b9cf7596a5493ac130e36f0c7fcc5e32},
}

@Article{Bertasi2009,
  author        = {Bertasi, P. and Bressan, M. and Peserico, E.},
  title         = {Psort, yet another fast stable sorting software},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2009},
  volume        = {5526 LNCS},
  pages         = {76-88},
  note          = {cited By 5},
  __markedentry = {[Nichl:6]},
  abstract      = {psort was the fastest sorting software in 2008 according to the Pennysort benchmark, sorting 181GB of data for 0.01 of computer time. This paper details its internals, and the careful fitting of its architecture to the structure of modern PCs-class platforms, allowing it to outperform state-of-the-art sorting software such as GNUsort or STXXL. © 2009 Springer Berlin Heidelberg.},
  affiliation   = {Dipartimento di Ingegneria dell'Informazione, Università Degli Studi di Padova, Italy},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-642-02011-7_9},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-68749083711&doi=10.1007%2f978-3-642-02011-7_9&partnerID=40&md5=c3f7bf8e062675384b950f74c9b83e34},
}

@Article{Cho2009,
  author          = {Cho, C.-B. and Zhang, W. and Li, T.},
  title           = {Thermal design space exploration of 3d die stacked multi-Core processors using geospatial-Based predictive models},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2009},
  volume          = {5419 LNCS},
  pages           = {102-120},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents novel 2D geospatial-based predictive models for exploring the complex thermal spatial behavior of three-dimensional (3D) die stacked multi-core processors at the early design stage. Unlike other analytical techniques, our predictive models can forecast the location, size and temperature of thermal hotspots. We evaluate the efficiency of using the models for predicting within-die and cross-dies thermal spatial characteristics of 3D multicore architectures with widely varied design choices (e.g. microarchitecture, floor-plan and packaging). Our results show the models achieve high accuracy while maintaining low complexity and computation overhead. ©Springer-Verlag Berlin Heidelberg 2009.},
  affiliation     = {Intelligent Design of Efficient Architecture Lab(IDEAL), Department of ECE, University of Florida, United States},
  author_keywords = {3D Die stacking; Analytical modeling; Multi-core architecture; Thermal/power Characterization},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-540-93799-9_7},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-67650694964&doi=10.1007%2f978-3-540-93799-9_7&partnerID=40&md5=9b5333dda46be49de5daa93b5c680cfc},
}

@Article{2009,
  title         = {Software Engineering - International Summer Schools, ISSSE 2006-2008, Revised Tutorial Lectures},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2009},
  volume        = {5413},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 9 papers. The topics discussed include: the future of software: adaptation and dependability; autonomic computing now you see it, now you don't: design and evolution of autonomic software systems; impact of usability on software requirements and design; service-oriented architectures testing: a survey; the PLASTIC framework and tools for testing service-oriented; architecture reconstructions: tutorial on reverse engineering to the architectural level; collaboration in distributed software development; web coast estimation and productivity benchmarking; and knowledge base and experience factory for empowering competitiveness.},
  document_type = {Conference Review},
  page_count    = {264},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-75549091790&partnerID=40&md5=13a06abc87867ff3e5d5f22f27d3571c},
}

@Article{Lee2009,
  author          = {Lee, T.-Y. and Fan, Y.-H. and Tsai, C.-C.},
  title           = {Adaptive multi-constraints in hardware-software partitioning for embedded multiprocessor FPGA systems},
  journal         = {WSEAS Transactions on Computers},
  year            = {2009},
  volume          = {8},
  number          = {2},
  pages           = {334-343},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {An embedded multiprocessor field programmable gate array (FPGA) system has a powerful and flexible architecture that the interaction between hardware circuits and software applications. Modern electronic products, such as portable devices, consumer electronics and telematics, can be evaluated rapidly in this platform via the implementation of a set of hardware and software tasks. However, the functionality is markedly increased, resulting in a significant raise in the number of hardware and software tasks. Consequently, too large of a solution space is formed to achieve hardware-software partitioning. Moreover, a partitioning result with low power consumption and fast execution time is difficult to obtain since meeting simultaneously multi-constraints from hundreds of thousands of combinations of hardware-software partitions is difficult. Thus, this work presents a hardware-software partitioning scheme that can obtain a partitioning result that satisfies multi-constraints from massive solution space. Specifically, this study attains a partitioning result with low power consumption and fast execution time. The effectiveness of the proposed approach is demonstrated by assessing a JPEG encoding system and a benchmark with 199 tasks.},
  affiliation     = {Graduate Institute of Computer and Communication, National Taipei Univ. of Technology, Taipei, Taiwan; Dept. of Computer Science and Information Eng., National Taitung Univ., Taitung, Taiwan; Dept. of Computer Science and Information Eng., Nanhua Univ., Chia-Yi, Taiwan},
  author_keywords = {Adaptive multi-constraints partitioning; Embedded multiprocessor system; FPGA system; Hardware-software codesign; Hardware-software partitioning},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-66349113705&partnerID=40&md5=808d59e73fbb00677b7f09b738115a06},
}

@Conference{Kamalakannan2009,
  author          = {Kamalakannan, S. and Gururajan, A. and Shahriar, M. and Hill, M.M. and Anderson, J. and Sari-Sarraf, H. and Hequet, E.F.},
  title           = {Assessing fabric stain release using a GPU implementation of statistical snakes},
  year            = {2009},
  volume          = {7251},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {Stain release is the degree to which a stained substrate approaches its original unsoiled appearance as a result of care procedure. Stain release has a significant impact on the pricing of the fabric and, hence, needs to be quantified in an objective manner. In this paper, an automatic approach for the objective assessment of fabric stain release that utilizes region-based statistical snakes, is presented. This deformable contour approach employs a pressure energy term in the parametric snake model in conjunction with statistical information (hence, statistical snakes) extracted from the image to segment the stain and subsequently assign a stain release grade. This algorithm has been parallelized on a General Purpose Graphical Processing Unit (GPGPU) for accelerated and simultaneous segmentation of multiple stains on a fabric. The computational power of the GPGPU is attributed to its hardware and software architecture, which enables multiple and identical snake kernels to be processed in parallel on several streaming processors. The detection and segmentation results of this machine vision scheme are illustrated as part of the validation study. These results establish the efficacy of the proposed approach in producing accurate results in a repeatable manner. In addition, this paper presents a comparison between the benchmarking results for the algorithm on the CPU and the GPGPU. © 2009 SPIE.},
  affiliation     = {Dept. of Electrical Engineering, Texas Tech Univ., Box 43102, Lubbock, TX 79409-3102, United States; Fiber and Biopolymer Research Institute, P.O. Box 45019, Lubbock, TX 79409-5019, United States},
  art_number      = {725108},
  author_keywords = {Energy-minimization; General Purpose Graphical Processing Units; Segmentation; Stain release; Statistical Snakes},
  document_type   = {Conference Paper},
  doi             = {10.1117/12.806370},
  journal         = {Proceedings of SPIE - The International Society for Optical Engineering},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-65949120945&doi=10.1117%2f12.806370&partnerID=40&md5=bb76c0c0fb18e65948889cc2feee4697},
}

@Article{Xiao2009,
  author          = {Xiao, W. and Ren, J. and Yang, Q.},
  title           = {A case for continuous data protection at block level in disk array storages},
  journal         = {IEEE Transactions on Parallel and Distributed Systems},
  year            = {2009},
  volume          = {20},
  number          = {6},
  pages           = {898-911},
  note            = {cited By 20},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents a study of data storages for continuous data protection (CDP). After analyzing the existing data protection technologies, we propose a new disk array architecture that provides Timely Recovery to Any Point-in-time, referred to as TRAP. TRAP stores not only the data stripe upon a write to the array but also the time-stamped Exclusive ors (xors) of successive writes to each data block. By leveraging the xor operations that are performed upon each block write in today's RAID4/5 controllers, TRAP does not incur noticeable performance overhead. More importantly, TRAP is able to recover data very quickly to any point-in-time upon data damage by tracing back the sequence and history of xors resulting from writes. What is interesting is that the TRAP architecture is very space efficient. We have implemented a prototype of the new TRAP architecture using software at the block level and carried out extensive performance measurements using TPC-C benchmarks running on Oracle and Postgres databases, TPC-W running on a MySQL database, and file system benchmarks running on Linux and Windows systems. Our experiments demonstrated that TRAP not only is able to recover data to any point-in-time very quickly upon a failure but also uses less storage space than traditional daily incremental backup/snapshot. Compared to the state-of-the-art CDP technologies, TRAP saves disk storage space by one to two orders of magnitude with a simple and a fast encoding algorithm. In addition, TRAP can provide two-way data recovery with the availability of only one reference image in contrast to the one-way recovery of snapshot and incremental backup technologies. © 2009 IEEE.},
  affiliation     = {Department of Electrical, Computer, and Biomedical Engineering, University of Rhode Island, Kingston, RI 02881, United States},
  author_keywords = {Data backup; Data protection and recovery; Data storage; Disk array; Disk I/O},
  document_type   = {Article},
  doi             = {10.1109/TPDS.2008.154},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-67349132286&doi=10.1109%2fTPDS.2008.154&partnerID=40&md5=ffe3388ac7e3df869bb26b3f96feb20f},
}

@Article{Li2009,
  author          = {Li, M. and Novo, D. and Bougard, B. and Carlson, T. and Van Der Perre, L. and Catthoor, F.},
  title           = {Generic multiphase software pipelined partial FFT on instruction level parallel architectures},
  journal         = {IEEE Transactions on Signal Processing},
  year            = {2009},
  volume          = {57},
  number          = {4},
  pages           = {1604-1615},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {The partial fast Fourier transform (PFFT) is an extended fast Fourier transformation (FFT) where only part of the input or output bins are used. By pruning useless data flow, it is possible to achieve a significant speedup in many important applications. Although theoretical aspects of the PFFT have been thoroughly studied in the past three decades, efficient and generic implementations were rarely reported. The most important obstacle for the optimization of the PFFT is the highly irregular data flow and the associated control flow. In addition, a size-N PFFT has 2N possibilities of data flow patterns, so finding a flexible but efficient implementation is very challenging. Our contribution is a generic method to map the highly irregular data flow of an arbitrary PFFT onto instruction level parallel architectures using software pipelining. By leveraging the algorithmic level flexibilities in a FFT, we select an appropriate data flow variant that enables aggressive optimizations in implementation schemes. Then, we apply a divide and conquer strategy, partitioning the PFFT into three phases. For each phase, we introduce specialized control structures, loop structures, address generation schemes and memory operations. This reduces cycle count, number of executed instructions and memory accesses. By studying ten representative benchmarks from wireless baseband applications, we are able to produce repeatable and successful results on the TMS320C6000. When comparing to two optimized FFT implementations, our work reduces the cycle count by 20.5% to 87.5%, executed instructions by 11.2% to 86.5% and L1D and L1P cache accesses by 16.1% to 79.4% and 19.5% to 87.1% respectively. To the best of our knowledge, this is the first reported work about a generic software pipelined PFFT for instruction level parallel architectures. © 2009 IEEE.},
  affiliation     = {ESAT, K.U.Leuven, Leuven, Belgium; Nomadic Embedded System Division, IMEC, Leuven, Belgium},
  author_keywords = {FFT; ILP; OFDMA; PFFT; VLIW},
  document_type   = {Article},
  doi             = {10.1109/TSP.2008.2010422},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-63449117777&doi=10.1109%2fTSP.2008.2010422&partnerID=40&md5=b327360c378e1ba92422fc469e0c7ced},
}

@Article{Osman2009,
  author          = {Osman, R. and Awan, I. and Woodward, M.E.},
  title           = {Application of Queueing Network Models in the Performance Evaluation of Database Designs},
  journal         = {Electronic Notes in Theoretical Computer Science},
  year            = {2009},
  volume          = {232},
  number          = {C},
  pages           = {101-124},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, we model database designs using queueing networks, giving visibility to the dynamic behaviour of the database design and allowing the database designer to experiment with different design decisions. Our approach is to abstract away the more detailed levels of the database system design by concentrating on the information that is available to the database designer at design time. It differs from other methods of database system performance evaluation in that the performance assessment is specifically targeted at the database design, not at the database system software architecture. We present a bottleneck evaluation of the Transaction Processing Performance Council TPC-C benchmark under different workload conditions and demonstrate how this affects database design decisions. © 2009 Elsevier B.V. All rights reserved.},
  affiliation     = {Department of Computing, University of Bradford, Bradford, West Yorkshire, United Kingdom},
  author_keywords = {Database design performance; queueing networks},
  document_type   = {Article},
  doi             = {10.1016/j.entcs.2009.02.053},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-62649103371&doi=10.1016%2fj.entcs.2009.02.053&partnerID=40&md5=03494e7c897b7e304d393890f3003699},
}

@Article{Gebhart2009,
  author        = {Gebhart, M. and Maher, B.A. and Coons, K.E. and Diamond, J. and Gratz, P. and Marino, M. and Ranganathan, N. and Robatmili, B. and Smith, A. and Burrill, J. and Keckler, S.W. and Burger, D. and McKinley, K.S.},
  title         = {An evaluation of the TRIPS computer system},
  journal       = {ACM SIGPLAN Notices},
  year          = {2009},
  volume        = {44},
  number        = {3},
  pages         = {1-12},
  note          = {cited By 27},
  __markedentry = {[Nichl:6]},
  abstract      = {The TRIPS system employs a new instruction set architecture (ISA) called Explicit Data Graph Execution (EDGE) that renegotiates the boundary between hardware and software to expose and exploit concurrency. EDGE ISAs use a block-atomic execution model in which blocks are composed of dataflow instructions. The goal of the TRIPS design is to mine concurrency for high performance while tolerating emerging technology scaling challenges, such as increasing wire delays and power consumption. This paper evaluates how well TRIPS meets this goal through a detailed ISA and performance analysis. We compare performance, using cycles counts, to commercial processors. On SPEC CPU2000, the Intel Core 2 outperforms compiled TRIPS code in most cases, although TRIPS matches a Pentium 4. On simple benchmarks, compiled TRIPS code outperforms the Core 2 by 10% and hand-optimized TRIPS code outperforms it by factor of 3. Compared to conventional ISAs, the block-atomic model provides a larger instruction window, increases concurrency at a cost of more instructions executed, and replaces register and memory accesses with more efficient direct instruction-to-instruction communication. Our analysis suggests ISA, microarchitecture, and compiler enhancements for addressing weaknesses in TRIPS and indicates that EDGE architectures have the potential to exploit greater concurrency in future technologies. Copyright © 2009 ACM.},
  affiliation   = {Department of Computer Sciences, University of Texas, Austin, United States},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-67650844210&partnerID=40&md5=d24771fcb77ad90f4f8005a8d0ec8096},
}

@Article{Howes2009,
  author        = {Howes, L.W. and Lokhmotov, A. and Donaldson, A.F. and Kelly, P.H.J.},
  title         = {Deriving efficient data movement from decoupled access/execute specifications},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2009},
  volume        = {5409 LNCS},
  pages         = {168-182},
  note          = {cited By 25},
  __markedentry = {[Nichl:6]},
  abstract      = {On multi-core architectures with software-managed memories, effectively orchestrating data movement is essential to performance, but is tedious and error-prone. In this paper we show that when the programmer can explicitly specify both the memory access pattern and the execution schedule of a computation kernel, the compiler or run-time system can derive efficient data movement, even if analysis of kernel code is difficult or impossible. We have developed a framework of C++ classes for decoupled Access/Execute specifications, allowing for automatic communication optimisations such as software pipelining and data reuse. We demonstrate the ease and efficiency of programming the Cell Broadband Engine architecture using these classes by implementing a set of benchmarks, which exhibit data reuse and non-affine access functions, and by comparing these implementations against alternative implementations, which use hand-written DMA transfers and software-based caching. © 2009 Springer Berlin Heidelberg.},
  affiliation   = {Department of Computing, Imperial College London, 180 Queen's Gate, London SW7 2AZ, United Kingdom; Codeplay Software, 45 York Place, Edinburgh EH1 3HP, United Kingdom},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-540-92990-1_14},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-59049083516&doi=10.1007%2f978-3-540-92990-1_14&partnerID=40&md5=a859bba205d0e126f1cf35d64fdc9074},
}

@Conference{Dugerdil2008,
  author          = {Dugerdil, P. and Jossi, S.},
  title           = {Empirical assessment of execution trace segmentation in reverse-engineering},
  year            = {2008},
  volume          = {SE},
  number          = {GSDCA/M/-},
  pages           = {20-27},
  note            = {cited By 5},
  __markedentry   = {[Nichl:6]},
  abstract        = {Reverse-engineering methods using dynamic techniques rests on the post-mortem analysis of the execution trace of the programs. However, one key problem is to cope with the amount of data to process. In fact, such a file could contain hundreds of thousands of events. To cope with this data volume, we recently developed a trace segmentation technique. This lets us compute the correlation between classes and identify cluster of closely correlated classes. However, no systematic study of the quality of the clusters has been conducted so far. In this paper we present a quantitative study of the performance of our technique with respect to the chosen parameters of the method. We then highlight the need for a benchmark and present the framework for the study. Then we discuss the matching metrics and present the results we obtained on the analysis of two very large execution traces. Finally we define a clustering quality metrics to identify the parameters providing the best results.},
  affiliation     = {Department of Information Systems, HEG-University of Applied Sciences, 7 rte de Drize, 1227 Geneva, Switzerland},
  author_keywords = {Dynamic analysis; Empirical study; Reverse-engineering; Software architecture},
  document_type   = {Conference Paper},
  journal         = {ICSOFT 2008 - Proceedings of the 3rd International Conference on Software and Data Technologies},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-57649233624&partnerID=40&md5=ef1743e1d9bc35a0aa1757c68db57b64},
}

@Article{2008,
  title         = {Performance Evaluation: Metrics, Models and Benchmarks - SPEC International Performance Evaluation Workshop, SIPEW 2008, Proceedings},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2008},
  volume        = {5119 LNCS},
  note          = {cited By 2},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 19 papers. The topics discussed include: SAP standard application benchmarks - it benchmarks with a business focus; the relationship of performance models to data; extracting response times from fluid analysis of performance models; approximate solution of a PEPA model of a key distribution center; a model transformation from the Palladio component model to layered queueing networks; model-driven generation of performance prototypes; generating probabilistic and intensity-varying workload for web-based software systems; tuning topology generators using spectral distributions; performance, benchmarking and sizing in developing highly scalable enterprise software; phase-type approximations for message transmission times in web services reliable messaging; a framework for simulation models of service-oriented architectures; and model-driven performability analysis of composite web services.},
  document_type = {Conference Review},
  page_count    = {331},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70349914521&partnerID=40&md5=6d04e7198d81b1e3a755ffee50aa1bad},
}

@Conference{Yang2008,
  author          = {Yang, C.-L. and Chang, Y.-K. and Chu, C.-P.},
  title           = {Modeling services to construct service-oriented healthcare architecture for digital home-care business},
  year            = {2008},
  pages           = {351-356},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {Using Information and Communication Technologies (ICT) to enable the daily activities and interests such as dining, medicine, lifestyle, traffic, education and entertainment has recently become a world wide trend. Moreover, Service-Oriented Architecture (SOA) is nowadays one of the most important techniques to realize services in industry. Therefore, we would attempt to give attention to what type of services ICT could realize for chronic patients and how this concept should contribute to their recovery. In this paper, we would like to share our experiences in creating innovative home-care business models. We first discuss the business modeling process, which contains generating care services concepts, investigating market, defining Key Services scenarios and cooperative policies. Second, we present the constructed SOA healthcare platform. Specifically, we explain the technical issues during the development of our business models. Finally, the business models and platform are evaluated using the Key Performance Indicators (KPI) developed in the study.},
  affiliation     = {Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan; Networks and Multimedia Institute, Tainan, Taiwan},
  author_keywords = {Business modeling; Digital home-care; KPI evaluation; SOA design},
  document_type   = {Conference Paper},
  journal         = {20th International Conference on Software Engineering and Knowledge Engineering, SEKE 2008},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-78651529151&partnerID=40&md5=4b8776a9994707901b775b71466d0628},
}

@Article{Kouvakis2008,
  author        = {Kouvakis, I. and Georgatos, F.},
  title         = {A report on the effect of heterogeneity of the grid environment on a grid job},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2008},
  volume        = {4818 LNCS},
  pages         = {476-483},
  note          = {cited By 2},
  __markedentry = {[Nichl:6]},
  abstract      = {Grids include heterogeneous resources, which are based on different hardware and software architectures or components. In correspondence with this diversity of the infrastructure, the execution time of any single job, as well as the total grid performance can both be affected substantially, which can be demonstrated by measurements. In need to effectively explore this issue, we decided to apply micro benchmarking tools on a subset of sites on the EGEE infrastructure, employing in particular the lmbench suite, for it includes latency, bandwidth and timing measurements. Furthermore we retrieved and report information about sites characteristics, such as kernel version, middleware, memory size, cpu threads and more. Our preliminary conclusion is that any typical grid can largely benefit from even trivial resource characterization and match-making techniques, if we take advantage of this information upon job scheduling. These metrics, which in this case were taken from the South Eastern Europe VO of EGEE, can provide a handle to compare and select the more suitable site(s), so that we can drive the grid towards maximum capacity and optimal performance. © 2008 Springer.},
  affiliation   = {Univerity of Aegean, Department of Mathematics, Greece; University of Cyprus, Department of Computer Science, Cyprus},
  document_type = {Conference Paper},
  doi           = {10.1007/978-3-540-78827-0_54},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-70350274319&doi=10.1007%2f978-3-540-78827-0_54&partnerID=40&md5=aa7d4f4d5eaab0b78ece54a536e77888},
}

@Article{Kovari2008,
  author          = {Kovari, B. and Albert, I. and Charaf, H.},
  title           = {A general approach to off-line signature verification},
  journal         = {WSEAS Transactions on Computers},
  year            = {2008},
  volume          = {7},
  number          = {10},
  pages           = {1648-1657},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {Although automatic off-line signature verification has been extensively studied in the last three decades, there are still a huge number of open questions and even the best systems are still struggling to get better error rates than 5%. This paper targets some of the weak spots of the research area, which are comparability, measurability and interoperability of different signature verification systems. After delivering an overview of some of the main research directions, this paper proposes a generic representation of signature verifiers. In the first part of the paper it is shown how existing verification systems compare to the generic model, detailing the differences and their resolutions. In the second part a signature verification framework is introduced, which was created based on the generic model. It is demonstrated how existing algorithms and even an existing signature verifier can be modularized and modified to allow an execution through the framework. Finally the benefits of the model are outlined including the unified benchmarking, comparability of different systems and the support for distributed software architectures like SOA.},
  affiliation     = {Department of Automation and Applied Informatics, Budapest University of Technology and Economics, Goldman Gyorgy ter 3., 1111 Budapest, Hungary},
  author_keywords = {Component based; Loose coupling; Off-line; Signature verification; Unified model},
  document_type   = {Review},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-59249092797&partnerID=40&md5=4dddc73773576bc8b731a5e890893b82},
}

@Article{Casas2008,
  author          = {Casas, A. and Carro, M. and Hermenegildo, M.V.},
  title           = {A high-level implementation of non-deterministic, unrestricted, independent and-parallelism},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2008},
  volume          = {5366 LNCS},
  pages           = {651-666},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {The growing popularity of multicore architectures has renewed interest in language-based approaches to the exploitation of parallelism. Logic programming has proved an interesting framework to this end, and there are parallel implementations which have achieved significant speedups, but at the cost of a quite sophisticated low-level machinery. This machinery has been found challenging to code and, specially, to maintain and expand. In this paper, we follow a different approach which adopts a higher level view by raising some of the core components of the implementation to the level of the source language. We briefly present an implementation model for independent and-parallelism which fully supports non-determinism through backtracking and provides flexible solutions for some of the main problems found in previous and-parallel implementations. Our proposal is able to optimize the execution for the case of deterministic programs and to exploit unrestricted and-parallelism, which allows exposing more parallelism among clause literals than fork-join-based proposals. We present performance results for an implementation, including data for benchmarks where and-parallelism is exploited in non-deterministic programs. © 2008 Springer Berlin Heidelberg.},
  affiliation     = {Depts. of Comp. Science and Electr. and Comp. Eng., Univ. of New Mexico, United States; School of Comp. Science, Univ. Politécnica de Madrid, IMDEA-Software, Spain},
  author_keywords = {And-parallelism; High-level implementation; Prolog},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-540-89982-2_53},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-58549088564&doi=10.1007%2f978-3-540-89982-2_53&partnerID=40&md5=7d062c349f2878ac7c8137a1bbf9f679},
}

@Article{Soundararajan2008,
  author          = {Soundararajan, K. and Brennan, R.W.},
  title           = {Design patterns for real-time distributed control system benchmarking},
  journal         = {Robotics and Computer-Integrated Manufacturing},
  year            = {2008},
  volume          = {24},
  number          = {5},
  pages           = {606-615},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, we describe the design and development of a simulation-agent interface for real-time distributed control system benchmarking. This work is motivated by the need to test the feasibility of extending agent-based systems to the physical device level in manufacturing and other industrial automation systems. Our work focuses on the development of hybrid physical/simulation environment that can be used to perform tests at both the physical device level, as well as the planning and scheduling level of control. As part of this work, we have extended the proxy design pattern for this application. This paper focuses on the resulting software design pattern for distributed control system benchmarking and provides examples of its use in our hybrid physical/simulation environment. © 2007 Elsevier Ltd. All rights reserved.},
  affiliation     = {Schulich School of Engineering, University of Calgary, 2500 University Dr. N.W, Calgary, Alta. T2N 1N4, Canada},
  author_keywords = {Discrete-event simulation; Distributed control; IEC 61499; Object-oriented programming},
  document_type   = {Article},
  doi             = {10.1016/j.rcim.2007.09.010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-46749136901&doi=10.1016%2fj.rcim.2007.09.010&partnerID=40&md5=5bc98b21f97e7206297b92e1e199cc44},
}

@Article{Tam2008,
  author          = {Tam, F.},
  title           = {On the development of standards based carrier grade platforms},
  journal         = {Computer Systems Science and Engineering},
  year            = {2008},
  volume          = {23},
  number          = {5},
  pages           = {317-328},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The remarkable pace of advancement in communications technologies and the exponential growth of the market have pressured network equipment providers into producing more features in products in a much faster rate at lower costs. We advocate the use of standards based availability management middleware in carrier grade platforms as a building block for network infrastructure products, and buying constituent components as a strategy for achieving the costs and cycle time reduction goals. We examine the required capabilities of the availability management middleware, articulate the needs and show a solution for the platform's support of software upgrade with no or minimum service outage, and propose dependability benchmarking as a means of choosing an off-the-shelf high availability middleware for the platform. The development of the appropriate standards and their supporting technologies, together with the preliminary results indicate that this approach can potentially deliver a good return on investment. © 2008 CRL Publishing Ltd.},
  affiliation     = {Nokia Research Center, Nokia Group, P.O. Box 407, FI-00045, Finland},
  author_keywords = {Dependability benchmarking; High availability middleware; Online software upgrade; Open standards; Robustness testing; Service availability; Software architectures},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-64549126996&partnerID=40&md5=1973392ba90bcbb724adc3888b0058e4},
}

@Conference{Fang2008,
  author        = {Fang, X. and Wang, D. and Chen, S.},
  title         = {SPVA: A Novel Digital Signal Processor Architecture for Software Defined Radio},
  year          = {2008},
  pages         = {856-859},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {In this paper, we propose a new digital signal processor architecture SPVA (Scalable Parallel VLIW architecture) for Software Defined Radio. The proposed architecture organizes function units into arithmetic units, and other units. The former are organized as SIMD clusters and the latter are organized as control clusters. Advantages of the proposed architecture include exploiting data parallelism through SIMD VLIW clusters, hiding memory latency through static schedule schemes, and exposing delay of branch and memory access to programmers and compilers. The experiments' results on critical benchmarks show that this architecture provides significant computation speedup compared to conventional VLIW DSP. © 2008 IEEE.},
  affiliation   = {School of Computer, National University of Defense Technology, 410073, Changsha, Hunan Province, China},
  art_number    = {4493629},
  document_type = {Conference Paper},
  doi           = {10.1109/AICCSA.2008.4493629},
  journal       = {AICCSA 08 - 6th IEEE/ACS International Conference on Computer Systems and Applications},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-50049134051&doi=10.1109%2fAICCSA.2008.4493629&partnerID=40&md5=ddf99c76d05cbf4e3673d01a91a6f74b},
}

@Conference{Hampton2008,
  author          = {Hampton, M. and Asanović, K.},
  title           = {Compiling for vector-thread architectures},
  year            = {2008},
  pages           = {205-215},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {Vector-thread (VT) architectures exploit multiple forms of parallelism simultaneously. This paper describes a compiler for the Scale VT architecture, which takes advantage of the VT features. We focus on compiling loops, and show how the compiler can transform code that poses difficulties for traditional vector or VLIW processors, such as loops with internal control flow or cross-iteration dependences, while still taking advantage of features not supported by multithreaded designs, such as vector memory instructions. We evaluate the compiler using several embedded benchmarks and show that we can obtain substantial speedups over a single-issue, in-order scalar machine. Copyright 2008 ACM.},
  affiliation     = {MIT Computer Science and Artificial Intelligence Laboratory, Cambridge, MA 02139, United States; Computer Science Division, EECS Department, University of California at Berkeley, Berkeley, CA 94720-1776, United States},
  author_keywords = {Code generation; Compilers; Vector processors},
  document_type   = {Conference Paper},
  doi             = {10.1145/1356058.1356085},
  journal         = {Proceedings of the 2008 CGO - Sixth International Symposium on Code Generation and Optimization},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-43449100381&doi=10.1145%2f1356058.1356085&partnerID=40&md5=bb72ee97ecb1b659d30a02bfc86cc66c},
}

@Conference{Phan2008,
  author        = {Phan, T. and Li, W.-S.},
  title         = {Load distribution of analytical query workloads for database cluster architectures},
  year          = {2008},
  pages         = {169-180},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {Enterprises may have multiple database systems spread across the organization for redundancy or for serving different applications. In such systems, query workloads can be distributed across different servers for better performance. A materialized view, or Materialized Query Table (MQT), is an auxiliary table with pre-computed data that can be used to significantly improve the performance of a database query. In this paper, we propose a framework for coordinating execution of OLAP query workloads across a database cluster with shared nothing architecture. Such coordination is complex since we need to consider (1) the time to build the MQTs, (2) the query execution impact of the MQTs, (3) whether the MQTs can fit in the disk space limitation, (4) server computation power, and (5) the effectiveness of the scheduling and placement algorithms in deriving a combination of configurations so that the workload can be completed in the shortest time period. We frame the problem as a combinatorial problem with a solution space that is exponential in the number of queries, MQTs, and servers. We provide a stochastic search heuristic that finds a near-optimal mapping of queries-to-servers and MQTs-to-servers within an arbitrarily bounded time and compare our solution with an exhaustive search and three standard greedy algorithms. Our search implementation produced schedules within 9% of the optimal found through an exhaustive search and produced better solutions than typical greedy algorithms for both TPC-H and synthetic benchmarks under a variety of experiments. For a key trial where disk space is limited, it produced 15% better results than the next best competitor, corresponding to an absolute wall clock advantage of over 10 hours. Copyright 2008 ACM.},
  affiliation   = {Yahoo, Inc., 701 First Ave., Sunnyvale, CA 94089, United States; IBM, Almaden Research Center, 650 Harry Road, San Jose, CA 95134, United States},
  document_type = {Conference Paper},
  doi           = {10.1145/1353343.1353367},
  journal       = {Advances in Database Technology - EDBT 2008 - 11th International Conference on Extending Database Technology, Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-43349093608&doi=10.1145%2f1353343.1353367&partnerID=40&md5=d9c298dc1c9d3b37c4b56f73d0df10bd},
}

@Article{Almasi2008,
  author        = {Almasi, G. and Asaad, S. and Bellofatto, R.E. and Bickford, H.R. and Blumrich, M.A. and Brezzo, B. and Bright, A.A. and Brunheroto, J.R. and Castanos, J.G. and Chen, D. and Chiu, G.L.-T. and Coteus, P.W. and Dombrowa, M.B. and Dozsa, G. and Eichenberger, A.E. and Gara, A. and Giampapa, M.E. and Giordano, F.P. and Gunnels, J.A. and Hall, S.A. and Haring, R.A. and Heidelberger, P. and Hoenicke, D. and Kochte, M. and Kopcsay, G.V. and Kumar, S. and Lanzetta, A.P. and Lieber, D. and Nathanson, B.J. and O'Brien, K. and Ohmacht, A.S. and Ohmacht, M. and Rand, R.A. and Salapura, V. and Sexton, J.C. and Steinmacher-Burow, B.D. and Stunkel, C. and Sugavanam, K. and Swetz, R.A. and Takken, T. and Tian, S. and Trager, B.M. and Tremaine, R.B. and Vranas, P. and Walkup, R.E. and Wazlowski, M.E. and Winograd, S. and Wisniewski, R.W. and Wu, P. and Busche, D.R. and Douskey, S.M. and Ellavsky, M.R. and Flynn, W.T. and Germann, P.R. and Hamilton, M.J. and Hehenberger, L. and Hruby, B.J. and Jeanson, M.J. and Kasemkhani, F. and Lembach, R.F. and Liebsch, T.A. and Lyndgaard, K.C. and Lytle, R.W. and Marcella, J.A. and Marroquin, C.M. and Mathiowetz, C.H. and Maurice, M.D. and Nelson, E. and Rickert, D.M. and Sellers, G.W. and Sheets, J.E. and Strissel, S.A. and Wait, C.D. and Winter, B.B. and Wood, C.J. and Zumbrunnen, L.M. and Rangarajan, M. and Allen, P.V. and Archer, C.J. and Blocksome, M. and Budnik, T.A. and Ellis, S.D. and Good, M.P. and Gooding, T.M. and Inglett, T.A. and Kaliszewski, K.T. and Knudson, B.L. and Lappi, C. and Leckband, G.S. and Lee, S. and Megerian, M.G. and Miller, S.J. and Mundy, M.B. and Musselman, R.G. and Musta, T.E. and Nelson, M.T. and Obert, C.F. and Van Oosten, J.L. and Orbeck, J.P. and Parker, J.J. and Poole, R.J. and Rodakowski, H.L. and Reed, D.D. and Scheel, J.J. and Sell, F.W. and Shok, R.M. and Solie, K.M. and Stewart, G.G. and Stockdell, W.M. and Tauferner, A.T. and Thomas, J. and Sharrar, R.H. and Schwartz, S. and Satterfield, D.L. and Chauvin, J.D. and Shmueli, E. and Archambault, R.G. and Martin, A.R. and Mendell, M.P. and Zhang, G. and Zhao, P.P. and Mani, I. and Nair, R. and Bendale, R.D. and Curioni, A. and Sabharwal, Y. and Doi, J. and Negishi, Y.},
  title         = {Overview of the IBM Blue Gene/P project},
  journal       = {IBM Journal of Research and Development},
  year          = {2008},
  volume        = {52},
  number        = {1-2},
  pages         = {199-220},
  note          = {cited By 207},
  __markedentry = {[Nichl:6]},
  abstract      = {On June 26, 2007, IBM announced the Blue Gene/P™ system as the leading offering in its massively parallel Blue Gene® supercomputer line, succeeding the Blue Gene/L™ system. The Blue Gene/P system is designed to scale to at least 262,144 quad-processor nodes, with a peak performance of 3.56 petaflops. More significantly, the Blue Gene/P system enables this unprecedented scaling via architectural and design choices that maximize performance per watt, performance per square foot, and mean time between failures. This paper describes our vision of this petascale system, that is, a system capable of delivering more than a quadrillion (10 15 ) floating-point operations per second. We also provide an overview of the system architecture, packaging, system software, and initial benchmark results. © Copyright 2008 by International Business Machines Corporation.},
  affiliation   = {IBM Thomas J. Watson Research Center, Yorktown Heights, NY 10598, United States; IBM Global Engineering Solutions, Rochester, MN 55901, United States; IBM Global Engineering Solutions, Bangalore, India; IBM Systems and Technology Group, Rochester, MN 55901, United States; IBM Systems and Technology Group, Research Triangle Park, NC 27709, United States; IBM Systems and Technology Group, Burlington, VT, United States; IBM Systems and Technology Group, Waltham, MA 02451, United States; IBM Integrated Technology Delivery, Rochester, MN 55901, United States; IBM Haifa Research Laboratory, Mount Carmel, Haifa 31905, Israel; IBM Software Group, Department of Rational, Markham, ON, Canada; IBM Software Group, Bangalore, India; IBM Sales and Distribution, San Diego, CA 92122, United States; IBM Zurich Research Laboratory, Zurich, Switzerland; IBM India Research Laboratory, New Delhi, India; IBM Yamato Laboratory, Yamato, Japan},
  document_type = {Review},
  doi           = {10.1147/rd.521.0199},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-40749160036&doi=10.1147%2frd.521.0199&partnerID=40&md5=41a0cd073271bfc2e50442c98fb2bc9b},
}

@Book{Kepner2008,
  title         = {Performance metrics and software architecture},
  year          = {2008},
  author        = {Kepner, J. and Meuse, T. and Schrader, G.E.},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {FIGURE 15-2 (Color figure follows p. 278.) Unprocessed (left) and processed (right) SAR data. The area that reflects a single pulse is large and an image of this raw data is very blurry (left). A SAR system provides multiple looks at the same area of the ground from multiple viewing angles. Combining these different viewing angles together produces a much sharper image (right). (From Bader et al., Designing scalable synthetic compact applications for benchmarking high productivity computing systems, CTWatch Quarterly 2(4B), 2006. With permission.) Handbook: A Systems Perspective FIGURE 15-3 System mode block diagram. SAR system mode consists of Stage 1 front-end sensor processing and Stage 2 back-end knowledge formation. In addition, there is significant IO to the storage system. (From Bader et al., Designing scalable synthetic compact applications for benchmarking high productivity computing systems, CTWatch Quarterly 2(4B), 2006. With permission.) FIGURE 15-4 (Color figure follows p. 278.) Compute Only Mode block diagram. Simulates a streaming sensor that moves data directly from front-end processing to back-end processing. (From Bader et al., Designing scalable synthetic compact applications for benchmarking high productivity computing systems. CTWatch Quarterly 2(4B), 2006. With permission.) Handbook: A Systems Perspective FIGURE 15-5 Algorithm flow. In Stage 1, the data are transformed in series of steps from a n mcs singleprecision complex valued array to a n mx s single-precision real valued array. At each step the processing is along either the rows or the columns. © 2008 by Taylor & Francis Group, LLC.},
  affiliation   = {MIT Lincoln Laboratory, Lexington, MA, United States},
  document_type = {Book Chapter},
  journal       = {High Performance Embedded Computing Handbook: A Systems Perspective},
  pages         = {303-334},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85032772201&partnerID=40&md5=ed0da09d1e6a496a89d6aea3341a11c8},
}

@Article{PowersJr.2008,
  author          = {Powers Jr., F.E. and Alaghband, G.},
  title           = {The hydra parallel programming system},
  journal         = {Concurrency Computation Practice and Experience},
  year            = {2008},
  volume          = {20},
  number          = {1},
  pages           = {1-27},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {The Hydra Parallel Programming System, a new parallel language extension to Java, and its supporting software are described. It is a fairly simple yet powerful language designed to address a number of areas that have not received much attention. One of these areas is the recompilation of parallel programs at runtime to allow a parallel program to adapt to the architecture it is executing on. The first version of this software system focuses on smaller Symmetric Multiprocessing and compatible architectures which are becoming more common. This particular class of machines has a great need for more options in the area of parallel programming among the vastly popular Java language programmers. Hydra programs will run as sequential Java on machines that do not have the parallel support or do not have an implemented Hydra runtime system without requirement of any modifications to the program. This paper describes the language, compares it with other languages (specifically with JOMP, an OpenMP implementation for Java), presents a brief discussion on compiling and executing Hydra programs, presents some sample benchmarks and their performance on three platforms, and concludes with a discussion of issues and future directions for Hydra. Copyright © 2007 John Wiley & Sons, Ltd.},
  affiliation     = {Department of Computer Science and Engineering, Health and Sciences Center, University of Colorado at Denver, P.O. Box 173364, Denver, CO 80217-3364, United States},
  author_keywords = {Multicore architectures; Parallel java; Parallel languages; Parallel processing; Shared memory parallel language},
  document_type   = {Article},
  doi             = {10.1002/cpe.1205},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-37349081340&doi=10.1002%2fcpe.1205&partnerID=40&md5=2bafc3638011c1c8c3e7c3428427e6a8},
}

@Conference{Grant2008,
  author          = {Grant, T.},
  title           = {Checklist for comparing emergency management information systems},
  year            = {2008},
  pages           = {752-763},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper describes a checklist that has been developed for comparing the functionality of emergency management control centres and their information systems. The intention is to interest the ISCRAM community in using the checklist in various applications and pooling experiences. The Control Centre Visit Checklist has evolved through four iterations. It has been used to study two military C2 systems and one non-military control system, and has been applied by students for course assignments. The paper focuses on the part of the checklist that evaluates the information system from the systems viewpoint. It describes the underlying applications architecture and process model. The Royal Netherlands Army's Battlefield Management System illustrates the application of the checklist. The results show that the checklist aids in identifying where C2 systems can be developed further. The next step is to perform a set of substantial pilot studies for diverse domains, including civilian emergency management systems.},
  affiliation     = {Netherlands Defence Academy, P.O. Box 90.002, 4800PA Breda, Netherlands},
  author_keywords = {Benchmarking; Command & control; Situation awareness; Software architecture},
  document_type   = {Conference Paper},
  journal         = {Proceedings of ISCRAM 2008 - 5th International Conference on Information Systems for Crisis Response and Management},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84905577791&partnerID=40&md5=3da43de38952cdff7d5f9213badce09e},
}

@Conference{Mendes2008,
  author          = {Mendes, M.R.N. and Bizarro, P. and Marques, P.},
  title           = {A framework for performance evaluation of Complex Event Processing systems},
  year            = {2008},
  pages           = {313-316},
  note            = {cited By 23},
  __markedentry   = {[Nichl:6]},
  abstract        = {Several new Complex Event Processing (CEP) engines have been recently released, many of which are intended to be used in performance sensitive scenarios - like fraud detection, traffic control, or health care systems. However, there is no standard means to assess the performance of a CEP engine. This omission is all the more relevant as there are currently many competing products, languages, architectures, data models, and data processing CEP techniques. A performance evaluation framework can help identify good design decisions and assist in improving engines. Here we demonstrate our work in progress: FINCoS, a framework that can be used to benchmark CEP systems. The proposed framework has five relevant characteristics: i. Flexible (e.g., it allows changing the workload on the fly to measure reactions to peak loads); ii. Independent of particular workloads; iii. Neutral (not bound to any specific CEP product); iv. Correctness check (validators can be plugged into the framework on demand to verify results); v. Scalable (many of its components, like event generators, can be centrally orchestrated and run in parallel). Note that the framework does not include a benchmark specification. In fact, it was designed such that diverse datasets and query scenarios can be easily attached and tested on several CEP engines. As such, this framework has three key benefits: first, it can be used by the CEP community to more quickly devise and experiment new benchmarks for event processing systems. Second, CEP vendors can employ the framework in conjunction with their own test datasets to benchmark their systems internally. Finally, customers can use it with their real data and select the CEP engine that best fits their needs. Copyright 2008 ACM.},
  affiliation     = {CISUC, University of Coimbra, Dep. Eng. Informática- Polo II, 3030-290 Coimbra, Portugal},
  author_keywords = {Complex Event Processing; Framework; Performance evaluation},
  document_type   = {Conference Paper},
  doi             = {10.1145/1385989.1386030},
  journal         = {Proceedings of the 2nd International Conference on Distributed Event-Based Systems, DEBS 2008},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-57549103196&doi=10.1145%2f1385989.1386030&partnerID=40&md5=b23149047732cde690454187fea39e3f},
}

@Article{Kim2007,
  author        = {Kim, J. and Ok, C. and Kumara, S.R.T. and Yee, S.-T.},
  title         = {Multiagent-based dynamic deployment planning in RTLS-enabled automotive shipment yard},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2007},
  volume        = {4509 LNAI},
  pages         = {38-49},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Real-time vehicle location information enables to facilitate more efficient decision-making in dynamic automotive shipment yard environment. This paper proposes a multiagent-based decentralized decision-making model for the vehicle deployment planning in a shipment yard. A multiagent architecture is designed to facilitate decentralized algorithms and coordinate different agents dynamically. The results of computational experiments show that the proposed deployment model outperforms a current deployment practice with respect to the deployment performance measures. © Springer-Verlag Berlin Heidelberg 2007.},
  affiliation   = {Department of Industrial and Manufacturing Engineering, Pennsylvania State University, Univeristy Park, PA 16802, United States; Manufacturing Systems Research Laboratory, General Motors, Warren, MI 48090, United States},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-37249080335&partnerID=40&md5=b02cca3813e0f76839cb2a57c2479151},
}

@Article{Bjoerndalen2007,
  author        = {Bjørndalen, J.M. and Anshus, O.J.},
  title         = {Trusting floating point benchmarks - are your benchmarks really data independent?},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2007},
  volume        = {4699 LNCS},
  pages         = {178-188},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {Benchmarks are important tools for studying increasingly complex hardware architectures and software systems. Two seemingly common assumptions are that the execution time of floating point operations do not change much with different input values, and that the execution time of a benchmark does not vary much if the input and computed values do not influence any branches. These assumption do not always hold. There is significant overhead in handling denormalized floating point values (a representation automatically used by the CPU to represent values close to zero) on-chip on modern Intel hardware, even if the program can continue uninterrupted. We have observed that even a small fraction of denormal numbers in a textbook benchmark significantly increases the execution time of the benchmark, leading to the wrong conclusions about the relative efficiency of different hardware architectures and about scalability problems of a cluster benchmark. © Springer-Verlag Berlin Heidelberg 2007.},
  affiliation   = {Department of Computer Science, University of Tromsø, NO-9037 Tromsø, Norway},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-38049034057&partnerID=40&md5=d93b9a8db29a1dc98f575a6865858571},
}

@Conference{Sawant2007,
  author          = {Sawant, A.P. and Bali, N.},
  title           = {DiffArchViz: A tool to visualize correspondence between multiple representations of a software architecture},
  year            = {2007},
  pages           = {121-128},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper describes a technique to visualize the software architecture of the Network Appliance Data ONTAP® 7G (ONTAP) storage server operating system. We integrate the Multi-Dimensional Scaling (MDS) algorithm into a visualization technique for converting raw source code into patterns that capture the software architecture. Individual components are drawn using graphical "glyphs" that vary their spatial position, color, and texture properties to encode each component's attribute values. The result is a display that can be used by viewers to rapidly and accurately analyze, explore, compare, and discover within the software architecture. We show how our visualization tool, known as DiffArchViz, can be used to study different architectural views representing multiple attributes of the software components. We also present a technique to visualize dynamic software architecture by examining the correspondence between multiple runtime profiles for a few storage server performance benchmarks. We apply our technique to multiple hardware platforms and versions of ONTAP.},
  affiliation     = {Department of Computer Science, North Carolina State University; Performance Engineering Group, Network Appliance, Inc.},
  art_number      = {4290710},
  author_keywords = {Architecture visualization; Information visualization; Perception; Software architecture; Software performance visualization; Software visualization},
  document_type   = {Conference Paper},
  doi             = {10.1109/VISSOF.2007.4290710},
  journal         = {VISSOFT 2007 - Proceedings of the 4th IEEE International Workshop on Visualizing Software for Understanding and Analysis},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-46449127144&doi=10.1109%2fVISSOF.2007.4290710&partnerID=40&md5=d46ebfd6dee35aeac16696d827aa2054},
}

@Conference{Jarraya2007,
  author          = {Jarraya, T. and Guessoum, Z.},
  title           = {A model-driven method for multi-agent development [Une méthode dirigée par les modèles pour le développement multi-agents]},
  year            = {2007},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {We propose a new multi-agent development method, named MDAD (Model Driven Agent Development). It is based on the MDA (Model Driven Architecture) paradigm. The aim of MDAD method is to reduce the cost of building MAS applications by starting from abstract specification of system thanks to MAS meta-models, and producing the final system by means of transformations of this specification into computational entities. We present in this paper the application of MDAD to the INAF framework. First we give an overview of MDA approach and its application to MAS. Thus, several abstract ion levels are determined and a set of metamodels is introduced. Then, we give the transformation rules used to produce INAF compliant models. MDAD method is illustrated with the timetable management benchmark.},
  affiliation     = {CReSTIC MODECO Team, Univ. de Reims Champagne Ardenne, IUT Rue des Crayeres BP 1035, 51687 Reims, France; LIP6 OASIS Team, Univ. de Pierre and Marie Curie, 8 Rue du Capitaine Scott, 75015 Paris, France},
  author_keywords = {Agent Oriented Software Engineering; MDA; Multi-Agent System},
  document_type   = {Conference Paper},
  journal         = {XXVeme Congres INFORSID 2007 - Informatique des Organisations et Systemes d'Information et de Decision},
  page_count      = {17},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84884659640&partnerID=40&md5=05daf405dd234ebda9c09f7cc7c40855},
}

@Article{Kimpe2007,
  author        = {Kimpe, D. and Lani, A. and Quintino, T. and Vandewalle, S. and Poedts, S. and Deconinck, H.},
  title         = {A study of real world I/O performance in parallel scientific computing},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2007},
  volume        = {4699 LNCS},
  pages         = {871-881},
  note          = {cited By 2},
  __markedentry = {[Nichl:6]},
  abstract      = {Parallel computing is indisputably present in the future of high performance computing. For distributed memory systems, MPI is widely accepted as a de facto standard. However, I/O is often neglected when considering parallel performance. In this article, a number of I/O strategies for distributed memory systems will be examined. These will be evaluated in the context of COOLFluiD, a framework for object oriented computational fluid dynamics. The influence of the system and software architecture on performance will be studied. Benchmark results will be provided, enabling a comparison between some commonly used parallel file systems. © Springer-Verlag Berlin Heidelberg 200.},
  affiliation   = {Technisch-Wetenschappelijk Rekenen, K.U.Leuven, Celestijnenlaan 200A, BE-3001 Leuven, Belgium; Centrum voor Plasma-Astrofysica, K.U.Leuven, Celestijnenlaan 200B, BE-3001 Leuven, Belgium; Von Karman Instituut, Waterloosesteenweg 72, BE-1640 Sint-Genesius-Rode, Belgium},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-38049044504&partnerID=40&md5=45aac5b2787aeefb40325c8f91b72c6d},
}

@Conference{Lelli2007,
  author          = {Lelli, F. and Maron, G. and Orlando, S.},
  title           = {Client side estimation of a remote service execution},
  year            = {2007},
  pages           = {295-302},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {many use cases, concerning the monitoring and controlling of real physical instruments, demand deep interaction between users and services that virtualize the access to such instruments/devices. In addition, in order to realize high interoperable solutions, SOA-based Web/Grid Service technologies must be adopted. When the access to one of these services is performed via internet using a Web Service call, the remote invocation time becomes critical in order to understand if an instrument can be controlled properly, or the delays introduced by the wire and the serialization/deserialization process are unacceptable. This paper thus presents methodologies and algorithms, based on a 2k factorial analysis and a Gaussian Majorization of previous service execution times, which enables the estimation of a generic remote method execution time. Furthermore it suggests three different software architectures, where the developed algorithms and methodology could be integrated in order to automatically profile the end-toend service. It is worth noting that our proposals are validated using suitable benchmarks and extensive tests coming out from a real (not simulated) environment In addition, the outcome of this paper have been used in the realization of a service for remote control, monitor, and manage of a pool of instruments/devices.},
  affiliation     = {Istituto Nazionale di Fisica Nucleare, Laboratori Nazionali di Legnaro (INFN-LNL), Legnaro (Padova), Italy; Dipartimento di Informatica, Universita Ca Foscari Venezia, Venezia, Italy},
  art_number      = {4674430},
  author_keywords = {Grid; Quality of service; Response time; Web Servic},
  document_type   = {Conference Paper},
  doi             = {10.1109/MASCOTS.2007.14},
  journal         = {IEEE International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems - Proceedings},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-57849142423&doi=10.1109%2fMASCOTS.2007.14&partnerID=40&md5=03faa3f083d962e5418dff084188668b},
}

@Article{Li2007,
  author          = {Li, P. and Wang, D. and Wang, H. and Lu, M. and Li, C. and Zheng, W.},
  title           = {Software Support for LIRAC Architecture},
  journal         = {Tsinghua Science and Technology},
  year            = {2007},
  volume          = {12},
  number          = {6},
  pages           = {700-706},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Memory limitations are always a focus of computer architecture. The live range aware cache (LIRAC) offers a way to reduce memory access using live range information. In the LIRAC system, scratch data need not be written back if the data will no longer be used. Three kinds of software support developed for LIRAC architecture use compiler analyses, binary analyses, and trace analyses. Trace analysis results show that LIRAC can eliminate 29% of cache write-backs on average and up to 83% in the best case for the SPEC CPU 2000 benchmark. These software techniques can show the feasibility and potential benefit of the LIRAC architecture. © 2007 Tsinghua University Press.},
  affiliation     = {Research Institute of Information Technology, National Laboratory for Information Science and Technology, Tsinghua University, Beijing, 100084, China},
  author_keywords = {cache; LIRAC; live range; memory hierarchy},
  document_type   = {Article},
  doi             = {10.1016/S1007-0214(07)70178-5},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-36549039477&doi=10.1016%2fS1007-0214%2807%2970178-5&partnerID=40&md5=c90ec14000483db6012d0590649728cd},
}

@Article{Jugwan2007,
  author          = {Jugwan, E. and Dohun, K. and Chanik, P.},
  title           = {L4oprof: A system-wide profiler using hardware PMU in L4 environment},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {2007},
  volume          = {4523 LNCS},
  pages           = {548-559},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The recent advance of L4 microkernel technology enables building a secure embedded system with comparable performance to a traditional monolithic kernel-based system. According to the different system software architecture, the execution behavior of an application in microkernel environment differs greatly from that in traditional monolithic environment. Therefore, we need a performance profiler to improve performance of the application in microkernel environment. Currently, L4's profiling tools provides only program-level information such as the number of function calls, IPCs, context switches, etc. In this paper, we present L4oprof, a System-Wide statistical profiler in L4 microkernel environment. L4oprof leverages the hardware performance counters of PMU on a CPU to enable profiling of a wide variety of hardware events such as clock cycles and cache and TLB misses. Our evaluation shows that L4oprof incurs 0-3% higher overhead than Linux OProfile. Moreover, the main cause of performance loss in L4Linux applications is shown compared with Linux applications. © Springer-Verlag Berlin Heidelberg 2007.},
  affiliation     = {Department of Computer Science and Engineering, Pohang University of Science and Technology, Pohang, Gyungbuk 790-784, South Korea},
  author_keywords = {Hardware PMU; L4 microkernel; Performance analysis; Performance measures; Performance monitoring; Statistical profiling},
  document_type   = {Conference Paper},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-38749107833&partnerID=40&md5=0a0abac97bc74a114c25bf4aedd299ef},
}

@Article{Jarraya2007a,
  author        = {Jarraya, T. and Guessoum, Z.},
  title         = {Towards a model driven process for multi-agent system},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2007},
  volume        = {4696 LNAI},
  pages         = {256-265},
  note          = {cited By 11},
  __markedentry = {[Nichl:6]},
  abstract      = {We propose a new multi-agent development method, named MDAD (Model Driven Agent Development). It is based on the MDA (Model Driven Architecture) paradigm. The aim of MDAD method is to reduce the cost of building MAS applications by starting from abstract specification of system thanks to MAS meta-models, and producing the final system by means of transformations of this specification into computational entities. We present in this paper the application of MDAD to the INAF framework. First we give an overview of MDA approach and its application to MAS. Thus, several abstraction levels are determined and a set of meta-models is introduced. Then, we give the transformation rules used to produce INAF compliant models. MDAD method is illustrated with the timetable management benchmark. © Springer-Verlag Berlin Heidelberg 2007.},
  affiliation   = {I3S RAINBOW Team, University of Nice, Poly'tech Nice, 930 route des Colles, 06903 Sophia Antipolis, France; LIP6 OASIS Team, Univ. of Pierre and Marie Curie, 104 avenue du Prsident, Kennedy 75016 Paris, France},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-38049115947&partnerID=40&md5=74557c2e723198ad908f8c3a095a2652},
}

@Article{Stanley-Marbell2007,
  author        = {Stanley-Marbell, P. and Marculescu, D.},
  title         = {Sunflower: Full-system, embedded microarchitecture evaluation},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2007},
  volume        = {4367 LNCS},
  pages         = {168-182},
  note          = {cited By 11},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper describes Sunflower, a full-system microarchitectural evaluation environment for embedded computing systems. The environment enables detailed microarchitectural simulation of multiple instances of complete embedded systems, their peripherals, and medium access control / physical layer communication between systems. The environment models the microarchitecture, computation and communication upset events under a variety of stochastic distributions, compute and communication power consumption, electrochemical battery systems, and power regulation circuitry, as well as analog signals external to the processing elements. The simulation environment provides facilities for speeding up simulation performance, which tradeoff accuracy of simulated properties for simulation speed. Through the detailed simulation of benchmarks in which the effect of simulation speedup on correctness can be accurately quantified, it is demonstrated that traditional techniques proposed for simulation speedup can introduce significant error when simulating a combination of computation and analog physical phenomena external to a processor. © Springer-Verlag Berlin Heidelberg 2007.},
  affiliation   = {Department of Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15217, United States},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-38149105706&partnerID=40&md5=9d484777de099dcb11a6d2da06ab8dbf},
}

@Article{Huang2007,
  author          = {Huang, J. and Li, B.H. and Chai, X. and Zhang, L.},
  title           = {Web-HLA and Service-Enabled RTI in the Simulation Grid},
  journal         = {Communications in Computer and Information Science},
  year            = {2007},
  volume          = {5},
  pages           = {195-204},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {HLA-based simulations in a grid environment have now become a main research hotspot in the M&S community, but there are many shortcomings of the current HLA running in a grid environment. This paper analyzes the analogies between HLA and OGSA from the software architecture point of view, and points out the service-oriented method should be introduced into the three components of HLA to overcome its shortcomings. This paper proposes an expanded running architecture that can integrate the HLA with OGSA and realizes a service-enabled RTI (SE-RTI). In addition, in order to handle the bottleneck problem that is how to efficiently realize the HLA time management mechanism, this paper proposes a centralized way by which the CRC of the SE-RTI takes charge of the time management and the dispatching of TSO events of each federate. Benchmark experiments indicate that the running velocity of simulations in Internet or WAN is properly improved. © Springer-Verlag Berlin Heidelberg 2006.},
  affiliation     = {Beijing University of Aeronautics and Astronautics, Beijing 100083, China; Beijing Simulation Center, Beijing 100854, China},
  author_keywords = {core RTI component (CRC); Grid; HLA/RTI; local RTI component (LRC); OMT},
  document_type   = {Conference Paper},
  doi             = {10.1007/978-3-540-77600-0_22},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84884997620&doi=10.1007%2f978-3-540-77600-0_22&partnerID=40&md5=2d204a6277bffe61e43fb20370844315},
}

@Conference{Yokota2007,
  author          = {Yokota, T. and Ootsu, K. and Baba, T.},
  title           = {Introducing entropies for representing program behavior and branch predictor performance},
  year            = {2007},
  note            = {cited By 8},
  __markedentry   = {[Nichl:6]},
  abstract        = {Predictors are inherent components of state-of-the-art microprocessors. Branch predictors are discussed actively from diverse perspectives. Performance of a branch predictor largely depends on the dynamic behavior of the executing program. Nevertheless, we have no effective metrics to represent the nature of program behavior quantitatively. In this paper, we introduce an information entropy idea to represent program behavior and branch predictor performance. Through simple application of Shannon's information entropy, we introduce new entropy, Branch History Entropy, which quantitatively represents the regularity level of program behavior. We show that the entropy also represents an index of prediction performance that is independent of prediction mechanisms. We further discuss branch predictor performance from a stereoscopic view of their typical organization. We propose two entropies: Table Reference Entropy and Table Entry Entropy. The former represents an unbalanced level of references of table entries. The latter offers the maximum expectation in prediction performance. We evaluated the proposed three entropies and prediction performance in various situations. Artificially generated branch patterns, as preliminary experiments, show an overview of the entropies and prediction performance. Subsequently, we present a comparison to the 2nd Championship Branch Predictor competition results and show the high potential of the proposed entropy. Finally, we present an actual view of our entropies and prediction performance as application results to SPEC CPU2000 benchmarks. Copyright 2007 ACM.},
  affiliation     = {Utsunomiya University, 7-2-1 Yoto, Utsunomiya, 321-8585, Japan},
  art_number      = {17},
  author_keywords = {Architecture; Branch predictors; Information entropy; Prediction performance; Program behavior},
  document_type   = {Article},
  doi             = {10.1145/1281700.1281717},
  journal         = {Proceedings of the 2007 Workshop on Experimental Computer Science},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-37849037259&doi=10.1145%2f1281700.1281717&partnerID=40&md5=45fb44b055870c8f75d9fe26855ae8f9},
}

@Conference{Sakata2007,
  author        = {Sakata, T. and Hirotsu, T. and Yamada, H. and Kataoka, T.},
  title         = {A cost-effective dependable microcontroller architecture with instruction-level rollback for soft error recovery},
  year          = {2007},
  pages         = {256-265},
  note          = {cited By 13},
  __markedentry = {[Nichl:6]},
  abstract      = {A cost-effective, dependable microcontroller architecture has been developed. To detect soft errors, we developed an electronic design automation (EDA) tool that generates optimized soft error-detecting logic circuits for flip-flops. After a soft error is detected, the error detection signal goes to a developed rollback control module (RCM), which resets the CPU and restores the CPU's register file from the backup register file using a rollback program routine. After the routine, the CPU restarts from the instruction executed before the soft error occurred. In addition, there is a developed error reset module (ERM) that can restore the RCM from soft errors. We also developed an error correction module (ECM) that corrects ECC errors in RAM after error detection with no delay overheads. Testing on a 32-bit RISC microcontroller and EEMBC benchmarks showed that the area overhead was under 59% and frequency overhead was under 9%. In a soft error injection simulation, the MTBF of random logic circuits, and the MTBF of RAM were 30 and 1.34 times longer, respectively, than those of the original microcontroller. © 2007 IEEE.},
  affiliation   = {Hitachi Research Laboratory, Hitachi Ltd. Standard Product Business Group, Renesas Technology Corp., Japan},
  art_number    = {4272977},
  document_type = {Conference Paper},
  doi           = {10.1109/DSN.2007.5},
  journal       = {Proceedings of the International Conference on Dependable Systems and Networks},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-36048961837&doi=10.1109%2fDSN.2007.5&partnerID=40&md5=375dde7d4b291ad19837cb75a4f4d0a6},
}

@Conference{Li2007a,
  author        = {Li, X. and Adve, S.V. and Bose, P. and Rivers, J.A.},
  title         = {Architecture-level soft error analysis: Examining the limits of common assumptions},
  year          = {2007},
  pages         = {266-275},
  note          = {cited By 41},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper concerns the validity of a widely used method for estimating the architecture-level mean time to failure (MTTF) due to soft errors. The method first calculates the failure rate for an architecture-level component as the product of its raw error rate and an architecture vulnerability factor (AVF). Next, the method calculates the system failure rate as the sum of the failure rates (SOFR) of all components, and the system MTTF as the reciprocal of this failure rate. Both steps make significant assumptions. We investigate the validity of the AVF+SOFR method across a large design space, using both mathematical and experimental techniques with real program traces from SPEC 2000 benchmarks and synthesized traces to simulate longer real-world workloads. We show that AVF+SOFR is valid for most of the realistic cases under current raw error rates. However, for some realistic combinations of large systems, long-running workloads with large phases, and/or large raw error rates, the MTTF calculated using AVF+SOFR shows significantdiscrepancies from that using first principles. We also show that SoftArch, a previously proposed alternative method that does not make the AVF+SOFR assumptions, does not exhibit the above discrepancies. ©2007 IEEE.},
  affiliation   = {Department of Computer Science, University of Illionis at Urbana-Champaign, United States; IBM T.J. Watson Research Center, Yorktown Heights, NY, United States},
  art_number    = {4272978},
  document_type = {Conference Paper},
  doi           = {10.1109/DSN.2007.15},
  journal       = {Proceedings of the International Conference on Dependable Systems and Networks},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-36048965581&doi=10.1109%2fDSN.2007.15&partnerID=40&md5=ddbd47db6a9bb349b1e582724e2571d8},
}

@Conference{Dubach2007,
  author          = {Dubach, C. and Cavazos, J. and Franke, B. and Fursin, G. and O'Boyle, M.F.P. and Temam, O.},
  title           = {Fast compiler optimisation evaluation using code-feature based performance prediction},
  year            = {2007},
  pages           = {131-142},
  note            = {cited By 38},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance tuning is an important and time consuming task which may have to be repeated for each new application and platform. Although iterative optimisation can automate this process, it still requires many executions of different versions of the program. As execution time is frequently the limiting factor in the number of versions or transformed programs that can be considered, what is needed is a mechanism that can automatically predict the performance of a modified program without actually having to run it. This paper presents a new machine learning based technique to automatically predict the speedup of a modified program using a performance model based on the code features of the tuned programs. Unlike previous approaches it does not require any prior learning over a benchmark suite. Furthermore, it can be used to predict the performance of any tuning and is not restricted to a prior seen trans-formation space. We show that it can deliver predictions with a high correlation coefficient and can be used to dramatically reduce the cost of search. Copyright 2007 ACM.},
  affiliation     = {HiPEAC, United Kingdom; Institute for Computing Systems Architecture, University of Edinburgh, United Kingdom; ALCHEMY Group, INRIA Futurs and LRI, Paris-Sud University, France},
  author_keywords = {Architecture; Artificial neural networks; Compiler optimisation; Machine learning; Performance modelling},
  document_type   = {Conference Paper},
  doi             = {10.1145/1242531.1242553},
  journal         = {2007 Computing Frontiers, Conference Proceedings},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-35348881568&doi=10.1145%2f1242531.1242553&partnerID=40&md5=68f577dee34104441ef3c61982cf83ab},
}

@Conference{Chitchyan2007,
  author        = {Chitchyan, R. and Rashid, A. and Moreira, A. and Araújo, J. and Clements, P. and Baniassad, E. and Tekinerdogan, B.},
  title         = {Early aspects at ICSE 2007: Workshop on aspect-oriented requirements engineering and architecture design},
  year          = {2007},
  pages         = {127-128},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The "Early Aspects @ ICSE'07" is the 11th workshop in the series of Early Aspects workshops [1] which focuses on aspect identification during the requirements engineering and architecture derivation activities. The specific aim of the present workshop is twofold: (a) to initiate creation of an Early Aspects application demonstration and comparisons benchmark; and (b) to solicit submission of new research. © 2007 IEEE.},
  affiliation   = {Lancaster University, United Kingdom; New University of Lisbon, Portugal; Software Engineering Institute, United States; Chinese University of Hong Kong, Hong Kong, Hong Kong; University of Twente, Netherlands},
  art_number    = {4222708},
  document_type = {Conference Paper},
  doi           = {10.1109/ICSECOMPANION.2007.32},
  journal       = {Proceedings - International Conference on Software Engineering},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34548757892&doi=10.1109%2fICSECOMPANION.2007.32&partnerID=40&md5=297d304f5524b52869243fdd1ff0a125},
}

@Conference{Mavroidis2007,
  author        = {Mavroidis, I. and Papaefstathiou, I. and Pnevmatikatos, D.},
  title         = {Hardware implementation of 2-Opt local search algorithm for the traveling salesman problem},
  year          = {2007},
  pages         = {41-47},
  note          = {cited By 7},
  __markedentry = {[Nichl:6]},
  abstract      = {In this paper we discuss how one of the most famous local optimization algorithms for the Traveling Salesman Problem, the 2-Opt, can be efficiently implemented in hardware for Euclidean TSP instances up to a few hundred cities. We introduce the notion of "symmetrical 2-Opt moves" which allows us to uncover fine-grain parallelism when executing the specified algorithm. We propose a novel architecture that exploits this parallelism. A subset of the TSPLIB benchmark is used to evaluate the proposed architecture and its ASIC implementation, which exhibits better final results and an average speedup of 20 when compared with the state-of-the-art software implementation. Our approach produces, to the best of our knowledge, the fastest to date TSP 2-Opt solver for small-scale Euclidean TSP instances. © 2007 IEEE.},
  affiliation   = {Microprocessor and Hardware Lab. (MHL), Technical University of Crete (TUC), Kounoupidiana, Crete, GR73100, Greece},
  art_number    = {4228483},
  document_type = {Conference Paper},
  doi           = {10.1109/RSP.2007.24},
  journal       = {Proceedings of the International Workshop on Rapid System Prototyping},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34548714164&doi=10.1109%2fRSP.2007.24&partnerID=40&md5=8d1d1caf374e5665bb77f5e821922f6f},
}

@Article{Tsouloupas2007,
  author          = {Tsouloupas, G. and Dikaiakos, M.D.},
  title           = {GridBench: A tool for the interactive performance exploration of Grid infrastructures},
  journal         = {Journal of Parallel and Distributed Computing},
  year            = {2007},
  volume          = {67},
  number          = {9},
  pages           = {1029-1045},
  note            = {cited By 17},
  __markedentry   = {[Nichl:6]},
  abstract        = {As Grids rapidly expand in size and complexity, the task of benchmarking and testing, interactive or unattended, quickly becomes unmanageable. In this article we describe the difficulties of testing/benchmarking resources in large Grid infrastructures and we present the software architecture implementation of GridBench, an extensible tool for testing, benchmarking and ranking of Grid resources. We give an overview of GridBench services and tools, which support the easy definition, invocation and management of tests and benchmarking experiments. We also show how the tool can be used in the analysis of benchmarking results and how the measurements can be used to complement the information provided by Grid Information Services and used as a basis for resource selection and user-driven resource ranking. In order to illustrate the usage of the tool, we describe scenarios for using the GridBench framework to perform test/benchmark experiments and analyze the results. © 2007 Elsevier Inc. All rights reserved.},
  affiliation     = {Department of Computer Science, University of Cyprus, 75 Kallipoleos Street, CY-1678 Nicosia, Cyprus},
  author_keywords = {Benchmarking; Grids; Performance; Ranking; Testing},
  document_type   = {Article},
  doi             = {10.1016/j.jpdc.2007.04.009},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34547196921&doi=10.1016%2fj.jpdc.2007.04.009&partnerID=40&md5=8fefae36053121ecb8e8c06f73b8a982},
}

@Conference{Hunt2007,
  author          = {Hunt, G. and Aiken, M. and Fähndrich, M. and Hawblitzel, C. and Hodson, O. and Larus, J. and Levi, S. and Steensgaard, B. and Tarditi, D. and Wobber, T.},
  title           = {Sealing OS processes to improve dependability and safety},
  year            = {2007},
  pages           = {341-354},
  note            = {cited By 28},
  __markedentry   = {[Nichl:6]},
  abstract        = {In most modern operating systems, a process is a hardware-protected abstraction for isolating code and data. This protection, however, is selective. Many common mechanisms - -dynamic code loading, run-time code generation, shared memory, and intrusive system APIs - -make the barrier between processes very permeable. This paper argues that this traditional open process architecture exacerbates the dependability and security weaknesses of modern systems. As a remedy, this paper proposes a sealed process architecture, which prohibits dynamic code loading, self-modifying code, shared memory, and limits the scope of the process API. This paper describes the implementation of the sealed process architecture in the Singularity operating system, discusses its merits and drawbacks, and evaluates its effectiveness. Some benefits of this sealed process architecture are: improved program analysis by tools, stronger security and safety guarantees, elimination of redundant overlaps between the OS and language runtimes, and improved software engineering. Conventional wisdom says open processes are required for performance; our experience suggests otherwise. We present the first macrobenchmarks for a sealed-process operating system and applications. The benchmarks show that an experimental sealed-process system can achieve performance competitive with highly-tuned, commercial, open-process systems. Copyright 2007 ACM.},
  affiliation     = {Microsoft Research, One Microsoft Way, Redmond, WA 98052, United States},
  author_keywords = {Open process architecture; Sealed kernel; Sealed process architecture; Software isolated process (SIP)},
  document_type   = {Conference Paper},
  doi             = {10.1145/1272996.1273032},
  journal         = {Operating Systems Review (ACM)},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34548049533&doi=10.1145%2f1272996.1273032&partnerID=40&md5=28fb455ff97e4ccaf4de37435b3923b1},
}

@Article{TravininBliss2007,
  author          = {Travinin Bliss, N. and Kepner, J.},
  title           = {pMatlab parallel MATLAB library},
  journal         = {International Journal of High Performance Computing Applications},
  year            = {2007},
  volume          = {21},
  number          = {3},
  pages           = {336-359},
  note            = {cited By 32},
  __markedentry   = {[Nichl:6]},
  abstract        = {MATLAB® has emerged as one of the languages most commonly used by scientists and engineers for technical computing, with approximately one million users worldwide. The primary benefits of MATLAB are reduced code development time via high levels of abstractions (e.g. first class multi-dimensional arrays and thousands of built in functions), interpretive, interactive programming, and powerful mathematical graphics. The compute intensive nature of technical computing means that many MATLAB users have codes that can significantly benefit from the increased performance offered by parallel computing. pMatlab provides this capability by implementing parallel global array semantics using standard operator overloading techniques. The core data structure in pMatlab is a distributed numerical array whose distribution onto multiple processors is specified with a "map" construct. Communication operations between distributed arrays are abstracted away from the user and pMatlab transparently supports redistribution between any block-cyclic-overlapped distributions up to four dimensions. pMatlab is built on top of the MatlabMPI communication library and runs on any combination of heterogeneous systems that support MATLAB, which includes Windows, Linux, MacOS X, and SunOS. This paper describes the overall design and architecture of the pMatlab implementation. Performance is validated by implementing the HPC Challenge benchmark suite and comparing pMatlab performance with the equivalent C+MPI codes. These results indicate that pMatlab can often achieve comparable performance to C+MPI, usually at one tenth the code size. Finally, we present implementation data collected from a sample of real pMatlab applications drawn from the approximately one hundred users at MIT Lincoln Laboratory. These data indicate that users are typically able to go from a serial code to an efficient pMatlab code in about 3 hours while changing less than 1% of their code. © 2007 SAGE Publications.},
  affiliation     = {MIT Lincoln Laboratory, 244 Wood Street, Lexington, MA 02420, United States},
  author_keywords = {HPC challenge; Parallel computing; Parallel MATLAB; Parallel programming models},
  document_type   = {Article},
  doi             = {10.1177/1094342007078446},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34447556978&doi=10.1177%2f1094342007078446&partnerID=40&md5=0a7cc2aa7ee922cafc13b0ecb5a78597},
}

@Article{Zhu2007,
  author          = {Zhu, W. and Niu, Y. and Gao, G.R.},
  title           = {Performance portability on EARTH: A case study across several parallel architectures},
  journal         = {Cluster Computing},
  year            = {2007},
  volume          = {10},
  number          = {2},
  pages           = {115-126},
  note            = {cited By 3},
  __markedentry   = {[Nichl:6]},
  abstract        = {Due to the increase of the diversity of parallel architectures, and the increasing development time for parallel applications, performance portability has become one of the major considerations when designing the next generation of parallel program execution models, APIs, and runtime system software. This paper analyzes both code portability and performance portability of parallel programs for fine-grained multi-threaded execution and architecture models. We concentrate on one particular event-driven fine-grained multi-threaded execution model - EARTH, and discuss several design considerations of the EARTH model and runtime system that contribute to the performance portability of parallel applications. We believe that these are important issues for future high end computing system software design. Four representative benchmarks were conducted on several different parallel architectures, including two clusters listed in the 23rd supercomputer TOP500 list. The results demonstrate that EARTH based programs can achieve robust performance portability across the selected hardware platforms without any code modification or tuning. © Springer Science+Business Media, LLC 2007.},
  affiliation     = {Department of Electrical and Computer Engineering, University of Delaware, Newark, DE 19716, United States},
  author_keywords = {Code portability; EARTH; Fine-grained multithreading; Performance portability; Programming execution model},
  document_type   = {Article},
  doi             = {10.1007/s10586-007-0011-1},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34248674645&doi=10.1007%2fs10586-007-0011-1&partnerID=40&md5=61fab285e54175e5d26a8227e0bb9d8f},
}

@Article{Zhu2007a,
  author          = {Zhu, L. and Bui, N.B. and Liu, Y. and Gorton, I.},
  title           = {MDABench: Customized benchmark generation using MDA},
  journal         = {Journal of Systems and Software},
  year            = {2007},
  volume          = {80},
  number          = {2},
  pages           = {265-282},
  note            = {cited By 11},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper describes an approach for generating customized benchmark suites from a software architecture description following a Model Driven Architecture (MDA) approach. The benchmark generation and performance data capture tool implementation (MDABench) is based on widely used open source MDA frameworks. The benchmark application is modeled in UML and generated by taking advantage of the existing community-maintained code generation "cartridges" so that current component technology can be exploited. We have also tailored the UML 2.0 Testing Profile so architects can model the performance testing and data collection architecture in a standards compatible way. We then extended the MDA framework to generate a load testing suite and automatic performance measurement infrastructure. This greatly reduces the effort and expertise needed for benchmarking with complex component and Web service technologies while being fully MDA standard compatible. The approach complements current model-based performance prediction and analysis methods by generating the benchmark application from the same application architecture that the performance models are derived from. We illustrate the approach using two case studies based on Enterprise JavaBean component technology and Web services. © 2006 Elsevier Inc. All rights reserved.},
  affiliation     = {Empirical Software Engineering Program, National ICT Australia Ltd., School of Computer Science and Engineering, University of New South Wales, Garden Street, Eveleigh NSW 1430, Australia; School of Computer Science and Engineering, University of New South Wales, Australia; Faculty of Information Technology, University of Technology Sydney, Australia},
  author_keywords = {Code generation; MDA; Model-driven development; Performance; Testing},
  document_type   = {Article},
  doi             = {10.1016/j.jss.2006.10.052},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33846238538&doi=10.1016%2fj.jss.2006.10.052&partnerID=40&md5=3e1d176915160ae8c0eb1f4e95ad8ba9},
}

@Article{Moench2007,
  author          = {Mönch, L.},
  title           = {Simulation-based benchmarking of production control schemes for complex manufacturing systems},
  journal         = {Control Engineering Practice},
  year            = {2007},
  volume          = {15},
  number          = {11},
  pages           = {1381-1393},
  note            = {cited By 39},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, benchmarking efforts for production control approaches applied to complex manufacturing systems are described. Complex manufacturing systems are characterized by a large number of products, an over time changing product mix, sequence-dependent set-up times, unrelated parallel machines, a mix of different process types, and internal and external disturbances. In order to ensure comparison possibilities among different production control approaches, the usage of a simulation test-bed and a software architecture that allows for plug in of different production control software are suggested. Different application areas, the advantages, and also the limitations of the suggested approach are discussed. © 2006 Elsevier Ltd. All rights reserved.},
  affiliation     = {Faculty for Mathematics and Computer Science, FernUniversität in Hagen, 58097 Hagen, Germany},
  author_keywords = {Benchmark examples; Complex manufacturing systems; Performance measures; Simulation-based performance evaluation; Software architecture},
  document_type   = {Article},
  doi             = {10.1016/j.conengprac.2006.05.010},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34548321751&doi=10.1016%2fj.conengprac.2006.05.010&partnerID=40&md5=b4f065493ed72b45a293210e54e6be55},
}

@Conference{Bo2006,
  author          = {Bo, J. and Jinfang, Z. and Junda, Z. and Liping, Q.},
  title           = {Fastpath-based VPN gateway over network processor},
  year            = {2006},
  number          = {525},
  pages           = {86},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Networks, along with the explosive growth of the Internet, developed quickly and make our lives more convenient and efficient. However, at the same time, people will face much more network security problems such as illegal observation, modification, IP Spoofing, etc. This article proposes a novel type of universal software architecture named Fastpath which is purposely designed and optimized for network element (NE) in the next generation networks (NGN), under Linux. Furthermore, it illustrates the experience in developing IPSec-based VPN gateway over IXP425, and offloading IPSec processing to NPEs and coprocessors by using APIs of software AccessLibrary through Open Crypto Framework (OCF). Finally, we investigate the performance issues both internally and externally. Through the benchmarks, we analyze the performance of the system and identify that Fastpath and network processor together can construct a universal and high-functional platform for NE which focuses on network processing.},
  affiliation     = {Dept. of Inf. Sci. and Electron. Eng., Zhejiang Univ., Hangzhou, Zhejiang 310027, China},
  author_keywords = {Fastpath; IPSec; IXP425; Network Element (NE); Network Processor; VPN},
  document_type   = {Conference Paper},
  doi             = {10.1049/cp:20061251},
  journal         = {IET Conference Publications},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34250615129&doi=10.1049%2fcp%3a20061251&partnerID=40&md5=b860486c8ba71c8301b919657a8a0a2d},
}

@Article{Soundararajan2006,
  author        = {Soundararajan, K. and Brennan, R.W.},
  title         = {Design patterns for distributed control system benchmarking},
  journal       = {IFIP International Federation for Information Processing},
  year          = {2006},
  volume        = {220},
  pages         = {99-108},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {In this paper, we describe the design and development of a simulation-agent interface for real-time distributed control system benchmarking. This work is motivated by the need to test the feasibility of extending agent-based system to the physical device level in manufacturing and other industrial automation systems. Our work focuses on the development of hybrid physical/simulation environment that can be used to perform tests at both the physical device level, as well as the planning and scheduling level of control. As part of this work, we have extended the proxy design pattern for this application. This paper focuses on the resulting software design pattern for distributed control system benchmarking and provides examples of its use in our hybrid physical/simulation environment. © 2006 International Federation for Information Processing.},
  affiliation   = {Schulich School of Engineering, University of Calgary, 2500 University Dr. N. W., Calgary, AB T2N 1N4, Canada},
  document_type = {Article},
  doi           = {10.1007/978-0-387-36594-7_11},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33947168254&doi=10.1007%2f978-0-387-36594-7_11&partnerID=40&md5=2a1ce5328c62497924622d7a5fe17b16},
}

@Conference{Fahey2006,
  author        = {Fahey, M.R.},
  title         = {Portable performance optimizations based on a performance history of the fusion microturbulence code GYRO},
  year          = {2006},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {We will show the performance history of the fusion code GYRO on two diverse architectures, namely a Cray XT3 and a Cray X1E. The history shows performance of selected versions of GYRO for two standard benchmarks. For this comparison, we choose one compiler version on each machine and keep it constant for all the tests and run the tests over a short time frame for consistency. We will present Cray X1E and Cray XT3 results; we expect to present dual-core XT3 results as well, time-permitting, following an upgrade this summer. The contribution of this work will be a summary of the performance enhancements that are portable to three classes of machines: vector, scalar, and multi-core scalar. Getting good performance on modern HPC systems is more than just finding the right compiler options. Knowing what performance optimizations are portable across a variety of machines is essential. © 2006 IEEE.},
  art_number    = {1188650},
  document_type = {Conference Paper},
  doi           = {10.1145/1188455.1188650},
  journal       = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing, SC'06},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34548234526&doi=10.1145%2f1188455.1188650&partnerID=40&md5=cd97ac450a10f6a3700674feaa0fcb4f},
}

@Conference{Luszczek2006,
  author        = {Luszczek, P.R. and Bailey, D.H. and Dongarra, J.J. and Kepner, J. and Lucas, R.F. and Rabenseifner, R. and Takahashi, D.},
  title         = {The HPC Challenge (HPCC) benchmark suite},
  year          = {2006},
  note          = {cited By 117},
  __markedentry = {[Nichl:6]},
  abstract      = {In 2003, the DARPA's High Productivity Computing Systems released the HPCC suite. It examines the performance of HPC architectures using kernels with various memory access patterns of well known computational kernels. Consequently, HPCC results bound the performance of real applications as a function of memory access characteristics and define performance boundaries of HPC architectures. The suite was intended to augment the TOP500 list and by now the results are publicly available for 6 out of 10 of the world's fastest computers. Implementations exist in most of the major high-end programming languages and environments, accompanied by countless optimization efforts. The increased publicity enjoyed by HPCC doesn't necessarily translate into deeper understanding of the performance issues that HPCC benchmarks. And so this tutorial will introduce attendees to HPCC, provide tools to examine differences in HPC architectures, and give hands-on training that will hopefully lead to better understanding of parallel environments. © 2006 IEEE.},
  art_number    = {1188677},
  document_type = {Conference Paper},
  doi           = {10.1145/1188455.1188677},
  journal       = {Proceedings of the 2006 ACM/IEEE Conference on Supercomputing, SC'06},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34548294753&doi=10.1145%2f1188455.1188677&partnerID=40&md5=d9a2cf67f8e69b02e2344ee22c5c9496},
}

@Conference{Kong2006,
  author          = {Kong, J. and Zou, C.C. and Zhou, H.},
  title           = {Improving software security via runtime instruction-level taint checking},
  year            = {2006},
  pages           = {18-24},
  note            = {cited By 19},
  __markedentry   = {[Nichl:6]},
  abstract        = {Current taint checking architectures monitor tainted data usage mainly with control transfer instructions. An alarm is raised once the program counter becomes tainted. However, such architectures are not effective against non-control data attacks. In this paper we present a generic instruction-level runtime taint checking architecture for handling non-control data attacks. Under our architecture, instructions are classified as either Taintless-Instructions or Tainted-Instructions prior to program execution. An instruction is called a Tainted-Instruction if it is supposed to deal with tainted data. Otherwise it is called a Taintless-Instruction. A security alert is raised whenever a Taintless-Instruction encounters tainted data at runtime. The proposed architecture is implemented on the SimpleScalar simulator. The preliminary results from experiments on SPEC CPU 2000 benchmarks show that there are a significant amount of Taintless-Instructions. We also demonstrate effective usages of our architecture to detect buffer overflow and format string attacks. Copyright 2006 ACM.},
  affiliation     = {School of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL 32816, United States},
  author_keywords = {Buffer overflow; Format string; Hardware tagging},
  document_type   = {Conference Paper},
  doi             = {10.1145/1181309.1181313},
  journal         = {ASID'06: 1st Workshop on Architectural and System Support for Improving Software Dependability},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34547203977&doi=10.1145%2f1181309.1181313&partnerID=40&md5=ad6d57f23cec3839be80f754a77d9c9d},
}

@Conference{Seo2006,
  author        = {Seo, H. and Kim, S.W.},
  title         = {OpenMP directive extension for BlackFin 561 dual core processor},
  year          = {2006},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {Many researchers and vendors are exploiting the increasing number of transistors to build chip multiprocessors (CMPs) by partitioning a chip into multiple simple ILP cores. As in traditional multiprocessors, CMPs extract thread-level parallelism (TLP) from programs by running multiple independent program segments, i.e., threads, in parallel. Currently CMPs are used widely in high performance servers, and even in embedded systems. In this paper, we present an extension of the OpenMP shared directive for performance optimization on BlackFin 561 (ADSP-BF561) dual core processors. In order to support memory consistency between multiple cores, many architectures have been proposed. On the dual core processor, like ADSP-BF561, each core has its own private LI cache, and a shared L2 cache. In order to execute multithreaded parallel programs, we need to consider carefully where to allocate shared variables on targeted memory architecture. We could improve the speedup by up to 107% and reduce the energy consumption by up to 108% in our measured benchmarks with respect to no use of our extension. © 2006 IEEE.},
  affiliation   = {Compilers and Embedded Systems Laboratory, Department of Electronics and Computer Engineering, Korea University, Seoul, South Korea},
  art_number    = {4019870},
  document_type = {Conference Paper},
  doi           = {10.1109/CIT.2006.131},
  journal       = {Proceedings - Sixth IEEE International Conference on Computer and Information Technology, CIT 2006},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-34547265718&doi=10.1109%2fCIT.2006.131&partnerID=40&md5=89c323682b065762555337f785f49651},
}

@Conference{Takashi2006,
  author          = {Takashi, S. and Eiji, K. and Suguru, Y. and Heiichi, Y.},
  title           = {Performance anomalies of advanced web server architectures in realistic environments},
  year            = {2006},
  volume          = {1},
  pages           = {169-174},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {When we discuss performance of network servers, simple benchmark tests can mislead us into inaccurate and wrong results. In this paper, we present precise results of web server performance evaluations on emulated networks whose parameters include network delay and packet loss ratio. We chose three types of target web servers: Apache, thttpd, and TUX. Their software architectures are largely different from each other and their pros and cons are discussed from a general viewpoint in a number of previous literatures. However, we still found some performance anomalies in the experimental results, which revealed implementation issues in those servers.},
  affiliation     = {Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara, 6300101, Japan},
  art_number      = {1625550},
  author_keywords = {Apache; Delay; Loss; Network; Performance; Server; thttpd; TUX; Web},
  document_type   = {Conference Paper},
  journal         = {8th International Conference Advanced Communication Technology, ICACT 2006 - Proceedings},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33750958322&partnerID=40&md5=d1b1401a927320d87e4cd204351807b5},
}

@Article{Aniel-Buchheit2006,
  author          = {Aniel-Buchheit, S.},
  title           = {Simulation of the VVER-1000 pump start-up experiment in the OECD/DOE/CEA V1000CT benchmark by the FLICA4/CRONOS2 coupled code system},
  journal         = {Progress in Nuclear Energy},
  year            = {2006},
  volume          = {48},
  number          = {8},
  pages           = {773-789},
  note            = {cited By 1},
  __markedentry   = {[Nichl:6]},
  abstract        = {In the framework of joint effort between the Nuclear Energy Agency (NEA) of OECD, the United States Department of Energy (US DOE), and the Commissariat à l'Energie Atomique (CEA), France, a coupled 3-D thermal-hydraulics/neutron kinetics benchmark was defined. The overall objective of OECD/NEA V1000CT benchmark [Ivanov, B., Ivanov, K., Groudev, P., Pavlova, M., Hadjiev, V., 2002. VVER-1000 Coolant Transient Benchmark (V1000-CT). Phase 1 - Final Specifications, NEA/NSC/DOC] is to assess computer codes used in the analysis of VVER-1000 reactivity transients where mixing phenomena (mass flow and temperature) in the reactor pressure vessel are complex. Original data from the Kozloduy-6 Nuclear Power Plant are available for the validation of computer codes: one experiment of pump start-up (V1000CT-1) and one experiment of steam generator isolation (V1000CT-2). The CEA presented results for the V1000CT-1 Exercise 2 using a coupling of FLICA4 [Toumi, I., Gallo, D., Bergeron, A., Royer, E., Caruge, D., 2000. FLICA4: a three dimensional two-phase flow computer code with advanced numerical methods for nuclear applications. Nuclear Engineering and Design 200, 139-155] and CRONOS2 [Akherraz, B., Baudron, A.M., Lautard, J.J., Magnaud, C., Moreau, F., Schneider, D., Gonzales, M., 2004. Manuel de Référence CRONOS 2.6. Technical Report SERMA/LENR/RT/04-3433/A, CEA] via the coupling tool ISAS [Toumi, I., et al., 1995. Specifications of the general software architecture for code integration in ISAS. Euratom Fusion Technology, ITER task S81TT-01/1]. The FLICA4/CRONOS2 VVER-1000 model is based on the data available in the benchmark specifications. This paper summarizes the FLICA4/CRONOS2 model build-up with the associated sensitivity studies and presents the CEA results for V1000CT-1 Exercise 2 as well as a comparison with experimental results at hot power steady state (HP SS). © 2006.},
  affiliation     = {DM2S/SFME/Laboratoire d'Etudes Thermiques des Reacteurs, CEA Saclay, 91 191 Gif sur Yvette Cedex, France},
  author_keywords = {Benchmark; Code coupling; Neutronic; Pump start-up; Thermal-hydraulic; VVER},
  document_type   = {Article},
  doi             = {10.1016/j.pnucene.2006.06.006},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33751175154&doi=10.1016%2fj.pnucene.2006.06.006&partnerID=40&md5=cbd1fd74c89243a4b83f113fe1b96d3b},
}

@Conference{Chen2006,
  author          = {Chen, G. and Li, F. and Kandemir, M. and Irwin, M.J.},
  title           = {Reducing NoC energy consumption through compiler-directed channel voltage scaling},
  year            = {2006},
  volume          = {2006},
  pages           = {193-203},
  note            = {cited By 11},
  __markedentry   = {[Nichl:6]},
  abstract        = {While scalable NoC (Network-on-Chip) based communication architectures have clear advantages over long point-to-point communication channels, their power consumption can be very high. In contrast to most of the existing hardware-based efforts on NoC power optimization, this paper proposes a compiler-directed approach where the compiler decides the appropriate voltage/frequency levels to be used for each communication channel in the NoC. Our approach builds and operates on a novel graph based representation of a parallel program and has been implemented within an optimizing compiler and tested using 12 embedded benchmarks. Our experiments indicate that the proposed approach behaves better - from both performance and power perspectives - than a hardware-based scheme and the energy savings it achieves are very close to the savings that could be obtained from an optimal, but hypothetical voltage/frequency scaling scheme. Copyright © 2006 ACM.},
  affiliation     = {Computer Science and Engineering Department, Pennsylvania State University, University Park, PA 16802, United States},
  author_keywords = {Compiler; Energy; Network-on-Chip},
  document_type   = {Conference Paper},
  journal         = {Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33746085616&partnerID=40&md5=8b201c26e2f7a1ec80e8be9261267804},
}

@Article{Hua2006,
  author          = {Hua, L. and Wei, J. and Niu, C.-L. and Zheng, H.-R.},
  title           = {High performance SOAP processing based on dynamic template-driven mechanism},
  journal         = {Jisuanji Xuebao/Chinese Journal of Computers},
  year            = {2006},
  volume          = {29},
  number          = {7},
  pages           = {1145-1156},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, the authors first analyze the performance of SOAP processing on Java platform, and identify that data model mapping between XML data and Java data is the main impact factor on performance. Therefore, the authors propose a new paradigm of data model mapping-Dynamic Early Binding which enables to improve SOAP processing by avoiding Java reflection operations and proactively generating processing codes. This dynamic early binding is realized by Data Mapping Template (DMT), which is specified by extended context free grammar and implemented by pushdown automaton with output. The DMTs are generated and compiled at runtime and drive the data mapping process with XML Pull Parsing. The authors illustrate the effectiveness by applying it into a high performance SOAP engine, SOAPExpress, and yielding over 100% speedups compared to Apache Axis 1.2 in Sun Microsystems' benchmark-WS Test 1.0.},
  affiliation     = {Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing 100080, China},
  author_keywords = {Data model mapping; DMT; Performance; SOA; SOAP; Web services},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33747443813&partnerID=40&md5=894887223ccdf473b9a3e188d496f6c4},
}

@Conference{Gokhale2006,
  author        = {Gokhale, S. and Gokhale, A. and Gray, J. and Vandal, P. and Praphamontripong, U.},
  title         = {Performance analysis of the reactor pattern in network services},
  year          = {2006},
  volume        = {2006},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {The growing reliance on services provided by software applications places a high premium on the reliable and efficient operation of these applications. A number of these applications follow the event-driven software architecture style since this style fosters evolvability by separating event handling from event demultiplexing and dispatching functionality. The event demultiplexing capability, which appears repeatedly across a class of event-driven applications, can be codified into a reusable pattern, such as the Reactor pattern. In order to enable performance analysis of event-driven applications at design time, a model is needed that represents the event demultiplexing and handling functionality that lies at the heart of these applications. In this paper, we present a model of the Reactor pattern based on the well-established Stochastic Reward Net (SRN) modeling paradigm. We discuss how the model can be used to obtain several performance measures such as the throughput, loss probability and upper and lower bounds on the response time. We illustrate how the model can be used to obtain the performance metrics of a Virtual Private Network (VPN) service provided by a Virtual Router (VR). We validate the estimates of the performance measures obtained from the SRN model using simulation. © 2006 IEEE.},
  affiliation   = {University of Connecticut, Dept. of Computer Science and Engineering, Storrs, CT 06269, United States; Vanderbilt University, Dept. of Electrical Engineering and Computer Science, Nashville, TN 37235, United States; University of Alabama at Birmingham, Dept of Computer and Information Science, Birmingham, AL, United States},
  art_number    = {1639639},
  document_type = {Conference Paper},
  doi           = {10.1109/IPDPS.2006.1639639},
  journal       = {20th International Parallel and Distributed Processing Symposium, IPDPS 2006},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33847148619&doi=10.1109%2fIPDPS.2006.1639639&partnerID=40&md5=6b55a08be80a3b101568655c5afbf259},
}

@Conference{2005,
  title         = {Software Architecture, 2005. WICSA 2005. 5th Working IEEE/IFIP Conference on 2005},
  year          = {2005},
  volume        = {2005},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 56 papers. The topics discussed include: quantitative observation and theoretical construction in software architecture; what architects should know about reverse engineering and reengineering; sparking research ideas from the friction between doctrine and reality; on the meeting of software architecture and reverse engineering; using architectural perspectives; customized benchmark generation using MDA; extending the ATAM architecture evaluation to product line architectures; predicting change impact in architecture design with Bayesian belief networks; a survey of the use and documentation of architectural design rationale; a declarative approach to architectural reflection; architecting session report; measuring architecting effort; architecture description languages in practice session report; components and services session report; architectural design decision session report; an architecture and its rationale; and position on ontology-based architecture.},
  document_type = {Conference Review},
  journal       = {Proceedings - 5th Working IEEE/IFIP Conference on Software Architecture, WICSA 2005},
  page_count    = {292},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33947173297&partnerID=40&md5=4727cfe72b62f7148c1256f93f4105f4},
}

@Conference{Bagby2005,
  author          = {Bagby, M. and Romero, R. and Sulprizio, B. and Uda, H. and Jaquish, J. and Harris Jr., F.C.},
  title           = {DiRT - Dust in Real-time: The specification process},
  year            = {2005},
  volume          = {2},
  pages           = {807-813},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {Dust in Real-time (DiRT) is a 3D dust visualization program and interactive computer benchmarking tool designed to model real world dust dynamics in a virtual environment, in real-time. The interactive benchmarking tool allows users to track and gauge the performance of their system's ability to render the dust by issuing real-time system and environment related reports. The system reports update, in accordance with the user's quality and realism settings. Similar to a video game, users will be able to "play" during the simulation by way of a simple vehicle simulator. This paper presents details of DiRT "s UML requirements specification, software architecture specifics, high and low-level design details, user interface principles and snapshots.},
  affiliation     = {Department of Computer Science and Engineering, University of Nevada, Reno, Reno, NV 89557, United States},
  author_keywords = {Benchmarking; Dust; Real-time computation; Requirements specification; Software architecture; Virtual environment},
  document_type   = {Conference Paper},
  journal         = {Proceedings of the 2005 International Conference on Software Engineering Research and Practice, SERP'05},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-60749134411&partnerID=40&md5=96b0daefd46035aed4d84337f41614f1},
}

@Article{Zhang2005,
  author        = {Zhang, C. and Gao, D. and Jacobsen, H.-A.},
  title         = {Generic middleware substrate through modelware},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2005},
  volume        = {3790 LNCS},
  pages         = {314-333},
  note          = {cited By 14},
  __markedentry = {[Nichl:6]},
  abstract      = {Conventional middleware architectures suffer from insufficient module-level reusability and the ability to adapt in face of functionality evolution and diversification. To overcome these deficiencies, we propose the Modelware methodology adopting the Model Driven Architecture (MDA) approach and aspect oriented programming (AOP). We advocate the use of models and views to separate intrinsic functionalities of middleware from extrinsic ones. This separation effectively lowers the concern density per component and fosters the coherence and the reuse of the components of middleware architectures. Comparing to the conventionally designed version, Modelware improves the standard benchmark performance by as much as 40% through architectural optimizations. Our evaluation also shows that Modelware considerably reduces coding efforts in supporting the funcitonal evolution of middleware and dramatically different application domains. © IFIP International Federation for Information Processing 2005.},
  affiliation   = {University of Toronto, Canada},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33646762822&partnerID=40&md5=8bb168b6f719c02739d3fd2f6f83d42e},
}

@Conference{Richter2005,
  author        = {Richter, K. and Jersak, M. and Ernst, R.},
  title         = {Early architecture exploration with SymTA/S},
  year          = {2005},
  pages         = {125-134},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {With increasingly parallel development of a system's hardware-software architecture on the one hand, and its functionality on the other hand, system integration, verification, and test happens ever later in the design process. In order to ultimately avoid costly re-designs, the system architecture has to more or less meet all requirements on the first try. In other words, the system architects face the challenge to make sufficiently good estimates and choices very early in the design when the implementation is not yet or -in case of re-used or supplied parts- at most partially available. This paper addresses the major limitations of the state-of-the-art benchmarking approach and outlines a structured and systematic architecture evaluation procedure. Based on the SymTA/S tool, the proposed approach explicitly supports estimated data and thereby enables a variety of architectural options to be explored and optimized in early design stages.},
  affiliation   = {Institute of Computer and Communication Network Engineering, Technical University at Braunschweig, Hans-Sommer-Strasse 66, D-38106 Braunschweig, Germany},
  document_type = {Conference Paper},
  journal       = {Tagungsband - Dagstuhl-Workshop MBEES: Modellbasierte Entwicklung eingebetteter Systeme, MBEES 2005},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84873453923&partnerID=40&md5=06e830f9b9a99a71cb163f0fc44e3338},
}

@Conference{2005a,
  title         = {2005 IEEE Symposium on Visual Languages and Human-centric Computing},
  year          = {2005},
  volume        = {2005},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 67 papers. The topics discussed include: unified modeling language 2.0; immersive integration for virtual and human-centered environments; a stagecast retrospective; a suite of visual languages for statistical survey specification; executable visual contracts; benchmarking for graph transformation; spoken programs; a toolkit for addressing HCI issues in visual language environments; visual specifications of correct spreadsheets; estimating the numbers of end users and enduser programmers; navigating software architectures with constant visual complexity; easing program comprehension by sharing navigation data; achieving flexibility in direct-manipulation programming environments by relaxing the edit-time grammar; a visual language and environment for specifying design tool event handling; a trainable system for recognizing diagrammatic sketch languages; applying eye-movement tracking to program visualization; and the design of an array visualization.},
  document_type = {Conference Review},
  journal       = {Proceedings - 2005 IEEE Symposium on Visual Languages and Human-Centric Computing},
  page_count    = {361},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33746439802&partnerID=40&md5=4fdaa5046a224c109a1f9b6024db467c},
}

@Article{Liu2005,
  author          = {Liu, Y. and Fekete, A. and Gorton, I.},
  title           = {Design-level performance prediction of component-based applications},
  journal         = {IEEE Transactions on Software Engineering},
  year            = {2005},
  volume          = {31},
  number          = {11},
  pages           = {928-941},
  note            = {cited By 62},
  __markedentry   = {[Nichl:6]},
  abstract        = {Server-side component technologies such as Enterprise JavaBeans (EJBs). NET, and CORBA are commonly used in enterprise applications that have requirements for high performance and scalability. When designing such applications, architects must select a suitable component technology platform and application architecture to provide the required performance. This is challenging as no methods or tools exist to predict application performance without building a significant prototype version for subsequent benchmarking. In this paper, we present an approach to predict the performance of component-based server-side applications during the design phase of software development. The approach constructs a quantitative performance model for a proposed application. The model requires inputs from an application-independent performance profile of the underlying component technology platform, and a design description of the application. The results from the model allow the architect to make early decisions between alternative application architectures in terms of their performance and scalability. We demonstrate the method using an EJB application and validate predictions from the model by implementing two different application architectures and measuring their performance on two different implementations of the EJB platform. © 2005 IEEE.},
  affiliation     = {IEEE, Australia; IEEE Computer Society, Australia; National ICT Australia (NICTA), NSW 1430, Australia; School of Information Technologies, Madsen Building F09, University of Sydney, NSW 2006, Australia},
  author_keywords = {Performance measures; Quality analysis and evaluation; Software architectures},
  document_type   = {Article},
  doi             = {10.1109/TSE.2005.127},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-33746830359&doi=10.1109%2fTSE.2005.127&partnerID=40&md5=905d679f5308e0fa4e364808cdf6cacb},
}

@Conference{Liu2005a,
  author        = {Liu, Y. and Gorton, I.},
  title         = {Performance prediction of J2EE applications using messaging protocols},
  year          = {2005},
  volume        = {3489},
  pages         = {1-16},
  note          = {cited By 13},
  __markedentry = {[Nichl:6]},
  abstract      = {Predicting the performance of component-based applications is difficult due to the complexity of the underlying component technology. This problem is exacerbated when a messaging protocol is introduced to create a loosely coupled software architecture. Messaging uses asynchronous communication, and must address quality of service issues such as message persistence and flow control. In this paper, we present an approach to predicting the performance of Java 2 Enterprise Edition (J2EE) applications using messaging services. The prediction is done during application design, without access to the application implementation. This is achieved by modeling the interactions among J2EE and messaging components using queuing network models, calibrating the performance model with architecture attributes associated with these components, and populating the model parameters using a lightweight, application-independent benchmark. Benchmarking avoids the need for prototype testing in order to obtain the value of model parameters, and thus reduces the performance prediction effort. A case study is carried out to predict the performance of a J2EE application with asynchronous communication. Analysis of the resulting predictions shows the error is within 15%. © Springer-Verlag Berlin Heidelberg 2005.},
  affiliation   = {National ICT Australia (NICTA), NSW 1430, Australia},
  document_type = {Conference Paper},
  journal       = {Lecture Notes in Computer Science},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-24944513000&partnerID=40&md5=c2056d281a3807a4c31d79f733115574},
}

@Article{Brightwell2005,
  author          = {Brightwell, R. and Camp, W. and Cole, B. and DeBenedictis, E. and Leland, R. and Tomkins, J. and Maccabe, A.B.},
  title           = {Architectural specification for massively parallel computers: An experience and measurement-based approach},
  journal         = {Concurrency Computation Practice and Experience},
  year            = {2005},
  volume          = {17},
  number          = {10},
  pages           = {1271-1316},
  note            = {cited By 13},
  __markedentry   = {[Nichl:6]},
  abstract        = {In this paper, we describe the hardware and software architecture of the Red Storm system developed at Sandia National Laboratories. We discuss the evolution of this architecture and provide reasons for the different choices that have been made. We contrast our approach of leveraging high-volume, mass-market commodity processors to that taken for the Earth Simulator. We present a comparison of benchmarks and application performance that support our approach. We also project the performance of Red Storm and the Earth Simulator. This projection indicates that the Red Storm architecture is a much more cost-effective approach to massively parallel computing. Published in 2005 by John Wiley & Sons, Ltd.},
  affiliation     = {Sandia National Laboratories, Scalable Computer Systems, P.O. Box 5800, Albuquerque, NM 87185-1110, United States; Department of Computer Science, University of New Mexico, Albuquerque, NM 87131-0001, United States},
  author_keywords = {Amdahl; Commodity processors; Distributed memory; Massively parallel computing; Shared memory; Supercomputing; Vector processors},
  document_type   = {Article},
  doi             = {10.1002/cpe.893},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-23244450120&doi=10.1002%2fcpe.893&partnerID=40&md5=7267e9e49d2e3b5db7d3fc29d79393e5},
}

@Article{Grundy2005,
  author          = {Grundy, J. and Cai, Y. and Liu, A.},
  title           = {SoftArch/MTE: Generating distributed system test-beds from high-level software architecture descriptions},
  journal         = {Automated Software Engineering},
  year            = {2005},
  volume          = {12},
  number          = {1},
  pages           = {5-39},
  note            = {cited By 17},
  __markedentry   = {[Nichl:6]},
  abstract        = {Most distributed system specifications have performance benchmark requirements, for example the number of particular kinds of transactions per second required to be supported by the system. However, determining the likely eventual performance of complex distributed system architectures during their development is very challenging. We describe SoftArch/MTE. a software tool that allows software architects to sketch an outline of their proposed system architecture at a high level of abstraction. These descriptions include client requests, servers, server objects and object services, database servers and tables, and particular choices of middleware and database technologies. A fully-working implementation of this system is then automatically generated from this high-level architectural description. This implementation is deployed on multiple client and server machines and performance tests are then automatically run for this generated code. Performance test results are recorded, sent back to the SoftArch/MTE environment and are then displayed to the architect using graphs or by annotating the original high-level architectural diagrams. Architects may change performance parameters and architecture characteristics, comparing multiple test run results to determine the most suitable abstractions to refine to detailed designs for actual system implementation. Further tests may be run on refined architecture descriptions at any stage during system development. We demonstrate the utility of our approach and prototype tool and the accuracy of our generated performance test-beds, for validating architectural choices during early system development. © 2005 Springer Science + Business Media, Inc.},
  affiliation     = {Department of Electrical and Computer Engineering, University of Auckland, Private Bag 92019, Auckland, New Zealand; Department of Computer Science, University of Auckland, Private Bag 92019, Auckland, New Zealand; Software Architectures and Component Technologies, CSIRO, Mathematical and Information Sciences, Locked Bag 17, North Ryde, NSW 1670, Sydney, Australia},
  author_keywords = {CASE tools; Performance engineering; Rapid prototyping; Software architecture evaluation; Test-bed generation},
  document_type   = {Review},
  doi             = {10.1023/B:AUSE.0000049207.62380.74},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-18744370792&doi=10.1023%2fB%3aAUSE.0000049207.62380.74&partnerID=40&md5=93cca6119dd1ca64a610090bfbbc435c},
}

@Article{Kawadia2005,
  author          = {Kawadia, V. and Kumar, P.R.},
  title           = {Principles and protocols for power control in wireless ad hoc networks},
  journal         = {IEEE Journal on Selected Areas in Communications},
  year            = {2005},
  volume          = {23},
  number          = {1},
  pages           = {76-88},
  note            = {cited By 283},
  __markedentry   = {[Nichl:6]},
  abstract        = {Transmit power control is a prototypical example of a cross-layer design problem. The transmit power level affects signal quality and, thus, impacts the physical layer, determines the neighboring nodes that can hear the packet and, thus, the network layer affects interference which causes congestion and, thus, affects the transport layer. It is also key to several performance measures such as throughput, delay, and energy consumption. The challenge is to determine where in the architecture the power control problem is to be situated, to determine the appropriate power level by studying its impact on several performance issues, to provide a solution which deals properly with the multiple effects of transmit power control, and finally, to provide a software architecture for realizing the solution. We distill some basic principles on power control, which inform the subsequent design process. We then detail the design of a sequence of increasingly complex protocols, which address the multidimensional ramifications of the power control problem. Many of these protocols have been implemented, and may be the only implementations for power control in a real system. It is hoped that the approach in this paper may also be of use in other topical problems in cross-layer design.},
  affiliation     = {Department of Electrical Engineering, Coordinated Science Laboratory, University, of Illinois, Urbana-Champaign, Urbana, IL 61801, United States; BBN Technologies, Cambridge, MA 02140, United States},
  author_keywords = {Design principles; Linux implementation; Power control},
  document_type   = {Conference Paper},
  doi             = {10.1109/JSAC.2004.837354},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-11844279744&doi=10.1109%2fJSAC.2004.837354&partnerID=40&md5=d8e5e9f0b692a73c0f16c06b94f6e9c0},
}

@Conference{Muskens2004,
  author        = {Muskens, J. and Chaudron, M. and Lange, C.},
  title         = {Investigations in applying metrics to multi-view architecture models},
  year          = {2004},
  volume        = {30},
  pages         = {372-379},
  note          = {cited By 11},
  __markedentry = {[Nichl:6]},
  abstract      = {The goal of our research is to develop industry-proof software architecture and design metrics. We identify a number of problems that arise in computing software architecture and design metrics in industrial settings that were not encountered in computing source-code metrics. These problems include the absence of a single, unifying representation for architectures and they arise from the fact that architecture diagrams are used in an informal manner. In this paper we describe our approach towards defining metrics for architectures and designs which are represented in the 4+1 views paradigm using UML. We report our experiences with architectural metrics in industrial settings.},
  affiliation   = {Department of Mathematics Science, Technische Universiteit Eindhoven, P.O. Box 513, 5600 MB Eindhoven, Netherlands},
  document_type = {Conference Paper},
  journal       = {Conference Proceedings of the EUROMICRO},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-11844299011&partnerID=40&md5=534805cc92d97851b555408317a887cf},
}

@Article{Blain2004,
  author          = {Blain, D. and Garzon, M. and Shin, S.-Y. and Zhang, B.-T. and Kashiwamura, S. and Yamamoto, M. and Kameda, A. and Ohuchi, A.},
  title           = {Development, evaluation and benchmarking of simulation software for biomolecule-based computing},
  journal         = {Natural Computing},
  year            = {2004},
  volume          = {3},
  number          = {4},
  pages           = {427-442},
  note            = {cited By 7},
  __markedentry   = {[Nichl:6]},
  abstract        = {Simulators for biomolecular computing, (both in vitro and in silico), have come to play an important role in experimentation, analysis, and evaluation of the efficiency and scalability of DNA and biomolecule based computing. Simulation in silico of DNA computing is useful to support DNA-Computing algorithm design and to reduce the cost and effort of lab experiments. Although many simulations have now been developed, there exists no standard for simulation software in this area. Reliability, performance benchmarks, user interfaces, and accessibility are arguably the most important criteria for development and wide spread use of simulation software for BMC. The requirements and evaluation of such software packages for DNA computing software are discussed, particularly questions about software development, appropriate user environments, standardization of benchmark data sets, and centrally available common repositories for software and/or data. © 2004 Kluwer Academic Publishers.},
  affiliation     = {Computer Science Division, The University of Memphis, Memphis, TN 38152-3240, United States; Biointelligence Lab., Sch. of Computer Science/Engineering, Seoul National University, 151-742 Seoul, South Korea; Graduate School of Engineering, Hokkaido University, North 13, West 8, Kita-ku, Sapporo 060-8628, Japan; Japan Science/Technology Corporation, Saitama, Japan; Grad. Sch. of Info. Sci./Technology, Hokkaido University, North 14, West 9, Kita-ku, Sapporo 060-0814, Japan},
  author_keywords = {Access and common repository; Benchmarking; DNA-based computing; Evaluation; Simulation; Software architecture; User interfaces},
  document_type   = {Article},
  doi             = {10.1007/s11047-004-2644-9},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-13944274132&doi=10.1007%2fs11047-004-2644-9&partnerID=40&md5=a2e5c5983586551409866d473f55c101},
}

@Conference{Pocheville2004,
  author          = {Pocheville, A. and Kheddar, A. and Yokoi, K.},
  title           = {I-TOUCH: A generic multimodal framework for industry virtual prototyping},
  year            = {2004},
  pages           = {65-66},
  note            = {cited By 10},
  __markedentry   = {[Nichl:6]},
  abstract        = {Simulations based on virtual reality techniques make often special arrangements for haptic rendering. In fact, in most cases, haptic rendering drives the design of the simulation engine. This work proposes alternative software architecture to handle multimodal and human centered interactive rendering with a particular emphasis for the computer haptics problem. Namely, the architecture allows handling both haptic devices requirements, in terms of high refresh rates, and physically-based simulations requirements, in terms of CPU time. The developed I-TOUCH framework is designed to address these issues; in the meantime, it provides an open architecture and powerful tools to benchmark robustness of subsequent algorithms. All undergoing developments are being tested with actual industry virtual prototyping scenarios, the complexity of some of which highlights the extent of the fundamental problems to overcome. © 2004 IEEE.},
  affiliation     = {Laboratoire Systèmes Complexes, CNRS, UEVE, 40, rue du Pelvoux, 91020 Evry, France; AIST-CNRS Joint Robotics Laboratory, JRL IS, Natl Inst of AIST, Tsukuba Central 2, Japan},
  author_keywords = {Computer haptics; Haptic feedback; I-TOUCH; Virtual prototyping},
  document_type   = {Conference Paper},
  journal         = {2004 1st IEEE Technical Exhibition Based Conference on Robotics and Automation, Proceedings, TExCRA 2004},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-20844463232&partnerID=40&md5=b00d4ee0fe6094f44632b5e1ebb7c847},
}

@Conference{Liu2004,
  author          = {Liu, T.J.},
  title           = {Software architecture of a network fault management system in real time},
  year            = {2004},
  pages           = {134-140},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper describes an software architecture of an integrated network Fault Management system, which monitors the service fault and outage alarms in real time fashion. In addition, the performance evaluation is conducted. The response time is recorded and studied by employing WAS (Web Application Stress Tool) with Windows 2000 server operating system in a networked distributed system. The simulated environment involves an isolated LAN with 100 Mb bandwidth and the deployment of 16 Windows 2000 Servers as regional network data operators. The adoption of SonicMQ provides the pub/sub mechanism to publish the GUI Map into display in real time. The Orbix Notification Server is used to receive the simulated alarms from the network system. After the data correlation process, the output and correlated data will be saved into SQL database of Framework Server and then data will be published into Service Side SQL tables, e.g. Node, Facility and Service tables. After analysis of Alarm Analyzer, Alarm Publisher will send the result through SonicMQ Pub/Sub server to a simulated display, which is Message Monitor in this project This work benchmarks the performance of a simulated Network Fault management application in real time by employing WAS - Web Stress Test Tool.},
  affiliation     = {New Jersey City University, United States},
  author_keywords = {Alarm; Analyzer; Fault Management; Persistor; Publisher; Subscriber},
  document_type   = {Conference Paper},
  journal         = {Proceedings of the International Conference on Communications in Computing, CIC'04},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-16244384512&partnerID=40&md5=4784ec1cfb334e12e046f9ef2342b9fa},
}

@Article{Stewart2004,
  author          = {Stewart, C. and Shen, K. and Dwarkadas, S. and Scott, M.L. and Yin, J.},
  title           = {Profile-driven component placement for cluster-based online services},
  journal         = {IEEE Distributed Systems Online},
  year            = {2004},
  volume          = {5},
  number          = {10},
  note            = {cited By 6},
  __markedentry   = {[Nichl:6]},
  abstract        = {The growth of the Internet and of various intranets has spawned a wealth of online services, most of which are implemented on local-area clusters using remote invocation (for example, remote procedure call/remote method invocation) among manually placed application components. Component placement can be a significant challenge for large-scale services, particularly when application resource needs are workload dependent. Automatic component placement has the potential to maximize overall system throughput. The key idea is to construct (offline) a mapping between input workload and individual-component resource consumption. Such mappings, called component profiles, then support high-performance placement. Preliminary results on an online auction benchmark based on J2EE (Java 2 Platform, Enterprise Edition) suggest that profile-driven tools can identify placements that achieve near-optimal overall throughput.},
  affiliation     = {University of Rochester, United States; IBM Research; Computer Science Dept., Univ. of Rochester, Rochester, NY 14627-0226, United States; IBM T.J. Watson Research Center, Hawthorne, NY 10532, United States},
  author_keywords = {Clusters; Component placement; Component profile; Enterprise javabeans; Online services; Rubis},
  document_type   = {Review},
  page_count      = {15},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-9144239457&partnerID=40&md5=2fd02422f14e7ac3f3b871c34142430f},
}

@Conference{Denaro2004,
  author          = {Denaro, G. and Polini, A. and Emmerich, W.},
  title           = {Early performance testing of distributed software applications},
  year            = {2004},
  pages           = {94-103},
  note            = {cited By 80},
  __markedentry   = {[Nichl:6]},
  abstract        = {Performance characteristics, such as response time, throughput and scalability, are key quality attributes of distributed applications. Current practice, however, rarely applies systematic techniques to evaluate performance characteristics. We argue that evaluation of performance is particularly crucial in early development stages, when important architectural choices are made. At first glance, this contradicts the use of testing techniques, which are usually applied towards the end of a project. In this paper, we assume that many distributed systems are built with middleware technologies, such as the Java 2 Enterprise Edition (J2EE) or the Common Object Request Broker Architecture (CORBA). These provide services and facilities whose implementations are available when architectures are defined. We also note that it is the middleware functionality, such as transaction and persistence services, remote communication primitives and threading policy primitives, that dominate distributed system performance. Drawing on these observations, this paper presents a novel approach to performance testing of distributed applications. We propose to derive application-specific test cases from architecture designs so that performance of a distributed application can be tested using the middleware software at early stages of a development process. We report empirical results that support the viability of the approach.},
  affiliation     = {Università di Milano Bicocca, Dipartimento di Informatica, I-20126, Milano, Italy; ISTI-CNR, Via Moruzzi, 1, I-56124 Pisa, Italy; University College London, Dept. of Computer Science, WC1E 6BT, London, United Kingdom},
  author_keywords = {Distributed Software Architecture; Middleware; Performance Analysis Models; Software Performance; Software Testing},
  document_type   = {Conference Paper},
  journal         = {Proceedings of the Fourth International Workshop on Software and Performance, WOSP'04},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-3543055843&partnerID=40&md5=d3cd5a67c076e39857ff9bc2d93f1a65},
}

@Conference{Menasce2004,
  author        = {Menascé, D.A. and Ruan, H. and Gomaa, H.},
  title         = {A framework for QoS-aware software components},
  year          = {2004},
  pages         = {186-196},
  note          = {cited By 28},
  __markedentry = {[Nichl:6]},
  abstract      = {The next generation of software systems will be highly distributed, component-based, service-oriented, will need to operate in unattended mode and possibly in hostile environments, will be composed of a large number of "replaceable" components discoverable at run-time, and will have to run on a multitude of unknown and heterogeneous hardware and network platforms. This paper focuses on service oriented-architectures in which each component provides a set of interrelated services to other components. These components are QoS-aware (i.e., aware of Quality of Service requirements) and are capable of engaging in QoS negotiations with other components of a distributed application. The main contributions of this paper are: i) the description of an architecture for QoS-aware software components that are able to negotiate QoS requirements with other components, ii) the specification of the protocols used for QoS negotiation and admission control at the QoS-aware components, iii) a report on the implementation of a QoS-aware component, and iv) the experimental validation of the ideas presented in the paper.},
  affiliation   = {Dept. of Computer Science, George Mason University, Fairfax, VA 22030, United States; Dept. of Information Engineering, George Mason University, Fairfax, VA 22030, United States},
  document_type = {Conference Paper},
  journal       = {Proceedings of the Fourth International Workshop on Software and Performance, WOSP'04},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-1842440585&partnerID=40&md5=fff6ccaebf7a6d58c35307d871c9e9f3},
}

@Conference{Botturi2004,
  author        = {Botturi, D. and Castellani, A. and Moschini, D. and Fiorini, P.},
  title         = {Performance evaluation of task control in teleoperation},
  year          = {2004},
  volume        = {2004},
  number        = {4},
  pages         = {3690-3695},
  note          = {cited By 2},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper presents theoretical and experimental results of robot interaction control schemes for a teleoperation system whose slave end effector comes in contact with a compliant surface. A salient feature of this work a comparison of different control schemes implemented on a system whose slave consists of an industrial robot with an open software architecture and is equipped with a wrist force sensor. Three control strategies are considered; one based on a single regulator PID giving the nominal performance, the second based on impedance control, and the last one based on a hybrid system approach. The performance of the various schemes is compared on a contact and puncturing task of an elastic membrane. The comparison is first carried out in simulation and then simulations are validated with experiments. Quantitative as well as subjective performance measures are compared to assess the performance of the different control schemes.},
  affiliation   = {Department of Computer Science, University of Verona, Italy},
  document_type = {Conference Paper},
  journal       = {Proceedings - IEEE International Conference on Robotics and Automation},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-3042648649&partnerID=40&md5=d01cf60b2e98aaeae52648336a1bdb5d},
}

@Article{Okamura2004,
  author          = {Okamura, H. and Kuroki, S. and Dohi, T. and Osaki, S.},
  title           = {A reliability growth model for modular software},
  journal         = {Electronics and Communications in Japan, Part II: Electronics (English translation of Denshi Tsushin Gakkai Ronbunshi)},
  year            = {2004},
  volume          = {87},
  number          = {5},
  pages           = {43-53},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {This paper presents a probabilistic model for evaluating the reliability of software having modular structure. In particular, it proposes a new reliability-evaluating model based on the structure of software by integrating the Jelinski-Moranda model, which is the most basic black box model, and the Littlewood model, which is a representative white box model. The performance of the proposed model is compared and studied in reference to the benchmark model of Cheung and the dependency of the architecture and the reliability growth in software reliability evaluation is discussed. © 2004 Wiley Periodicals, Inc.},
  affiliation     = {Dept. of Information Engineering, Hiroshima University, Higashi-Hiroshima, 739-8527, Japan; IBM Japan, Ltd., Tokyo, 106-8711, Japan; Faculty of Math. Sci. and Info. Eng., Nanzan University, Seto, 4 89-0863, Japan},
  author_keywords = {Markov processes; Reliability growth; Software architecture; Software modules; Software reliability},
  document_type   = {Article},
  doi             = {10.1002/ecjb.20075},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-2442562256&doi=10.1002%2fecjb.20075&partnerID=40&md5=5483aeb6bd6f963ede98b5b300b13ab9},
}

@Article{Dimitrov2003,
  author        = {Dimitrov, R. and Skjellum, A.},
  title         = {Software architecture and performance comparison of MPI/Pro and MPICH},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2003},
  volume        = {2659},
  pages         = {307-315},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper presents a comparison of two implementations of the MPI standard [1] for message passing: MPI/Pro, a commercial implementation of the MPI standard from MPI Software Technology, Inc., and MPICH, an open source, high-performance, portable MPI implementation. This paper reviews key distinguishing architectural features of the two MPI implementations and presents comparative performance results from micro benchmarks and real applications. A discussion on the impact of MPI library architecture on performance is also offered. © Springer-Verlag Berlin Heidelberg 2003.},
  affiliation   = {MPI Software Technology, Inc., 101 S. Lafayette St, Starkville, MS 39759, United States},
  document_type = {Review},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-35248857845&partnerID=40&md5=eb426a1beb1389ed51b5201178aa87ab},
}

@Article{Kise2003,
  author        = {Kise, K. and Honda, H. and Yuba, T.},
  title         = {SimAlpha version 1.0: Simple and readable alpha processor simulator},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2003},
  volume        = {2823},
  pages         = {122-136},
  note          = {cited By 5},
  __markedentry = {[Nichl:6]},
  abstract      = {We have developed a processor simulator SimAlpha Version 1.0 for research and education activities. Its design policy is to keep the source code readable (enjoyable and easy to read) and simple. SimAlpha is written in C++ and the source code consists of only 2,800 lines. This paper describes the software architecture of SimAlpha by referring to its source code. To show an example of SimAlpha in practical use, we present the ideal instruction-level parallelism of SPEC CINT95 and CINT2000 benchmarks measured with a modified version of SimAlpha. © Springer-Verlag Berlin Heidelberg 2003.},
  affiliation   = {Graduate School of Information Systems, University of Electro-Communications, 1-5-1 Chofugaoka, Chofu-shi, Tokyo 182-8585, Japan; Information Infrastructure and Applications, PRESTO, Japan Science and Technology Corporation (JST), Japan},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-21144432085&partnerID=40&md5=5bcda69f53e742655a5115c0c8ce87d0},
}

@Conference{Hertel2003,
  author        = {Hertel, D.W. and Brogan, J.G.},
  title         = {Polaroid Scanner-Based Image Quality Measuring System},
  year          = {2003},
  pages         = {140-146},
  note          = {cited By 4},
  __markedentry = {[Nichl:6]},
  abstract      = {Image quality measuring techniques are an essential part of the Polaroid Image Quality Methodology. For spatial image quality evaluation, a flatbed scanner-based measuring system has been developed. It comprises image quality metrics as well as diagnostic tools for print uniformity and registration. Visual models are used to calculate graininess and sharpness from the measured physical image quality functions. The results are the basis for estimating system image quality. Special attention has been given to the software architecture to create a de-centralized, efficient, low-cost measuring system that can easily be deployed on-site. Modular architecture facilitates the addition of new metrics and diagnostic tools. Fully automated to eliminate user error, the software also addresses workflow efficiency with features such as a user-friendly interface, batch processing of scans, and auto-archiving of processed image files. Detailed reports in Excel spreadsheets allow seamless integration into the evaluation process and database. The system has become instrumental in the research and development of both silver halide and digital imaging systems, and is the workhorse for product evaluation, benchmarking, and competitive product analysis. It has been successfully deployed within Polaroid and at its program partners to monitor quality at media coating and hardware assembly facilities.},
  affiliation   = {Polaroid Corporation, Waltham, MA, United States},
  document_type = {Conference Paper},
  journal       = {Society for Imaging Science and Technology: Image Processing, Image Quality, Image Capture, Systems Conference},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-1542272397&partnerID=40&md5=38e94c4c0e81113223effa2b031902cf},
}

@Conference{2003,
  title         = {Proceedings: 25th International Conference on Software Engineering},
  year          = {2003},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contains 123 papers. Topics discussed include software engineering component technologies, testing, design recovery and documentation, formal methods, software design, software process, program analysis, software architecture, software understanding, consistency management and quality assurance, automotive software engineering, process analysis and improvement, process and tools, testing and fault correction, extreme programming, undergraduate education, course delivery and evaluation and processes and methodology.},
  document_type = {Conference Review},
  journal       = {Proceedings - International Conference on Software Engineering},
  page_count    = {827},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0037819590&partnerID=40&md5=e2cd92846942a0119e300268e04629cb},
}

@Conference{Crnkovic2002,
  author          = {Crnkovic, I. and Schmidt, H. and Stafford, J. and Wallnau, K.},
  title           = {5th ICSE workshop on component-based software engineering: Benchmarks for predictable assembly},
  year            = {2002},
  pages           = {655-656},
  note            = {cited By 4},
  __markedentry   = {[Nichl:6]},
  abstract        = {This workshop brings together researchers and practitioners from the community interested in predictable assembly from certifiable components. The goal of this workshop is to ensure continued collaboration among the members of this community. One output of the workshop will be an understanding of composition theory and how it applies to community model problems that were suggested at the workshop on component-based software engineering held at ICSE in 2001. A second output will be the identification of research opportunities that lie on the perimeter of predictable assembly.},
  affiliation     = {Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
  author_keywords = {Analysis; Certification; Component; Composition languages; Software architecture; Trusted components},
  document_type   = {Conference Paper},
  journal         = {Proceedings - International Conference on Software Engineering},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0036039193&partnerID=40&md5=9e2319c4d727ab0de3c9b14582d6c579},
}

@Conference{2002,
  title         = {Proceedings - International Software Metrics Symposium},
  year          = {2002},
  volume        = {2002-January},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 24 papers. The topics discussed include: an empirical validation of the relationship between the magnitude of relative error and project size; estimating software project effort by analogy based on linguistic values; dynamic coupling measures for object-oriented software; architectural tradeoffs at the object level; Gemini: maintenance support environment based on code clone analysis; avoiding architectural degeneration: an evaluation process for software architecture; software quality analysis code clones in industrial legacy software; an industrial case study to examine a non-traditional inspection implementation for requirements specifications; investigating the influence of inspector capability factors with four inspection techniques on inspection performance; and software inspection benchmarking - a qualitative and quantitative comparative opportunity.},
  document_type = {Conference Review},
  journal       = {Proceedings - International Software Metrics Symposium},
  page_count    = {237},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84948471889&partnerID=40&md5=74dd63d67a10c69cf53c02fc8951d72c},
}

@Article{2002a,
  title         = {IFIPWG 7.3 International Symposium on Computer Modeling, Measurement, and Evaluation, Performance 2002},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2002},
  volume        = {2459},
  pages         = {1-500},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contain 20 papers. The special focus in this conference is on Computer Modeling, Measurement, and Evaluation. The topics include: Multiple classes of positive customers, signals, and product form results; spectral expansion solutions for markov-modulated queues; M/G/1-type markov processes; an algorithmic approach to stochastic bounds; dynamic scheduling via polymatroid optimization; workload modeling for performance evaluation; capacity planning for web services; end-to-end performance of web services; benchmarking models and tools for distributed web-server systems; from an algebraic formalism to an architectural description language; automated performance and dependability evaluation using model checking; measurement-based analysis of system dependability using fault injection and field failure data; software reliability and rejuvenation; performance validation of mobile software architectures; performance issues of multimedia applications; heuristic phase type and MAP fitting of heavy tailed and fractal like samples; optimization of bandwidth and energy consumption in wireless local area networks; next generation internet computing and experiences of deploying a large scale testbed for e-science applications.},
  document_type = {Conference Review},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84957059459&partnerID=40&md5=94fc0f99ef2cdd1bd6cd1fba5814609e},
}

@Article{Sekharan2002,
  author        = {Sekharan, C.N. and Saranathan, K. and Sivakumar, R. and Taherbhai, Z.},
  title         = {Scalability and performance of multi-threaded algorithms for international fare construction on high-performance machines},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {2002},
  volume        = {2552},
  pages         = {452-460},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {We describe the design, implementation and performance of a project for constructing international fares at United Airlines. An efficient fare construction engine allows an airline to simplify, and automate the process of pricing fares in lucrative international markets. The impact of a fare engine to the revenues of a large airline has been estimated at between $20M and $60M per year. The goal is to design an efficient software system for handling 10 Gb of data pertaining to base fares from all airlines, and to generate over 250 million memory-resident records (fares). The software architecture uses a 64-bit, object- oriented, and multi-threaded approach and the hardware platforms used for benchmarking include a 24-CPU, IBM S-80 and 32-CPU, Hewlett-Packard Superdome. Two competing software designs using (i) dynamic memory and (ii) static memory are compared. Critical part of the design includes a scheduler that uses a heuristic for load balancing which is provably within a constant factor of optimality. Performance results are presented and discussed. © Springer-Verlag Berlin Heidelberg 2002.},
  affiliation   = {Department of Computer Science, Loyola University of Chicago, 6525 N. Sheridan RoadIL 60626, United States; Information Sciences Division- WHQKB, United Airlines, P.O.Box 66100, Chicago, IL 60666-0100, United States},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84945293716&partnerID=40&md5=2346ddf717350e23dc17fc12ee2f7b48},
}

@Article{Goddard2001,
  author        = {Goddard, S. and Jeffay, K.},
  title         = {Managing latency and buffer requirements in processing graph chains},
  journal       = {Computer Journal},
  year          = {2001},
  volume        = {44},
  number        = {6},
  pages         = {486-503},
  note          = {cited By 12},
  __markedentry = {[Nichl:6]},
  abstract      = {Real-time signal-processing applications for high assurance systems are commonly designed using a processing-graph software architecture. Here we demonstrate the management of latency and buffer requirements in such an architecture-the US Navy's processing graph method (PGM). By applying recent results in real-time scheduling theory to the subset of PGM employed by the US DARPA rapid prototyping of application-specific signal processors (RASSP) synthetic aperture radar (SAR) benchmark application, we identify inherent real-time properties of nodes in a PGM graph, and demonstrate how these properties can be exploited to perform useful and important system-level analyses such as schedulability analysis, end-to-end latency analysis, and memory requirements analysis. More importantly, we develop relationships between properties such as latency and buffer bounds and show how one may be traded off for the other.},
  affiliation   = {Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE 68588-0115, United States; Department of Computer Science, University of North Carolina at Chapel Hill, Chapel Hill, NC 27599-3175, United States},
  document_type = {Article},
  doi           = {10.1093/comjnl/44.6.486},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0035659186&doi=10.1093%2fcomjnl%2f44.6.486&partnerID=40&md5=da6c435b558eaad578a4f65ede597c04},
}

@Article{Blake2001,
  author        = {Blake, M.B.},
  title         = {Agent-oriented approaches to B2B interoperability},
  journal       = {Knowledge Engineering Review},
  year          = {2001},
  volume        = {16},
  number        = {4},
  pages         = {383-388},
  note          = {cited By 15},
  __markedentry = {[Nichl:6]},
  abstract      = {The use of agents in electronic commerce has been explored greatly over the past several years. A large majority of this effort is toward commerce where businesses have direct transactions with consumers (B2C). However, the transactions that occur between businesses (B2B) are far more prevalent than B2C. Research where agents are used for B2B can be classified in five basic areas, service discovery, mediation, negotiation, process management (be it workflow or supply-chain management), and evaluation. At the 2001 International Bi-Conference Sessions on Agent-Based Approaches to B2B Interoperability (AgentB2B), practitioners were invited to present their research and industry efforts in each of these areas. This paper summaries the work and conclusions presented at these two events.},
  affiliation   = {Department of Computer Science, Georgetown University, Washington, DC 20057, United States},
  document_type = {Review},
  doi           = {10.1017/S0269888901000236},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0347504537&doi=10.1017%2fS0269888901000236&partnerID=40&md5=49e9442e82e4c5c76d79f17767cec7bb},
}

@Conference{Sohda2001,
  author        = {Sohda, Y. and Nakada, H. and Matsuoka, S. and Ogawa, H.},
  title         = {Implementation of a portable software DSM in Java},
  year          = {2001},
  pages         = {163-172},
  note          = {cited By 13},
  __markedentry = {[Nichl:6]},
  abstract      = {Rapid commoditization of advanced hardware and progress of networking technology is now making wide area high-performance computing a.k.a, the 'Grid' Computing a reality. Since a Grid will consist of vastly heterogeneous sets of compute nodes, especially commodity clusters, some have articulated the use of Java as a suitable technology to satisfy portability across different machines. Since Java's natural model of parallelism is shared memory multithreading, one will have to support distributed shared memory (DSM) in a portable manner; however, none of the previous work on implementing Java on DSM has been a portable solution. Instead, we propose a software architecture whose goal is to achieve portability of DSM implementations across different commodity clustering platforms, while restricting the programming model somewhat, and implemented a prototype system, JDSM. Benchmark results show that the current implementation on Java incurs increased memory coherency maintenance cost compared to C-based DSMs, thus limiting scalability to some degree, and we are currently working on a solution to alleviate this cost.},
  affiliation   = {Tokyo Institute of Technology, Tokyo, Japan; Electrotechnical Laboratory, Tsukuba, Japan},
  document_type = {Conference Paper},
  journal       = {ACM 2001 Java Grande/ISCOPE Conference},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0035789920&partnerID=40&md5=4669e06825aa85fe9adcb4cd5935e0b9},
}

@Conference{2001,
  title         = {2001 IEEE aerospace conference proceedings},
  year          = {2001},
  volume        = {6},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The proceedings contains 46 papers from the 2001 IEEE Aerospace Conference Proceedings. Topics discussed include: adaptive guidance system for hypersonic reusable launch vehicles; adaptive guidance and control for autonomous launch vehicles; data embedding in audio signals; a benchmark evaluation of network intrusion detection systems; a process for introducing agent technology into space missions; multi-agent system for formation flying missions; and a software architecture for intelligent synthesis environments.},
  document_type = {Conference Review},
  journal       = {IEEE Aerospace Conference Proceedings},
  page_count    = {507},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0034849355&partnerID=40&md5=75226ff8e3afece0e7f4cdc57663a15c},
}

@Article{Mattsson2000,
  author          = {Mattsson, M. and Bosch, J.},
  title           = {Stability assessment of evolving industrial object-oriented frameworks},
  journal         = {Journal of Software Maintenance and Evolution},
  year            = {2000},
  volume          = {12},
  number          = {2},
  pages           = {79-102},
  note            = {cited By 15},
  __markedentry   = {[Nichl:6]},
  abstract        = {Object-oriented framework technology has become a common reuse technology in software development. As with all software, frameworks evolve over time. Once the framework has been deployed, new versions of a framework potentially cause a high maintenance cost for the products built with the framework. This fact, in combination with the high costs of developing and evolving a framework, make it important for organizations to achieve a controlled and predictable evolution of the framework's functionality and costs. We present a metrics-based framework stability assessment method, which has been applied on two industrial frameworks from the telecommunication and graphical user interface domains. First, we discuss the framework concept and the frameworks studied. Then, the stability assessment method is presented including the metrics used. The results from applying the method, as well as an analysis of each of the frameworks, are described. We continue with a set of observations regarding the method, including framework differences that seem to be invariant with respect to the method. A set of framework stability indicators based on the results is then presented. Finally, we assess the method against issues related to the management and evolution of frameworks, framework deployment, change impact analysis and benchmarking. Copyright © 2000 John Wiley & Sons, Ltd.},
  affiliation     = {Dept. Software Eng. and Comp. Sci., University of Karlskrona/Ronneby, S-372 25 Ronneby, Sweden; University of Karlskrona/Ronneby, S-372 25 Ronneby, Sweden; Vaxj University, Sweden; Department of Software Engineering, University of Karlskrona/Ronneby, Sweden; University of Twente, Netherlands; Lund University, Sweden},
  author_keywords = {Framework assessment; Framework evolution; Framework stability; Object-oriented framework; Object-oriented metrics; Software architecture},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0034148902&partnerID=40&md5=47648dd9ec48d96734360b26dbf3bb66},
}

@Conference{Viademonte2000,
  author        = {Viademonte, Sergio and Burstein, Frada and Beckenkamp, Fabio G.},
  title         = {Empirical study of distribution based on voyager: a performance analysis},
  year          = {2000},
  pages         = {37},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The paper describes the model, implementation and experimental evaluation of a distributed Kohonen Neural Network application (Kohonen Application). The aim of this research is to empirically verify the suitability and the performance of a distributed application based on mobile objects and, in perspective, intelligent agents. This research is aims to provide distribution features in decision support systems. The experiment was based in the Java-ABC project. The Java-ABC project is concerned with flexible software architecture for decision support systems, which rely on artificial neural network (ANN) technology. Three parameters: used CPU, used memory (RAM) and time consumed by each Kohonen Application, were taken as evaluation measures in this experiment. Three hardware environments were used: two PCs (one local and one remote) under Windows NT with different RAM capacity and a SUN Ultra under SunOS 5.6. This paper presents the comparison of performance measures from our experimental studies and the analysis of the results. In conclusion, the paper presents the implications of these results for the area of distributed intelligent decision support and future directions of this work.},
  affiliation   = {Monash Univ, Australia},
  document_type = {Conference Paper},
  journal       = {Proceedings of the Hawaii International Conference on System Sciences},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0033886046&partnerID=40&md5=a2285482bd1377822f4c845d8ca62df5},
}

@Article{Fiutem1999,
  author          = {Fiutem, R. and Antoniol, G. and Tonella, P. and Merlo, E.},
  title           = {ART: An Architectural Reverse Engineering Environment},
  journal         = {Journal of Software Maintenance and Evolution},
  year            = {1999},
  volume          = {11},
  number          = {5},
  pages           = {339-364},
  note            = {cited By 9},
  __markedentry   = {[Nichl:6]},
  abstract        = {When programmers perform maintenance tasks, program understanding is often required. One of the first activities in understanding a software system is identifying its subsystems and their relations, i.e., its software architecture. Since a large part of the effort is spent in creating a mental model of the system under study, tools can help maintainers in managing the evolution of legacy systems by showing them architectural information. This paper describes an environment for the architectural recovery of software systems called the architectural recovery tool (ART). The environment is based on a hierarchical architectural model that drives the application of a set of recognizers, each producing a different architectural view of a system or of some of its parts. Recognizers embody know ledge about architectural clichés and use flow analysis techniques to make their output more accurate. To test the accuracy and effectiveness of the ART, a suite of public domain applications containing interesting architectural organizations was selected as a benchmark. Results are presented by showing ART performance in terms of precision and recall of the architectural concept retrieval process. The results obtained show that cliché-based architectural recovery is feasible and the recovered information can be valuable support in reengineering and maintenance activities. Copyright © 1999 John Wiley & Sons, Ltd.},
  affiliation     = {ITC-IRST, Ist. Ric. Scientifica e Tecnologica, I-38050 Povo (Trento), Italy; Dept. of Elec. and Comp. Engineering, Ecole Polytechnique, C.P. 6079, Succ. Centre Ville, Montreal, Que., Canada; Sodalia S.p.A., 38100 Trento, Italy; Politecnico of Milano, Italy; Ist. Ric. Scientifica e Tecnologica, Trento, Italy; Sodalia S.P.A., Trento, Italy; University of Padua, Italy; University of Padua, ITA, Italy; IRST, Trento, Italy; Department of Computer Science, Ecole Polytechnique de Montreal, Canada; McGill University, Montreal, Que., Canada; University of Turin, Italy},
  author_keywords = {Cliché; Flow analysis; Matching; Program understanding; Reverse architecturing},
  document_type   = {Article},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0033340014&partnerID=40&md5=1b3532a43dc3ce8108d2ef026b1ab1f6},
}

@Conference{Smith1998,
  author        = {Smith, Connie U. and Williams, Lloyd G.},
  title         = {Performance evaluation of a distributed software architecture},
  year          = {1998},
  volume        = {1},
  pages         = {337-346},
  note          = {cited By 5},
  __markedentry = {[Nichl:6]},
  abstract      = {There is growing recognition of the importance of the role of architecture in determining the quality of a software system. While a good architecture cannot guarantee attainment of quality goals, a poor architecture can prevent their achievement. It is particularly important to evaluate the performance of a distributed system architecture. Errors made early can cause excessive overhead for communication and coordination and they are far more difficult - if not impossible - to correct with tuning. This paper discusses assessment of the performance characteristics of distributed software architectures in early life cycle stages. The techniques are described and illustrated with a simple example.},
  affiliation   = {Performance Engineering Services, Santa Fe, United States},
  document_type = {Conference Paper},
  journal       = {CMG Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0032284255&partnerID=40&md5=c2bd14ce8b7556b565a83006e2f51a54},
}

@Conference{Nguyen1997,
  author        = {Nguyen, Khien B. and Haiges, John G.},
  title         = {GPS integration receiver-programmable (GPS-PRO). Open architecture receiver design and unique applications},
  year          = {1997},
  volume        = {1},
  pages         = {665-674},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {With the widespread applications of GPS and the ever changing and evolving GPS requirements and capabilities, both in the commercial and military markets, there is a need for a flexible open architecture GPS receiver that would support laboratory development testing of GPS receiver design concepts, integration schemes, and GPS applications. The experience of the Central Engineering Activity (CEA) GPS Laboratory at NRaD San Diego supports the need for such a receiver. The special investigations and R&D GPS projects conducted by NRaD frequently require novel integration or data collection capabilities. These would only be readily available in a flexible receiver with a high degree of adaptability to the laboratory's ever-changing needs. The GPS Integration Receiver-Programmable (GPS-PRO) receiver, which utilizes an open hardware and software architecture, provides the necessary flexibility to perform this function. The GPS-PRO uses Commercial Off-the-Shelf (COTS) hardware, an open architecture for external sensors, external communications and control, digital control, open software architecture, functional breakout points, and an upgradable daughter board concept for application specific systems. Such a receiver configuration provides the flexibility to interchange or insert new hardware or software modules in order to evaluate their performance in a controlled environment or provide an integrated system solution. The GPS-PRO flexible (programmable) functionality allows for its use as a design tool for R&D and hardware in-the-loop testing, application specific tailoring for low cost, small size customer demands, while providing benchmark performance capabilities.},
  affiliation   = {NRaD San Diego},
  document_type = {Conference Paper},
  journal       = {Proceedings of ION GPS},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0031356620&partnerID=40&md5=7443188b6f144675d9f638d6f35896b9},
}

@Article{Chebira1997,
  author          = {Chebira, A. and Madani, K. and Mercier, G.},
  title           = {Multi-neural networks hardware and software architecture: Application of the divide to simplify paradigm DTS},
  journal         = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year            = {1997},
  volume          = {1240 LNCS},
  pages           = {841-850},
  note            = {cited By 2},
  __markedentry   = {[Nichl:6]},
  abstract        = {We present in this paper the implementation of the data driven method we called DTS (Divide to Simplify), that builds dynamically a Multi-Neural Network Architecture. The Multi-Neural Network architecture, we propose, solves a complex problem by splitting it into several easier problems. We have previously present a software version of the DTS multi- neural network architecture. The main idea of the DTS approach is to use a set of small and specialized mapping neural networks, or Slave Neural Networks (SNN), that are guided by a prototype based neural network, or Master Neural Network (MNN). In this paper, the MNN manages a set of hardware digital neural networks. Learning is performed in few milliseconds. We get a very good rate of classification when using the two spirals problem as a benchmark.},
  affiliation     = {Laboratoire D'Etudes et de Recherches en Instrumentation, Signaux et Systèmes, Division Réseaux Neuronaux, Université Paris XII, I.U.T. De Creteil-Senart Rue Pierre Point, F-77127 Lieusaint, France},
  author_keywords = {Cooperative and parallel architecture; Divide to simplify; IBM zero instruction set computer ZISC-036; Kohonen self organization maps; Multi-neural networks systems},
  document_type   = {Conference Paper},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-77949334993&partnerID=40&md5=a6b6d160ef34432208e26435ad54381e},
}

@Article{Altilar1997,
  author        = {Altilar, D.T. and Paker, Y. and Sahiner, A.V.},
  title         = {A parallel architecture for video processing},
  journal       = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  year          = {1997},
  volume        = {1225},
  pages         = {929-939},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {Video data consists of a sequence of frames that is produced at a constant rate and many applications in real-time require the processing of these frames executing compute intensive algorithms. To handle many of such applications in real-time, we developed a new architecture based on parallel processing. The parallel architecture for video processing has been developed at Queen Mary and Westlield College as a part of an European Union RACE II project called MONALISA. A multiprocessing kernel and a high level software environment called SAPS (self adapting parallel server) model has been developed for this architecture. This environment makes it possible to introduce a number of load balancing and data decomposition schemes that can be realised automatically in realtime by the kernel without any explicit inputs from the user. The developed architecture aims at applications such as image analysis algorithms, camera tracking, mixing captured foreground images and synthetically generated background images using depth values in real-time for Virtual Studios. In this paper we focus on the software architecture, frame buffer management and frame buffer access protocols. The system architecture and hardware is explained first. The standard frame buffer access protocol, SFBA, and the dedicated frame buffer access protocol, DFBA either of which address different needs of video processing are introduced. System performance evaluation, benchmark results and an analysis of DFBA protocol are given in detail. © 1997, Springer-Verlag Berlin Heidelberg. All right reserved.},
  affiliation   = {University of London, Queen Mary & Wesffield College, Department of Computer Science, Mile End Road, London, E1 4NS, United Kingdom},
  document_type = {Conference Paper},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84978943110&partnerID=40&md5=031aae1693bdfde53a79f849625f8292},
}

@Conference{Ridge1997,
  author        = {Ridge, Daniel and Becker, Donald and Merkey, Phillip and Sterling, Thomas},
  title         = {Beowulf: Harnessing the power of parallelism in a Pile-of-PCs},
  year          = {1997},
  volume        = {2},
  pages         = {79-91},
  note          = {cited By 90},
  __markedentry = {[Nichl:6]},
  abstract      = {The rapid increase in performance of mass market commodity microprocessors and significant disparity in pricing between PCs and scientific workstations has provided an opportunity for substantial gains in performance to cost by harnessing PC technology in parallel ensembles to provide high end capability for scientific and engineering applications. The Beowulf project is a NASA initiative sponsored by the HPCC program to explore the potential of Pile-of-PCs and to develop the necessary methodologies to apply these low cost system configurations to NASA computational requirements in the Earth and space sciences. Recently, a 16 processor Beowulf costing less than $50,000 sustained 1.25 Gigaflops on a gravitational N-body simulation of 10 million particles with a Tree code algorithm using standard commodity hardware and software components. This paper describes the technologies and methodologies employed to achieve this breakthrough. Both opportunities afforded by this approach and the challenges confronting its application to real-world problems are discussed in the framework of hardware and software systems as well as the results from benchmarking experiments. Finally, near term technology trends and future directions of the Pile-of-PCs concept are considered.},
  affiliation   = {USRA Cent of Excellence in Space, Data and Information Sciences, Greenbelt, United States},
  document_type = {Conference Paper},
  journal       = {IEEE Aerospace Applications Conference Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0030783352&partnerID=40&md5=81c9f259bd295a748bf3e53a6df18eb1},
}

@Article{Mascarenhas1996,
  author          = {Mascarenhas, E. and Rego, V.},
  title           = {Ariadne: Architecture of a portable threads system supporting thread migration},
  journal         = {Software - Practice and Experience},
  year            = {1996},
  volume          = {26},
  number          = {3},
  pages           = {327-356},
  note            = {cited By 32},
  __markedentry   = {[Nichl:6]},
  abstract        = {Threads exhibit a simply expressed and powerful form of concurrency, easily exploitable in applications that run on both uni- and multi-processors, shared- and distributed-memory systems. This paper presents the design and implementation of Ariadne: a layered, C-based software architecture for multi-threaded distributed computing on a variety of platforms. Ariadne is a portable user-space threads system that runs on shared- and distributed-memory multiprocessors. Thread-migration is supported at the application level in homogeneous environments (e.g., networks of SPARCs and Sequent Symmetrys, Intel hypercubes). Threads may migrate between processes to access remote data, preserving locality of reference for computations with a dynamic data space. Ariadne can be tuned to specific applications through a customization layer. Support is provided for scheduling via a built-in or application-specific scheduler, and interfacing with any communications library. Ariadne currently runs on the SPARC (SunOS 4.x and SunOS 5.x), Sequent Symmetry, Intel i860, Silicon Graphics workstation (IRIX), and IBM RS/6000 environments. We present simple performance benchmarks comparing Ariadne to threads libraries in the SunOS 4.x and SunOS 5.x systems.},
  affiliation     = {Department of Computer Sciences, Purdue University, West Lafayette, IN 47907, United States},
  author_keywords = {Distributed; Migration; Parallel; Process; Scheduling; Thread},
  document_type   = {Article},
  doi             = {10.1002/(SICI)1097-024X(199603)26:3<327::AID-SPE12>3.0.CO;2-H},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0030106770&doi=10.1002%2f%28SICI%291097-024X%28199603%2926%3a3%3c327%3a%3aAID-SPE12%3e3.0.CO%3b2-H&partnerID=40&md5=3a75ddb576770ff375f78dfd719343ba},
}

@Conference{Rajkumar1995,
  author        = {Rajkumar, Ragunathan and Gagliardi, Mike and Sha, Lui},
  title         = {Real-time publisher/subscriber inter-process communication model for distributed real-time systems: design and implementation},
  year          = {1995},
  pages         = {66-75},
  note          = {cited By 87},
  __markedentry = {[Nichl:6]},
  abstract      = {Distributed real-time systems are becoming more pervasive in many domains including process control, discrete manufacturing, defense systems, air traffic control, and on-line monitoring systems in medicine. The construction of such systems, however, is impeded by the lack of simple yet powerful programming models and the lack of efficient, scalable, dependable and analyzable interfaces and their implementations. We argue that these issues need to be resolved with powerful application-level toolkits similar to that provided by ISIS [2]. In this paper, we consider the inter-process communication requirements which form a fundamental block in the construction of distributed real-time systems. We propose the real-time publisher/subscriber model, a variation of group-based programming and anonymous communication techniques, as a model for distributed real-time inter-process communication which can address issues of programming ease, portability, scalability and analyzability. The model has been used successfully in building a software architecture for building upgradable real-time systems. We provide the programming interface, a detailed design and implementation details of this model along with some preliminary performance benchmarks. The results are encouraging in that the goals we seek look achievable.},
  affiliation   = {Carnegie Mellon Univ, Pittsburgh, United States},
  document_type = {Conference Paper},
  journal       = {Real-Time Technology and Applications - Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0029211731&partnerID=40&md5=355d3d092c01ed90b0414b52ba045fc3},
}

@Conference{Kazman1994,
  author          = {Kazman, R. and Bass, L.},
  title           = {Evaluating user interface tools},
  year            = {1994},
  volume          = {1994-April},
  pages           = {429-430},
  note            = {cited By 0},
  __markedentry   = {[Nichl:6]},
  abstract        = {The evaluation and selection of user interface tools is a continuing problem for builders of interactive systems. Tool evaluations found in the literature tend to concentrate on features, without any consideration of the types of interfaces to be constructed or the life cycle of the systems within which these user interfaces operate. In this tutorial, we present a methodology for the evaluation of user interface tools that is based on a knowledge of the types of interfaces being built and the life cycle expectations. The methodology uses two techniques for gaining this understanding of usage characteristics: benchmarking and software architectural analysis. Benchmarking is a technique widely used in the evaluation of other types of tools but rarely used in the user interface world. Software architectural analysis is a newly developed technique for the comparison of systems. It involves a discussion of the various elements of software architecture-structure, functionality and allocation-and uses these notions to analyze user interface software architecture. We demonstrate the methodology through example evaluations of several well-known tools such as Interviews, Tcl/Tk, Interface Architect, TAE+ and so forth. We also evaluate models of user interface software: "monolithic", Seeheim and PAC. © 1994 ACM.},
  affiliation     = {Department of Computer Science, University of Waterloo, Waterloo, ON N2L 3G1, Canada; Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA 15213, United States},
  author_keywords = {Software architecture; Tools and techniques},
  document_type   = {Conference Paper},
  doi             = {10.1145/259963.260427},
  journal         = {Conference on Human Factors in Computing Systems - Proceedings},
  source          = {Scopus},
  url             = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-84943518124&doi=10.1145%2f259963.260427&partnerID=40&md5=dc2618335dce47c3a8979813c78b1a09},
}

@Book{Frysinger1993,
  title         = {Hydrological modelling and GIS; the Sandia environmental decision support system},
  year          = {1993},
  author        = {Frysinger, S.P. and Thomas, R.P. and Parsons, A.M.},
  note          = {cited By 3},
  __markedentry = {[Nichl:6]},
  abstract      = {Sharing a common user interface philosophy and software architecture, the members of the SEDSS family are designed to facilitate human decision making with respect to the hydrological aspects of hazardous waste management. The Monitor Well Network Designer (MWND), the first member of the SEDSS family, is designed to satisfy the United States' Resource Conservation and Recovery Act (RCRA) groundwater monitoring regulations. RCRA requires that a hazardous waste management facility have a groundwater monitoring system consisting of at least one upgradient and three downgradient wells. SEDSS uses a probabilistic approach which defines and quantifies the performance measures to evaluate monitor well network effectiveness. -from Authors},
  affiliation   = {AT & T Bell Labs, Holmdel, NJ 07733, USA},
  document_type = {Article},
  journal       = {Applications of geographic information systems in hydrology and water resources management. Proc. international conference, Vienna, 1993},
  pages         = {45-50},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0027449894&partnerID=40&md5=be79fd614502d0c1265081ea2cc6edb8},
}

@Article{Frysinger1993a,
  author        = {Frysinger, S.P. and Thomas, R.P. and Parsons, A.M.},
  title         = {Hydrological modelling and GIS: the Sandia environmental decision support system},
  year          = {1993},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The Sandia Environmental Decision Support System (SEDSS) is a family of workstation applications being developed jointly by Sandia National Laboratories and AT&T Bell Laboratoris. Sharing a common user interface philosophy and software architecture, the members of the SEDSS family are designed to facilitate human decision making with respect to the hydrological aspects of hazardous waste management. The Monitor Well Network Designer (MWND), the first member of the SEDSS family, is designed to satisfy the United States' Resource Conservation and Recovery Act (RCRA) groundwater monitoring regulations. RCRA requires that a hazardous waste management facility have a groundwater monitoring system consisting of at least one upgradient and three downgradient wells. These regulations are subjective, making it difficult for the owner/operator and the regulators to resolve whether a monitor well network satisfies the regulatory requirements. To minimize this subjectivity, Sandia developed a probabilistic approach which defines and quantifies the performance measures to evaluate monitor well network effectiveness (Parsons & Davis, 1991). MWND implements this strategy, integrating a highly interactive graphical user interface, various modelling and simulation processes, and the GRASS Geographical Information System (GIS) to yield an Environmental Decision Support System (EDSS) which is easily used by both regulators and operators to evaluate assumptions about a site's hydrological characteristics and negotiate the design of a monitor well network. (Authors)},
  affiliation   = {AT&T Bell Lab, Holmdel, NJ 07733, USA},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-85041147158&partnerID=40&md5=574af5b52e0fa7fd15b4297a2aabb276},
}

@Conference{Anon1986,
  author        = {Anon},
  title         = {PROCEEDINGS - IEEE CONTROL SYSTEMS SOCIETY THIRD SYMPOSIUM ON COMPUTER-AIDED CONTROL SYSTEM DESIGN (CACSD).},
  year          = {1986},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {The following topics are dealt with: software architectures; computer-aided control system design packages; algorithms; benchmarks and simulation; artificial intelligence; and applications. Abstracts of individual papers can be found under the relevant classification codes in this or other issues.},
  document_type = {Conference Review},
  page_count    = {179},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0022935034&partnerID=40&md5=e293e12f52dfec2ba41ead0e3ef44ca4},
}

@Conference{Gorin1986,
  author        = {Gorin, A.L. and Shoenfelt, J.E. and Lewine, R.N.},
  title         = {SPEECH RECOGNITION ON THE DADO/DSP MULTIPROCESSOR.},
  year          = {1986},
  pages         = {361-363},
  note          = {cited By 1},
  __markedentry = {[Nichl:6]},
  abstract      = {The investigation of multiprocessor architectures and parallel algorithms for speech recognition is important. Large-vocabulary speech recognition is a computationally intensive problem, which can require orders-of-magnitude acceleration over uniprocessors to achieve real-time performance. Also, there is still much algorithm development work to be done, which requires a programmable computer rather than a hardware implementation. The authors describe a massively parallel hardware/software architecture that is applicable to accelerating a wide class of large-vocabulary speech recognition algorithms. The general principles of applicability supporting this claim are described. Timing and sizing results obtained by applying these principles to Rabiner's level-building DTW algorithm for connected-word recognition are given. Finally, a benchmark algorithm is described that demonstrates the programmability and performance of the architecture.},
  affiliation   = {AT&T Bell Lab, Whippany, NJ, USA, AT&T Bell Lab, Whippany, NJ, USA},
  document_type = {Conference Paper},
  journal       = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0022876441&partnerID=40&md5=5a6ea370e5159d556604dea4a989e565},
}

@Article{Korn1981,
  author        = {Korn, Granino A.},
  title         = {MULTIPROCESSOR DESIGNS SURPASS SUPERMINI ALTERNATIVES FOR CONTINUOUS SYSTEM SIMULATION.},
  journal       = {Electronic Systems Technology and Design/Computer Design's},
  year          = {1981},
  volume        = {20},
  number        = {5},
  pages         = {95-101},
  note          = {cited By 5},
  __markedentry = {[Nichl:6]},
  abstract      = {This state-of-the-art report extrapolates from benchmark studies to compare cost and effectiveness of alternative hardware and software architectures assembled specifically to perform continuous system simulation.},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0019561529&partnerID=40&md5=c4e62f19ae316a3986e9d15baa813ff7},
}

@Article{Hurtado1974,
  author        = {Hurtado, Henry J. and Steele, S.A.},
  title         = {COMPUTER SELECTION FOR COMMAND AND CONTROL SYSTEMS.},
  journal       = {RCA Eng},
  year          = {1974},
  volume        = {19},
  number        = {5},
  pages         = {28-31},
  note          = {cited By 0},
  __markedentry = {[Nichl:6]},
  abstract      = {This paper discusses the procedures for selecting a computer to satisfy the various real-time requirements of a command and control system. The evaluation-and-selection process is described, with some detail devoted to the use of benchmarks. The basic considerations include hardware and software architecture, languages, support software, simulation requirements, I/O capability, and use of existing software - and how these considerations influence the cost/performance ratio. The emphasis is on a single or basic computer system in a multiprogramming environment as opposed to a distributed and/or multi-processing system.},
  document_type = {Article},
  source        = {Scopus},
  url           = {https://www2.scopus.com/inward/record.uri?eid=2-s2.0-0016027178&partnerID=40&md5=1e8179fb85931cc4391ebfde304dc3ec},
}

@Comment{jabref-meta: databaseType:bibtex;}
